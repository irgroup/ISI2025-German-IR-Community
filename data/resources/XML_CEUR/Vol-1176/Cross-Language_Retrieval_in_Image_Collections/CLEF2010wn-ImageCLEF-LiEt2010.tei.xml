<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.44,151.73,291.59,12.58;1,233.70,169.13,139.26,12.58">A Text-Based Approach to the ImageCLEF 2010 Photo Annotation Task</title>
				<funder ref="#_XCvqg6e">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder>
					<orgName type="full">Centre for Next Generation Localisation</orgName>
					<orgName type="abbreviated">CNGL</orgName>
				</funder>
				<funder>
					<orgName type="full">Dublin City University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.70,196.02,26.81,9.02"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localisation</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.85,196.02,51.29,9.02"><forename type="first">Jinming</forename><surname>Min</surname></persName>
							<email>jmin@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localisation</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.55,196.02,71.49,9.02"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localisation</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.44,151.73,291.59,12.58;1,233.70,169.13,139.26,12.58">A Text-Based Approach to the ImageCLEF 2010 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BBAF28274CA3263DBFE386F0F33D4251</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Photo annotation</term>
					<term>Document expansion</term>
					<term>Feature extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The challenges of searching the increasingly large collections of digital images which are appearing in many places mean that automated annotation of images is becoming an important technology. We describe our participation in the ImageCLEF 2010 Visual Concept Detection and Annotation Task. Our approach used only the textual features (Flickr user tags and EXIF information) provided with the images to perform automatic annotation. Our method explores the use of a combination of techniques to address the annotation problem. Our results indicate that the techniques works reasonably given the limitations inherent in using only textual data for this task. We identify the drawbacks of our approach and how these might be addressed and optimized in further work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The exponential increase in the number of images available on the World Wide Web has led to a great interest in the topic of Automatic Image Annotation (AIA) to support applications such as effective search of online image collections. This paper describes details of our participation in the ImageCLEF 2010 Photo Annotation task which aims to explore methods for automatic annotation of large photo collections. The task involves assigning 93 concepts to images from the MIR Flickr 25,000 image dataset. The training and test sets consist of 8,000 and 10,000 images respectively. The Flickr images in each collection include user assigned tags and EXIF data for the photos where they are present. Automatic image annotation can broadly be classified into three approaches: visual, textual and hybrid models. In our work for ImageCLEF 2010 we concentrated only on use of text metadata for this task.</p><p>We submitted one run for the annotation task. The focus of our work was to attempt to exploit different methods to derive more text information from available resources to do the automatic annotation. In this our participation in the task we extracted features from the training set; used document expansion to enrich the existing text information resources; and extracted identified additional features. This paper is organized as follows: Section 2 describes our indexing and retrieval methods, Section 3 gives our experimental results and finally Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Metadata Processing and Retrieval Strategies</head><p>Attempting to annotate images based on the available text information poses a significant challenge. Images are provided with tags of varying quality and scope manually assigned by users and with standard EXIF information. Investigation revealed of the provided Flickr dataset revealed that some images do not in fact have any user tags at all. In our experiments, we investigated approaches to making use of the limited information which was available to capture more features from both the training set and test set to assist with the annotation. These methods included document expansion and feature extraction which are introduced in the following subsections. The stages of processing and annotation are summarised in Figure <ref type="figure" coords="2,442.94,269.52,3.77,9.02">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Flow diagram of image annotation approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Expansion</head><p>The limitations of the text descriptions provided with images can lead to significant problems for reliable processing of the images in applications such as search tools and classifiers. Particular problems can arise due to mismatch between the manually assigned tags when comparing images and when attempting to identify images relevant to user queries in retrieval application, and due to the general inadequacy of the tags assigned by users. In our approach to this task we sought to enrich text information about images by using a process of document expansion <ref type="bibr" coords="2,403.49,582.17,10.64,9.02" target="#b5">[6]</ref>. In document expansion the existing text metadata for an image is used as a query to a text information resource. Items retrieved in response to the query are then processed to identify terms strongly associated with the image's metadata. These words can then be added to the metadata, in the same manner as queries are expanded in traditional query expansion methods. For our work we use DBpedia as an external information resource for expansion of the image metadata "documents".</p><p>We used document expansion to expand the image metadata, but also the concepts which are to be used to annotate the images. Each concept usually consists of only 1 or 2 words. Thus it is hard to reliably match concepts to image metadata. Thus it is interesting to try to expand concepts to include words related to the concept or which describe the concept. We thus hoped that after this expansion, concepts could be more reliably matched to image metadata. To perform concept expansion each of the concepts was treated as a query and again applied to external DBpedia information resource. Selected expansion terms were then added to the concept.</p><p>Our document expansion method uses the Okapi feedback method <ref type="bibr" coords="3,437.13,218.76,10.63,9.02" target="#b6">[7]</ref>. For expansion of the concepts, we assumed that the top 100 retrieved ranked DBpedia documents were relevant to the concept, we then added 10 top scoring words from the retrieved documents to the concept. For user tags a slightly more complex procedure was used. We still added 10 words to the metadata data of each image. However, since some user tags are sentences, they may contain stop words or other words which are not central to the focus of the tag. If we use the simple document expansion method which treats every word with the same weight, some stop words or other words not related to the topic of the tag may be added to user tags. To help avoid this problem, we used the document expansion method introduced in <ref type="bibr" coords="3,386.16,322.20,10.61,9.02" target="#b0">[1]</ref>. In this procedure user tags are first reduced by removing stop words and other words not likely to be significant to the document. The document expansion stage is then performed to add additional words to the image metadata. To perform the concept assignment, words in the expanded concepts and metadata documents were first stemmed, the similarity between each expanded image tag and concept was then computed to perform the annotation.</p><p>While this approach has the potential to assign good concept annotations for images which have manual tags to seed the expansion process, it does not work well for images which do not have manual tags as a starting point for expansion. In order to be able to annotate these images another method is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>The annotation scheme has been setup in such a way to make it easy to extend it with new keywords without having to go through all images again <ref type="bibr" coords="3,387.74,504.71,10.67,9.02" target="#b1">[2]</ref>. In this part, we present a further method we used to refine the annotation process. The ImageCLEF 2010 task provides 93 annotation concepts. The relation between these concepts is another useful way for us to perform the annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Affiliation Between Concepts</head><p>From the training set, some general concepts can be found. They cover a number of proper subtopics, see Table <ref type="table" coords="3,240.30,596.69,3.76,9.02" target="#tab_0">1</ref>. We used a simple greedy algorithm method to assign an affiliation relation selection. The algorithm operates as follows:</p><p>Greedy Algorithm 1 (affiliation):</p><p>1. for each concept c i (0 &lt;= i &lt;= 92), count how many times it appears in image collection, Nc i 2. when concept c i appears, count how many times another concept c j (0 &lt;= j &lt;=92, j!=i) appears, Nc j 3. compute P ij = Nc j / Nc i 4. if P ij &gt;= 0.97, then we assume, concept c j is the subtopic of concept c i</p><p>The value 0.97 was selected empirically for this collection. According to this relationship, if any subtopic is annotated in one photo, then its corresponding general topic will be annotated in the same photo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Opposite Relation Between Concepts</head><p>In addition to the affiliation, an opposite relationship was also identified, see examples in Table <ref type="table" coords="4,204.01,478.08,3.77,9.02" target="#tab_1">2</ref>. Similar to the affiliation relation method, a greedy algorithm was used to identify these relations. The algorithm operates as follows:</p><p>Greedy Algorithm 2 (opposite relationship):</p><p>1. for each concept c i (0 &lt;= i &lt;= 92), count how many times c i appears in the image collection, Nc ia 2. when c i occurs, count how many times another concept c j (0&lt;= j &lt;=92, j!=i) does not occur, Nc jn 3. compute P ij = Nc jn / Nc ia 4. for each pair of concepts c i and c j , compute P ji = Nc in / Nc ja 5. if P ij &gt;= 0.7 &amp; P ji &gt;= 0.7, we assume concept c i and c j are an opposite pair of concepts Nc ia = number of times concept c i appears in the image collection Nc jn = the number of times concept c j does not appear in the image collection when c i appears</p><p>This relationship means that if one concept occurred in a photo, its opposite concept is unlikely to have occurred in the same photo. The value 0.7 was again chosen empirically. In this experiment, only two of these opposite pairs were found (the pair with '*' mark in Table <ref type="table" coords="5,268.84,184.26,3.61,9.02" target="#tab_1">2</ref>). How to find more opposite pairs is another challenge for our future work in this kind of task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Extract features from EXIF file</head><p>For concept classification, assignment of each concept was treated as an individual classification task. Thus for each concept, we consider that there is an annotated image collection. We find the common features of all images in this collection from their EXIF information file. Then this common feature is used to annotate this concept on test set.</p><p>EXIF metadata represents a number of properties and settings of the digital camera at the time of taking picture <ref type="bibr" coords="5,269.46,445.74,10.64,9.02" target="#b1">[2]</ref>. This includes the information:</p><p>• Camera itself: brand… • Camera settings: exposure, aperture, focal length, ISO speed… • Image settings: orientation, resolution, compression…</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Time and Date</head><p>Because not all of these fields are present in every EXIF file and the time restrictions to perform this task. We did not use EXIF collection effectively. We only extracted the Date and Time properties from EXIF metadata. Pictures which were taken at times between 08.00 and 17.00 were annotated as the day time concept and other times are assumed to be associated with a night concept. Further features could be extracted if more time were available to analyze this EXIF metadata. Further study of EXIF metadata is also planned in future work for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Combination</head><p>To calculate the concept assignments the features need to combined to produce a final result. Following the application of document expansion, we get a binary result matrix A. All other feature functions are then applied on this matrix. The final combination result is calculated using the following equation: A represents the document expansion binary result matrix; + represents application of the following method on the previous matrix; Rb is the affiliation relation method; Rc is the opposite relation method; ⊗ is exlusive or symbol; D is the binary result matrix achieved by the EXIF metadata method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Submission and Evaluation</head><p>We made only one submission for this task. This used all the methods introduced above in combination to annotate the test dataset. The official result of this run is reported in Table <ref type="table" coords="6,196.15,377.52,3.74,9.02" target="#tab_2">3</ref>.</p><p>For this task, 64 runs were submitted in total, only two runs chose to use the textbased approach (our submission and another from the MLKD group). Based on the reported MAP measure, these two runs got very close results, and were ranked at positions 42 (MLKD group) and 45 (our run) out of 64 submitted runs, respectively. The best run used a hybrid approach. For each concept, the EER (Equal Error Rate) and AUC (Area Under Curve) were calculated. The results of each concept are shown in Figure <ref type="figure" coords="6,413.92,570.60,5.01,9.02" target="#fig_1">2</ref> (the x axis indicates the 93 concepts; the y axis indicates the Accuracy Rate). From the figure we can see that the results of our experiment are variable, it can be noted that some concepts are not detected at all. One of the main reasons underlying poor results is that the text resource available for some concepts is not sufficient for this task. In particular, some images do not have tags and EXIF files at all. This is obviously a big problem when using a text only based approach to doing the annotation task. Another issue is that both the EER and AUC evaluation methods require confidence scores of each annotated concept. However, our method cannot provide this score information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented and analysed our submission to the ImageCLEF 2010 Photo Annotation Task and compared our results to those of other participants. Although the text-based approach only achieves moderate and inconsistent results, it has potential to be improved further. In this experiment we used document expansion to enhance image text metadata. In future work we plan to explore used of other external information resources for this task. Some images do not have tags and EXIF information and thus cannot be annotated at all. How to identify more features and information from this limited resource is a big challenge for text-based approach. All of these problems define our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,231.90,181.81,131.56,12.26;6,124.74,207.96,27.25,9.02"><head>Final</head><label></label><figDesc>result = (A + Rb + Rc) ⊗ Dwhere:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,208.74,149.76,189.16,9.02;7,126.30,170.40,345.72,144.78"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2(a). EER and AUC of concept 0 to 46</figDesc><graphic coords="7,126.30,170.40,345.72,144.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,126.30,301.74,345.60,164.64"><head></head><label></label><figDesc></figDesc><graphic coords="2,126.30,301.74,345.60,164.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,189.48,309.54,234.06,106.96"><head>Table 1 .</head><label>1</label><figDesc>Some examples of the affiliation in 93 concepts</figDesc><table coords="4,203.70,333.69,219.84,82.80"><row><cell>General Concept</cell><cell>Sub Concept</cell></row><row><cell>Sky</cell><cell>clouds, shadow</cell></row><row><cell>Water</cell><cell>lake, rive, sea</cell></row><row><cell>City_life</cell><cell>car, vehicle, bicycle, ship, train,</cell></row><row><cell></cell><cell>airplane</cell></row><row><cell>Animals</cell><cell>dog, cat, bird, horse, fish</cell></row><row><cell>winter</cell><cell>snow</cell></row><row><cell>architecture</cell><cell>building-sight, church, bridge</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,175.74,218.76,255.29,107.92"><head>Table 2 .</head><label>2</label><figDesc>Some examples of the opposite relation in 93 concepts</figDesc><table coords="5,211.56,254.43,195.00,72.24"><row><cell>Concept</cell><cell>Opponent Concept</cell></row><row><cell>Indoor</cell><cell>Outdoor</cell></row><row><cell>*day</cell><cell>*night</cell></row><row><cell>No_visual_Time</cell><cell>day, night</cell></row><row><cell>*no_person</cell><cell>*single_person, female,</cell></row><row><cell></cell><cell>male, baby, child,</cell></row><row><cell></cell><cell>teenager, adult, old_person</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,131.94,458.04,311.84,74.68"><head>Table 3 .</head><label>3</label><figDesc>Result of Runs evaluated by MAP, EER and AUC</figDesc><table coords="6,131.94,482.19,311.84,50.52"><row><cell>Submission run</cell><cell>MAP</cell><cell>Avg. EER</cell><cell>Avg. AUC</cell></row><row><cell>Text-Based Run</cell><cell>0.2284</cell><cell>0.4508</cell><cell>0.1944</cell></row><row><cell>(DCU__1277149866992_</cell><cell></cell><cell></cell><cell></cell></row><row><cell>_test_annotation.txt)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>This research is supported by the <rs type="funder">Science Foundation Ireland</rs> (Grant <rs type="grantNumber">07/CE/I1142</rs>) as part of the <rs type="funder">Centre for Next Generation Localisation (CNGL)</rs> at <rs type="funder">Dublin City University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XCvqg6e">
					<idno type="grant-number">07/CE/I1142</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,128.12,265.53,342.60,8.10;8,136.08,275.85,120.12,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,261.68,265.53,149.53,8.10">Document Expansion for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,417.03,265.53,53.69,8.10;8,136.08,275.85,40.48,8.10">Proceedings of RIAO 2010</title>
		<meeting>RIAO 2010<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.12,286.71,330.43,8.10" xml:id="b1">
	<monogr>
		<ptr target="http://press.liacs.nl/mirflickr/" />
		<title level="m" coord="8,136.09,286.71,143.55,8.10">MIRFLICKR Image Collection Website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.51,297.66,342.16,9.02;8,136.08,308.79,191.32,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,239.07,298.35,216.61,8.10">Overview of the WikipediaMM Task at ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,136.08,308.79,107.78,8.10">Working Notes of CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.11,319.17,342.46,8.10;8,136.08,329.49,99.82,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,218.46,319.17,127.00,8.10">I2R ImageCLEF Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,380.55,319.17,90.03,8.10;8,136.08,329.49,16.22,8.10">Working Notes of CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.11,339.87,342.48,8.10;8,136.08,350.19,270.89,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,248.90,339.87,221.69,8.10;8,136.08,350.19,63.65,8.10">Joint Equal Contribution of Global and Local Features for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kameyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,215.62,350.19,107.72,8.10">Working Notes of CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.11,360.57,342.47,8.10;8,136.08,370.89,333.30,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,322.36,360.57,148.22,8.10;8,136.08,370.89,126.65,8.10">DCU at WikipediaMM 2009: Document expansion from wikipedia abstracts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,278.10,370.89,107.72,8.10">Working Notes of CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.12,381.27,342.49,8.10;8,136.08,391.59,334.60,8.10;8,136.08,401.97,98.08,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,437.98,381.27,32.62,8.10;8,136.08,391.59,28.13,8.10">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,182.67,391.59,231.31,8.10">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
