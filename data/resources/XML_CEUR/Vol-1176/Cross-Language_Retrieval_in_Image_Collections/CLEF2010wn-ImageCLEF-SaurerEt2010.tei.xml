<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.59,116.95,326.17,12.62;1,234.96,134.89,145.43,12.62">Visual localization using global visual features and vanishing points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.65,172.77,59.57,8.74"><forename type="first">Olivier</forename><surname>Saurer</surname></persName>
							<email>saurero@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Geometry Group</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.97,172.77,91.66,8.74"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
							<email>fraundorfer@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Geometry Group</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.84,172.77,63.87,8.74"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<email>marc.pollefeys@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Geometry Group</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.59,116.95,326.17,12.62;1,234.96,134.89,145.43,12.62">Visual localization using global visual features and vanishing points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5AED891523100E6038E249EDBCCC3586</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual place recognition</term>
					<term>semantic annotation of space</term>
					<term>visual localization</term>
					<term>vanishing points</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a visual localization approach for mobile robots. Robot localization is performed as location recognition. The approach uses global visual features (e.g. GIST) for image similarity and a geometric verification step using vanishing points. Location recognition is an image search to find the most similar image in the database. To deal with partial occlusions, which lower image similarity and lead to ambiguity, vanishing points are used to ensure that a matching database image was taken from the same viewpoint as the query image from the robot. Our approach will assign a query image to a location learned from a training dataset, to an "Unknown" location or in case of too much uncertainty the algorithm would refrain from a decision. The algorithm was evaluated under the ImageCLEF 2010 RobotVision competition 1 . The results on the datasets of this competition are published in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent approaches to visual robot localization using local image features and visual words proved to work very well <ref type="bibr" coords="1,304.42,494.24,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,316.60,494.24,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="1,326.00,494.24,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="1,335.41,494.24,3.51,8.74" target="#b2">3</ref>]. An underlying assumption for these methods however is, that one already collected images for all possible locations in a database. A scenario, where a database was created using images of one floor of a building and having the robot localize itself on a different floor of the building would be beyond the capabilities of these methods. This is exactly the scenario that was created for the ImageCLEF 2010 RobotVision competition <ref type="bibr" coords="1,467.31,554.01,9.96,8.74" target="#b6">[7]</ref>. The goal was to train the robot with locations (e.g. office, kitchen, printer room) from one floor, so that it can identify the corresponding locations on the other floor, where the locations differ in details like different chairs, different desks, different posters, different curtains, etc. In this paper we describe an approach that is targeted towards resolving this scenario. The approach works by using a global image descriptor that captures the large scale features of the location, but not the fine details. This would allow to match up two locations that share Fig. <ref type="figure" coords="2,154.40,245.81,4.13,7.89">1</ref>. Two images from the location class 'Meetingroom' on different floors. To identify these two images as matching an image descriptor needs to identify the large scale similarities (room configuration, table position) despite the obvious differences on the small scale.</p><p>the similar overall structure but differ on the fine details. Fig. <ref type="figure" coords="2,409.75,311.25,4.98,8.74">1</ref> illustrates this concept. The two images show two meeting rooms from the different floors. The table, chairs and pictures on the wall are different but the overall structure is similar. There is a table in the center of the room, which creates a strong horizontal edge feature. The outline of the room walls itself creates also strong edge features converging in a similar manner. These are the features that we would like to capture. To achieve this our approach uses GIST <ref type="bibr" coords="2,388.87,382.98,10.52,8.74" target="#b5">[6]</ref> as a global visual descriptor. In addition to visual similarity we propose a subsequent geometric verification check. For geometric verification we compare the vanishing points of matching images, which are computed from line features in the images. This geometric check ensures, that images are matched up only, if they are taken in the same geometric setting (e.g. a similar sized room) and from the same viewpoint.</p><p>In the experiments using the dataset of the ImageCLEF 2010 RobotVision competition <ref type="bibr" coords="2,190.02,466.95,10.52,8.74" target="#b6">[7]</ref> we demonstrate that using GIST it is possible to capture these larger scale similarities and that it is possible to match up the locations like the one depicted in Fig. <ref type="figure" coords="2,228.56,490.86,3.87,8.74">1</ref>. We also show that the vanishing points are useful for geometric verification and improve the localization results. Finally we report the scores achieved in the ImageCLEF 2010 RobotVision competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The GIST descriptor used in our approach was first introduced by Oliva et al. in <ref type="bibr" coords="2,160.74,585.38,9.96,8.74" target="#b5">[6]</ref>. It was used in <ref type="bibr" coords="2,242.51,585.38,10.52,8.74" target="#b8">[9]</ref> for place and object recognition. They showed that it is possible to distinguish between different places or rather scenes using the GIST descriptor. In particular they presented classification results on the following scenes: building, street, tree, sky, car, streetlight, person. In our current work we show that it is possible to use GIST for place recognition in typical indoor environments. In addition we added a geometric verification step targeted to indoor environments. GIST was also used in <ref type="bibr" coords="2,348.94,657.11,10.52,8.74" target="#b4">[5]</ref> for place recognition using panoramic images. There the GIST descriptor was adapted to the properties of panoramic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GIST descriptor and Vanishing Points</head><p>Before presenting our pipeline for semantic labeling of space, we first discuss the GIST descriptor which was introduced by Oliva et al. in <ref type="bibr" coords="3,375.26,199.41,9.96,8.74" target="#b5">[6]</ref>. The GIST descriptor represents scenes from the encoding of the global configuration, ignoring most of the details and object information present in the scene. We then further discuss the concept of vanishing points, which are projections of points laying at infinity. They provide information on the relative camera orientation with respect to the scene and are used as a geometric verification after image retrieval using the GIST descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GIST descriptor</head><p>The GIST descriptor was proposed by Oliva et al. in <ref type="bibr" coords="3,365.35,320.66,10.52,8.74" target="#b5">[6]</ref> for scene categorization without the need for segmentation and processing of objects. The structure of the scene is estimated using a few perceptual dimensions such as naturalness, openness, roughness, expansion, ruggedness which describe the spatial properties of the scene. The dimensions are reliably estimated using spectral and coarsely localized information, where membership in semantic categories such as streets, highways, etc. are projected close together in a multidimensional space. The low dimensional representation of a scene is represented by a 960 dimensional descriptor, which allows quick retrieval of similar images from a large database. In the following, image search consists in finding the set of images with the smallest L2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vanishing points</head><p>The premise to find vanishing points are man-made environments containing parallel straight lines. When looking at the perspective projection of three dimensional parallel lines, they intersect in one common point in the image plane, the so called vanishing point (VP) <ref type="bibr" coords="3,291.06,525.61,9.96,8.74" target="#b3">[4]</ref>. Vanishing points therefor represent the projections of 3D points laying at infinity, since parallel lines intersect at infinity.</p><p>To estimate the vanishing points, we first detect edges using canny edge detection and extract long straight lines from the edge segments. The straight lines are used as input for our RANSAC (random sample consensus) algorithm, which estimates multiple vanishing points in a single image. The algorithm first randomly selects two lines and computes their intersection point P . If at least 20 of the lines passes through the intersection point P , the point is re-estimated using a non-linear optimization, where all supporting lines are included in the optimization process. The supporting lines are then removed from the input set and the procedure is repeated until either no further lines are available or no further vanishing point is found.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Place recognition</head><p>The proposed pipeline for semantic labeling of space is illustrated in Fig. <ref type="figure" coords="4,452.69,358.05,3.87,8.74" target="#fig_2">3</ref>. The method classifies an image into one of the following three categories, which is either a label learned from a training dataset, the "Unknown" label or in some cases the algorithm would make no decision.</p><p>In a first step a database of GIST descriptors is build from the training dataset. Our database consists of 4780 images and is represented by a kd-tree for fast k-nearest neighbors search, we chose k to be 10 in our experiments. In a first step we query the database with the query image q, for its 10 nearest neighbors stored in the result set r. Images in the result set r with a L2 distance to the query image q, greater than a given threshold (0.6 in our experiments) are removed from r. If r is empty, the image q is labeled as "Unknown". Otherwise the set r is further matched to a set of ambiguous images, which were previously learned from the training dataset, see Fig. <ref type="figure" coords="4,282.62,501.61,3.87,8.74">4</ref>. If the set of ambiguous images in the set r is greater than the set of non-ambiguous images, the algorithm refrains a decision on the image q, due to lack of confidence. Otherwise, a geometric verification is applied to the remaining set of non-ambiguous images. The geometric verification compares the angular distance of vanishing points between the query image and the non-ambiguous images. Images with a large angular distance (0.34 in our experiments) are removed from the set r. Finally, the query image is assigned the label of the image with the smallest angular distance or is assigned the label "Unknown" if the set r is empty.</p><p>To find vanishing point matches between two images, we first normalize the VP vector to unit length, such that the VP lays on the surface of a Gaussian sphere. Then, for each VP in one image we do an exhaustive search for the closest VP in the other image i.e., the VP with smallest angular distance. We assume that two similar scenes match, if their appearance is similar i.e., similar GIST descriptor and similar vanishing points, meaning the camera has a similar point of view of the 3D scene being observed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Our algorithm was evaluated at the 3rd edition of the Robot Vision challenge, held in conjunction with IROS 2010. The challenge addresses the problem of classifying rooms and functional areas based on a pair of stereo images. Three image sets were provided, one training set for learning, one validation set for the participants to validate their algorithm and one testing set used for the competition. All three sets were captured in the same building, but on different floors. All three floors have a set of common rooms, such as Offices, Toilet, Printer Area, Corridor, etc. and rooms which are only present in one of the dataset such as Kitchen, Lab, Elevator, etc.. Sample images of the training sets are provided in Fig. <ref type="figure" coords="5,224.34,470.70,3.87,8.74">5</ref>. Task 1 of the competition asked to build a system which can answer the question "Where am I?", given one pair of stereo images. The answer can either be a previously learned label, the "Unknown" label if the system is presented with a new location not contained in the training set or it can refrain a decision by leaving the image unclassified. The performance of the algorithm was evaluated using the following scoring scheme:</p><p>-+1.0 point for each correctly classified image.</p><p>--1.0 point for each misclassified image. -0.0 point for each image that was not classified. -+2.0 points for a correct detection of unknown category.</p><p>--2.0 points for an incorrect detection unknown category.</p><p>Our system ranked first, with 677 points in the 3rd edition of the Robot Vision challenge. The winning run used the following configuration: a search window size of 10 images, a minimum GIST distance threshold of 0.6, and a minimum mean angular distance threshold of 0.34. Door frames, walls and whiteboards were learned and added to the ambiguous location set as well as the following four rooms Kitchen, Small Office and Large Office.</p><p>Bellow we further discuss the benefit of the geometric verification. The evaluation is based on the validation set, which contains 2069 images, where 14.4% of the image labels are unknown to the training set. Without geometric verification an image match is obtained by searching the training set for the image with the smallest L2 GIST distance. Using the geometric verification an image match is obtained by choosing the image with the smallest mean angular distance between the query image and the images obtained from the k-nearest neighbors, with k = 30. Table <ref type="table" coords="6,265.04,585.20,4.98,8.74">1</ref> lists the recognition rate of each category known to the training set. Overall, the geometric verification performed slightly superior (recognition rate of 43.15%) to the pure GIST based method (recognition rate of 42.03%). The Meeting Room category achieved an improvement of over 8%.</p><p>For the label Corridor and Large Office the pure GIST method performs better. The reason our method provides a lower performance on the Corridor category is that many images are misclassified at transitional places, where the robot moves from the corridor into a room. Fig. <ref type="figure" coords="7,341.75,500.17,4.98,8.74">6</ref> illustrates such a misclassification. In the Large Office category the misclassified images are mainly classified as Kitchen or as Small Office. Fig. <ref type="figure" coords="7,287.89,524.08,4.98,8.74">7</ref> illustrated a misclassification based on the GIST method, which is corrected by the geometric verification.</p><p>Our unoptimized Matlab implementation takes 51.21 seconds on a 2.66GHz Core2 Quad CPU, to classify 2069 images using precomputed GIST descriptors and precomputed vanishing points. Extracting GIST descriptors takes in average 1.91 seconds on a 487 × 487 pixel image. We make use of the freely available Matlab code provided by Antonio Torralba<ref type="foot" coords="7,319.71,607.56,3.97,6.12" target="#foot_2">2</ref> . We use our own Matlab implementation to extract vanishing points. In average it takes 0.65 seconds to extract the vanishing points of one image.  <ref type="table" coords="8,163.44,219.72,4.13,7.89">1</ref>. Recognition rate obtained from the validation dataset. Note that the validation set does not hold the label Kitchen, therefor the recognition rate for that label is 0.00%. See text for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a system for visual localization using global visual features (GIST) and a geometric verification based on vanishing points. We have shown that the geometric verification can indeed improve the recognition rate when used together with global visual features. The evaluation on the ImageCLEF 2010 RobotVision dataset showed that the approach manages to recognize similar locations despite of differences on the small scale. The evaluation however also revealed that the approach has difficulties in handling 'Unknown' locations. 'Unknown' locations are sometimes matched with locations from the training set and known locations are sometimes classified as 'Unknown' locations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,244.04,345.83,7.89;4,134.77,255.02,237.90,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left, lines are classified to the according VPs. On the right the VPs are shown, except the one located far from the image center (infinity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,149.71,289.41,330.88,8.74;4,134.77,301.36,112.05,8.74"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 shows line sets supporting different VPs. Each color represents the support of a different VP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,222.00,286.07,171.35,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,210.12,345.83,7.89;6,134.77,221.10,345.83,7.86;6,134.77,232.06,150.92,7.86;6,144.42,252.74,79.53,58.60"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Ambiguous places are represented by similar GIST descriptors with different image labels. We have trained two classes of ambiguous images, door frames (left image) and walls, whiteboards (right image).</figDesc><graphic coords="6,144.42,252.74,79.53,58.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,134.77,240.11,345.82,7.89;7,134.77,251.09,345.82,7.86;7,134.77,262.05,345.83,7.86;7,134.77,273.01,229.56,7.86;7,362.34,295.58,103.74,76.58"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. False image match due to ambiguous labeling of the training set. a) shows the original query image. b) shows the image with the smallest GIST distance 0.41 and a mean angular distance of 0.28. c) shows the image with the smallest angular distance, GIST distance 0.46 and a mean angular distance of 0.01.</figDesc><graphic coords="7,362.34,295.58,103.74,76.58" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.79,113.30,7.86;1,258.02,656.03,6.34,5.24;1,267.93,657.79,75.65,7.86"><p>This approach was ranked 1 st in the ImageCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2010" xml:id="foot_1" coords="1,368.15,657.79,103.85,7.86"><p>RobotVision competition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="7,144.73,657.79,237.97,7.86"><p>http://people.csail.mit.edu/torralba/code/spatialenvelope/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We would like to thank <rs type="person">Georges Baatz</rs> for sharing his vanishing point detection code.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,514.65,342.24,7.86;8,146.91,525.61,333.68,7.86;8,146.91,536.57,114.68,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,348.06,514.65,132.53,7.86;8,146.91,525.61,195.65,7.86">Fast and incremental method for loop-closure detection using bags of visual words</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,350.13,525.61,130.46,7.86">Robotics, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1037" />
			<date type="published" when="2008-10">Oct 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,547.70,342.24,7.86;8,146.91,558.66,333.68,7.86;8,146.91,569.61,311.34,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,261.81,547.70,218.78,7.86;8,146.91,558.66,100.83,7.86">FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<ptr target="http://ijr.sagepub.com/cgi/content/abstract/27/6/647" />
	</analytic>
	<monogr>
		<title level="j" coord="8,256.28,558.66,196.79,7.86">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,580.74,342.24,7.86;8,146.91,591.70,333.68,7.86;8,146.91,602.66,145.42,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,309.74,580.74,170.85,7.86;8,146.91,591.70,175.44,7.86">Combining monocular and stereo cues for mobile robot localization using visual words</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,343.77,591.70,136.82,7.86;8,146.91,602.66,79.80,7.86">Proc. International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,613.79,342.24,7.86;8,146.91,624.75,25.60,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,254.26,613.79,176.47,7.86">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,635.88,342.24,7.86;8,146.91,646.84,333.67,7.86;8,146.91,657.79,149.57,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,257.20,635.88,218.77,7.86">Experiments in place recognition using gist panoramas</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,160.47,646.84,320.11,7.86;8,146.91,657.79,32.77,7.86">IEEE Workshop on Omnidirectional Vision, Camera Netwoks and Non-Classical Cameras</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,120.67,342.24,7.86;9,146.91,131.63,333.68,7.86;9,146.91,142.59,46.33,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,244.94,120.67,235.65,7.86;9,146.91,131.63,91.14,7.86">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,245.97,131.63,171.17,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,153.55,342.25,7.86;9,146.91,164.51,313.40,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,381.68,153.55,98.91,7.86;9,146.91,164.51,56.99,7.86">The robot vision task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,236.77,164.51,133.98,7.86">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,175.46,342.24,7.86;9,146.91,186.42,333.68,7.86;9,146.91,197.38,89.65,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,311.88,175.46,123.52,7.86">City-scale location recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,459.20,175.46,21.39,7.86;9,146.91,186.42,253.39,7.86">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,208.34,342.24,7.86;9,146.91,219.30,284.29,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,396.13,208.34,84.46,7.86;9,146.91,219.30,157.25,7.86">Context-based vision system for place and object recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.24,219.30,26.11,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
