<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.06,115.96,341.25,12.62;1,201.55,133.89,212.25,12.62;1,236.68,151.82,142.00,12.62">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
				<funder ref="#_Q6bsX4F">
					<orgName type="full">Ministry of Economics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.42,189.49,67.25,8.74"><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
							<email>stefanie.nowak@idmt.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IDMT</orgName>
								<address>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.83,189.49,59.64,8.74"><forename type="first">Mark</forename><surname>Huiskes</surname></persName>
							<email>mark.huiskes@liacs.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Leiden Institute of Advanced Computer Science</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.06,115.96,341.25,12.62;1,201.55,133.89,212.25,12.62;1,236.68,151.82,142.00,12.62">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81D31CFADF35001BEAD8E4C68915191B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2010 Photo Annotation Task poses the challenge of automated annotation of 93 visual concepts in Flickr photos. The participants were provided with a training set of 8,000 Flickr images including annotations, EXIF data and Flickr user tags. Testing was performed on 10,000 Flickr images, differentiated between approaches considering solely visual information, approaches relying on textual information and multi-modal approaches. Half of the ground truth was acquired with a crowdsourcing approach. The evaluation followed two evaluation paradigms: per concept and per example. In total, 17 research teams participated in the multi-label classification challenge with 63 submissions. Summarizing the results, the task could be solved with a MAP of 0.455 in the multi-modal configuration, with a MAP of 0.407 in the visual-only configuration and with a MAP of 0.234 in the textual configuration. For the evaluation per example, 0.66 F-ex and 0.66 OS-FCS could be achieved for the multi-modal configuration, 0.68 F-ex and 0.65 OS-FCS for the visual configuration and 0.26 F-ex and 0.37 OS-FCS for the textual configuration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The steadily increasing amount of multimedia data poses challenging questions on how to index, visualize, organize, navigate or structure multimedia information. Many different approaches are proposed in the research community, but often their benefit is not clear as they were evaluated on different datasets with different evaluation measures. Evaluation campaigns aim to establish an objective comparison between the performance of different approaches by posing well-defined tasks including datasets, topics and measures. This paper presents an overview of the ImageCLEF 2010 Photo Annotation Task. The task aims at the automated detection of visual concepts in consumer photos. Section 2 introduces the task and describes the database, the annotation process, the ontology and the evaluation measures applied. Section 3 summarizes the approaches of the participants to solve the task. Next, the results for all configurations are presented and discussed in Section 4 and Section 5, respectively. Finally, Section 6 summarizes and concludes the paper.</p><p>The ImageCLEF Visual Concept Detection and Annotation Task poses a multilabel classification challenge. It aims at the automatic annotation of a large number of consumer photos with multiple annotations. The task can be solved by following three different approaches:</p><p>1. Automatic annotation with content-based visual information of the images. 2. Automatic annotation with Flickr user tags and EXIF metadata in a purely text-based scenario. 3. Multi-modal approaches that consider both visual and textual information like Flickr user tags or EXIF information.</p><p>In all cases the participants of the task were asked to annotate the photos of the test set with a predefined set of keywords (the concepts), allowing for an automated evaluation and comparison of the different approaches. Concepts are for example abstract categories such as Family&amp;Friends or Partylife, the Time of Day (Day, Night, sunny, ), Persons (no person, single person, small group or big group), Quality (blurred, underexposed ) and Aesthetics; 52 from the 53 concepts that were used in the ImageCLEF 2009 benchmark are used again <ref type="bibr" coords="2,416.84,353.87,9.96,8.74" target="#b0">[1]</ref>. In total the number of concepts was extended to 93 concepts. In contrast to the annotations from 2009, the new annotations were obtained with a crowdsourcing approach that utilizes Amazon Mechanical Turk. The task uses a subset of the MIR Flickr 25,000 image dataset <ref type="bibr" coords="2,228.20,401.69,10.52,8.74" target="#b1">[2]</ref> for the annotation challenge. The MIR Flickr collection supplies all original tag data provided by the Flickr users (noted as Flickr user tags). In the collection there are 1386 tags which occur in at least 20 images, with an average total number of 8.94 tags per image. These Flickr user tags are made available for the textual and multi-modal approaches. For most of the photos the EXIF data is included and may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Objectives</head><p>This year the focus of the task lies on the comparison of the strengths and limitations of the different approaches:</p><p>-Do multi-modal approaches outperform text only or visual only approaches? -Which approaches are best for which kind of concepts? -Can image classifiers scale to the large number of concepts and data?</p><p>Furthermore, the task challenges the participants to deal with an unbalanced number of annotations per photo, an unbalanced number of photos per concept, the subjectivity of concepts like boring, cute or fancy and the diversity of photos belonging to the same concept. Further, the textual runs have to cope with a small number of images without EXIF data and/or Flickr user tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation Process</head><p>The complete dataset consists of 18,000 images annotated with 93 visual concepts. The manual annotations for 52 concepts were acquired by Fraunhofer IDMT in 2009. (The concept Canvas from 2009 was discarded.) Details on the manual annotation process and concepts, including statistics on concept frequencies can be found in <ref type="bibr" coords="3,245.21,186.36,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="3,257.39,186.36,7.01,8.74" target="#b0">1]</ref>. In 2010, 41 new concepts were annotated with a crowdsourcing approach using the Amazon Mechanical Turk. In the following, we just focus on the annotation process of these new concepts.</p><p>Amazon Mechanical Turk (MTurk, www.mturk.com) is an online marketplace in which mini-jobs can be distributed to a crowd of people. At MTurk these minijobs are called HITs (Human Intelligence Tasks). They represent a small piece of work with an allocated price and completion time. The workers at MTurk, called turkers, can choose the HITs they would like to perform and submit the results to MTurk. The requester of the work collects all results from MTurk after they are completed. The workflow of a requester can be described as follows: 1) design a HIT template, 2) distribute the work and fetch results and 3) approve or reject work from turkers. For the design of the HITs, MTurk offers support by providing a web interface, command line tools and developer APIs. The requester can define how many assignments per HIT are needed, how much time is allotted to each HIT and how much to pay per HIT. MTurk offers several ways of assuring quality. Optionally, the turkers can be asked to pass a qualification test before working on HITs, multiple workers can be assigned the same HIT and requesters can reject work in case the HITs were not finished correctly. The HIT approval rate each turker achieves by completing HITs can be used as a threshold for authorization to work. Before the annotations of the ImageCLEF 2010 tasks were acquired, we performed a pre-study to investigate if annotations from nonexperts are reliable enough to be used in an evaluation benchmark. The results were very promising and encouraged us to adapt this service for the 2010 task. Details of the pre-study can be found in <ref type="bibr" coords="3,312.98,461.33,9.97,8.74" target="#b3">[4]</ref>.</p><p>Design of HIT templates: In total, we generated four different HIT templates at MTurk. For all concepts, the annotations per photo were obtained three times. Later the final annotations are built from the majority vote of these three opinions. For the annotation of the 41 new concepts we made use of the pre-knowledge that we have from the old annotations. Therefore the 41 concepts were structured into four groups:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Vehicles</head><p>The ImageCLEF 2009 dataset contains a number of photos annotated with the concept Vehicle. These photos were further annotated with the concepts car, bicycle, ship, train, airplane and skateboard. A textbox offered the possibility to input further categories. The turkers could select a checkbox saying that no vehicle is depicted in the photo to cope with the case of false annotations. The corresponding survey with guidelines can be found in Figure <ref type="figure" coords="3,472.84,643.61,3.88,8.74" target="#fig_0">1</ref>.</p><p>Each HIT was rewarded with 0.01$. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Animals</head><p>The ImageCLEF 2009 photo collection already contains several photos that were annotated with the concept animals. The turkers at Amazon were asked to further classify these photos in the categories dog, cat, bird, horse, fish and insect. Again, a textbox offered additional input possibilities. For each HIT a reward of 0.01$ was paid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Persons</head><p>The dataset contains photos that were annotated with a person concept (single person, small group or big group of persons). These photos were further classified with human attributes like female, male, Baby, Child, Teenager, Adult and old person. Each HIT was rewarded with 0.01$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">General annotations</head><p>For the following 22 concepts, no prior information could be used. Therefore the concepts were annotated in all 18,000 photos. The HIT was designed as a survey with 6 questions aiming to annotate the categories "content elements" (Architecture, Street, Church, Bridge, Park Garden, Rain, Toy, Musical Instrument and Shadow ), "persons" (bodypart), "events" (Travel, Work, Birthday), "image representation" (Visual Arts, Graffiti, Painting), "impression" (artificial, natural, technical, abstract) and "feelings" (boring, cute). Each HIT was rewarded with 0.03$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ontology</head><p>The concepts were organised in an ontology. For this purpose the Consumer Photo Tagging Ontology <ref type="bibr" coords="5,246.48,278.22,10.52,8.74" target="#b2">[3]</ref> of 2009 was extended with the new concepts. The hierarchy allows making assumptions about the assignment of concepts to documents. For instance, if a photo is classified to contain trees, it also contains plants. Then, next to the is-a relationship of the hierarchical organization of concepts, also other relationships between concepts can determine label assignments. The ontology requires for example that for a certain sub-node only one concept can be assigned at a time (disjoint items) or that a special concept (e.g. portrait) postulates other concepts like persons or animals. The ontology allows the participants to incorporate knowledge in their classification algorithms, and to make assumptions about which concepts are probable in combination with certain labels. Further, it is used in the evaluation of the submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation Measures</head><p>The evaluation follows the concept-based and example-based evaluation paradigms. For the concept-based evaluation the Average Precision (AP) is utilized. This measure showed better characteristics than the Equal Error Rate (EER) and Area under Curve (AUC) in a recent study <ref type="bibr" coords="5,329.80,485.10,9.97,8.74" target="#b4">[5]</ref>. For the example-based evaluation we apply the example-based F-Measure (F-ex). The Ontology Score of last year was extended with a different cost map that is based on Flickr metadata <ref type="bibr" coords="5,134.77,520.97,10.52,8.74" target="#b5">[6]</ref> and serves as additional evaluation measure. It is called Ontology Score with Flickr Context Similarity (OS-FCS) in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Submission</head><p>The participants submitted their results for all photos in a single text file that contains the photo ID as first entry per row followed by 93 floating point values between 0 and 1 (one value per concept). The floating point values are regarded as confidence while computing the AP. After the confidence values for all photos, the text file contains binary values for each photo (so again each line contains the photo ID followed by 93 binary values). The measures F-ex and OS-FCS need a binary decision about the presence or absence of the concepts. Instead of applying a strict threshold at 0.5 of the confidence values, the participants have the possibility to threshold each concept for each image individually. All groups had to submit a short description of their runs and state which configuration they chose (annotation with visual information only, annotation with textual information only or annotation with multi-modal information). In the following the visual configuration is abbreviated with "V", the textual with "T" and the multi-modal one with "M".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>In total 54 groups registered for the visual concept detection and annotation task, 41 groups signed the license agreement and were provided with the training and test sets, 17 of them submitted results in altogether 63 runs. The number of runs was restricted to a maximum of 5 runs per group. There were 45 runs submitted in the visual only configuration, 2 in the textual only configuration and 16 in the multi-modal configuration. BPACAD|SZTAKI <ref type="bibr" coords="6,248.16,320.51,11.45,8.77" target="#b6">[7]</ref>: The team of the Computer and Automation Research Institute of the Hungarian Academy of Science submitted one run in the visual configuration. Their approach is based on Histogram of Oriented Gradients descriptors which were clustered with a 128 dimensional Gaussian Mixture Model. Classification was performed with a linear logistic regression model with a χ 2 kernel per category.</p><p>CEA-LIST: The team from CEA-LIST, France submitted one run in the visual configuration. They extract various global (colour, texture) and local (SURF) features. The visual concepts are learned with a fast shared boosting approach and normalized with a logistic function.</p><p>CNRS|Telecom ParisTech <ref type="bibr" coords="6,283.52,440.48,11.45,8.77" target="#b7">[8]</ref>: The CNRS group of Telecom ParisTech, Paris, France participated with five multi-modal runs. Their approach is based on SIFT features represented by multi-level spatial pyramid bag-of-words. For classification a one-vs-all trained SVM is utilized.</p><p>DCU <ref type="bibr" coords="6,178.90,488.51,11.45,8.77" target="#b8">[9]</ref>: The team of Dublin City University, Ireland submitted one run in the textual configuration. They followed a document expansion approach based on the Okapi feedback method to expand the image metadata and concepts and applied DBpedia as external information source in this step. To deal with images without any metadata, the relationships between concepts in the training set is investigated. The date and time information of the EXIF metadata was extracted to predict concepts like Day.</p><p>HHI <ref type="bibr" coords="6,174.59,572.40,16.80,8.77" target="#b9">[10]</ref>: The team of Fraunhofer HHI, Berlin, Germany submitted five runs in the visual-only configuration. Their approach is based on the bag of words approach and introduces category specific features and classifiers including quality related features. They use opponent SIFT features with dense sampling and a sharpness feature and base their classification on a multi-kernel SVM classifier with χ 2 distance. Second, they incorporate a post-processing approach that considers relations and exclusions between concepts. Both extensions resulted in an increase in performance compared to the standard bag-of-words approach.</p><p>IJS <ref type="bibr" coords="7,170.97,118.96,16.80,8.77" target="#b10">[11]</ref>: The team of Jožef Stefan Institute, Slovenia and Department of Computer Science, Macedonia submitted four runs in the visual configuration. They use various global and local image features (GIST, colour histograms, SIFT) and learn predictive clustering trees classifiers. For each descriptor a separate classifier is learned and the probabilities output of all classifiers is combined for the final prediction. Further, they investigate ensembles of predictive clustering tree classifiers. The combination of global and local features leads to better results than using local features alone.</p><p>INSUNHIT <ref type="bibr" coords="7,212.37,216.45,16.80,8.77" target="#b11">[12]</ref>: The group of the Harbin Institute of Technology, China participated with five runs in the visual configuration. They use dense SIFT features as image descriptors and classify with a naïve-bayes nearest neighbour approach. The classifier is extended with a random sampling image to class distance to cope with imbalanced classes.</p><p>ISIS <ref type="bibr" coords="7,174.24,278.08,16.79,8.77" target="#b12">[13]</ref>: The Intelligent Systems Lab of the University of Amsterdam, The Netherlands submitted five runs in the visual configuration. They use a dense sampling strategy that combines a spatial pyramid approach and saliency points detection, extract different SIFT features, perform a codebook transformation and classify with a SVM approach. The focus lies on the improvement of the scores in the evaluation per image. They use the distance to the decision plane in the SVM as probability and determine the threshold for binary annotation from this distance.</p><p>LEAR and XRCE <ref type="bibr" coords="7,243.60,375.57,16.80,8.77" target="#b13">[14]</ref>: The team of LEAR and XEROX, France made a joint contribution with a total of ten runs, five submitted in the visual and five in the multi-modal configuration. They use SIFT and colour features on several spatial scales and represent them as improved Fisher vectors in a codebook of 256 words. The textual information is represented as a binary presence/absence vector of the most common 698 Flickr user tags. For classification a linear SVM is compared to a k -NN classifier with learned neighbourhood weights. Both classification models are computed with the same visual and textual features and late and early fusion approaches are investigated. All runs considering multi-modal information outperformed the runs in the visual configuration.</p><p>LIG <ref type="bibr" coords="7,172.99,496.97,16.80,8.77" target="#b14">[15]</ref>: The team of Grenoble University, France submitted one run in the visual configuration of the Photo Annotation task. They extract colour SIFT features and cluster them with a k -means clustering procedure in 4000 clusters. For classification a SVM with RBF kernel is learned in an one-against-all approach and based on the 4000 dimensional histogram of word occurrences.</p><p>LSIS <ref type="bibr" coords="7,176.78,558.60,16.80,8.77" target="#b15">[16]</ref>: The Laboratory of Information Science and Systems, France submitted two runs in the visual configuration. They propose features based on extended local binary patterns extracted with spatial pyramids. For classification they use a linear max-margin SVM classifier.</p><p>MEIJI <ref type="bibr" coords="7,186.37,608.27,16.80,8.77" target="#b16">[17]</ref>: The group of Meiji University, Kanagawa, Japan submitted in total five runs. They followed a conceptual fuzzy set approach applied to visual words, a visual words baseline with SIFT descriptors and a combination with a Flickr User Tag system using TF-IDF. Classification is based on a matching of visual word combinations between the training casebase and the test image.</p><p>For the visual word approach the cosine distance is applied for similarity determination. In total, two runs were submitted in the visual configuration and three in the multi-modal one. Their multi-modal runs outperform the visual configurations.</p><p>MLKD: The team of the Aristotle University of Thessaloniki, Greece participated with three runs; one in each configuration. For the visual and the textual runs ensemble classifier chains are used as classifiers. The visual configuration applies C-SIFT features with a Harris-Laplace salient point detector and clusters them in a 4000 word codebook. As textual features, the 250 most frequent Flickr user tags of the collection are represented in a binary feature vector per image. The multi-modal configuration chooses the confidence score of the model (textual or visual) per concept for which a better AP was determined in the evaluation phase. As a result, the multi-modal approach outperforms the visual and the textual models.</p><p>Romania <ref type="bibr" coords="8,197.08,286.78,16.80,8.77" target="#b17">[18]</ref>: The team of the University Bucharest, Romania participated with five runs in the visual configuration. Their approach considers the extraction of colour histograms and combine them with a method of structural description. The classification is performed using a Linear Discriminant Analysis (LDA) and a weighted average retrieval rank (ARR) method. The annotations resulting from the LDA classifier were refined considering the joint probabilities of concepts. As a result the ARR classification outperforms the LDA classification.</p><p>UPMC/LIP6 <ref type="bibr" coords="8,220.68,370.69,16.80,8.77" target="#b18">[19]</ref>: The team of University Pierre et Marie Curie, Paris, France participated in the visual and the multi-modal configuration. They submitted a total of five runs (3V, 2M). Their approach investigates the fusion of results from different classifiers with supervised and semi-supervised classification methods. The first model is based on fusing outputs from several Rank-ingSVM classifiers that classified the images based on visual features (SIFT, HSV, Mixed+PCA). The second model further incorporates unlabeled data from the test set for which the initial classifiers are confident to assign a certain label and retrains the classifiers based on the augmented set. Both models were tested with the additional inclusion of Flickr user tags using the Porter stemming algorithm. For both models the inclusion of user tags improved the results.</p><p>WROCLAW <ref type="bibr" coords="8,218.23,502.42,16.80,8.77" target="#b19">[20]</ref>: The group of Wroclaw University, Poland submitted five runs in the visual configuration. They focus on global colour and texture features and adapt an approach which annotates photos through the search for similar images and the propagation of their tags. In their configurations several similarity measures (Minkowski distance, Cosine distance, Manhattan distance, Correlation distance and Jensen-Shannon divergence) are investigated. Further, an approach based on a Penalized Discriminant Analysis classifier was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents the results of the Photo Annotation Task 2010. First, the overall results of all teams independent of the configuration are presented. In the following subsections the results per configuration are highlighted. In Table <ref type="table" coords="10,192.55,118.99,4.98,8.74" target="#tab_0">1</ref> the results for the evaluation per concept independent of the applied configuration are illustrated for the best run of each group. The results for all runs can be found at the Photo Annotation Task website<ref type="foot" coords="10,428.70,141.33,3.97,6.12" target="#foot_0">1</ref> . The task could be solved best with a MAP of 0.455 (XRCE) followed by a MAP of 0.437 (LEAR). Both runs make use of multi-modal information. Table <ref type="table" coords="10,428.43,166.81,4.98,8.74" target="#tab_1">2</ref> illustrates the overall ranking for the results of the evaluation per example. The table is sorted descending for the F-ex measure. The best results were achieved in a visual configuration with 0.68 F-ex (ISIS) and in a multi-modal configuration with 0.66 OS-FCS (XRCE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results for the visual configuration</head><p>Table <ref type="table" coords="10,163.35,265.01,4.98,8.74" target="#tab_2">3</ref> shows the results of the best run of each group that participated in the visual configuration evaluated with all three evaluation measures. The best results in the visual configuration were achieved by the ISIS team in terms of MAP and F-ex and the XRCE team in terms of OS-FCS. Both teams get close results in the concept-based evaluation (1.7% difference) while there is a bigger gap in the example-based evaluation (4.1% and 4.4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results for the textual configuration</head><p>The results for the two textual runs are presented in Table <ref type="table" coords="10,412.63,375.17,3.88,8.74" target="#tab_3">4</ref>. Both groups achieve close results in the concept-based evaluation. However, the examplebased evaluation measures show a significant difference between the results of both teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results for the multi-modal configuration</head><p>Table <ref type="table" coords="10,161.78,461.41,4.98,8.74" target="#tab_4">5</ref> depicts the results for the best multi-modal configuration of each group. As already stated the run of XRCE achieves the best overall results in terms of MAP and OS-FCS. In terms of OS-FCS, the results of XRCE in the multi-modal configuration are around 23% better than the second best configuration of the MEIJI team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The following section discusses some of the results in more detail. The best results for each concept are summarized in Table <ref type="table" coords="10,321.51,577.54,3.88,8.74">6</ref>. On average the concepts could be detected with a MAP of 0.48 considering the best results per concept from all configurations and submissions. From 93 concepts, 61 could be annotated best with a multi-modal approach, 30 with a visual approach and two with a textual one. Most of the concepts were classified best by one configuration of the XRCE, ISIS or LEAR group. The best classified concepts are the ones from the mutually exclusive categories: Neutral-Illumination (98.2% AP, 94% F), No-Visual-Season (96,5% AP, 88% F), No-Persons (91.9% AP, 68% F), No-Blur (91.5% AP, 68% F). Following, the concepts Outdoor (90.9% AP, 50% F), Sky, (89.5% AP, 27% F) Day (88.1% AP, 51% F) and Clouds (85.9% AP, 14% F) were annotated with a high AP. The concepts with the worst annotation quality were abstract (4.6% AP, 1% F), old-person (11.6% AP, 2% F), work (13.1% AP, 3% F), technical (14.2% AP, 4% F), Graffiti (14.5% AP, 1% F), and boring (16.2% AP, 6% F). The percentages in parentheses denote the detection performance in AP and the frequency (F) of the concept occurrence in the images of the test set. Although there is a trend that concepts that occur more frequently in the image collection can be detected better, this does not hold for all concepts. Figure <ref type="figure" coords="13,386.51,250.50,4.98,8.74" target="#fig_1">2</ref> shows the frequency of concepts in the test collection plotted against the best AP achieved by any submission. Although the performance of the textual runs is much lower in average than in the visual and textual runs, there are two concepts that can be annotated best in a textual configuration: skateboard and abstract. The concept skateboard was just annotated in six images of the test set and twelve of the training set. In the user tags of three images the word "skateboard" was present, while two images have no user tags and the sixth image does not contain words like "skateboard" or "skateboarding". It seems as if there is not enough visual information available to learn this concept while the textual and multi-modal approaches can make use of the tags and extract the correct concept from the tags for at least half of the images. The concept abstract was annotated more often (1,2% in the test set and 4,7% in the training set).</p><p>Further, one can see a great difference in annotation quality between the old concepts from 2009 that were carefully annotated by experts (number 1-52) and the new concepts (number 53-93) annotated with the service of Mechanical Turk. The average annotation quality in terms of MAP for the old concepts is 0.57 while it is 0.37 for the new concepts. The reason for this is unclear. One reason may lie in the quality of the annotations of the non-experts. However, recent studies found that the quality of crowdsourced annotations is similar to the annotation quality of experts <ref type="bibr" coords="14,281.82,178.77,15.49,8.74" target="#b20">[21,</ref><ref type="bibr" coords="14,298.98,178.77,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="14,308.38,178.77,11.63,8.74" target="#b21">22]</ref>. Another reason could be the choice and difficulty of the new concepts, as some of them are not as obvious and objective as the old ones. Further, some of the new concepts are special and their occurrence in the dataset is lower ( 7% in average) than the occurrence of the old concepts ( 17% in average).</p><p>One possibility to determine the reliability of a test collection is to calculate Cronbach's alpha value <ref type="bibr" coords="14,259.61,250.84,14.62,8.74" target="#b22">[23]</ref>. It defines a holistic measure of reliability and analyses the variance of individual test items and total test scores. The measure returns a value ranging between zero and one, for which bigger scores indicate a higher reliability. The Cronbach's alpha values show a high reliability for the whole test collection with 0.991, 0.991 for the queries assessed by experts and 0.956 for the queries assessed by MTurk. Therefore the scores point to a reliable test collection for both the manual expert annotations and the crowdsourced annotations and cannot explain the differences in MAP by the annotating systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The ImageCLEF 2010 Photo Annotation Task posed a multi-label annotation challenge for visual concept detection in three general configurations (textual, visual and multi-modal). The task attracted a considerable number of international teams with a final participation of 17 teams that submitted a total of 63 runs. In summary, the challenge could be solved with a MAP of 0.455 in the multi-modal configuration, with a MAP of 0.407 in the visual only configuration and with a MAP of 0.234 in the text configuration. For the evaluation per example 0.66 F-ex and 0.66 OS-FCS could be achieved for the multi-modal configuration, 0.68 F-ex and 0.65 OS-FCS for the visual configuration and 0.26 F-ex and 0.37 OS-FCS for the textual configuration. All in all, the multi-modal approaches got the best scores for 61 out of 93 concepts, followed by 30 concepts that could be detected best with the visual approach and two that won with a textual approach. As just two runs were submitted in the textual configuration, it is not possible to determine the abilities of purely textual classifiers reliably. In general, the multi-modal approaches outperformed visual and textual configurations for all teams that submitted results for more than one configuration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,147.65,502.13,303.12,7.86;4,138.22,115.83,338.91,371.50"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: MTurk HIT template for the annotation of specific vehicle concepts.</figDesc><graphic coords="4,138.22,115.83,338.91,371.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,152.11,473.01,311.14,7.86;13,134.77,305.50,345.84,152.71"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Frequency of labels in test set plotted against best AP of submissions.</figDesc><graphic coords="13,134.77,305.50,345.84,152.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,120.83,345.83,262.53"><head>Table 1 :</head><label>1</label><figDesc>Summary of the results for the evaluation per concept. The table shows the MAP for the best run per group and the averaged MAP for all runs of one group and indicates the configuration of the run.</figDesc><table coords="9,167.31,173.21,277.67,210.15"><row><cell></cell><cell></cell><cell>BEST RUN</cell><cell></cell><cell cols="2">AVERAGE RUNS</cell></row><row><cell cols="6">TEAM RUNS RANK MAP Conf. RANK MAP Conf.</cell></row><row><cell>XRCE</cell><cell>5</cell><cell>1 0.455</cell><cell>M</cell><cell cols="2">7.2 0.408 M+V</cell></row><row><cell>LEAR</cell><cell>5</cell><cell>3 0.437</cell><cell>M</cell><cell cols="2">7.8 0.392 M+V</cell></row><row><cell>ISIS</cell><cell>5</cell><cell>5 0.407</cell><cell>V</cell><cell>7.0 0.401</cell><cell>V</cell></row><row><cell>HHI</cell><cell>5</cell><cell>16 0.350</cell><cell>V</cell><cell>18.4 0.350</cell><cell>V</cell></row><row><cell>IJS</cell><cell>4</cell><cell>20 0.334</cell><cell>V</cell><cell>22.5 0.326</cell><cell>V</cell></row><row><cell>MEIJI</cell><cell>5</cell><cell>23 0.326</cell><cell>M</cell><cell cols="2">36.0 0.269 M+V</cell></row><row><cell>CNRS</cell><cell>5</cell><cell>28 0.296</cell><cell>M</cell><cell>30.0 0.293</cell><cell>M</cell></row><row><cell>BPACAD</cell><cell>1</cell><cell>33 0.283</cell><cell>V</cell><cell>33.0 0.283</cell><cell>V</cell></row><row><cell>Romania</cell><cell>5</cell><cell>34 0.259</cell><cell>V</cell><cell>43.8 0.221</cell><cell>V</cell></row><row><cell>INSUNHIT</cell><cell>5</cell><cell>36 0.237</cell><cell>V</cell><cell>41.0 0.230</cell><cell>V</cell></row><row><cell>MLKD</cell><cell>3</cell><cell>37 0.235</cell><cell>M</cell><cell>45.0 0.215</cell><cell>all</cell></row><row><cell>LSIS</cell><cell>2</cell><cell>38 0.234</cell><cell>V</cell><cell>38.5 0.234</cell><cell>V</cell></row><row><cell>DCU</cell><cell>1</cell><cell>44 0.228</cell><cell>T</cell><cell>44.0 0.228</cell><cell>T</cell></row><row><cell>LIG</cell><cell>1</cell><cell>46 0.225</cell><cell>V</cell><cell>46.0 0.225</cell><cell>V</cell></row><row><cell>WROCLAW</cell><cell>5</cell><cell>50 0.189</cell><cell>V</cell><cell>53.4 0.183</cell><cell>V</cell></row><row><cell>UPMC</cell><cell>5</cell><cell>54 0.182</cell><cell>M</cell><cell cols="2">59.0 0.160 M+V</cell></row><row><cell>CEA-LIST</cell><cell>1</cell><cell>61 0.147</cell><cell>V</cell><cell>61.0 0.147</cell><cell>V</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,403.84,345.83,251.57"><head>Table 2 :</head><label>2</label><figDesc>Summary of the results for the evaluation per example. The table shows the F-ex and the OS-FCS and the configuration used for the best run per group sorted by F-ex.</figDesc><table coords="9,181.41,456.22,249.46,199.19"><row><cell cols="6">TEAM RANK F-ex Conf. RANK OS-FCS Conf.</cell></row><row><cell>ISIS</cell><cell>1 0.680</cell><cell>V</cell><cell>10</cell><cell>0.601</cell><cell>V</cell></row><row><cell>XRCE</cell><cell>5 0.655</cell><cell>M</cell><cell>1</cell><cell>0.657</cell><cell>M</cell></row><row><cell>HHI</cell><cell>8 0.634</cell><cell>V</cell><cell>3</cell><cell>0.640</cell><cell>V</cell></row><row><cell>LEAR</cell><cell>15 0.602</cell><cell>M</cell><cell>32</cell><cell>0.411</cell><cell>M</cell></row><row><cell>IJS</cell><cell>18 0.596</cell><cell>V</cell><cell>12</cell><cell>0.595</cell><cell>V</cell></row><row><cell>MEIJI</cell><cell>23 0.572</cell><cell>M</cell><cell>30</cell><cell>0.428</cell><cell>M</cell></row><row><cell>Romania</cell><cell>29 0.531</cell><cell>V</cell><cell>17</cell><cell>0.562</cell><cell>V</cell></row><row><cell>LSIS</cell><cell>30 0.530</cell><cell>M</cell><cell>21</cell><cell>0.536</cell><cell>V</cell></row><row><cell>WROCLAW</cell><cell>34 0.482</cell><cell>V</cell><cell>41</cell><cell>0.379</cell><cell>V</cell></row><row><cell>LIG</cell><cell>35 0.477</cell><cell>V</cell><cell>22</cell><cell>0.530</cell><cell>V</cell></row><row><cell>CEALIST</cell><cell>37 0.451</cell><cell>V</cell><cell>28</cell><cell>0.458</cell><cell>V</cell></row><row><cell>BPACAD</cell><cell>38 0.428</cell><cell>V</cell><cell>29</cell><cell>0.439</cell><cell>V</cell></row><row><cell>CNRS</cell><cell>43 0.351</cell><cell>M</cell><cell>31</cell><cell>0.421</cell><cell>M</cell></row><row><cell>MLKD</cell><cell>49 0.260</cell><cell>T</cell><cell>42</cell><cell>0.379</cell><cell>M</cell></row><row><cell>INSUNHIT</cell><cell>53 0.209</cell><cell>V</cell><cell>43</cell><cell>0.372</cell><cell>V</cell></row><row><cell>UPMC</cell><cell>55 0.186</cell><cell>M</cell><cell>55</cell><cell>0.351</cell><cell>M</cell></row><row><cell>DCU</cell><cell>60 0.178</cell><cell>T</cell><cell>60</cell><cell>0.304</cell><cell>T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,128.46,345.83,229.65"><head>Table 3 :</head><label>3</label><figDesc>Summary of the results for the evaluation per concept in the visual configuration. The table shows the MAP, F-ex and OS-FCS for the best run per group sorted by MAP.</figDesc><table coords="11,179.31,180.84,253.67,177.28"><row><cell cols="5">TEAM RANK MAP RANK F-ex RANK OS-FCS</cell></row><row><cell>ISIS</cell><cell>1 0.407</cell><cell>1 0.680</cell><cell>8</cell><cell>0.601</cell></row><row><cell>XRCE</cell><cell>6 0.390</cell><cell>6 0.639</cell><cell>1</cell><cell>0.645</cell></row><row><cell>LEAR</cell><cell>9 0.364</cell><cell>15 0.582</cell><cell>28</cell><cell>0.387</cell></row><row><cell>HHI</cell><cell>11 0.350</cell><cell>7 0.634</cell><cell>2</cell><cell>0.640</cell></row><row><cell>IJS</cell><cell>15 0.334</cell><cell>14 0.596</cell><cell>10</cell><cell>0.595</cell></row><row><cell>BPACAD</cell><cell>20 0.283</cell><cell>30 0.428</cell><cell>27</cell><cell>0.439</cell></row><row><cell>Romania</cell><cell>21 0.259</cell><cell>22 0.531</cell><cell>15</cell><cell>0.562</cell></row><row><cell>INSUNHIT</cell><cell>23 0.237</cell><cell>38 0.209</cell><cell>31</cell><cell>0.372</cell></row><row><cell>LSIS</cell><cell>24 0.234</cell><cell>23 0.530</cell><cell>19</cell><cell>0.536</cell></row><row><cell>LIG</cell><cell>30 0.225</cell><cell>27 0.477</cell><cell>20</cell><cell>0.53</cell></row><row><cell>MEIJI</cell><cell>31 0.222</cell><cell>18 0.559</cell><cell>34</cell><cell>0.363</cell></row><row><cell>WROCLAW</cell><cell>34 0.189</cell><cell>26 0.482</cell><cell>30</cell><cell>0.379</cell></row><row><cell>MLKD</cell><cell>40 0.177</cell><cell>37 0.224</cell><cell>37</cell><cell>0.359</cell></row><row><cell>UPMC</cell><cell>42 0.148</cell><cell>43 0.174</cell><cell>40</cell><cell>0.348</cell></row><row><cell>CEALIST</cell><cell>43 0.147</cell><cell>29 0.451</cell><cell>26</cell><cell>0.458</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,393.84,345.83,87.19"><head>Table 4 :</head><label>4</label><figDesc>Summary of the results for the evaluation per concept in the textual configuration. The table shows the MAP, F-ex and OS-FCS for the best run per group sorted by MAP.</figDesc><table coords="11,186.80,446.22,238.68,34.81"><row><cell cols="5">TEAM RANK MAP RANK F-ex RANK OS-FCS</cell></row><row><cell>MLKD</cell><cell>1 0.234</cell><cell>1 0.260</cell><cell>1</cell><cell>0.368</cell></row><row><cell>DCU</cell><cell>2 0.228</cell><cell>2 0.178</cell><cell>2</cell><cell>0.304</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,516.76,345.83,131.02"><head>Table 5 :</head><label>5</label><figDesc>Summary of the results for the evaluation per concept in the multi-modal configuration. The table shows the MAP, F-ex and OS-FCS for the best run per group sorted by MAP.</figDesc><table coords="11,185.52,569.14,241.24,78.65"><row><cell cols="5">TEAM RANK MAP RANK F-ex RANK OS-FCS</cell></row><row><cell>XRCE</cell><cell>1 0.455</cell><cell>1 0.655</cell><cell>1</cell><cell>0.657</cell></row><row><cell>LEAR</cell><cell>3 0.437</cell><cell>3 0.602</cell><cell>5</cell><cell>0.411</cell></row><row><cell>MEIJI</cell><cell>6 0.326</cell><cell>6 0.573</cell><cell>3</cell><cell>0.428</cell></row><row><cell>CNRS</cell><cell>9 0.296</cell><cell>9 0,.351</cell><cell>4</cell><cell>0.421</cell></row><row><cell>MLKD</cell><cell>14 0.235</cell><cell>13 0.257</cell><cell>12</cell><cell>0.379</cell></row><row><cell>UPMC</cell><cell>15 0.182</cell><cell>15 0.186</cell><cell>15</cell><cell>0.351</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="10,144.73,657.44,211.83,7.47"><p>http://www.imageclef.org/2010/PhotoAnnotation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the <rs type="institution">CLEF</rs> campaign for supporting the ImageCLEF initiative. This work was partly supported by grant <rs type="grantNumber">01MQ07017</rs> of the <rs type="programName">German research program THESEUS</rs> funded by the <rs type="funder">Ministry of Economics</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Q6bsX4F">
					<idno type="grant-number">01MQ07017</idno>
					<orgName type="program" subtype="full">German research program THESEUS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="12,159.78,117.38,3.59,7.86">6</ref>: This table presents the best annotation performance per concept, achieved by any team in any configuration, in terms of AP. It lists the concept name, the AP score, the team that achieved the score and the configuration of the run.</p><p>Concept AP Team Conf.</p><p>Concept AP Team Conf. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.96,145.25,337.63,7.86;15,151.52,156.21,329.07,7.86;15,151.52,167.16,329.07,7.86;15,151.52,178.12,329.07,7.86;15,151.52,189.08,329.07,7.86;15,151.52,200.04,232.79,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,250.13,145.25,230.46,7.86;15,151.52,156.21,125.10,7.86">Overview of the CLEF 2009 Large-Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,363.38,167.16,117.21,7.86;15,151.52,178.12,329.07,7.86;15,151.52,189.08,221.56,7.86">Multilingual Information Access Evaluation Vol. II Multimedia Experiments: Proceedings of the 10th Workshop of the Cross-Language Evaluation Forum (CLEF 2009)</title>
		<title level="s" coord="15,151.52,200.04,141.42,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="15,142.96,211.53,337.63,7.86;15,151.52,222.49,307.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,260.32,211.53,148.98,7.86">The MIR Flickr Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,432.43,211.53,48.17,7.86;15,151.52,222.49,275.37,7.86">Proc. of the ACM International Conference on Multimedia Information Retrieval</title>
		<meeting>of the ACM International Conference on Multimedia Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,233.98,337.63,7.86;15,151.52,244.94,329.07,7.86;15,151.52,255.90,329.07,7.86;15,151.52,266.86,244.81,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,248.29,233.98,232.30,7.86;15,151.52,244.94,35.99,7.86">A Consumer Photo Tagging Ontology: Concepts and Annotations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,212.51,244.94,268.08,7.86;15,151.52,255.90,329.07,7.86;15,151.52,266.86,125.13,7.86">THESEUS/ImageCLEF Pre-Workshop 2009, Co-located with the Cross-Language Evaluation Forum (CLEF) Workshop and 13th European Conference on Digital Libraries ECDL</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,278.35,337.63,7.86;15,151.52,289.31,329.07,7.86;15,151.52,300.27,329.07,7.86;15,151.52,311.23,175.37,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,244.39,278.35,236.20,7.86;15,151.52,289.31,270.78,7.86">How reliable are Annotations via Crowdsourcing: a Study about Inter-annotator Agreement for Multi-label Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,444.69,289.31,35.90,7.86;15,151.52,300.27,325.19,7.86">MIR &apos;10: Proceedings of the International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,322.72,337.63,7.86;15,151.52,333.67,329.07,7.86;15,151.52,344.63,329.07,7.86;15,151.52,355.59,166.15,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,355.35,322.72,125.25,7.86;15,151.52,333.67,271.71,7.86">Performance Measures for Multilabel Evaluation: a Case Study in the Area of Image Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,444.76,333.67,35.83,7.86;15,151.52,344.63,325.19,7.86">MIR &apos;10: Proceedings of the International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,367.08,337.63,7.86;15,151.52,378.04,329.07,7.86;15,151.52,389.00,249.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,348.06,367.08,132.54,7.86;15,151.52,378.04,224.09,7.86">The Effect of Semantic Relatedness Measures on Multi-label Classification Evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Llorente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,402.40,378.04,78.19,7.86;15,151.52,389.00,166.80,7.86">ACM International Conference on Image and Video Retrieval</title>
		<imprint>
			<publisher>CIVR</publisher>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,400.49,337.63,7.86;15,151.52,411.45,300.71,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,434.89,400.49,45.70,7.86;15,151.52,411.45,68.89,7.86">SZTAKI @ ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daróczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Petrás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nemeskey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pethes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,242.77,411.45,118.10,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,422.94,337.63,7.86;15,151.52,433.90,329.07,7.86;15,151.52,444.86,223.54,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,227.25,422.94,253.34,7.86;15,151.52,433.90,324.67,7.86">TELECOM ParisTech at ImageCLEF 2010 Photo Annotation Task: Combining Tags and Visual Features for Learning-Based Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,165.60,444.86,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,456.35,337.63,7.86;15,151.52,467.31,325.33,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,280.48,456.35,200.11,7.86;15,151.52,467.31,93.40,7.86">A Text-Based Approach to the ImageCLEF 2010 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,267.39,467.31,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,478.80,337.97,7.86;15,151.52,489.76,329.07,7.86;15,151.52,500.72,223.54,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,151.52,489.76,324.70,7.86">Augmenting Bag-of-Words -Category Specific Features and Concept Reasoning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mbanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ndjiki-Nya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,165.60,500.72,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,512.21,337.97,7.86;15,151.52,523.17,329.07,7.86;15,151.52,534.12,172.33,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,379.26,512.21,101.33,7.86;15,151.52,523.17,270.00,7.86">Detection of Visual Concepts and Annotation of Images using Predictive Clustering Trees</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,446.53,523.17,34.06,7.86;15,151.52,534.12,80.97,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,545.61,337.97,7.86;15,151.52,556.57,317.37,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,307.90,545.61,172.69,7.86;15,151.52,556.57,85.34,7.86">Random Sampling Image to Class Distance for Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,259.43,556.57,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,568.06,337.97,7.86;15,151.52,579.02,329.07,7.86;15,151.52,589.98,49.40,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,297.36,568.06,183.23,7.86;15,151.52,579.02,145.02,7.86">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,320.99,579.02,120.20,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,601.47,337.97,7.86;15,151.52,612.43,329.07,7.86;15,151.52,623.39,209.46,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,435.74,601.47,44.85,7.86;15,151.52,612.43,308.71,7.86">LEAR and XRCE&apos;s participation to Visual Concept Detection Task -ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,151.52,623.39,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,634.88,337.97,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,263.86,634.88,216.74,7.86;15,151.52,645.84,116.02,7.86">MRIM-LIG at ImageCLEF 2010 Visual Concept Detection and Annotation task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Batal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,293.63,645.84,122.22,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,119.67,337.96,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,84.09,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,244.44,119.67,236.15,7.86;16,151.52,130.63,179.26,7.86">Linear SVM for LSIS Pyramidal Multi-Level Visual only Concept Detection in CLEF 2010 Challenge</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,355.70,130.63,120.69,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,152.55,337.97,7.86;16,151.52,163.51,329.07,7.86;16,151.52,174.47,146.42,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,314.07,152.55,166.53,7.86;16,151.52,163.51,249.70,7.86">Meiji University at the ImageCLEF2010 Visual Concept Detection and Annotation Task: Working notes</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Motohashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,421.36,163.51,59.23,7.86;16,151.52,174.47,55.06,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,185.43,337.97,7.86;16,151.52,196.39,255.56,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,251.56,185.43,229.04,7.86;16,151.52,196.39,24.44,7.86">A Novel Structural-Description Approach for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vertan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,197.61,196.39,118.11,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,207.35,337.97,7.86;16,151.52,218.30,329.07,7.86;16,151.52,229.26,25.60,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,427.74,207.35,52.85,7.86;16,151.52,218.30,123.40,7.86">UPMC/LIP6 at ImageCLEFannotation 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,298.29,218.30,119.12,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,240.22,337.97,7.86;16,151.52,251.18,329.07,7.86;16,151.52,262.14,84.09,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,251.55,240.22,229.04,7.86;16,151.52,251.18,175.39,7.86">The Wroclaw University of Technology Participation at ImageCLEF 2010 Photo Annotation Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,353.79,251.18,122.61,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,273.10,337.97,7.86;16,151.52,284.06,329.07,7.86;16,151.52,295.02,74.62,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,257.28,273.10,223.32,7.86;16,151.52,284.06,121.58,7.86">Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,299.92,284.06,180.67,7.86;16,151.52,295.02,41.77,7.86">SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,305.98,337.96,7.86;16,151.52,316.94,329.07,7.86;16,151.52,327.89,265.32,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,307.59,305.98,172.99,7.86;16,151.52,316.94,127.69,7.86">Data Quality from Crowdsourcing: a Study of Annotation Selection Criteria</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,300.52,316.94,180.07,7.86;16,151.52,327.89,232.55,7.86">Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing</title>
		<meeting>the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,338.85,337.97,7.86;16,151.52,349.78,206.19,7.89" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,198.63,338.85,227.35,7.86">Test theory for evaluating reliability of IR test collections</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bodoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,432.96,338.85,47.64,7.86;16,151.52,349.81,107.52,7.86">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1117" to="1145" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
