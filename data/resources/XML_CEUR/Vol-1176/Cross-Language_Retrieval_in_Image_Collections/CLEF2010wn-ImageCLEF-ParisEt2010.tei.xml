<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.16,115.75,310.90,12.90;1,147.00,133.75,321.38,12.90;1,273.12,151.63,69.07,12.90">Linear SVM for new Pyramidal Multi-Level Visual only Concept Detection in CLEF 2010 Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.60,189.19,74.31,9.62"><forename type="first">Sébastien</forename><surname>Paris</surname></persName>
							<email>sebastien.paris@lsis.org</email>
							<affiliation key="aff0">
								<orgName type="department">LSIS DYNI</orgName>
								<orgName type="institution">Univ Paul Cézanne</orgName>
								<address>
									<addrLine>av Escadrille Normandie-Niemen</addrLine>
									<postCode>13397</postCode>
									<settlement>MARSEILLE CEDEX 20</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.44,189.19,68.91,9.62"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="department">LSIS DYNI</orgName>
								<orgName type="institution">Univ du Sud-Toulon-Var</orgName>
								<address>
									<addrLine>av de l&apos;Université -BP20132</addrLine>
									<postCode>83957 GARDE</postCode>
									<settlement>LA</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.16,115.75,310.90,12.90;1,147.00,133.75,321.38,12.90;1,273.12,151.63,69.07,12.90">Linear SVM for new Pyramidal Multi-Level Visual only Concept Detection in CLEF 2010 Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">70B0F423095B341C457706CE342F72C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the Visual Concept Detection of CLEF 2010 Challenge, using only visual information, we propose a novel multi-level spatial pyramidal (sp) features : the spELBP (Extended Local Binary Pattern). In this paper we first present these features and few others that are similar : the spELBOP (Extended Local Binary Orientation Pattern), and the spHOEE (Histogram of Oriented Edge Energy). Then we discuss why our features feed state-of-the-art linear SVM algorithms for the Detection of Concept. Our scores are ranked, over the 15 participating teams, 8 th according to the F-mesure evaluation, and 9 th according to the MAP evaluation. We compare each topic score to the best system, and we finaly discuss on further extension of our approach 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The VCDT 2010 challenge (see <ref type="bibr" coords="1,273.82,446.95,29.45,9.62" target="#b9">[NH10]</ref>) consists in detection of visual concept.</p><p>Basically, this challenge can be considered as a supervised classification problem, more precisely by training models on efficient features with a "one-againstall" approach. In recent years in computer vision, in order to reduce the semantic gap in object categorization problem, two popular approaches have emerged offering efficient performances. The first one, a.k.a. "Bag of Words" (BoW) (see [ZZY + ,YYGH]), consists in building a dictionary of visual words given a large pool of feature vectors, usually some SIFT descriptors <ref type="bibr" coords="1,376.81,530.59,22.00,9.62">[Low]</ref>. SIFT descriptors can be computed over a regular spatial grid or on interest point outputs of specific detectors (corners, edges, blobs, . . . ) such Harris or Lowe detectors <ref type="bibr" coords="1,428.39,554.59,28.48,9.62" target="#b1">[HS88,</ref><ref type="bibr" coords="1,456.87,554.59,18.99,9.62" target="#b2">Low]</ref>. Following a dictionary learning step usually done by a vector quantification of all the total amount of feature vectors. The vector quantification is usually done by a K-means or GMM algorithms <ref type="bibr" coords="1,292.22,590.35,33.03,9.62">[YYGH]</ref>. More efficient dictionaries can be retrieved with sparse learning tools [WYKY + ].</p><p>The second approach is based on Local Binary Pattern (LBP) descriptors (a.k.a. CENTRIST in <ref type="bibr" coords="1,236.05,626.23,19.85,9.62">[WR]</ref>). The feature vector is defined by occurrences of each 256 patterns encoding the neighborhood relation to a central pixel.</p><p>For both approaches, adding a multi-level pyramidal architecture, permits to improve considerably the performances. This technic divides the image in sub-windows and weights adequately each corresponding feature vectors before concatenation (see <ref type="bibr" coords="2,218.51,154.51,21.96,9.62">[LSP]</ref>). The price of this kind of architecture is to deal with much larger vectors as input of classifiers. Large-scales binary supervised classification problems arise naturally with theses descriptors.</p><p>The next section describes more precisely the descriptors we developped in the challenge, especially the novel descriptor spELBOP. The fourth section overviews the large-scale binary classifier we use : the linear SVM Classifier TRON (L2 regularized with a L2 loss function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pyramidal Multi-Level Features</head><p>For each of the three following descriptors, a spatial pyramid architecture is used to divide the entire image I into N s possibly overlapping sub-windows. More precisely, a L levels pyramid is defined for l = 1, . . . , L, where image I of size N y × N y is divided into possibly overlapping sub-windows of size h l × w l . Histograms are computed for each sub-windows and weighted by</p><formula xml:id="formula_0" coords="2,143.28,328.15,337.27,30.47">c l = max j=1,...,L {hj } h l . max j=1,...,L {wj } w l</formula><p>. Finally, concatenation of the N s weighted histograms defines the global feature vector. In our implementation, h l = ⌊N y.r y,l ⌋ and w l = ⌊N x.r x,l ⌋ where r y,l and r x,l are elements of vectors r y and r x . Shifts in x-y axis are defined by integers δ y,l = ⌊N y.d y,l ⌋ and δ x,l = ⌊N x.d x,l ⌋ where d y,l and d x,l are elements of vectors d y and d x respectively. Overlapping windows can be obtained if d y,l ≤ r y,l and/or d x,l ≤ r x,l . The total number of sub-windows is equal to N s = l=1,...,L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⌊</head><p>(1r y,l ) (d y,l + 1)</p><formula xml:id="formula_1" coords="2,345.36,422.73,61.21,23.89">⌋.⌊ (1 -r x,l ) (d x,l + 1) ⌋.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The spHOEE Feature</head><p>Following <ref type="bibr" coords="2,179.05,488.11,45.46,9.62">[DT,MBM]</ref>, a histogram of the L1-normalized orientation edge energy filter responses is constructed for the N o different orientations. Theses responses are obtained by convolution of the gray image with two odd elongated oriented filters (horizontal and vertical gradients) at scale σ. L1-normalized magnitudes with a block of size h n ×w n and signed angles are computed from theses gradients.</p><p>Each N s sub-window histogram is computed efficiently thanks to the integral histogram method. The total dimension of the feature vector is d = N s.N o.</p><p>The spHOEE feature (a.k.a. spHOG in <ref type="bibr" coords="2,310.09,571.87,47.83,9.62">[MBM,MB]</ref>) offers state-of-the-art performances in databases such CALTECH 101 or INRIA pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The novel spELBP Feature</head><p>Local Binary Pattern (LBP) are powerful parametric descriptors encoding relation between intensity of a central pixel and intensities of its 8 adjacent neighbors (see [LZL + 07]). Widely used in face recognition (see <ref type="bibr" coords="2,364.45,655.75,34.74,9.62" target="#b10">[SGM09]</ref>), LBP shows also their efficiency in scene categorization ( <ref type="bibr" coords="3,307.52,118.63,20.51,9.62">[WR]</ref>) compared to BoW with SIFT descriptors. In [LZL + 07], a multi-scale extension (MSLBP) consists in encoding relation of a central block of pixels of size s × s with its 8 neighbors capturing more global details. Each block area is computed with the help of the integral image. We propose here a spatial pyramid architecture for the MSLBP so-called spELBP. This novel descriptor captures details of the scale s at given sub-windows location. Let S the number of scales, the total dimension of the spELBP descriptor is d = 256.N s.S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The spELBOP Feature</head><p>This novel descriptor is derived from the two last. Here instead of encoding the raw pixel values of a block of size s, we propose to encode the orientations of the block. As with the spHOEE features, orientations are retrieved by i) applying convolution with the two odd elongated oriented filters at scale σ and ii) computing the signed angles. The total dimension of the spELBOP descriptor is the same as the latter, i.e. d = 256.N s.S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large Scale SVM</head><p>Learning a topic (a room) with the one-against-all approach is equivalent to a binary supervised classification task. We deal with a training set D = {x i , y i }, i = 1, . . . , N where x i ⊂ R d represents a feature vector and y i ∈ {-1, 1} its corresponding label. Max-margin classifiers like SVM are known to offer state-of-theart performances. However with high dimension feature vectors and numerous examples, training SVM can be too computational expensive (∼ O(dN 3 )). For large scale problems, one alternative is to use a max-margin linear classifier which offers often the same amount of performances than the non-linear version [LWK,WYKY + ]. The linear SVM used here consists in finding the hyperplane parameter w minimizing the sum of a L2 loss function and a L2 regulation term:</p><formula xml:id="formula_2" coords="3,207.36,489.88,273.28,30.31">min w 1 2 w T w + C N i=1 max 1 -y i w T x i , 0 2 . (1)</formula><p>In <ref type="bibr" coords="3,146.28,529.51,26.22,9.62">[LWK]</ref>, the problem is solved with a Trust Region Newton algorithm (TRON). We use the modified version of TRON proposed by ( <ref type="bibr" coords="3,373.45,541.51,27.55,9.62">[MBM]</ref>) managing dense features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results at VCDT 2010</head><p>In the CLEF VCDT 2010 task, preliminary tests on the development set resulted in selecting the spELBP feature for our final system. We choose a L = 3 levels pyramid</p><formula xml:id="formula_3" coords="3,174.12,631.63,128.94,23.18">r x = r y = d x = d y = 1, 1 2 , 1<label>4</label></formula><p>T leading to N s = 42 sub-windows and a total of d = 10752 dimensions for this feature.</p><p>For each topic, hyperparameters C or λ of classifiers are tuned with a 5 cross-validation by minimizing the Balanced Error Rate (BER). Then models are learned on entire training sets.</p><p>All our run are visual only runs (we only use the image pixels to detect the topics).</p><p>Over the 15 participating teams, and for the example-based evaluation applying the F-Measure, our system has the 8th rank. For the seconde evaluation, our system has the 9th rank according to the concept-based Mean Average Precision (MAP) (this measure showed better characteristics than the EER and AUC in a recent study).  The best MAP run is also ISIS run with 0.407, while the best LSIS run MAP equals 0.234.</p><p>The figure <ref type="figure" coords="5,199.93,595.99,4.98,9.62" target="#fig_0">1</ref> gives the Average Precision (AP) for each topic for the best LSIS run. In order to compare our scores to the state of the art, we plot in figure <ref type="figure" coords="5,134.76,619.87,4.98,9.62" target="#fig_1">2</ref> the AP of the best run of the challenge versus our. We then clearly see that the scores are well correlated, and that LSIS AP are below (or for some topics equal) to the ISIS AP. This loss of precision for the LSIS run is figured for each topic in 3. We see then that the difference is about 0.15 points of AP in average. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and perspectives</head><p>In this paper, the VCDT 2010 challenge is solved by a large-scale classifier trained on multi-level descriptors and is ranked 8th for the F-mesure evaluation over the 15 teams. Training theses classifiers is extremely fast compared to the classical SVM counterpart. The Average Precision of our system is below the best run of the challenge with a nearly constant decrease of around 0.15. We could then assume that our approach could be globaly improved, as it seems to generalize well to all topics. Further improvements can be expected by adding denseSIFT descriptor with sparse learning and spatial pooling (see [WYKY + ]), and using Multiple Kernel Learning method for fusionning features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,177.36,516.66,260.53,8.66"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Average Precision of the best LSIS run for all the topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,135.24,374.34,344.79,8.66"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average Precision of the best ISIS run versus best LSIS run for all the topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.76,377.34,345.92,8.66;6,134.76,388.26,178.00,8.66"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Difference, for each topic, of the Average Precision between the best LSIS run and the best run of VCDT2010 (from ISIS).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,656.46,275.82,8.66"><p>This work has been supported by ANR-06-MDCA-002 AVEIR ANR</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,178.79,602.34,301.88,8.66;6,178.80,613.26,96.28,8.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,305.36,602.34,175.31,8.66;6,178.80,613.26,35.46,8.66">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,233.88,613.26,36.05,8.26">CVPR&apos;05</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,178.80,623.94,301.72,8.66;6,178.80,634.86,116.19,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,293.50,623.94,149.50,8.66">A combined corner and edge detector</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,462.48,623.94,18.03,8.26;6,178.80,634.86,87.94,8.26">Proc 4th Alvey Vision Conf</title>
		<meeting>4th Alvey Vision Conf</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,178.79,645.54,301.88,8.66;6,178.80,656.46,38.44,8.26" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,248.12,645.54,214.69,8.66">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,178.80,656.46,33.63,8.26">ICCV&apos;99</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,119.34,301.64,8.66;7,178.80,130.38,301.87,8.66;7,178.80,141.30,41.20,8.26" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,400.98,119.34,79.45,8.66;7,178.80,130.38,286.03,8.66">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.80,141.30,36.05,8.26">CVPR&apos;06</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,152.22,301.77,8.66;7,178.80,163.26,222.87,8.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,399.17,152.22,81.38,8.66;7,178.80,163.26,117.04,8.66">Trust region newton method for logistic regression</title>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruby</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Sathiya</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,303.96,163.26,83.60,8.26">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.68,174.18,14.75,8.66" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lzl</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.80,174.18,301.74,8.66;7,178.80,185.10,301.60,8.66" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,454.30,174.18,26.23,8.66;7,178.80,185.10,242.00,8.66">Learning multi-scale block local binary patterns for face recognition</title>
		<author>
			<persName coords=""><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ICB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,196.14,301.73,8.66;7,178.80,207.06,38.44,8.26" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,290.85,196.14,185.73,8.66">Max-margin additive classifiers for detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.80,207.06,33.63,8.26">ICCV&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,217.98,301.83,8.66;7,178.80,229.02,214.35,8.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,324.43,217.98,156.20,8.66;7,178.80,229.02,141.02,8.66">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.24,229.02,36.05,8.26">CVPR&apos;08</title>
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.80,239.94,301.83,8.66;7,178.80,250.86,301.81,8.66;7,178.80,261.90,45.27,8.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,290.25,239.94,190.38,8.66;7,178.80,250.86,185.60,8.66">New strategies for image annotation: Overview of the photo annotation task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,383.88,250.86,96.73,8.26;7,178.80,261.90,17.02,8.26">Working notes of CLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,272.82,301.63,8.66;7,178.80,283.74,301.84,8.66;7,178.80,294.78,116.92,8.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,411.93,272.82,68.49,8.66;7,178.80,283.74,268.98,8.66">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName coords=""><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,456.36,283.74,24.28,8.26;7,178.80,294.78,61.79,8.26">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.79,305.70,301.64,8.66;7,178.80,316.62,166.84,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,315.19,305.70,165.24,8.66;7,178.80,316.62,117.47,8.66">Where am i: Place instance and category recognition using spatial pact</title>
		<author>
			<persName coords=""><forename type="first">Jianixn</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,304.44,316.62,36.05,8.26">CVPR&apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,181.44,327.54,299.12,8.66;7,178.80,338.58,301.47,8.66;7,178.80,349.50,41.20,8.26" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,242.39,338.58,234.33,8.66">Locality-constrained linear coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fengjun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.80,349.50,36.05,8.26">CVPR&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.80,360.42,301.71,8.66;7,178.80,371.46,301.84,8.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,425.88,360.42,54.63,8.66;7,178.80,371.46,243.10,8.66">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,439.44,371.46,36.05,8.26">CVPR&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,178.80,382.38,301.64,8.66;7,178.80,393.30,301.86,8.66;7,178.80,404.34,58.71,8.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,312.10,393.30,152.74,8.66">Sift-bag kernel for video event analysis</title>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.80,404.34,26.13,8.26">MM&apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
