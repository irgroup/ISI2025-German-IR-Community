<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.84,115.90,315.67,12.90">RGU at ImageCLEF2010 Wikipedia Retrieval Task</title>
				<funder ref="#_QKYGECf">
					<orgName type="full">UK&apos;s Engineering and Physical Sciences Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.96,154.14,36.72,8.64"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang3@rgu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.26,154.14,46.72,8.64"><forename type="first">Dawei</forename><surname>Song</surname></persName>
							<email>d.song@rgu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.94,154.14,64.46,8.64"><forename type="first">Leszek</forename><surname>Kaliciak</surname></persName>
							<email>l.kaliciak@rgu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.84,115.90,315.67,12.90">RGU at ImageCLEF2010 Wikipedia Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6064132F32C92FFBB195FD4A86315B7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working notes paper describes our first participation in the Im-ageCLEF2010 Wikipedia Retrieval Task <ref type="bibr" coords="1,304.64,239.91,11.03,7.77" target="#b0">[1]</ref>. In this task, we mainly test our Quantum Theory inspired retrieval function on cross media retrieval. Instead of heuristically combining the ranking scores independently from different media types, we develop a tensor product based model to represent textual and visual content features of an image as a non-separable composite system. Such system incorporates the statistical/semantic dependencies between certain features. Then the ranking scores of the images are computed in a way as quantum measurement does. Meanwhile, we also test a new local feature that we have developed for content based image retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our participation to ImageCLEF we have submitted runs in the Wikipedia Retrieval Task, which are based on our Quantum Theory inspired retrieval model. This model applies tensor product to represent the textual and visual features of an image as a norder tensor in order to capture the non-separability of textual and visual features. The order of the tensor depends on the visual features that are going to be incorporated in the image retrieval.</p><p>We have tested this retrieval model on the ImageCLEF2007 data collection that has 20,000 images in total, and achieved some promising results. Therefore we would like to test it on a larger scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tensor Based Retrieval Model</head><p>Mathematically, the tensor product is used to construct a new vector space or a new tensor, where the relationship of the vector spaces can be expressed. In quantum mechanics, the tensor product can be used to expand Hilbert spaces or construct a composite system with separate systems.</p><p>Suppose an image represented in a single feature space to be a single system S i , then that image can be constructed as a composite system by tensor product the separated systems from different feature spaces:</p><formula xml:id="formula_0" coords="1,262.53,633.82,218.06,9.65">S = S 1 ⊗ S 2 • • • ⊗ S n<label>(1)</label></formula><p>Where S i is a system in a single feature space.</p><p>Next lets look at how we present a single system in the Hilbert space with Dirac notation. Here we will not introduce the detail of Dirac notation, readers who are interested can refer to Van Rijsbergen's book "The Geometry of Information Retrieval" <ref type="bibr" coords="2,462.68,143.22,11.69,8.64" target="#b1">[2]</ref>.</p><p>With quantum mechanics, traditionally vector based representation of documents are represented as superposed states. The textual feature of an image is:</p><formula xml:id="formula_1" coords="2,272.41,188.04,204.31,19.91">|T = i w ti |t i (<label>2</label></formula><formula xml:id="formula_2" coords="2,476.72,188.36,3.87,8.64">)</formula><p>where i w 2 ti = 1, and the amplitude w ti is proportional to the probability that the document is about the term t i . w ti can be set up with any term weighting scheme, and we adopted TF-IDF in our experiment.</p><p>Similarly, the histogram of content feature can also be represented as a superposed state in a content feature space:</p><formula xml:id="formula_3" coords="2,271.04,286.54,205.68,19.91">|F = i w fi |f i (<label>3</label></formula><formula xml:id="formula_4" coords="2,476.72,286.86,3.87,8.64">)</formula><p>where i w 2 fi = 1, f i is the a particular feature bin, and w fi is proportional to the number of pixels falling into the corresponding bin of the feature space.</p><p>When more than one features are used to represent an image, the representation will be:</p><formula xml:id="formula_5" coords="2,234.87,363.83,245.72,9.65">|D = |T ⊗ |F 1 ⊗ |F 2 • • • ⊗ |F n<label>(4)</label></formula><p>In our pilot study, we only combine one content feature with the textual feature.</p><formula xml:id="formula_6" coords="2,261.91,399.91,218.68,36.85">|D = |T ⊗ |F (5) = ij γ ij |t i ⊗ f j (6)</formula><p>Here, i and j are the dimensionalities of textual and content features. When textual and content feature are completely independent, then γ ij = w ti • w fj . However, this does not hold generally. Extra operation is necessary to reflect the non-separability of the two features. It can be operationalised as the co-occurrence or correlation of the features. The tensor product enables the expansion of feature spaces in a seamless way and incorporates the correlations between the feature spaces. With superposed representation, the similarity of a document and a query can be viewed as the probability that the document projects onto the sub-space that is expanded by the query features.</p><p>The probability that a document collapses to a state is:</p><formula xml:id="formula_7" coords="2,254.98,571.44,225.61,12.69">P (t i |d) = | t i |T | 2 = w 2 ti (7)</formula><p>It can be described as a projection onto a space spanned by |t i :</p><formula xml:id="formula_8" coords="2,253.85,610.57,222.87,12.69">P (t i |d) = t i |ρ d |t i = w 2 ti (<label>8</label></formula><formula xml:id="formula_9" coords="2,476.72,612.96,3.87,8.64">)</formula><p>where ρ d = |d d| is the density matrix of document d, and w 2 ti is the probability that the term t i appears in the document or the probability that the document is about the term.</p><p>With the textual and visual composite system, the density matrix of a document is:</p><formula xml:id="formula_10" coords="3,233.51,140.25,247.09,21.98">ρ D = |D D| = ij γ 2 ij |t i f j t i f j | (9)</formula><p>Then the similarity between a document and a query is:</p><formula xml:id="formula_11" coords="3,263.80,195.60,216.80,9.65">sim(d, q) = tr(ρ d ρ q ) (10)</formula><p>For a visual feature, each of its dimensions can be treated as orthogonal to other dimensions. Because the blue color pixel can never be counted as a red pixel. While for textual feature, two terms can be semantically related, e.g. cup and mug may refer to the same thing in one document. To represent the document with orthogonal textual basis, we can use a transformation matrix to fulfill the requirement:</p><formula xml:id="formula_12" coords="3,240.42,286.34,240.17,77.96">d = i w 2 i |t i t i | (11) = i w 2 i j U ij |e j k e k |U ik (12) = jk ρ jk |e j e k |<label>(13)</label></formula><p>In the current experiment, we assume that all the terms are orthogonal to simplify the calculation, then:</p><formula xml:id="formula_13" coords="3,219.11,407.55,261.48,77.73">sim(d, q) = tr( i d 2 i |t i t i |Q) (14) = tr( i d 2 i |t i t i | j q 2 j |t j t j |) (15) = i d 2 i q 2 i (<label>16</label></formula><formula xml:id="formula_14" coords="3,476.44,465.69,4.15,8.64">)</formula><p>3 Experiment Settings and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Processing</head><p>When we associated texts with images, we not only used annotation documents, but also used Wikipedia pages, hoping the original full text document can provide more semantic information.</p><p>Although in the Wikipedia task, the images may be annotated by more than one language, due to our language expertise we only parse the English language. If the images are not annotated by English, or do not appeared in English Wikipedia page, then this image will not be indexed. It also means that this image will never be retrieved in the runs using both text and content features as queries.</p><p>For the annotation files, we parsed all the terms from name, description, comment and caption entries if they contain any term. For the Wikipedia dump files, We parsed all the terms in the Wikipedia page, including the the link names within the page. However we do not go further into the linked page. The tags particular to the Wikipedia webpage are removed.</p><p>Same for the queries, we only use their English titles during the retrieval stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Content Feature</head><p>Apart from the content features provided by the organizer, we also used our local feature, which is based on the "bag of features"/"bag of visual words" approach. The feature extraction consists of following stages: image sampling, description of local patches, feature vector construction, visual dictionary generation and histogram computation. The number of sample points is 900 per image and the sampling is purely random. We open a square window -local patch (10 by 10 pixels wide) centred at the sample point. Each local patch is represented in a form of three colour moments computed for individual colour channels in HSV colour space. Thus obtained vector representation of a local patch has 9 dimensions. We apply the K-means clustering to the training set in order to obtain the codebook of visual words. Finally, we create a histogram of visual word counts by calculating manhattan distance between image patches and cluster centroids and generate a vector representation for each image from the collection. Thus obtained vector representation of an image has 40 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment runs and Results</head><p>With the textual and visual feature available, we submitted the following runs, some of which are based on content feature only and some are content and textual feature mixed.</p><p>-Text and content mixed retrieval • T+F L : retrieve on annotation first, then re-rank with our local feature • T+F C : retrieve on annotation first, then re-rank with cime • TXF L : quantum-like measurement on tensor product space of annotation vector and our local feature vector • TXF C : quantum-like measurement on tensor product space of annotation vector and feature cime vector • combine: T+F C based retrieval. When the length of result from T+F C is less than 1000, then the images from content retrieval are appended into the result list.</p><p>• W+F C : retrieve on Wikipedia file first, then re-rank with cime -Content only retrieval</p><p>• c leszek: city block distance with our new local feature • c add: city block distance with all content feature provided by ImageCLEF organiser</p><p>For the mixed retrieval, we did not run the mixed retrieval process on the whole collection due to the huge size. We ran the text retrieval first, then applied mixed retrieval to the re-ranking.</p><p>From table 1, we can see that the retrieval result based on content feature has extremely low MAP. The retrieval results from text and content mixed retrieval in Image-CLEF2010 is also considerably lower than our results from ImageCLEF2007 whose MAPs that are around 14%.</p><p>After further looking into the tensor product based experiments, we did find a bug in the code. Because a "+" operator is missing, which resulted in that only the last textual and content feature dimension had been used to re-rank the image. This accounts the poor performance of the submission runs.</p><p>We have corrected the code and re-run the experiments. The text based result is MAP=0.0939 and P@10=0.3485; The tensor product based result with our local feature is MAP=0.0665 and P@10=0.2000. There is a slightly improvement after removing the bug from the experiment, but it is still a lot of worse than text based retrieval, which needs a further investigation. In this notes paper, we reported our quantum theory inspired multimedia retrieval framework, which provides a formal and flexible way to expand the feature spaces, and seamlessly integrate different features. The similarity measurement between query and document follows quantum measurement.</p><p>We did not include the correlations between dimensions across different feature spaces in this year submission. We would like to investigate such issue in the continuing study, and further the study with entanglement concept in quantum mechanics.</p><p>In the current experiments, we assumed each word is orthogonal, which does not hold and can be relaxed in the future. We can solve this problem with either dimensionality reduction or utilize the thesaurus to remove the synonyms. This will also facilitate the ranking computation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,306.70,256.99,197.10"><head>Table 1 .</head><label>1</label><figDesc>Our runs in ImageCLEF20104 Conclusions and Future Works</figDesc><table coords="5,223.60,306.70,168.16,95.84"><row><cell>Run Modality</cell><cell>field(s) MAP P@10</cell></row><row><cell cols="2">combine Mixed TITLEIMG 0.0617 0.2271</cell></row><row><cell cols="2">T+F L Mixed TITLEIMG 0.0617 0.2257</cell></row><row><cell cols="2">TXF C Mixed TITLEIMG 0.0486 0.1443</cell></row><row><cell cols="2">TXF L Mixed TITLEIMG 0.0484 0.1500</cell></row><row><cell cols="2">T+F C Mixed TITLEIMG 0.0325 0.1143</cell></row><row><cell cols="2">W+F C Mixed TITLEIMG 0.0031 0.0086</cell></row><row><cell>c leszek Visual</cell><cell>IMG 0.0069 0.0614</cell></row><row><cell>c add Visual</cell><cell>IMG 0.0003 0.0100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is funded in part by the <rs type="funder">UK's Engineering and Physical Sciences Research Council</rs> (grant number: <rs type="grantNumber">EP/F014708/2</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QKYGECf">
					<idno type="grant-number">EP/F014708/2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,210.62,342.46,7.77;6,146.47,221.43,277.77,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,350.32,210.62,130.27,7.77;6,146.47,221.58,79.14,7.77">Overview of the wikipedia retrieval task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,242.58,221.43,104.48,7.72">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,232.39,342.47,7.93;6,146.47,243.50,43.59,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,251.87,232.39,140.70,7.72">The Geometry of Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
