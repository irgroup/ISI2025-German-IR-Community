<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.11,116.17,291.14,13.79;1,253.28,134.10,108.80,13.79">Overview of the Wikipedia Retrieval Task at ImageCLEF 2010</title>
				<funder ref="#_GKcWVv6">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Swiss National Fund</orgName>
					<orgName type="abbreviated">SNF</orgName>
				</funder>
				<funder ref="#_5JYTczx">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.08,171.64,70.80,9.88"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@telecom-bretagne.eu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institut Télécom</orgName>
								<orgName type="institution" key="instit2">Télécom Bretagne</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.21,171.64,80.26,9.88"><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
							<email>theodora.tsikrika@acm.org</email>
							<affiliation key="aff1">
								<orgName type="institution">CWI</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.81,171.64,52.99,9.88"><forename type="first">Jana</forename><surname>Kludas</surname></persName>
							<email>jana.kludas@unige.ch</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CUI</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.11,116.17,291.14,13.79;1,253.28,134.10,108.80,13.79">Overview of the Wikipedia Retrieval Task at ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">625F3AE95A340ECC63FC44A362FA0FBF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF's Wikipedia Retrieval task provides a testbed for the system-oriented evaluation of multimedia information retrieval from a collection of Wikipedia images. The aim is to investigate retrieval approaches in the context of a large and heterogeneous collection of images (similar to those encountered on the Web) that are searched for by users with diverse information needs. This paper presents an overview of the resources, topics, and assessments of the Wikipedia Retrieval task at ImageCLEF 2010, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Wikipedia Retrieval task is an ad-hoc image retrieval task. The evaluation scenario is thereby similar to the classic TREC ad-hoc retrieval task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. topics are not known to the system in advance). Given a multimedia query that consists of a title and one or more example images describing a user's multimedia information need, the aim is to find as many relevant images as possible from a Wikipedia image collection.</p><p>The Wikipedia Retrieval task differs from other benchmarks in multimedia information retrieval, like TRECVID, in the sense that the textual modality in the Wikipedia image collection contains less noise than the speech transcripts in TRECVID. Similarly to past years, participants are encouraged to develop approaches that combine the relevance of different media types into a single ranked list of results. A number of resources that support participants towards this research direction were provided this year.</p><p>The paper is organized as follows. First, we introduce the task's resources: the Wikipedia image collection and additional resources, the topics, and the assessments (Sections 2-4). Section 5 presents the approaches employed by the participating groups and Section 6 summarizes their main results. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task resources</head><p>The ImageCLEF 2010 Wikipedia collection consists of 237,434 Wikipedia images, their user-provided annotations, the Wikipedia articles that contain these images, and low-level visual features of these images. The collection was built to cover similar topics in English, German and French and it is based on the September 2009 Wikipedia dumps. Images are annotated in none, one or several languages and, wherever possible, the annotation language is given in the metadata file. The articles in which these images appear were extracted from the Wikipedia dumps and are provided as such. Image features were extracted using MM, CEA LISTs image indexing tool <ref type="bibr" coords="2,329.26,239.09,11.62,9.88" target="#b5">[6]</ref> and include both local (bags of visual words) and global features (texture, color and edges).</p><p>The main difference between the ImageCLEF 2010 Wikipedia collection and the INEX MM Wikipedia collection <ref type="bibr" coords="2,291.68,275.11,16.59,9.88" target="#b18">[19]</ref> used in the ImageCLEF WikipediaMM 2008-2009 tasks is that multilinguality has been added and both mono-and cross-lingual evaluations can be carried out. Another difference is that participants received for each image both its user-provided annotations, similarly to before, but also links to the article(s) which contain the image.</p><p>The collection contains 237,434 images and the associated annotations are distributed as follows:</p><p>-English only: 70,127 -German only: 50,291 -French only: 28,461 -English and German: 26,880 -English and French: 20,747 -German and French: 9,646 -English, German and French: 22,899 -Language undetermined: 8,144 -No textual annotation: 239</p><p>This distribution shows that the annotations in the ImageCLEF 2010 Wikipedia collection are heterogeneous, with nearly 10% of the collection with annotations in all three languages, 24% of the images with annotations in two languages out of three, 62% of the images with annotations in only one language, and the rest of the images with annotations for which language was not identified or no annotation exists. This distribution of annotations aims to encourage the investigation of multilingual approaches since they are likely to work better than monolingual approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metadata</head><p>Metadata are provided as a single metadata.zip archive which is split into 26 directories (from 1 to 26): "metadata/1" contains XML files from 0.xml to 9999.xml, "metadata/2" contains images from 10000.xml to 19999.xml, etc. Note that each directory may contain the metadata for less than 10,000 images, since some of the initial images were removed during the collection construction so as to eliminate duplicates and to ensure, to the extent possible, that the provided images are copyright free and valid. Textual annotations were extracted from the Wikimedia Commons files that describe the images and from the article(s) that contain the images. Annotations are grouped by language when it was possible to identify their language.</p><p>The main components of the .xml files (for an example see Figure <ref type="figure" coords="3,438.92,191.88,4.15,9.88">1</ref>) are:</p><p>-&lt;image&gt; -contains the unique ID of the image and a link to the image file.</p><p>-&lt;name&gt; -the name of the image as found in the Wikimedia Commons repository. No processing has been applied to this text. -&lt;text xml:lang="LANGUAGE"&gt; -where LANGUAGE if one of {en, de, fr}. These are textual annotations for which the language was identified.</p><p>• &lt;description&gt; was extracted from the Wikimedia Commons page of the image whenever the language of this text was explicitly marked in a normalized manner. • &lt;comment&gt; was extracted from the Wikimedia Commons page of the image whenever the language of this text was marked in a normalized manner. &lt;comment&gt; is a substring of the raw comment described below.</p><p>• &lt;caption&gt; is the text that accompanies the image in Wikipedia articles which are provided in the TEXT part of the collection (see Section 2.3) and are linked in the &lt;caption&gt; element. Sometimes Wikipedia articles contain images without captions. In such cases the &lt;caption&gt; element links to the article but is empty. Note that one image can appear in more than one article; in that case, all captions or links to the articles are provided.</p><p>-&lt;comment&gt; -raw annotation as found on the Wikimedia Commons page of the image. No processing was applied to this text and the comment can be in one or more languages (not necessarily one of {en, de, fr}). -&lt;license&gt; -licensing information extracted from the Wikimedia Commons page of the image.</p><p>Any or all of the textual annotations of the images can be missing and some of them can be redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Images</head><p>Fig. <ref type="figure" coords="4,152.89,300.95,3.73,9.88">1</ref>: Wikipedia image+metadata example from the ImageCLEF 2010 Wikipedia image collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text</head><p>Wikipedia article texts are provided in the "text" directory and are separated in language subdirectories ("text/en", "text/de", "text/fr"). Articles were renamed in order to have unique IDs in the collection and their IDs start at 300000 for English, 400000 for German and 500000 for French. They are grouped in 5 subdirectories for each language, with each such subdirectory containing 10,000 elements or less (for instance, "text/en/1/" includes articles from 300000 to 309999). Articles in which images appear are referenced in the &lt;caption&gt; element of the XML files contained in the "metadata" directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Additional Resources</head><p>Image features were also provided to support the participants in their investigation of multimodal approaches. Image features are provided in the "features" directory in binary files (one file per feature). Features were computed using the MM, CEA LIST's image indexing tool <ref type="bibr" coords="4,195.70,551.84,11.62,9.88" target="#b5">[6]</ref> and they include:</p><p>cime (features/cime.txt) a border/interior classification algorithm proposed by <ref type="bibr" coords="4,165.91,584.05,16.60,9.88" target="#b15">[16]</ref> which classifies pixels into interior or border and then builds a 64 bins histogram for each pixel type. The feature space is composed of 64 dimensions. -tlep (features/tlep.txt) a descriptor which combines image texture and color <ref type="bibr" coords="4,177.61,631.95,10.58,9.88" target="#b2">[3]</ref>. Two texture histograms are built for edge and non-edge pixels and a 64 bins color histogram compose the image description. The feature space is composed of 576 dimensions.</p><p>bag (features/bag.txt) a descriptor based on bags of visual words <ref type="bibr" coords="5,450.59,118.74,15.28,9.88" target="#b13">[14]</ref>. A vocabulary containing 5000 visual words was built from a random sample of the collection and all the images were then indexed with elements of this vocabulary. The feature space is composed of 5000 dimensions.</p><p>Each line in the feature files is composed of the image id (first column of each line) and the representation of the image in the feature space (from second to the last column of each line). Columns are separated by white spaces.</p><p>Along with the feature files a Perl script was provided to show how to exploit features in order to compute image similarities.</p><p>The additional resources are beneficial to researchers who wish to exploit visual evidence without performing image analysis. Of course, participants could also extract their own image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>The topics are descriptions of multimedia information needs that contain textual and visual hints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Format</head><p>These multimedia queries consist of a textual part, the query title, and a visual part, one or several example images. &lt;title&gt; query by keywords &lt;image&gt; query by image content (one or several) &lt;narrative&gt; description of query in which the definitive definition of relevance and irrelevance are given &lt;title&gt; The topic &lt;title&gt; simulates a user who does not have (or want to use) example images or other visual constraints. The query expressed in the topic &lt;title&gt; is therefore a text-only query. This profile is likely to fit most users searching digital libraries or the Internet.</p><p>Upon discovering that a text-only query does not produce many relevant hits, a user might decide to add visual hints and formulate a multimedia query.</p><p>&lt;image&gt; The visual hints are example images, which express the narrative of the topic.</p><p>&lt;narrative&gt; A clear and precise description of the information need is required in order to unambiguously determine whether or not a given document fulfils the given information need. In a test collection this description is known as the narrative. It is the only true and accurate interpretation of a user's needs. Precise recording of the narrative is important for scientific repeatability -there must exist, somewhere, a definitive description of what is and is not relevant to the user.</p><p>Textual terms and visual examples can be used in any combination in order to produce results. It is up to the systems how to use, combine or ignore this information; the relevance of a result does not directly depend on these constraints, but it is decided by manual assessments based on the &lt;narrative&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Development</head><p>The topics in the ImageCLEF 2010 Wikipedia Retrieval task (see Table <ref type="table" coords="6,450.41,204.41,3.60,9.88" target="#tab_0">1</ref>), created by the organizers of the task, aim to cover diverse information needs and to have a variable degree of difficulty. They were chosen from an initial pool of 137 candidate topics that were derived from a search log file and from the topics of the 2008 and 2009 WikipediaMM tasks. Candidate topics were run through the Cross Modal Search Engine<ref type="foot" coords="6,310.86,262.69,3.49,6.92" target="#foot_0">4</ref> (CMSE -developed by the University of Geneva) in order to get an indication of the number of relevant images in top results for baseline image only, text only and multimodal approaches. The final pool contains around 1/3 of topics that return good results for each type of retrieval. The topics range from simple, and thus relatively easy (e.g., "postage stamp"), to semantic, and hence highly difficult (e.g., "white house with garden"), with the latter forming the bulk of the topics. Semantic topics typically have a complex set of constraints, need world knowledge, and/or contain ambiguous terms, so they are expected to be challenging for current state-of-the-art retrieval algorithms. We encouraged the participants to use multimodal approaches since they are more appropriate for dealing with semantic information needs.</p><p>Image examples were selected from Flickr, after checking that they are uploaded under the Creative Commons license. Each topic has one or several image examples, chosen so as to illustrate the visual diversity of the topic. Query image examples and their low-level features are also associated to the collection in order to ensure repeatability of the experiments. On average, the 70 topics contain 1.68 images and 2.7 words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head><p>The Wikipedia Retrieval task is an image retrieval task, where an image with its metadata is either relevant or not (binary relevance). We adopted TREC-style pooling of the retrieved images with a pool depth of 100, resulting in pool sizes of between 1421 and 3850 images with a mean of 2659 and median of 2531. The evaluation was performed by three participant groups and by the organizers within a period of 4 weeks after the submission of runs. The assessors used a modified version of the web-based interface that was used last year and which has also been previously employed in the INEX Multimedia and TREC Enterprise tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>A total of 13 groups submitted 127 runs. The participation was significantly higher than last year both in terms of number of participants (13 vs. 8) and of submitted runs (127 vs. 57). Although the highest number of groups are located in European countries, the geographic spread of participants has increased this year, with North-American and Asian groups being better represented. Table <ref type="table" coords="9,175.22,375.33,4.98,9.88" target="#tab_3">2</ref> gives an overview of the types of the submitted runs. This year more multimodal (text/visual) than text-only runs were submitted. Table <ref type="table" coords="9,436.06,387.28,4.98,9.88" target="#tab_4">3</ref> presents the combinations of annotation and query languages used by participants in their textual and multimodal runs. A majority of submitted runs are multilingual in at least one of the two aspects. Many teams used both multilingual queries and multilingual annotations in order to maximize retrieval performance and the best results presented in the next section (see Tables <ref type="table" coords="9,443.45,447.06,4.98,9.88" target="#tab_5">4</ref> and<ref type="table" coords="9,472.29,447.06,4.15,9.88" target="#tab_6">5</ref>) validate this approach. Although runs that implicate English only queries are by far more frequent than runs implicating German and French only, some participants also submitted the latter type of runs. A short description of the participants' approaches follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHESHIRE (8 runs) [8]</head><p>Their focus was on textual retrieval and they proposed runs for English, French and German queries. The retrieval model used is logistic regression, complemented with blind relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAEDALUS (6 runs) [7] They proposed only textual runs and experimented</head><p>with corpus and topic expansion using several collection metadata, but also information about named entities and concepts included in DBPedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCU (3 runs) [9]</head><p>Their approach was based on document expansion with Wikipedia content and using the Okapi feedback algorithm. In addition, document reduction was exploited to weight the terms in the query. Okapi BM25 was used for the retrieval phase and only English queries were examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTH (20 runs) [1]</head><p>They experimented with the usage of all different modalities (textual descriptions in each language, image features) and discussed two types of fusion methods: score normalization and score combination.</p><p>They concluded that the text modality is by far more important than the image modality since the latter results only in little improvement when introduced into the retrieval framework. I2RCVIU (6 runs) <ref type="bibr" coords="10,216.95,154.33,16.60,9.57" target="#b17">[18]</ref> They presented results for mono-and multilingual textual runs as well as for multimodal runs. One interesting result they report is the use of visual near duplicates in order to boost images that are very similar to top results from textual runs. NUS (14 runs) They submitted both text and multimodal runs. For textual runs, they mapped image metadata to Wikipedia concepts, which were subsequently used during retrieval. For mixed runs, they also identify concepts from images. RGU (8 runs) <ref type="bibr" coords="10,197.57,249.16,16.60,9.57" target="#b10">[11]</ref> They extended their quantum theory approach first presented at ImageCLEF 2007. A tensor product model is developed to represent textual and visual features in a non-separable composite system. They also introduced a new "bag of visual words" inspired image features. SZTAKI (5 runs) <ref type="bibr" coords="10,212.17,296.57,11.62,9.57" target="#b4">[5]</ref> Their approach used Okapi BM25 text retrieval and Histogram of Oriented Gradients features clustered with Gaussian Mixture Models for image description. Query expansion with visual information was performed over textual results and this resulted in a slight improvement of the final results. TELECOM (16 runs) <ref type="bibr" coords="10,229.67,355.94,16.59,9.57" target="#b11">[12]</ref> Their approach is mainly based on query expansion with Wikipedia. Given a topic, related concepts are retrieved from Wikipedia and used to expand the initial query. Then results are re-ranked using query models extracted from Flickr. UAIC (2 runs) From the image's textual metadata, they generated image keywords which were filtered using a comparison to visually similar images. During retrieval, the topics were matched against the previously generated keywords. Visual similarity was equally used in order to rank retrieved images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED (20 runs) [2]</head><p>They implemented a variant of VSM approach with TF-IDF weights and used it for their best textual run. For the multimodal runs, they used late fusion with three different algorithms: automatic, query expansion and relevance feedback based on logistic regression. UNT (3 runs) <ref type="bibr" coords="10,197.02,510.14,16.60,9.57" target="#b12">[13]</ref> Their main goal was to explore the use of cross-lingual information retrieval. Instead of using a standard automatic query technique, they translated textual metadata to the language of the query and then performed mono-lingual retrieval. They also used manual query expansion in order to add possibly useful words to the query. This interactive retrieval technique improves the precision of top results, but does not improve the overall performance of the system. XRCE (16 runs) <ref type="bibr" coords="10,206.43,593.41,11.62,9.57" target="#b3">[4]</ref> They represented textual metadata using standard language models or a power law. Image content was described using Fisher Vectors improved with power and L2 normalization and spatial pyramid representations. They showed that, although text retrieval largely outperforms pure visual retrieval, an appropriate combination of the two modalities results in a significant improvement over each modality considered independently.</p><p>Tables <ref type="table" coords="11,165.78,149.48,4.98,9.88" target="#tab_5">4</ref> and<ref type="table" coords="11,194.51,149.48,4.98,9.88" target="#tab_6">5</ref> present the evaluation results for the 15 best performing runs and the best performing run for each team, respectively, ranked by Mean Average Precision (MAP). Compared to 2009, when the best submitted runs were textual, this year multimodal runs submitted by XRCE were ranked best with a MAP of 0.2765. The best textual run, also submitted by XRCE was ranked 12th and had a MAP of 0.2361. The results in Table <ref type="table" coords="11,335.52,209.25,4.98,9.88" target="#tab_6">5</ref> show that results for individual teams are more nuanced, with five teams having multimodal runs as their best submission. The complete list of results can be found at the ImageCLEF website<ref type="foot" coords="11,447.90,629.23,3.49,6.92" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance per modality for all topics</head><p>Here, we analyze the evaluation results using only the top 90% of the runs to exclude noisy and buggy results. Because there was only one visual only run among the top 90%, it was also discarded. Table <ref type="table" coords="12,355.23,164.73,4.98,9.88" target="#tab_7">6</ref> shows the average performance and standard deviation with respect to each modality. On average, the textual runs have a slightly better performance than multimodal ones with respect to all examined evaluation metrics (MAP, Precision at 20, and precision after R (= number of relevant) documents retrieved). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance per topic and per modality</head><p>To analyze the average difficulty of the topics, we classify the topics based on the AP values per topic averaged over all runs as follows:</p><p>easy: M AP &gt; 0.3 medium: 0.2 &lt; M AP &lt;= 0.3 hard: 0.1 &lt; M AP &lt;= 0.2 very hard: M AP &lt; 0.1.</p><p>Table <ref type="table" coords="12,175.95,488.49,4.98,9.88" target="#tab_8">7</ref> presents the up to 10 topics per class (i.e., easy, medium, hard, and very hard), together with the total number of topics per class. Out of 70 topics, 53 fall in the hard or very hard classes. This was actually intended during the topic development process, because we opted for highly semantic topics that are challenging for current retrieval approaches. 21 topics were very hard to solve, with four of them ("woman in red dress", "building site", "horseman", "people laughing") having a M AP &lt; 0.05 and being considered as unsolvable. The topic pool includes only four easy topics ("satellite image", "portrait of Jintao Hu", "ferrari red", "postage stamp"). A large number of the topics included in the easy and medium classes include a reference to a named entity ("Jintao Hu", "Ferrari", "ISS" or "Shiva") and, consequently, are easily retrieved with simple textual approaches. As for very hard topics, they often contain general terms ("woman", "people", "house" or "airplane"), which have a difficult semantic interpretation or high concept variation and are, hence, very hard to solve. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visuality of topics</head><p>We also analyzed the performance of runs that use only text (TXT) versus runs that use both text and visual resources (TXTIMG). Figure <ref type="figure" coords="13,390.69,356.72,4.98,9.88">2</ref> shows the average performance on each topic for all, text-only and text-visual runs. The text-based runs outperform the text-visual ones in 37 out of the 70, are outperformed by mixed runs in 31 cases and have the same performances in 2 cases. This indicates that less than half of the topics benefit from a multi modal approach.</p><p>The "visuality" of topics can be deduced from the performance of text-only and text-visual approaches that were presented in the last section. We consider that, if for a topic the text-visual approaches improve significantly the MAP over all runs (i.e., by diff(M AP ) &gt;= 0.01), then we could consider that to be a visual topic. In the same way, we can define topics as textual, if the text-only approaches improve significantly the MAP over all runs of a topic. Based on this analysis, 26 of the topics can be characterized as textual and 24 as visual. The remaining 20 topics, where no clear improvements are observed, are considered to be neutral.</p><p>Table <ref type="table" coords="13,176.25,524.35,4.98,9.88" target="#tab_9">8</ref> presents the topics in each group, as well as some statistics on the topic, their relevant documents, and their distribution over the classes that indicate their difficulty. There are small differences between the average number of words and example images for textual, neutral and visual topics. An important difference is observed for the number of relevant documents/topic, with a significantly higher number of such documents for visual topics compared to textual topics. This distribution of the number of relevant images indicates that a larger number of positive examples per query are needed for the visual features to be effective. Interestingly, the average mean average precision is distributed inversely, with a significantly higher average score for textual queries compared to visual ones (0.219 vs. 0.131). The distribution of the textual, visual and neutral topics over the classes expressing their difficulty shows that the vi-Fig. <ref type="figure" coords="14,174.11,362.05,3.73,9.88">2</ref>: Average topic performance over all, text-only, and mixed runs.</p><p>sual and neutral topics are more likely to fall into the hard/very hard class than the textual ones.</p><p>A closer look at the topics themselves indicates that textual ones often include a named entity ("ISS", "Ferrari", "Christmas", "Jintao Hu" etc.). Visual cues, which could be useful for topics that have a well defined semantic interpretation or a coherent visual aspect, do not help in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of Query Expansion and Relevance Feedback</head><p>Finally, we analyze the effect of the application of query expansion (QE) and relevance feedback (FB) techniques. Similarly to the analysis in the previous section, we consider the techniques to be useful for a topic, if they improved significantly the MAP all runs. Table <ref type="table" coords="14,318.83,541.67,4.98,9.88" target="#tab_10">9</ref> presents the best performing topics for these techniques and some statistics. Query expansion is useful for 34 topics and relevance feedback for 15. As with visual topics, query expansion seems to be useful for queries which have a lot of associated relevant documents and for queries that are either hard or very hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>For the first time this year, a multimodal approach performed best in the Wikipedia Retrieval task. It is encouraging to see more than half of the submitted runs were multimodal. This is possibly a consequence of the fact that visual descriptors were provided with the collection. A novelty this year was that participants were able to submit multilingual runs. Although a majority of runs focused either on a combination of topic languages or on English queries only, several groups submitted runs for German and French queries only. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,136.57,116.28,345.64,169.71"><head></head><label></label><figDesc></figDesc><graphic coords="4,136.57,116.28,345.64,169.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="14,136.57,116.25,345.80,230.85"><head></head><label></label><figDesc></figDesc><graphic coords="14,136.57,116.25,345.80,230.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,133.52,490.57,345.82,152.54"><head>Table 1 :</head><label>1</label><figDesc>Topics for the ImageCLEF 2010 Wikipedia Retrieval task: IDs, titles, the number of image examples providing additional visual information, and the number of relevant images in the collection.</figDesc><table coords="6,148.70,536.93,317.95,106.18"><row><cell>ID</cell><cell>Topic title</cell><cell cols="2"># image examples # relevant images</cell></row><row><cell>1 fractals</cell><cell></cell><cell>2</cell><cell>317</cell></row><row><cell cols="2">2 cockpit of an airplane</cell><cell>1</cell><cell>87</cell></row><row><cell cols="2">3 basketball game close up</cell><cell>2</cell><cell>116</cell></row><row><cell cols="2">4 Christmas tree</cell><cell>2</cell><cell>22</cell></row><row><cell cols="2">5 Oktoberfest beer tent</cell><cell>2</cell><cell>9</cell></row><row><cell cols="2">6 solar panels</cell><cell>2</cell><cell>101</cell></row><row><cell cols="2">7 lightning in the sky</cell><cell>1</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,148.70,117.28,317.95,536.97"><head>Table 1 -continued from previous page</head><label>1</label><figDesc></figDesc><table coords="7,148.70,129.63,317.95,524.61"><row><cell>ID</cell><cell>Topic title</cell><cell cols="2"># image examples # relevant images</cell></row><row><cell cols="2">8 tennis player on court</cell><cell>2</cell><cell>393</cell></row><row><cell cols="2">9 flying hot air balloon</cell><cell>2</cell><cell>30</cell></row><row><cell cols="2">10 horseman</cell><cell>2</cell><cell>96</cell></row><row><cell cols="2">11 landline telephone</cell><cell>1</cell><cell>27</cell></row><row><cell cols="2">12 DNA helix</cell><cell>1</cell><cell>39</cell></row><row><cell cols="2">13 trains and locomotives</cell><cell>2</cell><cell>687</cell></row><row><cell cols="2">14 videogames screenshot</cell><cell>2</cell><cell>114</cell></row><row><cell>15 cyclist</cell><cell></cell><cell>2</cell><cell>176</cell></row><row><cell cols="2">16 spider with cobweb</cell><cell>2</cell><cell>27</cell></row><row><cell cols="2">17 beach volleyball</cell><cell>2</cell><cell>7</cell></row><row><cell cols="2">18 stars and galaxies</cell><cell>2</cell><cell>384</cell></row><row><cell cols="2">19 lochs in Scotland</cell><cell>1</cell><cell>53</cell></row><row><cell cols="2">20 mountains with sky</cell><cell>2</cell><cell>969</cell></row><row><cell cols="2">21 Chernobyl disaster ruins</cell><cell>2</cell><cell>17</cell></row><row><cell cols="2">22 sharks underwater</cell><cell>2</cell><cell>27</cell></row><row><cell cols="2">23 emoticon smiley</cell><cell>2</cell><cell>8</cell></row><row><cell cols="3">24 Rorschach black and white 1</cell><cell>6</cell></row><row><cell cols="3">25 Shiva painting or sculpture 2</cell><cell>29</cell></row><row><cell cols="2">26 brain scan</cell><cell>2</cell><cell>24</cell></row><row><cell cols="2">27 active volcano with ash</cell><cell>1</cell><cell>75</cell></row><row><cell>cloud</cell><cell></cell><cell></cell></row><row><cell cols="2">28 palm trees</cell><cell>2</cell><cell>71</cell></row><row><cell cols="2">29 desert scenery</cell><cell>2</cell><cell>247</cell></row><row><cell>30 harbour</cell><cell></cell><cell>2</cell><cell>454</cell></row><row><cell cols="2">31 yellow buses</cell><cell>1</cell><cell>50</cell></row><row><cell cols="2">32 people laughing</cell><cell>2</cell><cell>51</cell></row><row><cell cols="2">33 close up of antenna</cell><cell>2</cell><cell>90</cell></row><row><cell cols="2">34 people playing guitar</cell><cell>2</cell><cell>348</cell></row><row><cell>35 race car</cell><cell></cell><cell>2</cell><cell>852</cell></row><row><cell cols="2">36 portrait of Jintao Hu</cell><cell>1</cell><cell>5</cell></row><row><cell cols="2">37 close up of bottles</cell><cell>1</cell><cell>237</cell></row><row><cell cols="2">38 baseball game</cell><cell>1</cell><cell>140</cell></row><row><cell cols="2">39 cactus in desert</cell><cell>1</cell><cell>13</cell></row><row><cell cols="2">40 ferrari red</cell><cell>1</cell><cell>185</cell></row><row><cell cols="2">41 polar bear</cell><cell>2</cell><cell>46</cell></row><row><cell cols="3">42 Paintings related to cubism 2</cell><cell>23</cell></row><row><cell cols="2">43 skyscraper in daylight</cell><cell>2</cell><cell>362</cell></row><row><cell>44 saturn</cell><cell></cell><cell>2</cell><cell>81</cell></row><row><cell cols="2">45 snowy winter landscape</cell><cell>2</cell><cell>376</cell></row><row><cell>46 sailboat</cell><cell></cell><cell>1</cell><cell>181</cell></row><row><cell cols="2">47 soccer stadium</cell><cell>1</cell><cell>366</cell></row><row><cell cols="2">48 civil airplane</cell><cell>1</cell><cell>633</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,148.70,117.28,315.00,297.46"><head>Table 1 -continued from previous page</head><label>1</label><figDesc></figDesc><table coords="8,148.70,129.63,315.00,285.11"><row><cell>ID</cell><cell>Topic title</cell><cell cols="2"># image examples # relevant images</cell></row><row><cell cols="2">49 surfing on waves</cell><cell>1</cell><cell>38</cell></row><row><cell cols="2">50 portraits of people</cell><cell>2</cell><cell>1727</cell></row><row><cell cols="3">51 aerial pictures of landscapes 3</cell><cell>678</cell></row><row><cell cols="2">52 satellite image</cell><cell>2</cell><cell>875</cell></row><row><cell cols="2">53 ISS international space sta-</cell><cell>1</cell><cell>178</cell></row><row><cell>tion</cell><cell></cell><cell></cell></row><row><cell cols="2">54 launching space shuttle</cell><cell>1</cell><cell>102</cell></row><row><cell cols="2">55 building site</cell><cell>1</cell><cell>125</cell></row><row><cell cols="2">56 musician on stage</cell><cell>1</cell><cell>568</cell></row><row><cell cols="2">57 road street signs</cell><cell>2</cell><cell>305</cell></row><row><cell cols="2">58 red fruits</cell><cell>2</cell><cell>146</cell></row><row><cell cols="2">59 cities at night</cell><cell>3</cell><cell>528</cell></row><row><cell cols="2">60 notes on music sheet</cell><cell>1</cell><cell>233</cell></row><row><cell cols="2">61 earth from space</cell><cell>2</cell><cell>89</cell></row><row><cell cols="2">62 Shopping in a market</cell><cell>2</cell><cell>224</cell></row><row><cell cols="2">63 postage stamp</cell><cell>3</cell><cell>866</cell></row><row><cell cols="2">64 woman in red dress</cell><cell>2</cell><cell>57</cell></row><row><cell cols="2">65 sea sunset or sunrise</cell><cell>1</cell><cell>116</cell></row><row><cell cols="2">66 bridges in daylight</cell><cell>2</cell><cell>793</cell></row><row><cell cols="2">67 white house with garden</cell><cell>1</cell><cell>77</cell></row><row><cell cols="2">68 historic castle</cell><cell>2</cell><cell>605</cell></row><row><cell cols="2">69 red tomato</cell><cell>1</cell><cell>33</cell></row><row><cell cols="2">70 close up of trees</cell><cell>2</cell><cell>603</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,218.34,127.10,178.66,98.77"><head>Table 2 :</head><label>2</label><figDesc>Types of the 127 submitted runs.</figDesc><table coords="9,244.14,139.60,127.07,86.28"><row><cell>Run type</cell><cell># runs</cell></row><row><cell>Text (TXT)</cell><cell>48</cell></row><row><cell>Visual (IMG)</cell><cell>7</cell></row><row><cell>Text/Visual (TXTIMG)</cell><cell>72</cell></row><row><cell>Query Expansion (QE)</cell><cell>18</cell></row><row><cell>Relevance Feedback (RF)</cell><cell>14</cell></row><row><cell>Pseudo RF</cell><cell>9</cell></row><row><cell>QE &amp; RF</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,249.64,345.81,100.16"><head>Table 3 :</head><label>3</label><figDesc>Annotation and query language combinations in the textual and multimodal runs.</figDesc><table coords="9,217.83,274.08,179.70,75.72"><row><cell></cell><cell cols="2">Annotation language</cell><cell></cell></row><row><cell cols="3">Query Language EN DE FR EN+DE+FR</cell><cell></cell></row><row><cell>EN</cell><cell>32 0 0</cell><cell>13</cell><cell>45</cell></row><row><cell>DE</cell><cell>0 4 0</cell><cell>0</cell><cell>4</cell></row><row><cell>FR</cell><cell>0 0 5</cell><cell>1</cell><cell>6</cell></row><row><cell>EN+DE+FR</cell><cell>1 0 0</cell><cell>64</cell><cell>65</cell></row><row><cell></cell><cell>33 4 5</cell><cell>78</cell><cell>120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,136.16,277.07,345.83,138.55"><head>Table 4 :</head><label>4</label><figDesc>Results for the top 15 runs.</figDesc><table coords="11,136.16,288.75,345.83,126.86"><row><cell cols="3">Rank Participant Run</cell><cell cols="3">Modality FB/QE AL</cell><cell>TL</cell><cell>MAP</cell><cell>P@10 P@20 R-prec.</cell></row><row><cell>1</cell><cell>xrce</cell><cell>XAFSQTAMP</cell><cell>Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2765 0.5814 0.5193 0.3465</cell></row><row><cell>2</cell><cell>xrce</cell><cell cols="2">XACFSRTAMP Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2681 0.5686 0.5257 0.3413</cell></row><row><cell>3</cell><cell>xrce</cell><cell cols="2">XAFSRTAMPRT Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2627 0.6114 0.5407 0.3289</cell></row><row><cell>4</cell><cell>xrce</cell><cell cols="2">XAFSRPTPAMP Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2591 0.5829 0.5143 0.3316</cell></row><row><cell>5</cell><cell>xrce</cell><cell cols="2">XACFSRTAMP2 Mixed</cell><cell cols="4">NOFB EN+FR+DE EN+FR+DE 0.2575 0.5957 0.5164 0.3257</cell></row><row><cell>6</cell><cell>xrce</cell><cell cols="2">XACFSRTAMP3 Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2532 0.5429 0.4986 0.3300</cell></row><row><cell>7</cell><cell>xrce</cell><cell cols="2">XAFSRTAMPRT2 Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2527 0.5971 0.5336 0.3200</cell></row><row><cell>8</cell><cell>xrce</cell><cell cols="2">XAFSRPTPAMP2 Mixed</cell><cell cols="4">NOFB EN+FR+DE EN+FR+DE 0.2495 0.5729 0.5157 0.3189</cell></row><row><cell>9</cell><cell>xrce</cell><cell cols="2">XAFSRTAMPRT3 Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2493 0.5171 0.4743 0.3233</cell></row><row><cell>10</cell><cell>xrce</cell><cell cols="2">XACFSRTAMP2 Mixed</cell><cell cols="4">NOFB EN+FR+DE EN+FR+DE 0.2424 0.5543 0.4907 0.3183</cell></row><row><cell>11</cell><cell>xrce</cell><cell cols="2">XAFSRTAMPRT4 Mixed</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2415 0.5414 0.4664 0.3165</cell></row><row><cell>12</cell><cell>xrce</cell><cell>ADDF</cell><cell>TXT</cell><cell>FB</cell><cell cols="3">EN+FR+DE EN+FR+DE 0.2361 0.4871 0.4393 0.3077</cell></row><row><cell>13</cell><cell>unt</cell><cell>untaTxEn</cell><cell>TXT</cell><cell cols="2">NOFB EN</cell><cell>EN</cell><cell>0.2251 0.4314 0.3871 0.3025</cell></row><row><cell>14</cell><cell>telecom</cell><cell>tefwm</cell><cell>TXT</cell><cell>QE</cell><cell cols="2">EN+FR+DE EN</cell><cell>0.2227 0.4829 0.4407 0.2953</cell></row><row><cell>15</cell><cell>unt</cell><cell>untaTxFr</cell><cell>TXT</cell><cell cols="2">NOFB FR</cell><cell>FR</cell><cell>0.2200 0.4229 0.3986 0.2855</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,136.16,477.15,347.85,122.61"><head>Table 5 :</head><label>5</label><figDesc>Results for the top 15 runs.</figDesc><table coords="11,136.16,488.84,347.85,110.92"><row><cell cols="3">Rank Participant Run</cell><cell cols="3">Modality FB/QE AL</cell><cell>TL</cell><cell>MAP</cell><cell>P@10 P@20 R-prec.</cell></row><row><cell>1</cell><cell>xrce</cell><cell>XAFSQTAMP</cell><cell cols="2">MIXED FB</cell><cell cols="4">EN+FR+DE EN+FR+DE 0.2765 0.5814 0.5193 0.3465</cell></row><row><cell>13</cell><cell>unt</cell><cell>untaTxEn</cell><cell>TXT</cell><cell cols="2">NOFB EN</cell><cell>EN</cell><cell cols="2">0.2251 0.4314 0.3871 0.3025</cell></row><row><cell>14</cell><cell>telecom</cell><cell>tefwm</cell><cell>TXT</cell><cell>QE</cell><cell cols="2">EN+FR+DE EN</cell><cell cols="2">0.2227 0.4829 0.4407 0.2953</cell></row><row><cell>16</cell><cell>i2rcviu</cell><cell cols="2">MONOLINGUAL TXT</cell><cell cols="2">NOFB EN</cell><cell cols="3">EN+FR+DE 0.2126 0.4486 0.4143 0.2832</cell></row><row><cell>20</cell><cell>dcu</cell><cell>dcuRunOkapi</cell><cell>TXT</cell><cell>QE</cell><cell>EN</cell><cell>EN</cell><cell cols="2">0.2039 0.4271 0.3907 0.2832</cell></row><row><cell>24</cell><cell>cheshire</cell><cell>BTEA</cell><cell>TXT</cell><cell>FB</cell><cell cols="4">EN+FR+DE EN+FR+DE 0.2014 0.4600 0.4036 0.2739</cell></row><row><cell>26</cell><cell>duth</cell><cell>D20MM</cell><cell cols="6">MIXED NOFB EN+FR+DE EN+FR+DE 0.1998 0.5200 0.4836 0.2820</cell></row><row><cell>34</cell><cell>uned</cell><cell>UUEYANTT</cell><cell>TXT</cell><cell cols="5">NOFB EN+FR+DE EN+FR+DE 0.1927 0.3914 0.3564 0.2663</cell></row><row><cell>48</cell><cell cols="2">daedalus DWCT</cell><cell>TXT</cell><cell>QE</cell><cell cols="4">EN+FR+DE EN+FR+DE 0.1820 0.4471 0.4029 0.2662</cell></row><row><cell>50</cell><cell>sztaki</cell><cell>bat</cell><cell cols="2">MIXED QE</cell><cell>EN</cell><cell>EN</cell><cell cols="2">0.1794 0.4857 0.4329 0.2318</cell></row><row><cell>66</cell><cell>nus</cell><cell>nustextonly</cell><cell>TXT</cell><cell cols="2">NOFB EN</cell><cell>EN</cell><cell cols="2">0.1581 0.3529 0.3264 0.2386</cell></row><row><cell cols="2">100 rgu</cell><cell>combine</cell><cell cols="3">MIXED NOFB EN</cell><cell>EN</cell><cell cols="2">0.0617 0.2271 0.2129 0.1221</cell></row><row><cell cols="2">110 uaic</cell><cell>dfiush</cell><cell cols="2">MIXED QE</cell><cell cols="4">EN+FR+DE EN+FR+DE 0.0423 0.1543 0.1529 0.0744</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,155.63,256.56,304.09,65.90"><head>Table 6 :</head><label>6</label><figDesc>Results per modality over all topics.</figDesc><table coords="12,155.63,269.05,304.09,53.41"><row><cell>Modality</cell><cell>MAP Mean SD Mean SD Mean SD P@20 R-prec.</cell></row><row><cell>All top 90% runs (112 runs)</cell><cell>0.1543 0.0641 0.3541 0.1 0.2213 0.0756</cell></row><row><cell cols="2">TXTIMG in top 90% runs (67 runs) 0.1504 0.0719 0.3519 0.1156 0.2148 0.0843</cell></row><row><cell>TXT in top 90% runs (45 runs)</cell><cell>0.1602 0.0505 0.3575 0.0728 0.2312 0.0598</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,134.77,127.10,345.82,170.43"><head>Table 7 :</head><label>7</label><figDesc>Topics classified based on their difficulty -the total number of topics per class is given in the table header. Up to 10 topics, the hardest ones in each class, are shown in the table.</figDesc><table coords="13,136.16,162.80,343.05,134.74"><row><cell>easy (4 topics)</cell><cell>medium (13 topics)</cell><cell>hard (32 topics)</cell><cell>very hard (21 top-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ics)</cell></row><row><cell>52 satellite image</cell><cell>23 emoticon smiley</cell><cell>69 red tomato</cell><cell>64 woman in red</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dress</cell></row><row><cell cols="2">36 portrait of Jintao Hu 9 flying hot air balloon</cell><cell>45 snowy winter landscape</cell><cell>55 building site</cell></row><row><cell>40 ferrari red</cell><cell>49 surfing on waves</cell><cell>22 sharks underwater</cell><cell>10 horseman</cell></row><row><cell>63 postage stamp</cell><cell>4 Christmas tree</cell><cell cols="2">42 Paintings related to cubism 32 people laughing</cell></row><row><cell></cell><cell>12 DNA helix</cell><cell>11 landline telephone</cell><cell>67 white house with</cell></row><row><cell></cell><cell></cell><cell></cell><cell>garden</cell></row><row><cell></cell><cell>18 stars and galaxies</cell><cell cols="2">27 active volcano with ash cloud 48 civil airplane</cell></row><row><cell></cell><cell cols="2">53 ISS international space station 68 historic castle</cell><cell>16 spider with cob-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>web</cell></row><row><cell></cell><cell>25 Shiva painting or sculpture</cell><cell>29 desert scenery</cell><cell>31 yellow buses</cell></row><row><cell></cell><cell>41 polar bear</cell><cell>65 sea sunset or sunrise</cell><cell>60 notes on music</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sheet</cell></row><row><cell></cell><cell>24 Rorschach black and white</cell><cell>14 videogames screenshot</cell><cell>19 lochs in Scott-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>land</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="15,134.77,127.10,345.83,223.83"><head>Table 8 :</head><label>8</label><figDesc>Best performing topics for textual and text-visual runs relative to the average over all runs.</figDesc><table coords="15,136.16,150.84,325.75,200.10"><row><cell></cell><cell>textual (21 topics)</cell><cell>visual (36 topics)</cell><cell>neutral (13 topics)</cell></row><row><cell>Topics</cell><cell>9 flying hot air balloon</cell><cell>1 fractals</cell><cell>7 lightning in the sky</cell></row><row><cell></cell><cell>69 red tomato</cell><cell>10 horseman</cell><cell>62 Shopping in a market</cell></row><row><cell></cell><cell cols="2">53 ISS international space station 13 trains and locomotives</cell><cell>61 earth from space</cell></row><row><cell></cell><cell>52 satellite image</cell><cell>15 cyclist</cell><cell>60 notes on music sheet</cell></row><row><cell></cell><cell cols="2">51 aerial pictures of landscapes 16 spider with cobweb</cell><cell>59 cities at night</cell></row><row><cell></cell><cell>5 Oktoberfest beer tent</cell><cell>17 beach volleyball</cell><cell>48 civil airplane</cell></row><row><cell></cell><cell>49 surfing on waves</cell><cell>18 stars and galaxies</cell><cell>47 soccer stadium</cell></row><row><cell></cell><cell>44 saturn</cell><cell>19 lochs in Scottland</cell><cell>43 skyscraper in daylight</cell></row><row><cell></cell><cell>41 polar bear</cell><cell>22 sharks underwater</cell><cell>35 race car</cell></row><row><cell></cell><cell>40 ferrari red</cell><cell>26 brain scan</cell><cell>30 harbour</cell></row><row><cell></cell><cell>4 Christmas tree</cell><cell>27 active volcano with ash</cell><cell>29 desert scenery</cell></row><row><cell></cell><cell></cell><cell>cloud</cell><cell></cell></row><row><cell></cell><cell>37 close up of bottles</cell><cell>28 palm trees</cell><cell>20 mountains with sky</cell></row><row><cell></cell><cell>36 portrait of Jintao Hu</cell><cell>3 basketball game close up</cell><cell>2 cockpit of an airplane</cell></row><row><cell></cell><cell>33 close up of antenna</cell><cell>31 yellow buses</cell><cell></cell></row><row><cell></cell><cell>25 Shiva painting or sculpture</cell><cell>32 people laughing</cell><cell></cell></row><row><cell cols="2">#images/topic 1.62</cell><cell>1.72</cell><cell>1.69</cell></row><row><cell cols="2">#words/topic 2.86</cell><cell>2.69</cell><cell>2.84</cell></row><row><cell>#reldocs</cell><cell>130.8</cell><cell>272.9</cell><cell>391.3</cell></row><row><cell>MAP</cell><cell>0.219</cell><cell>0.125</cell><cell>0.131</cell></row><row><cell>easy</cell><cell>3</cell><cell>1</cell><cell>0</cell></row><row><cell>medium</cell><cell>10</cell><cell>2</cell><cell>1</cell></row><row><cell>hard</cell><cell>8</cell><cell>17</cell><cell>7</cell></row><row><cell>very hard</cell><cell>0</cell><cell>16</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="16,134.77,127.10,345.83,227.42"><head>Table 9 :</head><label>9</label><figDesc>Best performing topics for query expansion (QE) and feedback (FB) runs relative to the average over all runs. Only the top 15 topics that benefit the from query expansion are presented here.</figDesc><table coords="16,187.15,162.80,241.05,191.73"><row><cell></cell><cell>QE (34 topics)</cell><cell>FB (15 topics)</cell></row><row><cell>Topics</cell><cell>9 flying hot air balloon</cell><cell>8 tennis player on court</cell></row><row><cell></cell><cell>70 close up of trees</cell><cell>69 red tomato</cell></row><row><cell></cell><cell>7 lightning in the sky</cell><cell>51 aerial pictures of landscapes</cell></row><row><cell></cell><cell>68 historic castle</cell><cell>5 Oktoberfest beer tent</cell></row><row><cell></cell><cell>66 bridges in daylight</cell><cell>44 saturn</cell></row><row><cell></cell><cell>63 postage stamp</cell><cell>43 skyscraper in daylight</cell></row><row><cell></cell><cell>6 solar panels</cell><cell>4 Christmas tree</cell></row><row><cell></cell><cell>58 red fruits</cell><cell>36 portrait of Jintao Hu</cell></row><row><cell></cell><cell>57 road street signs</cell><cell>33 close up of antenna</cell></row><row><cell></cell><cell>56 musician on stage</cell><cell>25 Shiva painting or sculpture</cell></row><row><cell></cell><cell>54 launching space shuttle</cell><cell>24 Rorschach black and white</cell></row><row><cell></cell><cell>52 satellite image</cell><cell>22 sharks underwater</cell></row><row><cell></cell><cell cols="2">51 aerial pictures of landscapes 21 Chernobyl disaster ruins</cell></row><row><cell></cell><cell>50 portraits of people</cell><cell>18 stars and galaxies</cell></row><row><cell></cell><cell>46 sailboat</cell><cell>11 landline telephone</cell></row><row><cell cols="2">#images/topic 2.62</cell><cell>3</cell></row><row><cell cols="2">#words/topic 1.79</cell><cell>1.8</cell></row><row><cell>#reldocs</cell><cell>353.3</cell><cell>144.2</cell></row><row><cell>avg. MAP</cell><cell>0.16</cell><cell>0.188</cell></row><row><cell>easy</cell><cell>2</cell><cell>1</cell></row><row><cell>medium</cell><cell>5</cell><cell>5</cell></row><row><cell>hard</cell><cell>18</cell><cell>8</cell></row><row><cell>very hard</cell><cell>9</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="6,144.73,656.56,128.06,8.89"><p>http://dolphin.unige.ch/cmse/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="11,144.73,657.28,237.30,7.05"><p>http://www.imageclef.org/2010/wikiMM-results</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p><rs type="person">Adrian Popescu</rs> was supported by the <rs type="funder">French ANR (Agence Nationale de la Recherche) vie</rs> the <rs type="projectName">Georama</rs> project (<rs type="grantNumber">ANR-08-CORD-009</rs>). <rs type="person">Theodora Tsikrika</rs> was supported by the <rs type="funder">European Union</rs> via the <rs type="funder">European Commission</rs> project <rs type="projectName">VITALAS</rs> (contract no. <rs type="grantNumber">045389</rs>). <rs type="person">Jana Kludas</rs> was funded by the <rs type="funder">Swiss National Fund (SNF)</rs>. The authors would also like to thank all the groups participating in the relevance assessment process.</p><p>The authors would like to thank <rs type="person">Hervé le Borgne</rs> and <rs type="person">Pierre-Alain Moëllic</rs> (<rs type="affiliation">CEA LIST</rs>) for providing the visual features for the collection.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GKcWVv6">
					<idno type="grant-number">ANR-08-CORD-009</idno>
					<orgName type="project" subtype="full">Georama</orgName>
				</org>
				<org type="funded-project" xml:id="_5JYTczx">
					<idno type="grant-number">045389</idno>
					<orgName type="project" subtype="full">VITALAS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.61,634.65,337.99,8.89;15,150.96,645.60,329.64,8.89;15,150.96,656.56,176.70,8.89" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="15,273.90,634.65,206.70,8.89;15,150.96,645.60,329.64,8.89;15,150.96,656.59,99.18,8.80">Chatzichristofis and Konstantinos Zagoris Multimedia Search with Noisy Modalities: Fusion and Multistage Retrieval In Working notes of the ImageCLEF 2010 Lab</title>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savvas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Padua, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,381.84,337.98,8.89;16,150.96,392.79,329.65,8.89;16,150.96,403.76,290.84,8.89" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="16,185.49,392.79,295.12,8.89;16,150.96,403.76,213.31,8.89">Experiences at ImageCLEF 2010 using CBIR and TBIR mixing information approaches In Working notes of the ImageCLEF 2010 Lab</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Garca-Serrano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Padua, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,414.82,337.98,8.89;16,150.96,425.78,243.78,8.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,304.78,414.82,175.80,8.89;16,150.96,425.78,27.50,8.89">Image classification using color, texture and regions</title>
		<author>
			<persName coords=""><forename type="first">Ya-Chun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu-Yuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,185.61,425.80,104.58,8.81">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="759" to="776" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,436.85,337.98,8.89;16,150.96,447.81,329.64,8.89;16,150.96,458.76,329.64,8.89;16,150.96,469.72,329.63,8.89;16,150.96,480.69,71.42,8.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,367.33,447.81,113.27,8.89;16,150.96,458.76,329.64,8.89;16,150.96,469.72,144.52,8.89">Minoukadeh XRCE&apos;s Participation in Wikipedia Retrieval, Medical Image Modality Classifiction and Ad-hoc Retrieval Tasks of ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keyvan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,314.66,469.75,162.09,8.81">Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,491.73,337.99,8.91;16,150.96,502.71,263.45,8.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,350.34,491.73,130.26,8.91;16,150.96,502.71,17.93,8.89">Bencz úr SZTAKI @ ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Bálint</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">István</forename><surname>Petrás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>András</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,182.59,502.74,154.30,8.81">Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,513.77,337.98,8.89;16,150.96,524.73,329.64,8.89;16,150.96,535.69,329.64,8.89;16,150.96,546.65,104.42,8.89;16,255.37,545.04,7.19,4.21;16,316.71,546.67,163.89,8.80;16,150.96,557.61,223.39,8.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,332.31,535.69,148.29,8.89;16,150.96,546.65,83.42,8.89">modular architecture for multimedia information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><surname>Delezoide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaël</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faiza</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meriama</forename><surname>Hède</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Laib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasredine</forename><surname>Moëllic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Semmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,239.74,546.65,15.64,8.89;16,255.37,545.04,7.19,4.21;16,316.71,546.67,163.89,8.80;16,150.96,557.64,70.93,8.80">In 8 th Workshop on Content-Based Multimedia Indexing (CBMI 2010</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>demo session</note>
</biblStruct>

<biblStruct coords="16,142.61,568.66,337.98,8.91;16,150.96,579.63,329.64,8.89;16,150.96,590.59,329.63,8.89;16,150.96,601.56,20.17,8.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="16,150.96,579.63,196.61,8.89;16,375.25,579.63,105.35,8.89;16,150.96,590.59,273.98,8.89">Expanding with Semantic Information from Context In Working notes of the ImageCLEF 2010 Lab</title>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>González-Crist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<pubPlace>Padua, Italy</pubPlace>
		</imprint>
	</monogr>
	<note>DAEDALUS at ImageCLEF Wikipedia Retrieval</note>
</biblStruct>

<biblStruct coords="16,142.61,612.62,337.98,8.89;16,150.96,623.58,263.46,8.89" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,213.37,612.62,267.22,8.89;16,150.96,623.58,185.94,8.89">Blind Relevance Feedback for the ImageCLEF Wikipedia Retrieval Task In Working notes of the ImageCLEF 2010 Lab</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Padua, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,634.65,337.99,8.89;16,150.95,645.60,329.64,8.89;16,150.95,656.56,109.20,8.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,379.04,634.65,101.55,8.89;16,150.95,645.60,176.67,8.89">Document Expansion for Text-based Image Retrieval at WikipediaMM</title>
		<author>
			<persName coords=""><forename type="first">Jinming</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,360.85,645.63,119.74,8.80;16,150.95,656.59,31.68,8.80">Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,119.43,338.36,8.91;17,150.96,130.40,329.63,8.89;17,150.96,141.39,329.63,8.81;17,150.96,152.32,329.63,8.89;17,150.96,163.27,115.64,8.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,433.26,130.42,47.33,8.81;17,150.96,141.39,114.13,8.81">Multilingual Information Access Evaluation</title>
	</analytic>
	<monogr>
		<title level="m" coord="17,286.33,141.39,194.26,8.81;17,150.96,152.35,213.16,8.81">II Multimedia Experiments: Proceedings of the 10th Workshop of the Cross-Language Evaluation Forum (CLEF</title>
		<title level="s" coord="17,391.36,152.32,89.23,8.89;17,150.96,163.27,51.20,8.89">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Henning</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Caputo</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,174.24,251.36,8.89;17,411.83,174.24,68.77,8.89;17,150.96,185.20,329.63,8.89;17,150.96,196.15,71.42,8.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,411.83,174.24,68.77,8.89;17,150.96,185.20,81.16,8.89">RGU at Image-CLEF</title>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leszek</forename><surname>Kaliciak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,150.96,185.20,325.79,8.89">Wikipedia Retrieval Task In Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,207.11,338.35,8.89;17,150.96,218.07,197.87,8.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,220.19,207.11,191.99,8.89">Télécom Bretagne at ImageCLEF WikipediaMM</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,448.89,207.14,31.69,8.81;17,150.96,218.09,120.35,8.81">Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,229.03,338.36,8.89;17,150.96,239.99,329.64,8.89;17,150.96,250.94,154.78,8.89" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="17,170.33,239.99,80.21,8.89;17,276.58,239.99,204.02,8.89;17,150.96,250.97,77.27,8.80">CLIR for Wikipedia Images In Working notes of the ImageCLEF 2010 Lab</title>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthikeyan</forename><surname>Pusapathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pok</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Knudson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<pubPlace>Padua, Italy</pubPlace>
		</imprint>
	</monogr>
	<note>UNT at ImageCLEF</note>
</biblStruct>

<biblStruct coords="17,142.24,261.91,338.35,8.89;17,150.96,272.87,307.07,8.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,290.80,261.91,189.78,8.89;17,150.96,272.87,173.11,8.89">Video Google: Efficient Visual Search for Videos, Towards Category-Level Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,331.45,272.89,22.93,8.81">LNCS</title>
		<imprint>
			<biblScope unit="volume">4170</biblScope>
			<biblScope unit="page">127144</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,283.82,338.35,8.89;17,150.96,294.78,329.64,8.89;17,150.96,305.74,329.64,8.89;17,150.96,316.70,313.35,8.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,260.45,294.78,220.14,8.89;17,150.96,305.74,128.07,8.89">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,296.84,305.77,183.76,8.81;17,150.96,316.72,91.96,8.81">Proceedings of the 14th annual ACM international conference on Multimedia</title>
		<meeting>the 14th annual ACM international conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,327.66,338.35,8.89;17,150.96,338.62,329.63,8.89;17,150.96,349.60,329.63,8.81;17,150.96,360.54,180.65,8.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,390.94,327.66,89.65,8.89;17,150.96,338.62,315.63,8.89">Falcão A compact and efficient image retrieval approach based on border/interior pixel classification</title>
		<author>
			<persName coords=""><forename type="first">Renato</forename><forename type="middle">O</forename><surname>Stehling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><forename type="middle">A</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,150.96,349.60,329.63,8.81;17,150.96,360.56,16.55,8.81">Proceedings of the 11th International Conference on Information and Knowledge Management</title>
		<meeting>the 11th International Conference on Information and Knowledge Management<address><addrLine>McLean, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,371.50,338.35,8.89;17,150.96,382.45,75.70,8.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,253.28,371.50,147.27,8.89">IIIT-H at ImageCLEF Wikipedia MM</title>
		<author>
			<persName coords=""><forename type="first">Srinivasarao</forename><surname>Vundavalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,438.61,371.53,41.98,8.81;17,150.96,382.48,49.77,8.81">CLEF 2009 working notes</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,393.41,338.35,8.89;17,150.96,404.38,291.21,8.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,350.03,393.41,130.56,8.89;17,150.96,404.38,25.53,8.89">R at ImageCLEF Wikipedi Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Kong-Wah</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sujoy</forename><surname>Roy I2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,210.37,404.40,154.29,8.81">Working notes of the ImageCLEF 2010 Lab</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,415.33,338.36,8.89;17,150.96,426.29,329.63,8.89;17,150.96,437.28,329.65,8.81;17,150.96,448.21,311.09,8.89" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,305.46,415.33,127.19,8.89">The INEX 2006 multimedia track</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roelof</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,366.58,426.32,114.01,8.81;17,150.96,437.28,329.65,8.81;17,150.96,448.23,40.29,8.81">Advances in XML Information Retrieval: 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2006</title>
		<title level="s" coord="17,197.52,448.23,84.30,8.81">Revised Selected Papers</title>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4518</biblScope>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
