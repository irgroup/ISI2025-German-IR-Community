<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.30,115.90,258.76,12.90;1,209.23,133.83,196.90,12.90">Multimedia Search with Noisy Modalities: Fusion and Multistage Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.87,171.88,60.72,8.64"><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Democritus</orgName>
								<orgName type="institution" key="instit2">University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.48,171.88,103.65,8.64"><forename type="first">Savvas</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Democritus</orgName>
								<orgName type="institution" key="instit2">University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,362.32,171.88,85.16,8.64"><forename type="first">Konstantinos</forename><surname>Zagoris</surname></persName>
							<email>kzagoris@ee.duth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Democritus</orgName>
								<orgName type="institution" key="instit2">University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.30,115.90,258.76,12.90;1,209.23,133.83,196.90,12.90">Multimedia Search with Noisy Modalities: Fusion and Multistage Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23C1BF6DCC3C9B2BC76A1E773DCBFD4F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report our experiences from participating to the controlled experiment of the ImageCLEF 2010 Wikipedia Retrieval task. We built an experimental search engine which combines multilingual and multi-image search, employing a holistic web interface and enabling the use of highly distributed indices. Modalities are searched in parallel, and results can be fused via several selectable methods. The engine also provides multistage retrieval, as well as a single text index baselines for comparison purposes. Experiments show that the value added by image modalities is very small when textual annotations exist. The contribution of image modalities is larger when search is performed in a 2-stage fashion, i.e., using image search for re-ranking a smaller set of only the top results retrieved by text. Furthermore, first splitting annotations to many modalities with respect to natural language and/or type and then fusing results has the potential of achieving better effectiveness than using all textual information as a single modality. Concerning fusion, the simple method of linearly combining evidence is found to be the most robust, achieving the best effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As digital information is increasingly becoming multimodal, the days of single-language text-only retrieval are numbered. Take as an example Wikipedia where a single topic may be covered in several languages and include non-textual media such as image, sound, and video. Moreover, non-textual media may be annotated with text in several languages in a variety of metadata fields such as object caption, description, comment, and filename. Current search engines usually focus on limited numbers of modalities at a time, e.g. English text queries on English text or maybe on textual annotations of other media as well, not making use of all information available. Final rankings are usually results of fusion of individual modalities, a task which is tricky at best especially when noisy modalities are involved.</p><p>In this paper we present the experiments performed by Democritus University of Thrace (DUTH), Greece, in the context of our participation to the ImageCLEF 2010, Wikipedia Retrieval task. <ref type="foot" coords="1,237.47,600.67,3.49,6.05" target="#foot_0">1</ref> The ImageCLEF 2010 Wikipedia collection has image as its primary medium, consisting of 237434 items, associated with noisy and incomplete user-supplied textual annotations and the Wikipedia articles containing the images. Associated annotations are written in any combination of English, German, French, or any other unidentified language. There are 70 test topics, each one consisting of a textual and a visual part: three title fields (one per language-English, German, French), and one or more example images. The exact details of the setting of the task, e.g., research objectives, collection etc., are provided in the overview paper <ref type="bibr" coords="2,381.57,155.18,10.58,8.64" target="#b6">[7]</ref>.</p><p>We built an experimental multimodal search engine, www.mmretrieval.net (Fig. <ref type="figure" coords="2,153.45,179.09,3.74,8.64" target="#fig_0">1</ref>), which allows multiple image and multilingual queries in a single search and makes use of the total available information in a multimodal collection. All modalities are indexed separately and searched in parallel, and results can be fused with different methods or ranked in a 2-stage fashion. The engine demonstrates the feasibility of the proposed architecture and methods, and furthermore enables a visual inspection of the results beyond the standard TREC-style evaluation. Using the engine, we experimented with different score normalization and combination methods for fusing results, as well as 2-stage retrieval by first thresholding the results obtained by secondary modalities and then re-ranking only the top results based on fusing the primary modalities.</p><p>The rest of the paper is organized as follows. In Section 2 we describe in more detail the fusion methods we experimented with and justify their use. In Section 3 we describe the MMretrieval engine, give the details on how the Wikipedia collection is indexed and a brief overview of the search methods that the engine provides. A comparative evaluation of most implemented methods is provided in Section 4; this is based solely on additional experiments performed, since we discovered a bug affecting all our official runs involving image modalities. Conclusions are drawn in Section 5. Let us consider a multimodal collection, i.e. a collection consisting of multiple descriptions for each of its items such as text, image, sound, etc. The term 'modality' seems to have been used interchangeably to 'medium' in the related literature. A question that arises is what an information medium really is. For example, text and image are usually seen as different media, but one can argue that text itself can come in different 'flavors' such as English, French, etc. Similarly, image information can come in different streams such as color, texture, or shape. In this respect, it might be more useful in IR to define modality as a description or representation of items in collection. For example, a multimodal collection may consist of modalities such as English text, French text, image texture, image color, etc., for each of its items.</p><p>Fusion in IR is the process of combining evidence about relevance from different sources of information, e.g. from a single modality via several retrieval models, or from several modalities. The relevance of an item to a query is usually captured in a numeric value or score. Scores for the same item across different sources may be incomparable. Thus, fusion usually consists of two components: score normalization and score combination. Assuming for now ideal scores, in the sense that are comparable across sources, we first investigate combination methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Combination</head><p>Let us assume that different sources of information produce comparable scores, e.g. probability of relevance estimates (prels), for all the items in a collection against a query. Let us investigate what the best combination of prels would be under different circumstances.</p><p>Standard fusion setups in textual IR consist of a single collection from which items are scored and ranked via several methods. Assuming that each method produces accurate prels, there is no issue of fusing the individual ranked-lists: each item would be assigned the same prel across retrieval methods, consequently using any of the methods in isolation would be sufficient. In practice, prel estimates are inaccurate with different degrees of inaccuracy, or noise, across methods. In this respect, in order to smooth out prel noise, prels for each item can be averaged or simply summed (for a constant number of sources) which has led to the popular combination method of CombSUM.</p><p>Alternatively, prel noise may be smoothed by multiplication (CombMULT), which can be seen as a proxy for their geometric average for a constant number of score sources. However, due to the fact that a single zero score would zero the multiplication, CompMULT is less robust than CombSUM. Beyond the robustness issue, we do not see a reason for preferring the geometric over the arithmetic average in the IR context. Consequently, we will not talk further about CombMULT in order to reduce the number of usable combinations.</p><p>Smoothing out noise with CombSUM implicitly assumes a non-skewed distribution of prels for an item across sources, where the average prel would make more sense. In the general case, noise can also be smoothed by taking the median prel (CombMED); this would eliminate sources with extreme noise better. Consequently, we argue that, theoretically, the most suitable combination is CombMED.</p><p>Let us now assume a multimodal collection. Having given a narrower definition of modality to mean a description or representation or a stream of information rather than medium, a standard fusion text setup mentioned above consists of a single text modality and several retrieval (or prel estimation) methods. In contrast, a multimodal setup consists of several descriptions per item as well as several suitable retrieval methods. For multimodal setups, using the same argument as above and assuming there is prel noise, a good combination method would still be CombSUM and the best theoretically would still be CombMED. However, the argument for using CombMED is now stronger as we explain next. Consider a highly imbalanced set of modalities, e.g. one with many more text modalities than image ones. The average or sum of prels for each item would be dominated (for the good or the worse) by the textual descriptions, opening also the question of what the best balance of media is. Using the median prel does not have this problem, eliminating also the best-balance question. In summary, we argue that CombMED is the best combination method for fusing prel estimates with unknown noise characteristics, in multimodal as well as standard fusion in text retrieval. When prels are accurate or noise-free, they should be identical per item across modalities or retrieval methods so fusion is redundant.</p><p>Estimation noise can result from two sources: the retrieval model internals or the descriptions. Let us assume ideal retrieval models and focus on descriptions. Take as an example user-supplied textual annotations for images, such as caption, description, comment. These are bound to be noisy in the form of missing information, e.g. a partial description of an image or no description given at all. In this respect, prels are likely to be underestimated. Similarly, using object recognition techniques one can describe several objects (e.g. sun and beach) but usually miss more global or greater semantics (e.g. vacation), with the effect of underestimating prels. Thus, one could argue that when descriptions are missing information then the most suitable combination method is the one that takes the maximum estimated prel across modalities, i.e. CombMAX.</p><p>In practice, however, and factoring in retrieval model internals, retrieval models are bound to be more noisy in an unpredictable way at high prels. For example, a high textual score is usually a result of a low document frequency of a query word, i.e. the score is based on statistics on sparse data which are unreliable. This may also make CombMAX unreliable. Additionally, in our current setup we employ image modalities for global image features rather than local, which may sometimes capture too much rather than miss information. It is not clear whether or when prels from image modalities are under-or over-estimated, so CombMAX may not be suitable; we will further investigate this experimentally.</p><p>Similarly, when noise comes mostly in the form incomplete or empty descriptions, CombMED may have a robustness issue similar to CombMULT but not so severe: it would return zero, if more than half of the modalities return zero scores. Consequently, although we argued that theoretically CombMED is the best combination, given the noise characteristics of the collection at hand and the highly modal approach we followed by splitting the metadata and articles into several modalities, CombMED is expected to be less robust than CombSUM.</p><p>Coming back to CompSUM, the implicit assumption made is that noise has similar characteristics across modalities. If noise levels are known for the modalities, this information can be used to take a weighted average of prels instead or simply a weighed linear combination (CompWSUM); the higher the noise of a modality, the lower its weight. Appropriate weights can be estimated from training data or supplied by users, thus CompWSUM is essentially parametric.</p><p>Concluding, we arrived by argument to three usable non-parametric score combination methods for the task at hand (in a descending order of expected effectiveness): CombSUM, CombMAX, CombMED. From parametric methods, CombWSUM is the most promising and it can also become simply CombSUM for equal weights across modalities. There are many other combinations in the literature, e.g. CombMNZ, which we have not considered here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Normalization</head><p>So far we have assumed ideally comparable scores across sources, e.g. in the form of probability of relevance estimates. But even most probabilistic models do not calculate the probability of relevance of items directly, but some order-preserving function of it <ref type="bibr" coords="5,143.55,345.44,10.58,8.64" target="#b2">[3]</ref>. Without training data, scores of some popular text retrieval models (e.g. tf.idf) can be turned to probabilities of relevance via the score-distributional method of <ref type="bibr" coords="5,466.20,357.40,10.79,8.64" target="#b0">[1]</ref>; however, the model does not seem to fit to the score distributions produced by our image descriptors <ref type="bibr" coords="5,207.81,381.31,10.58,8.64" target="#b5">[6]</ref>.</p><p>We resort to employing two methods, which we consider them as calibration rather than normalization methods, to calibrate scores across modalities per query:</p><p>-MinMax: It maps the resulting score range linearly to the [0, 1] interval.</p><p>-Zscore: A linear normalization which maps each score to the number of standard deviations it lies above or below the mean score.</p><p>MinMax does not guarantee any degree of comparability but it is a range calibrator, which may result to undesirable effects, for example: a modality with no relevant items would still return an item with a maximum score of 1. Z-score is more robust in this respect; it would calibrate high only the items separated from the heap of scores. Nevertheless, it also has its problems; it seems to assume a non-skewed distribution of scores, where the mean would be a meaningful 'neutral' score. As it is well-known, actual score distributions in text retrieval are highly skewed, clearly violating the assumption underlying Z-score. Although not very popular in IR, Z-score was used with reasonable success in <ref type="bibr" coords="5,177.38,571.94,10.58,8.64" target="#b1">[2]</ref>. As a third normalization method, we employed the non-linear Known-Item Aggregate CDF (KIACDF). KIACDF is similar to the HIS normalization introduced in <ref type="bibr" coords="5,466.48,596.66,10.58,8.64" target="#b1">[2]</ref>, except that know-item queries are used (instead of historical) in estimating score transfer functions. For each modality, we issued a uniform sample of 0.5% of the collection as known-item queries, aggregated the resulting scores from all queries, and calculated their CDF. For an ad-hoc query, each resulting item score is normalized to the value of the latter CDF at the score.</p><p>We introduce an experimental search engine for multilingual and multimedia information, employing a holistic web interface and enabling the use of highly distributed indices. Modalities are searched in parallel, and results can be fused via several selectable methods. The engine also provides multistage retrieval, as well as a single text index baseline for comparison purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing</head><p>To index images, we consider the family of descriptors known as Compact Composite Descriptors (CCDs). CCDs consist of more than one visual features in a compact vector, and each descriptor is intended for a specific type of image. We index with two descriptors from the family, which we consider them as capturing orthogonal information content, i.e., the Joint Composite Descriptor (JCD) <ref type="bibr" coords="6,330.24,288.71,11.62,8.64" target="#b3">[4]</ref> and the recently proposed Spatial Color Distribution (SpCD) <ref type="bibr" coords="6,243.46,300.66,10.58,8.64" target="#b4">[5]</ref>. JCD is developed for color natural images, while SpCD is considered suitable for colored graphics and artifficially generated images. Thus, we have 2 image indices.</p><p>The collection of images at hand, i.e. the ImageCLEF 2010 Wikipedia collection, comes with XML metadata consisting of a description, a comment, and multiple captions, per language (English, German, and French). Each caption is linked to the wikipedia article where the image appears in. Additionally, a raw comment is supplied which contains all the per-language comments and any other comment in an unidentified language; we do not use this field due to its great overlap with the per-language comments. Any of the above fields may be empty or noisy. Furthermore, a name field is supplied per image containing its filename. We do not use the supplied &lt;license&gt; field.</p><p>For text indexing and retrieval, we employ the Lemur Toolkit V4.11 and Indri V2.11 with the tf.idf retrieval model. <ref type="foot" coords="6,256.77,454.41,3.49,6.05" target="#foot_1">2</ref> In order to have clean global (DF) and local statistics (TF, document length), we split the metadata per language and index them separately preserving the fields. Lemur allows searching within fields and we use this facility, as we will see below, resulting in many modalities. This, together with a separate index for the name field, results in 4 indices. For English text, we enable Krovetz stemming; no stemming is done for other or unidentified languages in the current version of the system. Additionally, as a brute-force baseline, we also provide a single text index of all metadata and associated articles where no pre-processing (such as stemming) is done and no metadata fields or language information is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Searching</head><p>The web application is developed in the C#/.NET Framework 4.0 and requires a fairly modern browser as the underlying technologies which are employed for the interface are HTML, CSS and JavaScript (AJAX). Fig. <ref type="figure" coords="6,318.92,625.47,4.15,8.64" target="#fig_1">2</ref> illustrates an overview of the architecture. The user provides image and text queries through the web interface which are dispatched in parallel to the associated databases. Retrieval results are obtained from each of the databases, fused into a single listing, and presented to the user. Users can supply no, single, or multiple query images in a single search, resulting in 2 * i active image modalities, where i is the number of query images. Similarly, users can supply no text query or queries in any combination of the 3 languages, resulting in 5 * l active text modalities, where l is the number query languages: each supplied language results to 4 modalities, one per field described in the previous section, plus the name modality which we are matching with any language. The current beta version assumes that the user provides multilingual queries for a single search, while operationally query translation may be done automatically.</p><p>The results from each modality are fused by one of the supported methods. Fusion consists of two components: score normalization and combination. We provide two linear normalization methods, MinMax and Z-score, the non-linear KIACDF, and the ranked-based BordaCount in linear and non-linear forms. Combination of scores across modalities can be done with weighted summation (CombWSUM), multiplication (CompMULT), maximum (CombMAX), or median (CombMED).</p><p>In CombWSUM, the user may select a weigh factor w ∈ [0, 100], which determines the percentage contribution of the image modalities against the textual ones: scores from image modalities are multiplied with w/(2i100) and from text modalities with (100 -w)/(5l100), before summation.</p><p>For efficiency reasons, only the top-4000 results are asked from each modality. If a modality returns less than 4000 items, all non-returned items are assigned zero scores for the modality. When a modality returns 4000 items, all non-occurring items in the top-4000 are assigned half the score of the 4000th item.</p><p>Beyond fusion, the system provides baseline searches on the single text index in two flavors: metadata only (Baseline-Metadata), and metadata including associated articles (Baseline-Any). In baseline searches, multilingual queries are concatenated and issued as one.</p><p>Search can also be performed in a 2-stage fashion. First, the text-only results of the Baseline-Any are obtained. Then, the top-K results are re-ranked using only the image modalities which are fused by a selected method. By default, we estimate the optimal K for maximizing the recall-oriented T9U measure, i.e. 2 gain per relevant retrieved and 1 loss per non-relevant retrieved, via the score-distributional method of <ref type="bibr" coords="8,436.92,238.86,10.58,8.64" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>After the submission of our official runs, we discovered a bug in the image descriptors which affected all the runs except the text-only baselines. Fixing the bug improved effectiveness allover. Thus, we repeated all experiments with the bug-free version of the system and report them here together with additional ones. We use different runlabels than the official ones, providing more detail.</p><p>Table <ref type="table" coords="8,174.19,351.10,4.98,8.64" target="#tab_0">1</ref> shows that Baseline-Any performs significantly better than Baseline-Metadata. Moreover, since the associated articles constitute 3 of the modalities fused (1 per language), it also makes more sense to compare the fusion methods with Baseline-Any.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fusion</head><p>In Table <ref type="table" coords="9,173.54,138.10,4.98,8.64" target="#tab_0">1</ref> we can see that the best fusion method-and the only one beating the baseline-is the parametric CombWSUM, for both calibration methods and especially with small w (i.e. small image weight). Nevertheless, its effectiveness depends largely in the choice of w. Fig. <ref type="figure" coords="9,235.01,173.96,4.15,8.64" target="#fig_3">3</ref> shows the impact of different choices of w on MAP. For large w, CombWSUM degrades greatly with MinMax normalization. With Z-score, CombWSUM appears more robust; this is mostly a result of the highly skewed score distributions of the text modalities (in contrast to the rather symmetric ones of the image modalities <ref type="bibr" coords="9,197.40,453.20,10.46,8.64" target="#b5">[6]</ref>), producing much larger Z-scores for text than for image making CombWSUM less sensitive to w. We consider this as a drawback of Z-score rather than robustness. With MinMax, effectiveness keeps increasing with a decreasing w, which means that image modalities do not add value but rather have a negative impact.</p><p>For our 15 text and 2 image modalities (i.e. 1 query image), all modalities have roughly an equal contribution for w ≈ 12, while the corresponding w for 2 query images (i.e. 4 image modalities) is 21. Thus, for the current topic set which consists of many topics with 2 query images, CombWSUM becomes CombSUM for w ≈ 20 or 10, suggesting that CombSUM would have been the best non-parametric method. However, we consider this accidental.</p><p>Ignoring CombSUM, the best non-parametric method is CombMAX, which is close but not beating the baseline. CombMED is disappointing due to many modalities returning zero scores; the setup has lots of noise in the form of empty (or highly incomplete) metadata, making CombMED much less robust than summation methods. We have not yet run any experiments with CombMULT, but we expect similar or worse robustness issues than CombMED.</p><p>Since image does not seem to contribute much, there is another important conclusion we can draw. First splitting the text to several indices and further to several modalities and then retrieving by fusing results leads to improvements over using a single text index. This can be attributed to keeping cleaner keyword frequency statistics per language, separate text queries per language, as well as to using cleaner modalities conceptually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2-Stage</head><p>We tested a model of 2-stage retrieval. According to this model, image modalities are considered as primary, assuming the user is searching for images with visual similarity with the query images. Since the image low-level features do not behave very well in large databases, we performed the image search in a subset. First, the text-only results of the Baseline-Any are obtained. Then, the top-K results are re-ranked using only the image modalities which are fused by a selected method. We estimate the optimal K for maximizing the recall-oriented T9U measure, i.e. 2 gain per relevant retrieved and 1 loss per non-relevant retrieved, via the score-distributional method of <ref type="bibr" coords="10,392.34,281.77,10.58,8.64" target="#b0">[1]</ref>. The probability of relevance threshold that optimizes T9U is θ = 0.333. We also tried θ = 0.5 which is more precision-recall balanced and corresponds to minimizing the Error Rate.  Table <ref type="table" coords="10,175.29,560.80,4.98,8.64" target="#tab_1">2</ref> presents the results without and with relevance feedback (RF). We used pseudo/blind RF only for the first textual stage with the following parameters: top-4 items, 128 terms, and an original query weight of 0.8. These are arguably unusual RF parameters, mostly targeted to increasing the query length. It is suggested in <ref type="bibr" coords="10,437.41,596.66,11.62,8.64" target="#b2">[3]</ref> that the score-distributional method of <ref type="bibr" coords="10,257.05,608.62,11.62,8.64" target="#b0">[1]</ref> for estimating K or θ works better with long queries. While the results show that long queries indeed help the method (2-stage runs with RF perform better than the RF baseline in terms of MAP), the RF baseline performs much worse than the non-RF. The choice of the RF parameters was unfortunate for the performance of the first textual phase, affecting also the 2-stage method as a whole.</p><p>Overall, the performance of the 2-stage runs with RF is competitive, achieving MAP comparable to our best non-parametric CombMAX runs. More suitable RF parameters may have led to larger improvements. Also, tighter K or probability thresholds, e.g. θ = 0.5, seem to work better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Experiments</head><p>We performed a number of other experiments which we will not extensively report here, but provide only a summary.</p><p>We tried to enable RF (with the same parameters as for 2-stage) also for the fusion runs, but the results were inconsistent. While some runs improved, most of them presented lower effectiveness, pointing once more to the unfortunate choice of our RF parameters.</p><p>We also tried rank-based combinations with Borda Count in both linear and nonlinear fashion, but results were far behind the score-based combinations and 2-stage; MAP was in the area of 0.06 to 0.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Improvements</head><p>In retrospect, it seems that we have overlooked a few things which may have led to better effectiveness:</p><p>-We should have used the raw comment field. Although most of the times it has a large overlap with the per-language comments, it sometimes carries extra text which may have been useful. -We used stemming only for the identified English text. Stemming also the other languages could have improved effectiveness, especially when no relevance feedback is used. -We did not experiment with the relevance feedback parameters at all, but used rather unusual values which, based on previous literature, we assumed they would improve the quality of K in 2-stage runs. Although we achieved the target, other parameter values may have led to better effectiveness overall. -K is estimated to optimize a certain evaluation measure. We have tried two arbitrary measures, T9U and Error Rate. A suitable measure should be tight to the expected effectiveness of image search, a venue we have not explored.</p><p>For enhancing efficiency, the multiple indices should (and can easily) be moved to different hosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We reported our experiences from participating to the controlled experiment of the Im-ageCLEF 2010 Wikipedia Retrieval task. We built an experimental search engine which combines multilingual and multi-image search, employing a holistic web interface and enabling the use of highly distributed indices. Modalities are searched in parallel, and results can be fused via several selectable methods. The engine also provides 2-stage retrieval, as well as a single text index baselines for comparison purposes.</p><p>After various experiments, we arrived to a conclusion others had drawn before: the value added by image modalities is very small (or even negative) when textual annotations exist. This suggests that image retrieval is a problem which is far from being solved. In a more positive note, the contribution of image modalities is positive when search is performed in a 2-stage fashion, i.e., using image search for re-ranking a smaller set of only the top results retrieved by text. All these suggest that image retrieval can be reasonably effective in small databases, but it does not scale up well.</p><p>Focusing on text, first splitting annotations to many modalities with respect to natural language and/or type and then fusing results has the potential of achieving better effectiveness than using all textual information as a single modality. We attributed this to having cleaner keyword frequency statistics and separate text queries per language, as well as to using cleaner modalities conceptually.</p><p>Concerning fusion, the simple method of linearly combining evidence is found to be the most robust, achieving the best effectiveness. Combining by taking the max score across modalities is also competitive. Fusion is greatly affected by the degree of comparability of the scores combined. We tried two score calibration methods, Z-score and MinMax, and the latter achieved the best results. Nevertheless, whether any of the methods employed achieves comparability is questionable (Sec.2.2); we consider score normalization an open problem which its solution has the potential to greatly improve fusion, as well as result-merging in distributed retrieval.</p><p>All in all, we are satisfied with our results as first-time participants. Our best MAP result of 0.2561 (achieved post-submission with the bug-free version of the engine) would have ranked as the second-best run among the runs of all other participants. Moreover, we have experimentally identified some good methods for dealing with such tasks and directions for further improvements (Sec.4.4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,218.99,632.77,177.38,8.12;2,177.99,389.08,259.37,228.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The www.MMRetrieval.net search engine.</figDesc><graphic coords="2,177.99,389.08,259.37,228.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,255.57,394.97,104.21,8.12;7,203.93,162.67,207.50,217.57"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. System's architecture.</figDesc><graphic coords="7,203.93,162.67,207.50,217.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,193.53,407.73,21.41,7.77;8,315.20,407.73,102.26,7.77;8,193.53,418.74,226.06,8.12;8,193.53,429.70,226.06,8.12;8,193.53,440.72,226.06,8.06;8,193.53,451.68,226.06,8.06;8,193.53,462.64,154.72,8.06"><head></head><label></label><figDesc>CombWSUM, w = 10 0.2561 0.4971 0.4564 0.2997 MinMax CombWSUM, w = 20 0.2272 0.5257 0.4900 0.2781 Z-score CombWSUM, w = 50 0.1929 0.3714 0.3557 0.2373 Z-score CombWSUM, w = 33 0.1925 0.3671 0.3457 0.2329 Z-score CombWSUM, w = 20 0.1856 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,177.81,383.79,259.74,8.12;9,169.35,204.71,276.67,164.34"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MInMax or Z-score with CompWSUM: MAP as a function of w.</figDesc><graphic coords="9,169.35,204.71,276.67,164.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,170.47,337.50,21.41,7.77;10,338.26,337.50,102.27,7.77;10,170.47,348.86,49.17,7.77;10,335.65,348.51,107.00,8.12;10,170.47,359.53,146.94,8.06;10,335.65,359.82,107.00,7.77;10,170.47,370.49,135.22,8.06;10,335.65,370.78,107.00,7.77;10,170.47,381.45,137.73,8.06;10,335.65,381.74,107.00,7.77;10,170.47,392.41,144.44,8.06;10,335.65,392.70,107.00,7.77;10,170.47,403.37,145.93,8.06;10,335.65,403.66,107.00,7.77;10,170.47,414.33,136.71,8.06;10,335.65,414.61,107.00,7.77;10,170.47,425.62,272.18,8.12;10,170.47,436.64,272.18,8.06;10,170.47,447.60,272.18,8.06;10,170.47,458.56,272.18,8.06;10,170.47,469.52,272.18,8.06;10,170.47,480.48,272.18,8.06;10,170.47,491.73,64.03,7.77;10,335.65,491.38,107.00,8.12"><head></head><label></label><figDesc>MinMax CombSUM, θ = 0.333 0.1445 0.4129 0.3621 0.1978 2-Stage Z-Score CombSUM, θ = 0.5 0.1410 0.3914 0.3586 0.1977 2-Stage MinMax CombSUM, θ = 0.5 0.1401 0.3943 0.3579 0.1974 2-Stage Z-Score CombSUM, θ = 0.333 0.1400 0.4029 0.3579 0.1934 2-Stage Z-Score CombMAX, θ = 0.333 0.1359 0.3686 0.3200 0.1941 2-Stage Z-Score CombMAX, θ = 0.5 0.1295 0.3657 0.3336 0.1868 2-Stage MinMax CombSUM, θ = 0.5, RF 0.1689 0.4043 0.3579 0.2167 2-Stage Z-Score CombSUM, θ = 0.5, RF 0.1654 0.4000 0.3579 0.2132 2-Stage MinMax CombSUM, θ = 0.333, RF 0.1625 0.3914 0.3579 0.2075 2-Stage Z-Score CombMAX, θ = 0.5, RF 0.1618 0.3686 0.3443 0.2095 2-Stage Z-Score CombSUM, θ = 0.333, RF 0.1614 0.3929 0.3529 0.2080 2-Stage Z-Score CombMAX, θ = 0.333, RF 0.1583 0.3800 0.3457 0.2028 Baseline-Any, RF 0.1480 0.4086 0.3686 0.2149</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,155.23,462.92,304.90,168.19"><head>Table 1 .</head><label>1</label><figDesc>Fusion results, sorted on MAP. The best results per measure are in boldface.</figDesc><table coords="8,348.25,462.92,71.34,7.77"><row><cell>3629 0.3407 0.2267</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,509.32,345.83,19.08"><head>Table 2 .</head><label>2</label><figDesc>2-stage retrieval results, without and with RF, sorted on MAP. The best results per measure are in boldface.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.92,182.92,6.32"><p>http://www.imageclef.org/2010/wiki</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,657.92,145.26,6.32"><p>http://www.lemurproject.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.13,478.50,342.46,7.77;12,146.47,489.46,334.12,7.77;12,146.47,500.42,23.90,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,305.21,478.50,175.37,7.77;12,146.47,489.46,170.93,7.77">Where to stop reading a ranked list? Threshold optimization using truncated score distributions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,335.88,489.46,67.79,7.77">Proceedings SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,510.59,342.46,7.77;12,146.47,521.55,74.10,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,243.90,510.59,175.66,7.77">A signal-to-noise approach to score normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,436.76,510.59,43.83,7.77;12,146.47,521.55,47.96,7.77">Proceedings CIKM. ACM</title>
		<meeting>CIKM. ACM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,531.72,342.46,7.77;12,146.47,542.68,304.89,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,306.78,531.72,157.12,7.77">Score distributions in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,146.47,542.68,154.36,7.77">ICTIR. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5766</biblScope>
			<biblScope unit="page" from="139" to="151" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,552.85,342.46,7.77;12,146.47,563.81,334.12,7.77;12,146.47,574.77,23.90,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,321.60,552.85,158.99,7.77;12,146.47,563.81,193.94,7.77">Selection of the proper compact composite descriptor for improving content based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,357.93,563.81,70.73,7.77">Proceedings SPPRA</title>
		<meeting>SPPRA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,584.94,342.46,7.77;12,146.47,595.90,334.12,7.77;12,146.47,606.86,190.85,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,314.05,584.94,166.54,7.77;12,146.47,595.90,334.12,7.77;12,146.47,606.86,28.50,7.77">SpCD -Spatial Color Distribution Descriptor -A fuzzy rule-based compact composite descriptor appropriate for hand drawn color sketches retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,192.58,606.86,75.56,7.77">Proceedings ICAART</title>
		<meeting>ICAART</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,617.03,342.46,7.77;12,146.47,627.99,297.03,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,289.89,617.03,190.70,7.77;12,146.47,627.99,155.50,7.77">Late fusion of compact composite descriptorsfor retrieval from heterogeneous image databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,320.07,627.99,21.38,7.77">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.13,638.16,342.46,7.77;12,146.47,649.12,220.44,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,284.97,638.16,195.62,7.77;12,146.47,649.12,16.14,7.77">Overview of the wikipedia retrieval task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,181.09,649.12,106.58,7.77">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
