<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.49,115.96,284.37,12.62;1,197.18,133.89,221.00,12.62">ImageCLEF 2010 Working Notes on the Modality Classification subtask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.51,171.56,56.13,8.74"><forename type="first">Olivier</forename><surname>Pauly</surname></persName>
							<email>pauly@cs.tum.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.61,171.56,58.89,8.74"><forename type="first">Diana</forename><surname>Mateus</surname></persName>
							<email>mateus@cs.tum.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.15,171.56,57.70,8.74"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<email>navab@cs.tum.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.49,115.96,284.37,12.62;1,197.18,133.89,221.00,12.62">ImageCLEF 2010 Working Notes on the Modality Classification subtask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7B332B5E4838693A3128EED9FCF4C12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of this work is to investigate the performance of classical methods for feature description and classification, and to identify the difficulties of the ImageCLEF 2010 modality classification subtask. In this paper, we describe different approaches based on visual information for classifying medical images into 8 different modality classes. Since within the same class, images depict very different objects, we focus on global descriptors such as histograms extracted from scale-space, log-Gabor and phase congruency feature images. We also investigated different classification approaches based on support vector machines and random forests. A grid-search associated to a 10 folds cross-validation has been performed on a balanced set of 2390 images to find the best hyperparameters for the different models we propose. All experiments have been conducted with MATLAB on a Workstation with Intel Duo Core 3.16 Ghz and 4Gb of RAM. Our approach based on simple SVM and random forests give best performance and achieve respectively an overall f-measure of 74.13% and 73.59%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we investigate different classification approaches for the new Im-ageCLEF subtask modality classification <ref type="bibr" coords="1,313.43,488.75,9.96,8.74" target="#b0">[1]</ref>. The problem brings new challenges as in the database given as training set (see Fig. <ref type="figure" coords="1,353.79,500.70,4.09,8.74" target="#fig_0">1</ref>), images show a very high variability and very different kinds of anatomy appear within the same class. Furthermore, the problem is different from classical object recognition, in which features are especially designed for recognizing an object subject to different imaging conditions. In the present case, we aim at recognizing the imaging modality. Since we can not rely on the different structures appearing in images to discriminate them, we propose to focus on statistics based on global texture information. This is equivalent to making the assumption that each imaging modality shows different texture patterns. To characterize statistical texture information, we propose descriptors based on different feature images such as scalespace, log-Gabor and phase congruency feature images. Once a representation has been chosen to describe global textural statistics, a multi-class classification scheme is applied. Therefore, we investigated 3 different multi-class approaches: a simple multi-class SVM, a multi-kernel multi-class SVM and random forests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>The modality classification problem is an instance of a multi-class classification problem. We denote I : R 2 → R an image we want to classify into one of the 8 different classes {C k } k∈{1,••• ,8} , namely computerized tompography, graphics, magnetic resonance imaging, nuclear medicine, positron emission tomography, optical imaging, ultrasound and X-ray. I is described by a feature vector we denote x, and y its corresponding class label. The goal of multi-class classification is then to train a decision function Ψ such that:</p><formula xml:id="formula_0" coords="2,241.38,602.74,239.21,9.65">Ψ (x n ) = y n , ∀n ∈ {1, • • • , N }<label>(1)</label></formula><p>where {x n , y n } n∈{1,••• ,N } is the training set composed of image descriptors and their corresponding class labels. In the following section, we will show how to compute the different components of x.</p><p>3 Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Representation</head><p>As mentioned in the introduction, we will focus on statistical texture features to describe our images. We propose to use 3 different types of descriptors based on scale-space, log-Gabor and phase-congruency feature images.</p><p>Scale-space statistical descriptors A basic approach to characterize the image variations at different band of the frequency spectrum is to perform its multi-scale analysis <ref type="bibr" coords="3,221.63,244.30,9.96,8.74" target="#b1">[2]</ref>. By convolving the input image with a smoothing kernel, structure can be analyzed at different scales. As smoothing operator, a Gaussian kernel with increasing values of variance σ provides coarser resolutions:</p><formula xml:id="formula_1" coords="3,234.41,288.14,246.18,23.89">G σ (i, j) = 1 2πσ exp - (i 2 + j 2 ) 2σ<label>(2)</label></formula><p>where (i, j) ∈ [-s, s] 2 , (2s + 1) × (2s + 1) being the size of the filter mask. By convolving the input image I with the kernel in Eq.2, we obtain following feature images:</p><formula xml:id="formula_2" coords="3,281.08,363.50,199.51,9.65">L σ = I * G σ<label>(3)</label></formula><p>By doing this for a set of scales {σ m } m∈{1,••• ,M } , we obtain feature images</p><formula xml:id="formula_3" coords="3,134.77,395.38,77.85,11.15">{L σm } m∈{1,••• ,M } .</formula><p>For each of these feature images, we compute its corresponding intensity histograms which gives us a set of descriptors</p><formula xml:id="formula_4" coords="3,394.04,408.38,84.91,11.34">{H gauss m } m∈{1,••• ,M } .</formula><p>log-Gabor statistical descriptors A common approach to characterize textures is to use a bank of Gabor filters which permit to analyze different part of the frequency spectrum for different orientations <ref type="bibr" coords="3,363.23,463.63,9.96,8.74" target="#b2">[3]</ref>. As suggested in <ref type="bibr" coords="3,452.33,463.63,9.96,8.74" target="#b3">[4]</ref>, we use a logarithmic variant of these filters which have a Gaussian response when viewed on a logarithmic frequency scale. The transfer function of the log-Gabor filter is defined as:</p><formula xml:id="formula_5" coords="3,243.53,507.78,237.06,24.80">G(f ) = exp - (log(f /f 0 )) 2 (log(κ/f 0 )) 2<label>(4)</label></formula><p>where f 0 is the center frequency of the sinusoid and κ is a scaling factor of the bandwidth. To cover the whole frequency spectrum, a range of different scales and orientations must be considered while designing the filter bank. After convolving the input image I with the corresponding even-symmetric and odd-symmetric filters for a certain scale σ and orientation θ we obtain pairs if response images I e σ,θ and I o σ,θ . From these responses, we compute the corresponding amplitude and phase:</p><formula xml:id="formula_6" coords="3,252.64,635.80,227.95,12.55">A σ,θ = (I e σ,θ ) 2 + (I o σ,θ ) 2<label>(5)</label></formula><formula xml:id="formula_7" coords="3,253.47,654.05,227.12,12.69">Φ σ,θ = arctan I e σ,θ , I o σ,θ<label>(6)</label></formula><p>By doing this for a set of scales σ p∈{1,••• ,P } and orientations θ q∈{1,••• ,Q} , we obtain a set of feature images A σp,θq , Φ σp,θq p,q . For each of them, we compute the corresponding intensity histogram which finally gives us the descriptors H gabor p,q p,q .</p><p>Phase congruency statistical descriptors Phase congruency is an absolute measure of feature significance which is invariant to changes in illumination or contrast <ref type="bibr" coords="4,174.65,212.43,9.96,8.74" target="#b4">[5]</ref>. It is closely related to the concept of local energy model which postulates that significant features are perceived at points of maximum phase congruency. The phase congruency function is defined as:</p><formula xml:id="formula_8" coords="4,276.85,255.78,199.50,24.72">P C = E σ A σ (<label>7</label></formula><formula xml:id="formula_9" coords="4,476.35,262.52,4.24,8.74">)</formula><p>where E is the local energy and A σ the Fourier amplitudes. It is possible to approximate the function above by using quadrature filters such as the log-Gabor filters described in the previous part. Indeed, by using the response images I e σ,θ and I o σ,θ , we can approximate the phase congruency for a certain orientation as follows:</p><formula xml:id="formula_10" coords="4,227.67,352.68,252.92,35.73">P C θ = ( σ I e σ,θ ) 2 + ( σ I o σ,θ ) 2 σ (I e σ,θ ) 2 + (I o σ,θ ) 2<label>(8)</label></formula><p>For different orientations θ t∈{1,••• ,T } , we obtain the feature images</p><formula xml:id="formula_11" coords="4,416.40,398.49,75.21,11.15">{P C θt } t∈{1,••• ,T } .</formula><p>For each of them, we finally compute the corresponding intensity histogram which gives us the descriptors</p><formula xml:id="formula_12" coords="4,268.15,421.42,69.25,12.13">{H pc t } t∈{1,••• ,T } .</formula><p>Global descriptors After computing all these descriptors for an input image I, a global descriptor x is created by concatenating all histograms:</p><formula xml:id="formula_13" coords="4,191.45,483.47,289.14,43.65">x =    • • • H gauss m • • • scale-space , • • • H gabor p,q • • • log-Gabor , • • • H pc t • • • phase-congruency    T<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Different multi-class classification approaches</head><p>We investigated several approaches to classify images according to the descriptors we described in the previous section.</p><p>Simple Multi-Class SVM From a set of images and their corresponding labels, we compute their descriptors and generate a training set {x n , y n } n∈{1,••• ,N } .</p><p>Recall the classifcation problem statement, we are looking for a function Ψ such that:</p><formula xml:id="formula_14" coords="4,241.38,656.12,239.21,9.65">Ψ (x n ) = y n , ∀n ∈ {1, • • • , N }<label>(10)</label></formula><p>Let us first consider a 2 class classification problem where y n ∈ {-1, 1}. We model the function Ψ as:</p><formula xml:id="formula_15" coords="5,259.05,147.97,217.11,8.74">Ψ (x) = w , ϕ(x) + b (<label>11</label></formula><formula xml:id="formula_16" coords="5,476.16,147.97,4.43,8.74">)</formula><p>where ϕ is a non-linear mapping from the space spanned by our descriptors into a hidden feature space with higher dimensionality, w is a weighting vector and b a bias. w, ϕ and b define a hyperplane separating the different classes in the high-dimensional space. To find the optimal hyperplane, we maximize the margin between the classes and this, with a minimum of classification errors <ref type="bibr" coords="5,467.31,212.81,9.96,8.74" target="#b5">[6]</ref>. This can be written as the dual convex optimization problem:</p><formula xml:id="formula_17" coords="5,224.71,239.18,255.88,35.58">minimize 1 2 w 2 subject to: y n ( w , ϕ(x n ) + b) ≥ 1 (12)</formula><p>Since the problem may not be separable, we allow misclassification errors by introducing slack variables that induce a soft margin:</p><formula xml:id="formula_18" coords="5,213.73,309.43,262.44,45.05">minimize 1 2 w 2 + C N n=1 ξ n subject to: y n ( w , ϕ(x n ) + b) ≥ 1 -ξ n (<label>13</label></formula><formula xml:id="formula_19" coords="5,476.16,327.51,4.43,8.74">)</formula><p>where C is a tradeoff parameter weighting the impact of the errors and thus the flexibility of the model. After solving the dual, it can be shown that a solution w opt of this minimization is always a linear combination of the training vectors {x n } n with weights {α n } n∈{1,...,N } :</p><formula xml:id="formula_20" coords="5,262.93,414.67,213.23,30.20">w opt = N n=1 α n ϕ(x n ) (<label>14</label></formula><formula xml:id="formula_21" coords="5,476.16,425.08,4.43,8.74">)</formula><p>which leads to the following model for Ψ :</p><formula xml:id="formula_22" coords="5,185.62,466.49,290.54,30.20">Ψ (x) = N n=1 α i ϕ(x n ) • ϕ(x) + b = N n=1 α i K(x n , x) + b, (<label>15</label></formula><formula xml:id="formula_23" coords="5,476.16,476.90,4.43,8.74">)</formula><p>where K is the kernel associated to ϕ in the higher dimensional space. To handle complex non-linear relations between the descriptors and their labels, K is chosen as a RBF kernel, leading to the following decision function:</p><formula xml:id="formula_24" coords="5,220.19,542.22,255.97,30.20">Ψ (x) = N n=1 α n exp -γ x n -x 2 + b. (<label>16</label></formula><formula xml:id="formula_25" coords="5,476.16,552.63,4.43,8.74">)</formula><p>Now that we have a decision function for a binary classification problem, we need to extend it to multi-class. Two common methods are to combine several binary SVM into a multi-class model:</p><p>• One-vs-one: binary classifiers are built for each combination of classes • One-vs-rest: a binary classifier is built for each class against all the others Since the one-vs-rest combination is less suitable in terms of class balancing, we choose a one-vs-one approach as suggested by <ref type="bibr" coords="5,337.64,656.12,9.96,8.74" target="#b6">[7]</ref>.</p><p>Multi-Kernel Multi-Class SVM In the previous section, the descriptors x are a concatenation of different feature histograms. Each of these feature types spans its own space and may live in a different subspace. For this reason, it is advantageous to use a kernel for each feature type that is especially adapted to the features. This leads to a set of P decision functions where P is the number of feature types considered:</p><formula xml:id="formula_26" coords="6,185.75,196.10,290.41,30.20">Ψ (p) (x (p) ) = N n=1 α (p) n exp -γ (p) x (p) n -x (p) 2 + b (p) . (<label>17</label></formula><formula xml:id="formula_27" coords="6,476.16,206.51,4.43,8.74">)</formula><p>where x (p) is the descriptors of type p. The global decision function would be then a combination of each "specialized" decision function. In this paper, we investigate two combination approaches:</p><formula xml:id="formula_28" coords="6,213.89,276.82,266.71,30.20">linear multi-SVM: Ψ (x) = 1 P P p=1 Ψ (p) (x (p) )<label>(18)</label></formula><p>product multi-SVM:</p><formula xml:id="formula_29" coords="6,304.27,313.01,176.32,30.20">Ψ (x) = P p=1 Ψ (p) (x (p) )<label>(19)</label></formula><p>which we respectively call linear and product Multi-SVM in the remaining of this paper.</p><p>Random Forests Random forests are basically a ensemble of T independent random trees we denote Θ (t) t∈{1,••• ,T } which vote for the most popular class <ref type="bibr" coords="6,134.77,418.59,9.96,8.74" target="#b7">[8]</ref>. Each tree Θ (t) can be seen as an ensemble of random split functions which partition the feature space and give an estimate of the posterior probability distribution P (C k |x, Θ (t) ) for each class C k . The forest F = Θ (1) , • • • , Θ (T ) combines the trees estimates as follows:</p><formula xml:id="formula_30" coords="6,241.87,469.85,238.72,30.20">P (C k |x) = 1 T T t=1 P (C k |x, Θ (t) )<label>(20)</label></formula><p>The class of an unseen point is then determined by using a maximum a posteriori criterion:</p><formula xml:id="formula_31" coords="6,261.09,531.31,219.49,16.60">y = argmax k (P (C k |x))<label>(21)</label></formula><p>A clear advantage of random forests over the SVM approaches is their inherent multi-class characteristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>In this section, we compare the different approaches we decribed above by performing a 10-folds cross-validation on a set of 2390 labeled images distributed over the classes as follows:</p><p>• CT: Computerized tomography (314 images)</p><p>• GX: Graphics, typically drawing and graphs, (355 images)</p><p>• MR: Magnetic resonance imaging (299 images)</p><p>• NM: Nuclear Medicine (204 images)</p><p>• PET: Positron emission tomography including PET/CT (285 images)</p><p>• PX: optical imaging including photographs, micrographs, gross pathology etc (330 images) • US: ultrasound including (color) Doppler (307 images) • XR: x-ray including x-ray angiography (296 images) For each test, classes are rebalanced by using random subsampling. A gridsearch has been performed to find the best hyperparameters as shown on Fig. <ref type="figure" coords="7,472.33,244.88,4.13,8.74" target="#fig_1">2</ref>. For the SVM-based classifiers, the tradeoff parameter C and γ are optimized. Concerning the random forests, we are looking for the best number of trees and the best number of random tests at each node. Note that features are scaled so that each dimension is in the range [0, 1]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>To evaluate the different approaches, we compute several quality measures from the confusion matrix: overall accuracy, precision, recall, f-measure and error rate. Overall results summarized in Tab.1 show that the simple SVM and random forests perform better than the multi-SVM approaches. This is contrary to what could be expected from these methods designed to capture different aspects of the features. Note that in the multi-SVM approaches, each of the SVM was trained independently. A joint training of all kernels, called Multiple-Kernel Learning (MKL) <ref type="bibr" coords="7,168.94,620.25,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="7,181.12,620.25,11.62,8.74" target="#b9">10]</ref>, may perform better.</p><p>As feature representation, we relied on global statistical descriptors extracted from the textural information contained in the images from the 8 different modalities. To extract this textural information we used classical filters which are not especially adapted to the high variability we can observe in the different classes.</p><p>Thus, the resulting texture information may not be discriminative enough. Indeed, some images such as charts or drawing present almost no textural information. Moreover, by computing global statistics on these features, a lot of information which might be usefull for classification is discarded. In the present work, we did not make use of any RGB information, this would have definitely helped to discriminate images from classes which do not contain any color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the present work, our goal was to get a first insight into the ImageCLEF challenge by taking part to its new subtask modality classification. We presented different approaches for the classification of medical images into 8 imaging modalities. As a feature representation, we extracted global statistics computed from the textural information contained in the images. We then investigated four classification methods based on SVMs and random forests. Approaches based on simple SVM and random forests give the best performance and achieve respectively an overall f-measure of 74.13% and 73.59%. After analyzing our preliminary results, there is clearly a lot of room for improvement concerning our feature representation. In future work, we will focus on defining new sparse and more discriminative representations of the data which should lead to better classification results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,307.97,345.82,7.89;2,134.77,318.95,123.97,7.86;2,143.89,207.79,102.40,75.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of images taken from the training set showing a very high variability and different kinds of anatomy</figDesc><graphic coords="2,143.89,207.79,102.40,75.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,455.51,345.82,7.89;7,134.77,466.49,166.26,7.86;7,156.95,324.37,147.20,106.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Grid-search cross-validation to find the best hyperparameters for the single SVM (left) and the random forest (right)</figDesc><graphic coords="7,156.95,324.37,147.20,106.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,169.89,117.75,275.58,87.54"><head>Table 1 .</head><label>1</label><figDesc>Classification evaluation over all modality classes</figDesc><table coords="8,169.89,117.75,275.58,66.67"><row><cell cols="5">Overall Classification Results in %</cell><cell></cell></row><row><cell></cell><cell cols="5">Accuracy Precision Recall F-measure Error rate</cell></row><row><cell>Simple SVM</cell><cell>93.53</cell><cell>74.13</cell><cell>74.7</cell><cell>74</cell><cell>25.88</cell></row><row><cell>Multi SVM linear</cell><cell>91.31</cell><cell cols="2">65.25 69.99</cell><cell>65.11</cell><cell>34.75</cell></row><row><cell cols="2">Multi SVM product 91.34</cell><cell cols="2">65.38 69.79</cell><cell>65.33</cell><cell>34.63</cell></row><row><cell>Random Forest</cell><cell>93.45</cell><cell cols="2">73.81 74.78</cell><cell>73.59</cell><cell>26.19</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,568.94,342.24,7.86;8,146.91,579.90,333.68,7.86;8,146.91,590.86,103.55,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,146.91,579.90,229.39,7.86">Overview of the CLEF 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,382.90,579.90,97.69,7.86;8,146.91,590.86,16.79,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,601.88,342.24,7.86;8,146.91,612.84,293.29,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,344.46,601.88,136.13,7.86;8,146.91,612.84,90.95,7.86">Uniqueness of the gaussian kernel for scale-space filtering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Babaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Baudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,244.29,612.84,171.15,7.86">IEEE Trans. Pattern Anal. machine Intell</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,623.86,342.24,7.86;8,146.91,634.82,105.49,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,255.96,623.86,221.51,7.86">Unsupervised texture segmentation using Gabor filters</title>
		<author>
			<persName coords=""><surname>Ak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Farrokhnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,634.82,79.80,7.86">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,645.84,342.24,7.86;8,146.91,656.80,313.81,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,202.42,645.84,278.17,7.86;8,146.91,656.80,101.75,7.86">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,254.99,656.80,179.49,7.86">Journal of The Optical Society of America A</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.25,7.86;9,146.91,130.63,136.69,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,189.28,119.67,159.32,7.86">Image Features From Phase Congruency</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kovesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,355.83,119.67,124.77,7.86;9,146.91,130.63,62.55,7.86">Videre: A Journal of Computer Vision Research</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,141.59,294.52,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,252.96,141.59,98.08,7.86">Support Vector Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,358.49,141.59,53.14,7.86">Mach. Learn</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,152.55,342.24,7.86;9,146.91,163.51,228.98,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,251.67,152.55,228.92,7.86;9,146.91,163.51,35.31,7.86">A comparison of methods for multi-class support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,189.72,163.51,157.68,7.86">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,174.47,226.41,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,198.13,174.47,140.89,7.86">Random Forests, Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,185.43,342.24,7.86;9,146.91,196.39,312.47,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,328.66,185.43,151.93,7.86;9,146.91,196.39,93.44,7.86">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,247.48,196.39,186.17,7.86">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,207.34,337.98,7.86;9,146.91,218.30,333.67,7.86;9,146.91,229.26,18.43,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,454.47,207.34,26.12,7.86;9,146.91,218.30,206.34,7.86">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,360.43,218.30,115.90,7.86">J.Machine Learning Research</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
