<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.24,152.01,318.90,12.53;1,156.00,170.49,295.23,12.53;1,250.56,188.73,105.97,12.53">Medical Case-based Retrieval by Leveraging Medical Ontology and Physician Feedback: UIUC-IBM at ImageCLEF 2010</title>
				<funder ref="#_mYnRbWX">
					<orgName type="full">Alfred P.</orgName>
				</funder>
				<funder ref="#_yTAATda #_C2JS8Tx">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_bNCdbfW">
					<orgName type="full">IBM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.08,229.04,66.61,9.07"><forename type="first">Parikshit</forename><surname>Sondhi</surname></persName>
							<email>sondhi1@illinois.edu</email>
						</author>
						<author>
							<persName coords="1,252.96,229.04,46.79,9.07"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
							<email>jimeng@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.16,229.04,71.19,9.07"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@illinois.edu</email>
						</author>
						<author>
							<persName coords="1,387.83,229.04,71.99,9.07"><forename type="first">Robert</forename><surname>Sorrentino</surname></persName>
							<email>sorrentino@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.04,240.56,62.62,9.07"><forename type="first">Martin</forename><forename type="middle">S</forename><surname>Kohn</surname></persName>
							<email>marty.kohn@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.08,240.56,81.26,9.07"><forename type="first">Shahram</forename><surname>Ebadollahi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.60,240.56,37.36,9.07"><forename type="first">Yanen</forename><surname>Li</surname></persName>
							<email>yanenli2@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.24,152.01,318.90,12.53;1,156.00,170.49,295.23,12.53;1,250.56,188.73,105.97,12.53">Medical Case-based Retrieval by Leveraging Medical Ontology and Physician Feedback: UIUC-IBM at ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3866AD236CB292760B4CED7F93737F68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical information retrieval</term>
					<term>Language model</term>
					<term>Medical ontology</term>
					<term>MeSH</term>
					<term>UMLS</term>
					<term>Feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the experiment results of the UIUC-IBM team in participating in the medical case retrieval task of ImageCLEF 2010. We experimented with multiple methods to leverage medical ontology and user (physician) feedback; both have worked very well, achieving the best retrieval performance among all the submissions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Text Information Management group at the University of Illinois at Urbana-Champaign and the Healthcare Transformation group at IBM TJ Watson Research Center collaborated in participating in the medical case retrieval task of ImageCLEF 2010. This paper is a report of our experiments and findings based on preliminary analysis of the results of our submissions.</p><p>The medical case retrieval task involved searching medical literature to find cases similar to a sample case specified in a query case. The query case provided a text description of a patient's background, symptoms and relevant test findings as well as a set of images such as CT scans. The following are text descriptions in two representative queries with different lengths: Topic 17: "Female patient, 25 years old, with fatigue and a swallowing disorder (dysphagia worsening during a meal). The frontal chest X-ray shows opacity with clear contours in contact with the right heart border. Right hilar structures are visible through the mass. The lateral X-ray confirms the presence of a mass in the anterior mediastinum. On CT images, the mass has a relatively homogeneous tissue density." Topic 18: "Pain and incapacity to move after an accident. Slight deformation can be seen in the x-ray."</p><p>Their corresponding images are shown in Figure <ref type="figure" coords="2,318.49,208.40,3.77,9.07" target="#fig_0">1</ref>. The document collection is a set of literature articles published in Radiology and Radiographics. Each article also includes the text of the captions and a link to the html of the full text articles. Images from these articles are also provided. The retrieval task is to run a case query on this data set to retrieve all the similar cases to the query case from this set of articles. For this task, a "case" is regarded as equivalent to an article that covers a medical case. Thus from computational perspective, we can simply treat each article as a unit and cast the task as one to rank all the articles based on a query case, much similar to an ordinary text retrieval problem. A ranked list of up to 1,000 articles (i.e., cases) can be submitted for each query case, which would then be evaluated using standard retrieval measures. More detailed descriptions of this task and its design can be found at the overview paper by the organizers <ref type="bibr" coords="2,257.45,445.28,10.52,9.07" target="#b0">[1]</ref>.</p><p>Participants of this task in the past have found that although images are provided, matching cases solely based on text information seems to be not only sufficient, but also performs very well (see, e.g., <ref type="bibr" coords="2,242.40,485.84,10.43,9.07" target="#b1">[2]</ref>), thus we have focused on using only text information to perform medical case retrieval. Our goal of participation is two-fold: First, we would like to see how well a well-tuned state-of-the-art text retrieval model would work for this task. Second, we would like to see whether we can improve the state-of-the-art retrieval models by leveraging medical ontology and user (physician) feedback. Preliminary analysis of our experiment results shows that a standard retrieval model works reasonably well for this task, and both medical ontology and physician feedback can further improve retrieval accuracy over a standard state-of-the-art retrieval model.</p><p>In the rest of this paper, we first describe the retrieval methods we used in producing our runs, particularly how we leverage medical ontology and incorporate physician feedback, and then discuss our experiment results.</p><p>As our first line of experiments, we analyzed the performance of state of the art general retrieval methods for this task. This helped us assess the utility of such methods for the task and also obtain a strong baseline method for our further experiments with techniques to leverage medical ontology and physician feedback. These baseline experiments also allowed us to identify the differences between general retrieval and medical case retrieval and provide insights about how to address these differences by extending the standard state of the art retrieval methods. These extensions will be described in detail in the subsequent sections. For standard retrieval models, we used their implementations provided in the LEMUR retrieval toolkit (http://www.lemurproject.org/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KL-divergence Retrieval Model with Dirichlet Smoothing</head><p>Language modeling provides a systematic framework for designing retrieval models. One of the best-performing retrieval models based on language modeling is the Kullback-Leibler (KL) divergence retrieval model <ref type="bibr" coords="3,293.00,341.36,10.68,9.07" target="#b2">[3]</ref>. Given a query Q and a document D, this model would first estimate a unigram query language model  Q (i.e., a word distribution) based on a given query and a document language model  D for document D, and then score the document D with respect to query Q based on negative KL-divergence between the two language models, -D( Q || D ), defined below:</p><formula xml:id="formula_0" coords="3,204.72,408.48,193.12,27.44">-( || ) = - ( | ∈ ) ( | ) ( | )</formula><p>where V is the set of words in our vocabulary, and p(w|  Q ) and p(w|  D ) are the probabilities of word w given by the two language models, respectively. The negative KLdivergence intuitively measures the similarity of the query language model and the document language model, thus would favor a document with more query words.</p><p>The document language model p(w|  D ) is usually estimated using Dirichlet prior smoothing, which often performs the best <ref type="bibr" coords="3,290.65,507.44,10.90,9.07" target="#b2">[3]</ref>:</p><formula xml:id="formula_1" coords="3,238.80,524.48,136.09,26.37">( | ) = ( , ) + ( | ) | | +</formula><p>Where c(w,D) is the count of word w in document D, p(w|C) is a background/reference language model estimated based on all the documents in the collection and helps providing probabilities for words unseen in a document, and  is a smoothing parameter, which was tuned using last year's dataset with 5 queries. The optimal value was set at  = 4800.</p><p>The simplest way to estimate the query model  Q is to set p(w|  Q ) to the relative frequency of a word in the query: = Since this approach assigns zero probability to words not in the query, a potentially better way to estimate this model is to use a technique called pseudo relevance feedback, which we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo Relevance Feedback</head><p>Pseudo relevance feedback is a standard approach meant for improving retrieval performance via query expansion. It assumes top N documents in the ranked list generated by the baseline method as relevant and then picks a set of keywords from those documents and adds them to the query. Although not all the top-ranked documents are relevant, they do resemble relevant documents and often can suggest useful related terms to the query to expand and enrich a query representation. In general, such methods would pick terms that are far more common in the top-ranked documents in an initial retrieval result but not very common in the whole collection. With this strategy, we can estimate p(w|  Q ) based on both the query and the top-ranked documents. In our experiments, we used the mixture model approach described in <ref type="bibr" coords="4,247.18,333.68,10.52,9.07" target="#b3">[4]</ref>, which is one of the best-performing state of the art approaches to pseudo feedback. This approach is available in the Lemur toolkit that we used.</p><p>The mixture model pseudo feedback method has a few parameters, which we tuned using the 5 queries of last year's dataset. The best results were found when the number of documents used for feedback was set to top two documents.</p><p>Based on experiment results with last year's data set, we found that pseudo feedback generally improves performance over the simple relative frequency estimation method, though the improvement was largely variable. This is to be expected since with pseudo feedback, we simply blindly assumed the top ranked documents are relevant; in reality, these top-ranked documents are unlikely all relevant and may be distracting, thus hurting performance. Nevertheless, we decided to use the combination of Dirichlet prior smoothing with mixture model for pseudo feedback as our baseline method, which represents the best we could achieve with a state-of-the-art retrieval model out of the box from an existing retrieval toolkit with parameters tuned based on last year's data set. As we will discuss later, this baseline run actually worked very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Understanding the Challenges</head><p>From our experiments with the existing state of the art retrieval methods, we observed certain weaknesses that make them unsuitable for the case retrieval task. In this section, we discuss them in detail. In particular we focus on the unique characteristics that differentiate medical case retrieval from general retrieval.</p><p>We realized that the performance dropped for the following reasons:</p><p>Vocabulary Gap: Medical domain uses a highly specialized language, involving long multi-word expressions, term-order variations and abbreviations etc. It is often the case that, the same medical concept may have many different keyword variations. As a result, the keywords used in the query do not exactly match the conceptually similar, but morphologically different variants used in the documents. We term this as the vocabulary gap problem.</p><p>Non-Optimal Query Term Weighting: Case retrieval queries contain information regarding a patient's background, symptoms, any test results/observations etc. and are in general much longer than general search queries. As a result many of the query keywords are not very useful in identifying a case. The primary heuristic used in a standard retrieval model for judging the query keyword importance is based on the Inverse Document Frequency (IDF) (i.e., penalizing a word in the collection) does not seem to work well in this case. Based on this insight, we thus proposed to weigh keywords based on their semantic categories. The weight of keywords should also account for their semantic categories. Keywords belonging to certain categories like disease names (eg. cancer, diabetes etc.), symptoms (eg. dry cough, headache) etc. must be assigned high importance regardless of their IDF.</p><p>Missing Condition Names: Condition/disease names are usually the most discriminative keywords for finding similar cases. However such keywords representing potential diagnoses are often absent from the case descriptions provided as queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Leveraging Semantic Resources to Overcome the Challenges</head><p>Our subsequent experiments were aimed at overcoming the challenges described in the previous section. A major advantage in the biomedical domain is that a plethora of domain specific resources, such as the UMLS and MeSH ontologies, the MMTX toolkit etc. are available for language processing tasks. We believe that these can be used to address some of the limitations present in the general retrieval methods. We start by giving a brief description of these resources and then present our methods for leveraging them. The Specialist Lexicon provides a large English lexicon with an emphasis on biomedical words, including derivational knowledge. Tools have also been built around the UMLS to address terminological variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MeSH</head><p>MMTx: MetaMap Transfer (MMTx) is a tool to perform the task of mapping UMLS biomedical concepts and semantic groups to free text (http://mmtx.nlm.nih.gov/). The biomedical concepts used for mapping are taken from the Unified Medical Language System. The system is also capable of identifying multi-word expressions, synonyms, abbreviations, term variants and stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Keyword Weighing using UMLS Semantic Groups</head><p>In order to deal with the keyword weighing problem, we mapped the query keywords to UMLS semantic groups and then assigned weights based on the groups. Based on our analysis of all the semantic groups, we selected the following groups:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease or Syndrome, Body Part organ or organ component, Sign or Symptom, Finding, Acquired Abnormality, Congenital Abnormality, Mental or Behavioral Dysfunction, Neoplasm, Pharmacologic Substance</head><p>Query keywords belonging to each of these groups were assigned twice the weight of all other keywords. That is, their probabilities in the query language model are doubled. These groups were chosen specifically as most keywords belonging to these categories were found to be fairly discriminative while finding a relevant case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MeSH based Pseudo-Relevance Feedback</head><p>We note that physicians tend to use the following strategy to decide if a document is relevant to a given case:</p><p>1. Look at the available patient background, symptoms and test results information 2. Make a list of possible conditions based on the available information 3. All documents discussing those conditions have a high probability of being relevant</p><p>This approach gives an important insight. Keywords representing potential conditions, which are completely missing from our queries, are highly useful in identifying similar cases. Moreover, assuming the query case description is reasonably descriptive we can assume that there would only be a small number of conditions or potential diagnoses for that case. Thus if we can somehow guess these conditions and push up the documents that primarily talk about them, we should be able to improve performance.</p><p>This breaks down into two problems:</p><p>1. We need a way of knowing which conditions a given document talks about: Since each medical literature article indexed in Pubmed already has a set of MeSH terms assigned to it, we can easily filter out condition related MeSH terms to identify the prominent conditions the document talks about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We need some guess on what conditions the query case is likely to represent:</head><p>This is a harder problem and we deal with it in two ways. These are discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-N-based MeSH feedback</head><p>We make a list of all condition related MeSH terms present in the top N=10 documents in the initial ranked list generated by the baseline method. We then slightly reduce the weight of any documents below these top N that do not have any MeSH terms in common with this list. The method has an advantage in that it not only directly finds documents belonging to same conditions, but also it altogether avoids the problem of selecting and weighing the document keywords for pseudo relevance feedback. One limitation of the approach however is that we cannot re-rank the top N documents using it. We overcome this limitation in our second approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution based MeSH feedback</head><p>Let M be the set of all condition related MeSH terms. Then for a given query Q, the method works as follows:</p><p>1 i. For each query keyword q in S d :</p><p>1. If we have never encountered the keyword q in a document labeled with MeSH term m, then Score(m) = Score(m) +1 4. Sort all MeSH terms m in M in descending order of Score(m). 5. Select the top N=25 MeSH terms from the ranked list 6. Re-rank documents by reducing the weights of all documents not labeled with any of these N selected MeSH terms.</p><p>The intuition behind this method is that we assume that MeSH terms, whose documents contain a large number of query keywords, are more likely to represent the query. This approach is robust in that it takes into account the entire ranked list, rather than just the top ranked documents. It is therefore not affected by poor ranking generated by the baseline method. Additionally we are able to also re-rank the top results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approaches Utilizing Physician Feedback</head><p>Our next idea at dealing with the vocabulary gap problem was to let the doctors decide which keywords they considered most useful. Additionally what other related keywords they thought would be useful in detecting the right cases. This is to simulate an application scenario where the physician users would be able to use a search engine to reformulate the query with potentially more useful keywords.</p><p>We used these keywords to expand our queries. We assigned low weights to these keywords to prevent them from dominating the original keywords. We observed that this strategy helped improve performance across all queries. Additional keywords from doctors helped considerably in overcoming the vocabulary gap problem. In many cases the doctors provided condition/disease keywords representing potential diagnosis. This also helped greatly.</p><p>To further leverage feedback information from a user in an interactive retrieval system, we also experimented with relevance feedback, which is also a standard technique in improving search results. The idea is to ask a user to judge a small number of top-ranked documents as relevant or non-relevant, and the system could then use such judgments to improve the ranking of additional unseen documents for this user or improve ranking of all the documents for such a query case for future users. In our experiments, we asked two physicians to judge the top 20 documents and then used the same mixture model that we used for pseudo feedback to improve the estimation of query language model based on documents actually judged as relevant (as opposed to assuming top ranked documents to be relevant). However, our relevance feedback runs did not perform as well as we expected. This can be caused by multiple reasons, which we will further discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summarizing the Submitted Results</head><p>In this section we give a brief description of our runs and provide a preliminary analysis of the results. We submitted 10 runs; the first four runs (run IDs: 1-4) are completely automatic, while the rest six runs (run IDs: 5-10) utilized an additional set of manually provided keywords for query expansion. In addition, the last three runs (run IDs: 8-10) also utilize relevance feedback provided by the users. We now describe these runs in more detail. The order (sequence number) is the same as the run IDs that we used to label our runs (i.e., the first run described below is our submitted run with the run ID 1, and the second has the run ID 2, etc). The relations between these runs are shown in Figure <ref type="figure" coords="9,457.19,162.08,3.66,9.07" target="#fig_2">2</ref>.</p><p>1. 1276844704028__baselinefbsub: This was a basic retrieval run. It used the KLdivergence retrieval model with Dirichlet prior smoothing and pseudo-relevance feedback as described in Section 2. The performance of this run gives us a sense of what can be achieved using a well-tuned existing state-of-the-art method. feedback on top of Run 6. First, a set of results were generated using Run 6. A physician was then asked to identify the relevant documents among the top 20 results. The query was then re-executed using the same configuration as Run 6, but this time we used the documents labeled by the physicians for relevance feedback (instead of the pseudo-relevance feedback we were using earlier in Run 6). This run was to simulate "long-term relevance feedback". The rationale was that if one user submitted a query and clicked on certain relevant results, then we could collect the relevance judgments and learn from them to generate a better ranking for the same query in the future when a different user submits the same query (or a similar query). 9. 1276859235707__PhybaselineRelfbWMD_25_0.2sub: This run is similar Run 8 but the relevance feedback was applied on top of Run 7 instead of Run 6. Thus it was also to simulate "long-term relevance feedback". The main difference between Runs 8 and 9 is that Run 8 used top-N-based MeSH feedback whereas Run 9 used distributionbased MeSH feedback. Thus, comparing Run 8 and Run 9 can allow us to further compare the two MeSH feedback methods in the setting of relevance feedback. 10. 1276862759027__PhybaselineRelFbWMR_10_0.2_top20sub: Similar to Run 8, but this time we use the relevance judgments provided from the top 20 documents to rerank only the remaining documents. This was to simulate "short-term relevance feedback" (within the same search session). The rationale was that once we know a user found certain results as relevant, we can then simply use these judgments to generate a better ranking of the remaining (unseen) documents (i.e., documents originally ranked below top 20). In this case, the same user who provided feedback (i.e., relevance judgments) can benefit from his/her own judgments.  The performance figures of our 10 runs are shown in Table <ref type="table" coords="11,363.36,329.36,3.77,9.07" target="#tab_2">1</ref>. We observe that in terms of MAP, apart from Run 3 and Run 10, all other runs outperform the baseline run 1, indicating most of our extensions of the standard retrieval model are effective. Also, the manual runs usually perform better than the automatic runs, suggesting that the additional information obtained from the physicians is beneficial.</p><p>Looking at specific run comparisons, we can further draw the following conclusions:</p><p>1. UMLS-based keyword reweighting: Given that the performance of Run 4 based on all three metrics is better than Run 1, we can conclude that UMLS based keyword weighing is an effective strategy for improving retrieval performance for this task. 2. Top-N-based vs. distribution-based MeSH feedback: Given that the performance of Run 2 is better than Run 4, we can conclude that the top-N-based MeSH feedback method is effective and it can be added on top of UMLS-based keyword weighing to further improve performance. However, Run 3 is worse than Run 4, indicating that the distribution-based MeSH feedback method did not work well. These results, along with other results where we can compare these two feedback methods (i.e., Run 6 vs. Run 7; Run 8 vs. Run 9), all suggest that the top-N-based MeSH feedback method is more effective than the distribution-based feedback method. We suspect that the poor performance of distribution-based feedback may be due to the suboptimal setting of parameter values. We intend to further explore this issue in the future to better understand the cause. 3. Physician keywords: The runs 5, 6 and 7 perform considerably better than their corresponding baseline runs, i.e., Runs 4, 2, and 3. We can thus conclude that the additional keywords provided by physicians were greatly helpful. This result also provides us with some insight on how the case queries should be formulated. Apart from taking the actual case description as an input (something that was already available in the query), we can ask the users to provide a separate set of related keywords that they feel may be present in the relevant document. These can be assigned low weights (to avoid "query drift") and then used for query expansion. In other words, physicians can potentially formulate a more effective query than the case description in a current query. 4. Additive benefit of different heuristics: The continuous improvement in performance from run 1 to 4 to 2 to 6 shows that our strategies at dealing with different challenges continue to work well when combined. Specifically, UMLS keyword reweighting, top-N-based MeSH feedback, and physician keywords can be combined to achieve additive benefit, leading to the best performing Run 6. 5. Relevance feedback: The performance of relevance feedback based runs, 8 through 10, is lower compared to the corresponding baseline runs 6 and 7. This is a somewhat surprising result since in relevance feedback, we have available relevance judgments from the users for top-20 results which is a significant advantage over other runs. We thus expected that our relevance feedback runs would outperform all other runs. Our analysis suggests that in certain cases, the documents judged by our physician users as relevant were not judged as relevant in the official gold standard, and as a result, these relevance feedback runs have over-fitted the user-selected relevant documents, leading to inferior performance. The observation hints at a certain level of subjectivity being involved in the case retrieval problem. Due to the limited time available for preparing this paper, we did not have time to further analyze the reason, but this issue clearly warrants further experimentation, and we plan to explore it in the future.</p><p>We also did a comparison of our submitted runs with all the submissions from other participants of this task in terms of the Mean Average Precision (MAP) on all the topics and found that our 10 runs were ranked 1 through 7 and 9 through 11 among all the submissions. Additionally in 9 of the 14 topics, one of our runs performed the best among all the submissions. Figure <ref type="figure" coords="12,234.47,444.08,5.03,9.07" target="#fig_3">3</ref> shows the MAP scores of different teams. Runs 16 through 25, highlighted in red are ours while the remaining shown in blue are from other teams. One interesting observation is that our baseline run (i.e., Run 1, or 1276844704028__baselinefbsub) actually performed very well as compared with other submissions; indeed, only one run in the pool of submissions from other groups is slightly better than this run in MAP. Since this run represents a well-tuned standard retrieval model, this result suggests that such a well-tuned standard language modeling approach to retrieval remains a strong competitor for this task. Our overall superior performance has also clearly benefited from the use of this strong baseline method. However, it is very encouraging to see that several heuristic extensions that we developed can further outperform this strong baseline method, suggesting that there is clearly potential to further improve a state of the art general retrieval method for this special retrieval task by leveraging domain knowledge and user feedback. In this paper we described the details of our participation in the ImageClef 2010 medical case retrieval task. Our focus was primarily at identifying the major challenges arising from the differences between general retrieval and medical case retrieval, and then at developing methods for addressing them. We observed that taking into account the semantics of query keywords helped in assigning more appropriate keyword weights, and proposed a UMLS-based keyword reweighting strategy which is shown to be effective. We also proposed two novel methods (i.e., top-N-based and distribution-based) for leveraging MeSH terms to perform pseudo feedback and automatically re-rank documents.</p><p>The results show that the top-N-based method is more effective than the distribution-based method, and it can be combined with other heuristics to further improve retrieval accuracy. Finally we explored ways of dealing with the vocabulary gap issue, and found that additional related keywords provided by physician users can be used with low weights along with the original query case to greatly improve both precision and recall. However, while we expected relevance feedback to be beneficial, our results show that all the feedback runs were worse than their corresponding baseline runs, an issue to be further looked into. Overall, most of our strategies helped improve performance and our methods largely outperformed those other participating groups. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP Scores</head><p>Due to the time limit, we have not yet been able to conduct a thorough analysis of our experiment results and all the proposed methods. In the future, we will run additional experiments and perform more analysis of the proposed methods, in particular to better understand why relevance feedback did not help.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,188.64,309.61,230.04,8.21;2,185.52,237.36,102.48,60.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images for Topic 17 (left three) and Topic 18 (right one)</figDesc><graphic coords="2,185.52,237.36,102.48,60.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,143.21,468.56,201.65,9.07;7,139.44,480.08,259.02,9.07;7,139.44,491.36,120.79,9.07;7,175.44,502.88,264.47,9.73;7,175.44,514.40,255.83,9.73;7,175.44,525.92,139.12,9.73"><head></head><label></label><figDesc>. For every MeSH term m in M, set Score(m) =0 2. Retrieve a ranked list of all the documents L for the query Q 3. For each document d in L a. Identify the set of query keywords S d (subset of Q) found in d b. Identify the set of MeSH terms M d (subset of M) found in d c. For each MeSH term m in M d :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,121.44,582.97,364.38,8.21;10,121.44,593.05,294.72,8.21"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Dependency of different runs. Green boxes are the runs that improve MAP score from their parent runs. Red boxes are the runs that reduce MAP score from their parent runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,124.08,365.36,359.24,9.07"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MAP scores over all queries for all submitted runs. Our runs are highlighted in red</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,170.40,300.48,4.62,8.04;13,158.88,286.32,16.16,8.04;13,163.44,272.16,11.59,8.04;13,158.88,258.00,16.16,8.04;13,163.44,243.84,11.59,8.04;13,158.88,229.68,16.16,8.04;13,163.44,215.52,11.59,8.04;13,158.88,201.60,16.16,8.04;13,163.44,187.44,11.59,8.04;13,184.80,313.79,285.96,7.13;13,142.43,284.80,8.89,20.24;13,142.43,259.66,8.89,23.29;13,142.43,238.82,8.89,18.94;13,142.43,226.96,8.89,9.92;13,142.43,193.49,8.89,31.14"><head></head><label></label><figDesc>13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 MAP Score over all queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,121.44,538.88,364.83,106.99"><head></head><label></label><figDesc>Terms: Medical Subject Headings (MeSH) is the U.S. National Library of Medicine's controlled vocabulary used for indexing articles for MEDLINE/PubMed (http://www.nlm.nih.gov/mesh/). Each indexed research article is assigned a set of representative MeSH terms. MeSH terminology provides a consistent way to retrieve information that may use different terminology for the same concepts. MeSH database can be used to find Medical Subject Heading Terms and build a search strategy.</figDesc><table coords="5,121.44,613.76,364.83,32.11"><row><cell cols="8">(http://www.nlm.nih.gov/research/umls/). They are useful in investigating knowledge</cell></row><row><cell cols="8">representation and retrieval questions. UMLS contain three knowledge sources:</cell></row><row><cell cols="8">Metathesaurus, Semantic Network and Specialist Lexicon. The main component is the</cell></row><row><cell cols="8">Metathesaurus, which compiles and cross-references one hundred biomedical</cell></row><row><cell cols="8">terminologies (in version 2003AA: more than 800,000 concepts and 2,000,000 strings),</cell></row><row><cell cols="8">with their hierarchical and transversal relations. Its Semantic Network comprises 133</cell></row><row><cell cols="8">broad semantic groups and adds a common structure above these imported terminologies.</cell></row><row><cell cols="8">Unified Medical Language System: NLM's Unified Medical Language System (UMLS)</cell></row><row><cell cols="8">project develops and distributes multi-purpose, electronic "Knowledge Sources" and</cell></row><row><cell>associated</cell><cell>lexical</cell><cell>programs</cell><cell>for</cell><cell>system</cell><cell>developers</cell><cell>and</cell><cell>researchers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,121.44,225.44,364.95,354.19"><head></head><label></label><figDesc>2. 1276846614397__baselinefbWMR_10_0.2sub: This run added to Run 1 two additional heuristics: UMLS based keyword weighing and top-N-based MeSH feedback. For MeSH based re-ranking as discussed in section 4.2, the weights are reduced by 0.2. Comparing this run with Run 1 would allow us to see whether keyword weighting and top-N-based MeSH feedback are indeed effective. 3. 1276846564056__baselinefbWMD_25_0.2sub: This run is very similar to Run 2 except that we used the distribution-based MeSH feedback instead of the top-N-based MeSH feedback. For MeSH based re-ranking as discussed in section 4.2, the weights are reduced by 0.2. This run can be compared with Run 1 to see any improvement from keyword weighting and distribution-based MeSH feedback or compared with Run 2 to see which of the two MeSH feedback methods (i.e., top-N-based vs. This run added, on top of Run 3, additional keywords from physicians. Thus Runs 6 and 7 attempted to improve Runs 2 and 3, respectively, by adding additional keywords from physicians to further enrich the query. Meanwhile, comparing Run 6 and Run 7 can also further examine the relative effectiveness of the two MeSH feedback strategies. 8. 1276859628288__PhybaselineRelfbWMR_10_0.2sub: This run added relevance</figDesc><table coords="9,121.44,421.04,364.95,100.99"><row><cell>of keyword</cell></row><row><cell>reweighting.</cell></row><row><cell>5. 1276848633547__PhybaselinefbWsub: This run is similar to Run 4 but with</cell></row><row><cell>additional keywords from physicians included. When compared with 4, this run lets</cell></row><row><cell>us analyze the benefit from obtaining additional related query keywords from the</cell></row><row><cell>users.</cell></row><row><cell>6. 1276849026093__PhybaselinefbWMR_10_0.2sub: This run added, on top of Run 2,</cell></row><row><cell>additional keywords from physicians.</cell></row><row><cell>7. 1276850977593__PhybaselinefbWMD_25_0.2sub:</cell></row></table><note coords="9,139.44,351.92,130.43,9.07;9,121.44,363.44,364.32,9.07;9,139.44,374.96,346.31,9.07;9,139.44,386.48,346.71,9.07;9,139.44,398.00,346.62,9.07;9,139.44,409.52,346.51,9.07;9,139.44,421.04,296.10,9.07"><p>distribution-based) works better. 4. 1276846825574__baselinefbWsub: This run added to Run 1 only UMLS-based keyword reweighting. (In Runs 2 and 3, we added not only UMLS-based keyword reweighting, but also a MeSH feedback method.) Thus comparing this run with Run 1 can reveal any improvement from just using UMLS-based keyword reweighting, while comparing this run with Run 2 or Run 3 would allow us to see whether any of these MeSH feedback methods can further improve performance on top</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,131.04,150.56,331.64,155.74"><head>Table 1 .</head><label>1</label><figDesc>Summary of results for different runs</figDesc><table coords="11,131.04,173.77,331.64,132.53"><row><cell>Run</cell><cell>Type</cell><cell>Retrieval Methods</cell><cell>MAP for</cell><cell>Prec@10</cell><cell>Recall</cell></row><row><cell>ID</cell><cell></cell><cell></cell><cell>all</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>Automatic</cell><cell>Baseline Method</cell><cell>0.2754</cell><cell>0.4286</cell><cell>0.8081</cell></row><row><cell></cell><cell></cell><cell>(Standard Retrieval Model)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Automatic</cell><cell>Run 4 + topN-Mesh-FB</cell><cell>0.2902</cell><cell>0.4429</cell><cell>0.8464</cell></row><row><cell>3</cell><cell>Automatic</cell><cell>Run 4 + distr-Mesh-FB</cell><cell>0.2626</cell><cell>0.4</cell><cell>0.8464</cell></row><row><cell>4</cell><cell>Automatic</cell><cell>Run 1 + KeywordWeighting</cell><cell>0.2808</cell><cell>0.4429</cell><cell>0.8464</cell></row><row><cell>5</cell><cell>Manual</cell><cell>Run 4+ PhysicianKeywords</cell><cell>0.3441</cell><cell>0.4714</cell><cell>0.8618</cell></row><row><cell>6</cell><cell>Manual</cell><cell>Run 2+ PhysicianKeywords</cell><cell>0.3551</cell><cell>0.4714</cell><cell>0.8618</cell></row><row><cell>7</cell><cell>Manual</cell><cell>Run 3+PhysicianKeywords</cell><cell>0.3441</cell><cell>0.4714</cell><cell>0.8618</cell></row><row><cell>8</cell><cell>Manual</cell><cell>Run 6 + LongTermRelFB</cell><cell>0.3059</cell><cell>0.4571</cell><cell>0.8292</cell></row><row><cell>9</cell><cell>Manual</cell><cell>Run 7 + LongTermRelFB</cell><cell>0.2837</cell><cell>0.4571</cell><cell>0.8292</cell></row><row><cell>10</cell><cell>Manual</cell><cell>Run 6 + ShortTermRelFB</cell><cell>0.2713</cell><cell>0.4286</cell><cell>0.8292</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="3,313.44,623.68,18.00,6.67;3,315.84,634.72,9.64,6.19;3,331.44,627.92,2.52,9.07"><p>( , ) | | .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This paper is based upon work supported in part by an <rs type="funder">IBM</rs> <rs type="grantName">Faculty Award</rs>, an <rs type="funder">Alfred P.</rs> <rs type="grantName">Sloan Research Fellowship</rs>, and by the <rs type="funder">National Science Foundation</rs> under grants <rs type="grantNumber">IIS-0713581</rs> and <rs type="grantNumber">CNS-0834709</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bNCdbfW">
					<orgName type="grant-name">Faculty Award</orgName>
				</org>
				<org type="funding" xml:id="_mYnRbWX">
					<orgName type="grant-name">Sloan Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_yTAATda">
					<idno type="grant-number">IIS-0713581</idno>
				</org>
				<org type="funding" xml:id="_C2JS8Tx">
					<idno type="grant-number">CNS-0834709</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,134.40,306.73,351.30,8.21;14,134.88,317.05,350.74,8.21;14,134.88,327.37,20.25,8.21" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,426.00,306.73,59.70,8.21;14,134.88,317.05,152.93,8.21">Overview of the CLEF 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,304.73,317.05,123.32,8.21">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,136.32,343.69,349.66,8.21;14,134.88,354.01,350.97,8.21;14,134.88,364.57,23.38,8.21;14,183.84,364.57,302.07,8.21;14,134.88,374.89,237.01,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,448.80,343.69,37.18,8.21;14,134.88,354.01,350.97,8.21;14,134.88,364.57,19.49,8.21">Text-and Content-based Approaches to Image Retrieval for the ImageCLEF 2009 Medical Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2009/working_notes/CLEF2009WN-Contents.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,203.76,364.57,143.30,8.21">the Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,135.12,391.21,350.73,8.21;14,134.88,401.53,257.70,8.21" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="14,171.60,391.21,314.25,8.21;14,134.88,401.53,116.55,8.21">Statistical Language Models for Information Retrieval (Synthesis Lectures Series on Human Language Technologies)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,134.64,417.85,351.22,8.21;14,134.88,428.17,350.94,8.21;14,134.88,438.49,212.11,8.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,215.76,417.85,270.10,8.21;14,134.88,428.17,30.13,8.21">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,178.56,428.17,307.26,8.21;14,134.88,438.49,130.06,8.21">Proceedings of the Tenth ACM International Conference on Information and Knowledge Management (CIKM&apos;01)</title>
		<meeting>the Tenth ACM International Conference on Information and Knowledge Management (CIKM&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
