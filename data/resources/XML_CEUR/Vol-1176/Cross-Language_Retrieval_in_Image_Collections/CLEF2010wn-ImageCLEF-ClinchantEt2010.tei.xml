<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.67,100.02,385.66,12.62;1,95.52,117.95,411.97,12.62;1,231.06,135.88,140.87,12.62">XRCE&apos;s Participation in Wikipedia Retrieval, Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2010</title>
				<funder ref="#_FhG22hn">
					<orgName type="full">European</orgName>
				</funder>
				<funder ref="#_fPSW5Xx">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.79,173.56,85.24,8.74"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LIG</orgName>
								<orgName type="institution">Univ. Grenoble I</orgName>
								<address>
									<addrLine>BP 53</addrLine>
									<postCode>-38041</postCode>
									<settlement>Grenoble cedex 9, Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.92,173.56,70.75,8.74"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.22,173.56,65.31,8.74"><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.09,173.56,81.85,8.74"><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,461.49,173.56,31.71,8.74;1,181.98,185.52,43.09,8.74"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.63,185.52,61.33,8.74"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.11,185.52,89.11,8.74"><forename type="first">Keyvan</forename><surname>Minoukadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.67,100.02,385.66,12.62;1,95.52,117.95,411.97,12.62;1,231.06,135.88,140.87,12.62">XRCE&apos;s Participation in Wikipedia Retrieval, Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">51D28817064189464023D08FC5FA5213</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-modal Information Retrieval</term>
					<term>Image Modality Classification</term>
					<term>Medical Image Retrieval</term>
					<term>Wikipedia Retrieval</term>
					<term>Fisher Vector</term>
					<term>Lexical Entailment</term>
					<term>Query Expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year, XRCE participated in three main tasks of ImageCLEF 2010. The Visual Concept Detection and Annotation Task is presented in a separate paper. In this working note, we rather focus on our participation in the Wikipedia Retrieval Task and in two sub-tasks of the Medical Retrieval Task (Image Modality Classification and Ad-hoc Image Retrieval). We investigated mono-modal (textual and visual) and multi-modal retrieval and classification systems. For representing text we used either standard language model or a power law (log-logistic or smoothed power law) distribution-based information retrieval model. For representing images, we used Fisher Vectors improved by power and L2 normalizations and a spatial pyramid representation. With theses representations and simple linear classifiers we achieved excellent image modality classification both using mono-modal and combined textual and visual information. Concerning the retrieval performances, text based runs performed very well, but visual-only retrieval performances were in general poor showing that even state-of-the art image representations are insufficient to address these tasks accurately. However, we have shown that despite poor visual retrieval results, multimodal runs that combine both visual and textual retrieval scores, can outperform mono-modal systems as long as the information fusion is done appropriately. As a conclusion we can say that our participation in these tasks was successful, as the proposed systems obtained leading positions both in retrieval and modality classification and for each type of run: text, image or mixed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year, XRCE participated in three main tasks of ImageCLEF 2010. The Visual Concept Detection and Annotation Task is presented in a separate paper. In this working note, we rather focus on our participation in the Wikipedia Retrieval Task <ref type="bibr" coords="1,343.09,652.98,15.50,8.74" target="#b18">[19]</ref> and in two sub-tasks of the Medical Retrieval Task (Image Modality Classification and Ad-hoc Image Retrieval) <ref type="bibr" coords="1,414.48,664.93,14.61,8.74" target="#b14">[15]</ref>. In participating in these tasks, our first motivation was to investigate how well do our textual and visual retrieval and classification systems perform on Wikipedia and Medical Image Collections. Our second aim was to experiment with different information fusion strategies to cope with the multi-modal (textual and visual) aspect of the tasks.</p><p>Concerning the text-based retrieval, two information retrieval models were considered: a standard language model (similar to the techniques used in our past participation in other tasks of ImageCLEF <ref type="bibr" coords="2,145.47,103.05,15.50,8.74" target="#b13">[14]</ref>) and an information based model employing power law (log-logistic and smoothed power law) distribution proposed in <ref type="bibr" coords="2,249.39,115.01,9.96,8.74" target="#b2">[3]</ref>. These methods have shown high performances especially when combined with pseudo-relevance feedback or query expansion mechanisms were also applied.</p><p>In the case of the Medical Retrieval Task, our aim was to combine Lexical Textual Entailment inspired by Statistical Translation Models with query expansion mechanisms using external resources (in our case Wikipedia pages).</p><p>For representing images, we used the Improved Fisher Vectors (IFV) <ref type="bibr" coords="2,413.38,174.78,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="2,430.54,174.78,11.62,8.74" target="#b16">17]</ref>. Fisher Vectors have been successfully used in our previous ImageCLEF participations <ref type="bibr" coords="2,398.79,186.74,14.61,8.74" target="#b13">[14]</ref>, and for this challenge we used an improved version with power and L2 normalizations and a spatial pyramid representation as suggested in <ref type="bibr" coords="2,178.28,210.65,14.61,8.74" target="#b16">[17]</ref>. These IFVs were used as the image representation in all our tasks. They showed excellent performances in the Image Modality Classification Task both using them alone or combined with text representation. Concerning retrieval results, even with the latter recent advances in visual similarity <ref type="bibr" coords="2,218.17,246.52,14.60,8.74" target="#b17">[18]</ref>, using only images was not sufficient to address the Wikipedia and Medical Image Retrieval tasks. Indeed all visual-only runs performed poorly.</p><p>As the visual retrieval is an important element for the cross-modal similarities technique we used with success in the Photo Retrieval Tasks of past ImageCLEF sessions <ref type="bibr" coords="2,417.04,282.38,14.61,8.74" target="#b13">[14]</ref>, we did not experiment with these techniques for this year's challenges. We decided to use other score aggregation methods where we could easily handle the asymmetric roles of the two modalities during the fusion process. Hence we designed and compared several fusion strategies. We have shown that despite poor visual retrieval results, our proposed aggregation techniques were able to outperform both the image-only and text-only retrieval systems.</p><p>The rest of the paper is organized as follows. In Section 2 we describe our text retrieval and query expansion models. In Section 3 we briefly describe the image representation with the improved Fisher Vectors. In section 4 we present and compare different runs we submitted and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Retrieval</head><p>We start from a traditional bag-of-words representation of pre-processed texts where pre-processing includes tokenization, lemmatization, and standard stopword removal. However, in some cases lemmatization might lead to a loss of information. Therefore before building the bag-of-words representation we concatenated a lemmatized version of the document with the original document.</p><p>Two information retrieval models were considered: a standard language model (similar to the techniques used in our past participation in other tasks of ImageCLEF <ref type="bibr" coords="2,402.96,499.68,15.49,8.74" target="#b13">[14]</ref>) and an informationbased model employing a log-logistic distribution proposed in <ref type="bibr" coords="2,360.37,511.63,9.96,8.74" target="#b2">[3]</ref>. We also present a query expansion mechanism that appeared to be relevant to the Medical Retrieval Task. Finally, we briefly describe some details specific to the individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information Retrieval Techniques</head><p>To take into account the fact that one is comparing documents of different lengths, most IR models do not rely directly on the raw number of occurrences of words in documents, but rather on normalized versions of it. Language models for example use the relative frequency of words in the document and the collection<ref type="foot" coords="2,230.58,613.78,3.97,6.12" target="#foot_0">3</ref> :</p><formula xml:id="formula_0" coords="2,230.18,629.52,282.82,26.43">P (w|d) = λ x d w l d + (1 -λ) d x d w ) d l d (1)</formula><p>where x d w is the number of occurrences of word w in document d, l d is the length of d in tokens after lemmatization and C is the corpus. Then we can define the similarity between the query q = (q 1 , ..., q l ) and a document d using the cross-entropy function:</p><formula xml:id="formula_1" coords="2,201.93,707.70,311.07,19.61">CE(q|d) = qi P (q i |q) log( w P (q i |w)P (w|d))<label>(2)</label></formula><p>Other classical term normalization schemes include the well known Okapi normalization, as well as pivoted length normalization <ref type="bibr" coords="3,249.34,115.01,14.61,8.74" target="#b19">[20]</ref>. More recently, the concept of the amount of information brought by a term in a document has been considered in several IR models, inspired by the following observations made by Harter in <ref type="bibr" coords="3,236.93,138.92,14.61,8.74" target="#b10">[11]</ref>: the more a word deviates in a document from its average behavior given the collection, the more likely it is "significant" for this particular document. This can be easily captured in terms of information: if a word behaves in the document as expected in the collection, then it has a high probability p of occurrence in the document, according to the collection distribution, and the information it brings to the document, -log(p), is small. On the contrary, if it has a low probability of occurrence in the document, according to the collection distribution, then the amount of information it conveys is greater. This is the underlying idea of the information models proposed by Clinchant and Gaussier <ref type="bibr" coords="3,364.21,222.61,9.96,8.74" target="#b2">[3]</ref>: the log-logistic and smoothed power-law models.</p><p>These models are specified in three steps: the Divergence from Randomness (DRF) normalization of terms frequencies, the choice of a probability distribution to model these frequencies in the corpus and the mean information as the Relevance Score Vector (RSV). In the case of the log-logistic model, we have (for further details see <ref type="bibr" coords="3,310.26,282.38,10.30,8.74" target="#b2">[3]</ref>):</p><p>-DFR Normalization with parameter c:</p><formula xml:id="formula_2" coords="3,96.23,296.67,416.77,60.94">t d w = x d w log(1 + c avg l l d ) -T f w ∼ LogLogistic(λ w = Nw N ) -Ranking Model: RSV (q, d) = w∈q∩d x q w -log P (T f w &gt; t d w )<label>(3)</label></formula><p>where x d w and x q w are the numbers of occurrences of word w in document d and query q, N and N w are the numbers of documents in the corpus and the number of those containing w, avg l and l d are the average document length and the length of the document d.</p><p>In the case of the smoothed power-law model we have the same steps but the Relevance Score Vector in the Ranking Model is replaced by (see details in <ref type="bibr" coords="3,346.90,414.47,10.30,8.74" target="#b2">[3]</ref>):</p><formula xml:id="formula_3" coords="3,212.06,431.55,300.93,38.85">RSV (q, d) = w∈q∩d -x q w log( λ t d w t d w +1 w -λ w 1 -λ w ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lexical Entailment -Statistical Translation Model</head><p>Berger and Lafferty <ref type="bibr" coords="3,179.19,505.20,10.52,8.74" target="#b1">[2]</ref> addressed the problem of information retrieval as a statistical translation problem with the well-known noisy channel model. This model can be viewed as a probabilistic version of the generalized vector space model. The analogy with the noisy channel is the following one: To generate a query word, a word is first generated from a document and this word then gets "corrupted" into a query word. The key mechanism of this model is the probability P (v|u) that term u is "translated" by term v. These probabilities enable us to address a vocabulary mismatch, and some kinds of semantic enrichments. The problem now lies in the estimation of such probability models.</p><p>We refer here to a previous work <ref type="bibr" coords="3,248.81,600.84,10.52,8.74" target="#b3">[4]</ref> on lexical entailment models to estimate the probabilities. Lexical Entailment (LE) <ref type="bibr" coords="3,199.91,612.79,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,212.09,612.79,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="3,226.48,612.79,7.75,8.74" target="#b6">7]</ref> models the probability that one term entails another. It can be understood as a probabilistic term similarity or as a unigram language model associated to a word (rather than to a document or a query). Let u be a term in the corpus, then lexical entailment models compute a probability distribution over terms v of the corpus P (v|u). These probabilities can be used in information retrieval models to enrich queries and/or documents and to give a similar effect to use of a semantic thesaurus. However, lexical entailment is purely automatic, as statistical relationships are only extracted from the considered corpus. In practice, a sparse representation of P (v|u) is adopted, where we restrict v to be one of the N max terms that are the closest to u using an Information Gain metric<ref type="foot" coords="3,289.71,706.86,3.97,6.12" target="#foot_1">4</ref> .</p><p>To summarize our approach, documents containing images were extracted from the corpus and we computed a lexical entailment measure on this collection with the model GI-M3 as in <ref type="bibr" coords="4,489.68,115.01,10.52,8.74" target="#b3">[4]</ref> or using Chi-square statistics. Finally, we expanded the query terms with words that were the most similar to the latter in terms of the chosen lexical entailment measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ImageCLEF Task-specific Text Processing</head><p>In this section we give some further details on the text retrieval processes that were designed or applied to a given task.</p><p>Wikipedia The Wikipedia Corpus consisted of images with their captions extracted from Wikipedia in different languages namely French, English and/or German. In addition, the participants were provided with the original Wikipedia pages in one or several languages in wikitext format. Similarly the textual part of the query was multilingual.</p><p>For each image, in addition to the given metadata (image's captions), we extracted from the Wikipedia pages the paragraph of the Wikipedia page in which the image was mentioned. Thus, two indexes were built for the collection: one for the captions (metadata) of the images and one for the paragraphs. The combination of the retrieval scores based on those two types of text, was done by a simple late fusion (mean average) after having normalized the scores between 0 and 1.</p><p>To cope with the multilingual nature of the wikipedia collection, we adopted an early fusion approach that we previously experimented within CLEF'08 Adhoc <ref type="bibr" coords="4,388.33,353.41,9.96,8.74" target="#b4">[5]</ref>. This early fusion of text amounts to merging the different languages of a document into a single document. Different language representations of queries are also merged in a similar fashion. Multilingual queries are then matched with multilingual documents as if there was no difference between languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Task</head><p>In the Medical Retrieval Task, although participants were provided with articles containing the image, we used only the image captions to build the term frequencies and the distributions of these frequencies (see 2.1). Nevertheless, the text from the articles containing the image was used as a corpus during the topic enrichment step with the model described in 2.2.</p><p>Furthermore we also experimented with query expansion mechanisms using external data. Without such techniques, we would have vocabulary mismatches between queries and documents. For example, for the topic 16 'dermatofibroma' we found no document containing "dermatofibroma" as a word. The Lexical Entailment methods using the articles enabled us to address some of the issues, but the coverage of the retrieval system can be improved by using an external knowledge base. Ideally, the use of a thesaurus such as Mesh and the resources provides by UMLS would have been preferable. However, with the lacks of experience (this was our first participation in the Medical Task) and time for extracting useful information from these resources, we used our usual tools to improve the coverage for a given query with information extracted from Wikipedia pages.</p><p>To do so we proceeded as follows. For each query, a set of related pages in Wikipedia was found in order to cover all query terms:</p><formula xml:id="formula_4" coords="4,90.00,609.29,198.75,44.16">topic-1 | thoracic_aortic_dissection topic-2 | Acute_Myeloid_Leukemia topic-4 | congestive_heart_failure topic-5 | brachial plexus nerve_block</formula><p>We filtered redundant pages in order to have a unique coverage of query terms and to disambiguate some terms by taking the sense which was the most similar to the medical collection. Concerning the coverage, if the query contained a multiword expression related to a Wikipedia page, we used this page and not those related to the words within this multiword expression. For example, in topic 1, we can find the page "thoracic aortic dissection" in Wikipedia, then we use it and not the page "dissection". The disambiguation was done by computing a text similarity with the medical collection and by keeping the sense which had the higher score.</p><p>Then, for each of theses pages, we extracted only the hyperlinked text embedded in the body content. For each query, all those new terms were merged and we kept only the 20 terms that were the most frequent in the collection. This can be viewed as a query expansion mechanism provided by Wikipedia pages. Although, this expansion was expected to be noisy, it had the potential to improve recall. Finally, original textual runs were combined with these expanded query runs in order to maintain a high precision (see details in 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Representation</head><p>As for the image representation, we used an improved version <ref type="bibr" coords="5,368.17,221.96,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="5,385.33,221.96,12.73,8.74" target="#b16">17]</ref> of the Fisher Vector <ref type="bibr" coords="5,494.73,221.96,14.61,8.74" target="#b15">[16]</ref>. The Fisher Vector can be understood as an extension of the bag-of-visual-words (BOV) representation. Instead of characterizing an image with the number of occurrences of each visual word, it characterizes the image with the gradient vector derived from a generative probabilistic model. The gradient of the log-likelihood describes the contribution of the parameters to the generation process.</p><p>Assuming that the local descriptors I = {x t , x t ∈ R D , t = 1 . . . T } of an image I are generated independently by Gaussian mixture model (GMM) u λ (x) = M i=1 w i N (x|mu i , Σ i ), I can be described by the following gradient vector (see also <ref type="bibr" coords="5,316.07,319.24,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="5,333.23,319.24,11.62,8.74" target="#b15">16]</ref>):</p><formula xml:id="formula_5" coords="5,245.88,341.17,267.12,30.20">G I λ = 1 T T t=1 ∇ λ log u λ (x t )<label>(5)</label></formula><p>where λ = {w i , µ i , Σ i , i = 1 . . . M } are the parameters of the GMM. A natural kernel on these gradients is the Fisher Kernel <ref type="bibr" coords="5,222.67,395.26,14.61,8.74" target="#b11">[12]</ref>:</p><formula xml:id="formula_6" coords="5,157.06,417.45,355.94,13.38">K(I, J) = G I λ F -1 λ G J λ , F λ = E x∼u λ [∇ λ log u λ (x)∇ λ log u λ (x) ] .<label>(6)</label></formula><p>where F λ is the Fisher information matrix. As it is symmetric and positive definite, F -1 λ has a Cholesky decomposition F -1 λ = L λ L λ and K(I, J) can be rewritten as a dot-product between normalized vectors G λ with: G I λ = L λ G I λ . We will refer to G I λ as the Fisher Vector (FV) of the image I.</p><p>In the case of diagonal covariance matrices Σ i (we denote by σ 2 i the corresponding variance vectors), closed form formulas can be derived for <ref type="bibr" coords="5,134.33,517.86,14.76,8.74" target="#b16">[17]</ref>). As we do not consider G I While this representation was successfully used with the L1 normalization and L1 norm based similarity between the Fisher Vectors in our previous ImageCLEF participation <ref type="bibr" coords="5,438.53,559.66,14.61,8.74" target="#b13">[14]</ref>, this time we used an improved version of it with Power and L2 normalization and simple dot product similarity between them as suggested in <ref type="bibr" coords="5,223.40,583.57,14.61,8.74" target="#b16">[17]</ref>. In order to obtain these Improved Fisher Vectors we first use a Power Normalization with α = 0.5:</p><formula xml:id="formula_7" coords="5,90.00,500.79,423.00,25.80">G I w d i , G I µ d i , G I σ d i , for i = 1 . . . M , d = 1 . . . D (see details in</formula><formula xml:id="formula_8" coords="5,211.29,616.04,301.71,12.69">P N (G I λn ) = sign(G I λn )|G I λn | α , n = 1 . . . N.<label>(7)</label></formula><p>The aim of this normalization is to make the distribution of features in a given dimension n less peaky around zero. Then, these vectors are further L2 normalized to discard image-independent (i.e. background) information (see further details for both normalizations in <ref type="bibr" coords="5,425.56,664.61,14.76,8.74" target="#b16">[17]</ref>). Another improvement made to our Fisher Vector based image representation was the use of the spatial pyramid representation by Lazebnik et al . to take into account the rough geometry of a scene <ref type="bibr" coords="5,136.36,700.81,14.61,8.74" target="#b12">[13]</ref>. The main idea is to repeatedly subdivide the image and represent each layout as a concatenation of the representations (in our case Fisher Vectors) of individual sub-images. As we used three spatial layouts (1 × 1, 2 × 2, and 1 × 3), we obtained 3 image representations of respectively N , 4N and 3N dimensions.</p><p>As low level features we used our usual (see for example <ref type="bibr" coords="6,345.46,103.05,15.50,8.74" target="#b13">[14]</ref>) SIFT-like Orientation Histograms (ORH) and local color statistics (COL), i.e. local color means and standard deviations in the R,G and B channels, both extracted on regular multi-scale grids and reduced to 50 or 64 dimensional with Principal Component Analysis (PCA). With the three different spatial layouts mentioned above each image was finally represented by 6 different high level feature vectors (referred to as IFV for Improved Fisher Vector).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Wikipedia Retrieval</head><p>The Wikipedia Retrieval task consists of multilingual and multimedia retrieval. The collection contains images with their captions extracted from Wikipedia in different languages namely French, English and German. In addition, participants were provided with the original Wikipedia pages in wikitext format. The task consisted in retrieving as many relevant images as possible from the aforementioned collection, given a textual query translated in the three different languages and one or several query images.</p><p>We submitted different types of runs: mono-media runs and multimedia (mixed) runs with different fusion approaches.</p><p>Concerning the pure visual retrieval, we used 6 Improved Fisher Vector (IFV), corresponding to the two different low level features (ORH and COL) with the 3 spatial-layout (1x1, 2x2, 1x3) as described in section 3. The 6 IFVs were used independently to rank the Wikipedia images using the dot product as similarity measure and the 6 scores were weighted giving higher weights (0.7) to the IFV based on orientation histograms compared to the IFV based on color (0.3) before averaging them. The pure visual run results are mentioned in line 1 of Table <ref type="table" coords="6,424.19,400.41,3.87,8.74" target="#tab_0">1</ref>. The performances are very poor and this shows that even recent advances in visual similarities <ref type="bibr" coords="6,422.13,412.37,15.50,8.74" target="#b16">[17]</ref> are not sufficient to address the Wikipedia Retrieval task efficiently.</p><p>Pure text based retrieval performs much better. Details on the definition of the methods and the underlying retrieval models used are mentioned in section 2. Accordingly, from lines 2 to 4 of Table <ref type="table" coords="6,90.00,460.57,3.87,8.74" target="#tab_0">1</ref>, we report 3 pure text runs which are based on recent models presented in <ref type="bibr" coords="6,419.69,460.57,9.96,8.74" target="#b2">[3]</ref>. Line 2 is based on a smoothed power-law retrieval model while the latter two ones rely on a log-logistic probability distribution. It appeared that the smoothed power-law model performed better than the loglogistic ones, achieving 23.61% of Mean Average Precision (MAP). Nevertheless, in our combined modality runs, we used the former run when combining with visual retrieval. All text runs use both captions (metadata) and paragraphs as explained in section 2. However, the difference between the text runs LGD ALL METANOPRF PARAGPRF20 (line 3) and LGD ALL META PARAG (line 4) is that the former uses on the paragraphs, an additional pseudo-relevance feedback mechanism (PRF), introduced in <ref type="bibr" coords="6,184.57,556.21,9.96,8.74" target="#b2">[3]</ref>. This strategy appeared to be beneficial since the results obtained for the former run are better.</p><p>Runs reported from lines 5 to 16 are mixed, i.e. they make use of both visual and textual similarities. We grouped the runs according to the text run used in the combination. Consequently, from lines 5 to 10, we report the results of mixed runs that used the text run from line 3 (T2=LGD ALL METANOPRF PARAGPRF20) whereas from line 11 to line 16 we report results of the mixed runs that used the text run from line 4 (T3=LGD ALL META PARAG). The runs with a symbol (lines 5 and 12) in Table <ref type="table" coords="6,278.24,640.28,4.98,8.74" target="#tab_0">1</ref> were not submitted but we added them in order to have a better comparison between the results obtained using both aforementioned text runs.</p><p>As explained earlier, T2 gave a better result than T3. Moreover, whatever the combination technique we used, we always observe that the run using the PRF on the paragraphs dominates the other one. Therefore, we will only comment on the results of mixed runs reported from lines 5 to 10 in Table <ref type="table" coords="6,162.78,700.43,3.87,8.74" target="#tab_0">1</ref>. Similar conclusions can be deduced for the runs from line 11 to line 16.</p><p>From our past experiences with ImageCLEF <ref type="bibr" coords="6,300.93,712.77,14.61,8.74" target="#b13">[14]</ref>, we observed that textual similarity played a core role, but that it could be complemented with visual similarity if combined in an appropriate way. In other words, when combining image and text similarities in the context of multimedia retrieval, we paid attention to the fact that these two media should not be given, in general, symmetric roles during the fusion process.</p><p>Consequently, our first fusion strategy was to start with using the text similarities as a prefiltering stage before applying the visual similarities. In practice, we first selected the top 2000 images according to the textual similarities. Then, we re-ranked this pre-filtered top-list using the visual similarities only. This pre-filtering step based on textual similarities allowed us to significantly increase the pure visual run from 5.53% (line 1) to 18.05% of MAP (line 5). In particular, it gives better results than both image and text runs in terms of Precision at 20 (P@20). Still, this run entitled SEQ RERANKING-T2 (line 5) does not perform as well as pure textual similarities T2 (line 3) in terms of MAP.</p><p>Then, going beyond this simple re-ranking strategy, we investigated the combination of the re-ranked run SEQ RERANKING-T2, with the pure textual run T2. We basically normalized the two runs so that the scores were between 0 and 1 and we applied different aggregation functions. The different results are shown from line 6 to line 10.</p><p>Our first approach consisted in applying a simple weighted mean average operator. In that case, the submitted runs were given by the following aggregated score:</p><formula xml:id="formula_9" coords="7,245.71,305.41,263.04,11.72">w R s T RV (I) + w T s T R (I) (<label>8</label></formula><formula xml:id="formula_10" coords="7,508.76,307.49,4.24,8.74">)</formula><p>where I is an image of the collection, s T RV is the normalized score distribution based on the visual similarities but after having selected the top 2000 images according to the textual similarities, s T R is the normalized score distribution based on the textual similarities and w R and w T are their respective weights in the late fusion method. Lines 6, 8 and 10 with runs' titles beginning with FUSION are the results obtained with the following parameters: line 6 with w R = 0.5 and w t = 0.5; line 8 with w R = 0.7 and w t = 0.3; and line 10 with w R = 0.3 and w t = 0.7.</p><p>Our second fusion technique uses an aggregation operator that was introduced in <ref type="bibr" coords="7,475.86,415.64,9.96,8.74" target="#b0">[1]</ref>. This approach attempts to reinforce the aggregated score of an image assuming that there is a strong relationship between the two scores. In that case, the submitted run reported in line 7 of Table <ref type="table" coords="7,508.02,439.55,4.98,8.74" target="#tab_0">1</ref> (called CONFAGmin-T2) was given by the following aggregated score:</p><formula xml:id="formula_11" coords="7,205.94,474.70,307.06,9.65">s T RV (I) + s T R (I) + min(s T RV (I), s T R (I))<label>(9)</label></formula><p>Our last combination approach follows the idea that image similarities are less reliable then text similarities and when combining both of them one should adapt the combination weights accordingly. In that perspective, we propose to set the combination weights as functions of the visual similarity values. We particularly tested linear functions with w R (s T RV (I)) = αs T RV (I) and w T (s T Rv (I)) = 1 -αs T RV (I). This leads to the following aggregation method:</p><formula xml:id="formula_12" coords="7,216.39,567.47,292.19,11.72">α(s T RV (I)) 2 + (1 -αs T RV (I))s T R (I) (<label>10</label></formula><formula xml:id="formula_13" coords="7,508.58,569.54,4.43,8.74">)</formula><p>Line 9 corresponds to the results obtained using the aforementioned aggregation function with α = 0.7 (CONFα-T2).</p><p>From Table <ref type="table" coords="7,158.52,617.92,3.87,8.74" target="#tab_0">1</ref>, we can make the following observations:</p><p>-Image similarities are not sufficient to address the task accurately and text similarities perform much better. -Using as a preliminary stage the text similarities in order to select a first relevant list of images and re-ranking the latter list on the basis of image similarities only allows us to boost the precision (P@20) but not the MAP. The important conclusion to make here and towards multimedia retrieval, is that visual similarities might be relevant but they need to be filtered.</p><p>In that case, a text-based pre-filtering step is very important in order to select relevant visual similarities by removing images that are not relevant "semantically" despite of being similar to the image query "visually". -Combining textual similarities with text-based pre-filtered visual similarities, dramatically outperforms mono-media runs. Any of the aggregation functions used in fusing the two aforementioned runs allows us to increase both the MAP and the P@20 of the best mono-media runs. It appeared that the best aggregation strategy was the simplest one: the mean average. In that case, it improved the text run performance from MAP=20.45% to 27.65% which corresponds to more than a 35% increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Medical Image Modality Classification Task</head><p>Imaging modality is an important aspect of the image for medical retrieval. Therefore, within the Medical Retrieval Task in 2010 <ref type="bibr" coords="8,225.04,491.91,14.61,8.74" target="#b14">[15]</ref>, a first sub-task was image modality classification. Participants were provided a training set of 2000 images that have been classified into one out of 8 modalities (CT, MR, XR etc). These images also contained captions that participants could use in addition to the visual information both at training and test time. The measure used for this sub-task was the classification accuracy.</p><p>In our experiments we investigated mono-modal and mixed modality based classification. Concerning the pure visual-based classifiers, we trained 2 linear classifiers per modality (using the one-versus-all scheme) corresponding to the two different low level features (ORH and COL). Linear SVM classifiers with hinge loss using the primal formulation were trained with the Stochastic Gradient Descent (SGD) algorithm<ref type="foot" coords="8,242.32,597.93,3.97,6.12" target="#foot_2">5</ref> using the Improved Fisher Vectors (IFV) as described in section 3 and a single spatial layout (1x1). The SVM scores were combined by weighted averaging color and SIFT features for color images. We only used the ORH based IFV for gray-scale images. The weights for each modality were tuned by maximizing classification accuracy using a 5-fold cross validation scheme on the training set.</p><p>Concerning our text based modality classification, we used two different representations and hence two classifiers. The first one was based on a pattern matching (PM) technique where we searched for the modalities in the image captions. The second one was based on a binarized bagof-words representation.</p><p>The pattern matching was based on the information describing the modalities in the modal-ityClassificationREADME.txt file. Indeed, this file provides for each modality a description that contains a list of expressions mainly related to image sub-modalities grouped in that category (i.e. the PX modality description "PX: optical imaging including photographs, micrographs..." contains expressions "PX","optical imaging", "photograph", "micrograph"...). Therefore, for each image, we detected (matched) these expressions in the corresponding captions. However, within the corpus, many documents contain several images with the same caption while the latter do not necessarily have the same modalities. Therefore, we reduced<ref type="foot" coords="9,366.55,161.26,3.97,6.12" target="#foot_3">6</ref> the image caption for a targeted image name, to:</p><p>sentences related to the image reference <ref type="foot" coords="9,280.37,194.41,3.97,6.12" target="#foot_4">7</ref> , sentences containing multiple image references including the targeted one, and the introductory sentences (all sentences before the first occurrence of an image reference).</p><p>Consequently, the outputs of the pattern matching (PM) decision function can be a single modality, a set of potential modalities or an empty set.</p><p>The binarized bag-of-words representation consisted in a vector indicating for each word whether it appears in this document or not (in our case image caption). Note that for this representation we did not apply the aforementioned caption reduction but used them as they were provided. The feature vectors of the training set made of 2000 items with their modality labels, were then used to train a classifier per modality (one-versus-all scheme). The aim of this representation was to go beyond pattern matching and learn other words related to different modalities. To train the linear classifiers we used the logistic regression classifier (LRC) from the liblinear toolkit <ref type="foot" coords="9,118.78,347.76,3.97,6.12" target="#foot_5">8</ref> with Laplace Prior (L1 norm).</p><p>To combine the outputs of the two classifiers, we proceeded as follows:</p><p>-If we have no output from PM, the modality is given by:</p><formula xml:id="formula_14" coords="9,269.02,403.29,243.98,16.65">c * = arg max c∈C s c T (I)<label>(11)</label></formula><p>where s c T (I) is the output of the classifier LRC trained for the modality c.</p><p>-If PM outputs one or more modalities and c * is amongst them, the modality is again c * .</p><p>-Otherwise the modality amongst the PM outputs that has the highest score s c (I) was selected (C in equation 11 is reduced to the PM outputs).</p><p>In the case of mixed modalities, we first combined the LRC scores with the SGD visual scores (both normalized to get values between 0 and 1) before combining with the PM output as described above (s c T (I) is replaced by the combined scores in equation 11). The results of the different classifiers are shown in Table <ref type="table" coords="9,361.77,525.96,3.87,8.74" target="#tab_1">2</ref>. As these results show, the pure text modality classifier slightly outperforms the visual only information based classifier and both of them were outperformed by the combination of visual and textual modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Medical Ad-hoc Image Retrieval Task</head><p>For this task <ref type="bibr" coords="10,149.75,122.28,15.50,8.74" target="#b14">[15]</ref> the participants were given a set of 16 textual queries with 2-3 sample images for each query. The queries were classified into textual, mixed and semantic queries, based on the methods that are expected to yield the best results. The results of the modality classification can be used to filter the search in this sub-task. However, instead of filtering we preferred to aggregate the classification scores with the retrieval scores as explained below.</p><p>In our experiments we wanted to investigate both mono-modal and mixed-modality retrieval.</p><p>Visual Only Concerning the pure visual retrieval, in contrast to our Wikipedia runs, we used only the 3 Improved Fisher Vector (IFV), corresponding to the orientation histograms (ORH) with the 3 spatial-layout (1x1, 2x2, The 3 IFVs were used independently to rank the Wikipedia images using the dot product as similarity measure and the 3 scores were simply summed <ref type="foot" coords="10,488.62,245.54,3.97,6.12" target="#foot_6">9</ref> and then normalized to be between 0 and 1. We denote this score by s V R (I) (visual retrieval score). This score was then further combined with the visual only modality score as follows:</p><p>-First the query images were classified by the SGD classifiers using c * k (t) = arg max c∈C s c SGD (I t k ), where c * k (t) is the modality assigned to the image I t k of the topic t.</p><p>-If for all images I t k in a topic (if multiple image query) the same modality c * was assigned, the scores s c * SGD (I) were added to s V R (I) to rank the images of the dataset. The aim was to boost the scores of the images corresponding to that modality.</p><p>-In contrast, if we had difficulty to retain a given modality (as e.g. for the last topics) we preferred to rank the images based only on the retrieval scores s V R (I).</p><p>The performance of this run shown in line 1 of Table <ref type="table" coords="10,319.86,380.70,4.98,8.74">3</ref> is very poor, leading to the conclusion that visual only information is insufficient to address the task.</p><p>Text Only Concerning the textual retrieval, we submitted two types of text-only retrieval. For the first one, we simply ranked the text corresponding to some cross-entropy-based text similarity or Relevance Score Vector between the query text and the image caption. For the second, we aggregated these scores with text-only classification scores. First we describe a few textual retrieval runs (see section 2 for technical details). Several such textual runs can be created depending on:</p><p>-Different information retrieval models: Language Model or log-logistic model (the latter was used if LOGIT or LGD appears in the run names, the former otherwise). -Different smoothing parameters: we mostly used the classical Jelinek-Mercer interpolation except for the run DIR TXT (line 2) where the Dirichlet smoothing was used. -Different query and document enrichment models: statistical translation model (referred to as AX) and/or<ref type="foot" coords="10,158.99,558.53,7.94,6.12" target="#foot_7">10</ref> using the Wikipedia Corpus (referred to as WIKI). -Different statistics: using Chi2 with the log-logistic model is referred as CHI2 LOGIT and using Chi2 statistics instead of GI-M3 in the statistical translation model is referred as CHI2AX.</p><p>While only a single pure text retrieval run was submitted (line 2), nevertheless, we describe them here since they were used (referred to as textual retrieval score with s T R (I)) in the combination with the modality classification outputs and/or with the visual retrieval scores (see below). To aggregate the textual retrieval scores s T R (I) with the text based modality scores, we first extracted the modality c from the query text by pattern matching (PM) and selected the corresponding s c LRC (I) score 11 (see section 4.2). If no modality is found, only the s T R (I) is used to rank. Two such runs were submitted (lines 3 and 4 in Table <ref type="table" coords="11,278.33,103.05,3.87,8.74">3</ref>). While not directly comparable, we can nevertheless see that these runs led to better performances than DIR TXT suggesting that combining text retrieval with modality classification helps. Furthermore, WIKI AX MOD late led to a much better MAP than CHI2AX MOD late, showing the benefit of using external data (Wikipedia) for query expansion.</p><p>Mixing Textual and Visual Information Finally, we further used both visual and textual information. We also experimented with further these scores with modality classification scores as follows: w V s V R (I) + w M s c LRC (I) + w T (t)s T R (I).</p><p>Here s V R (I) and s T R (I) are the normalized visual and textual retrieval scores (see above) and s c LRC (I) is the output of the image modality classifier for the modality c using only textual information (LRC scores). The modality c is extracted from the query text of topic t by pattern matching (PM) and s c LRC (I) is set to zero if no modality was found. We made the weights of the textual retrieval score w T (t) dependent on the type of topic t. Hence, we used 0.5 for topics with query type V isual, 1 for type M ixed and 1.5 for type Semantic, in order to increase or decrease the importance of the textual score. The weights w V and w M took values 1 or 0. Hence setting w V to 0 lead to pure textual based retrieval runs discussed above (such as WIKI AX MOD late and CHI2AX MOD late depending on the s T R (I) used). If we further set w M to 0, we get runs without using the modality (e.g. DIR TXT). If we set w V = 1 and w M = 0 we get the classical late fusion (e.g. AX LGD IMG late) and setting w V = 1 and w M = 1 we get a late fusion based on textual and visual retrieval scores combined with modality classifier scores (CHI2 LOGIT IMG MOD late, WIKI LGD IMG MOD late).</p><p>Finally the runs AX rerank and AX rerank comb consist in ranking images based on s T R (I) and then re-ranking the top N=1000 relevant images using respectively s V R (I) and s V R (I) + w T (t)s T R (I).</p><p>The MAP, bPref and P10 results of the runs described above are shown in Table <ref type="table" coords="11,446.65,424.63,3.87,8.74">3</ref>. These results show that the poor performance of image retrieval for this task, decreases the performances when the visual modality is not appropriately combined with the other modalities (as in AX rerank or WIKI AX IMG MOD late). We can also note that the run AX rerank comb obtained the best performance on (MAP and bPref) while the run WIKI AX MOD late has the best performing P10 among all runs.</p><p>However, it is difficult to make conclusions about these runs that performed better than others from this table. Indeed, we need to evaluate different text runs with and without combining them with modality classifier and/or visual retrieval scores. Our intention is to do such analysis as soon as the image relevance scores are made available by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year we have participated with success in two new main tasks, namely the Wikipedia Retrieval Task and two sub-tasks of the Medical Retrieval Task (Image Modality Classification and Ad-hoc Retrieval). In all cases, we obtained leading positions both in retrieval and modality classification, and for each type of run: text-only, visual-only and mixed. We achieved excellent text based retrieval results and despite the fact that pure visual based retrieval led to poor results, when we appropriately combined them with our text ranking we were able to outperform the latter showing that multi-modal based systems can be better than mono-modal ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,268.79,522.70,9.64,7.16;5,274.56,527.37,2.66,4.37;5,283.30,517.86,196.20,8.74;5,480.09,516.28,3.53,6.12;5,479.50,522.72,4.73,6.12;5,488.59,517.86,24.41,8.74;5,90.00,533.35,147.70,8.74;5,238.29,531.77,3.53,6.12;5,237.70,538.19,8.53,7.16;5,242.55,542.86,2.66,4.37;5,250.55,533.35,25.30,8.74;5,276.44,531.77,3.53,6.12;5,275.85,538.19,8.55,7.16;5,280.46,542.86,2.66,4.37;5,288.71,533.35,175.48,8.74"><head></head><label></label><figDesc>w d i (the derivatives according to the weights), G I λ is the concatenation of the derivatives G I µ d i and G I σ d i and is therefore N = 2M D-dimensional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,90.00,99.97,423.00,236.38"><head>Table 1 .</head><label>1</label><figDesc>Wikipedia retrieval: overview of the performances of our different runs. For reference we have included the best performing run from the other participants (line 17).</figDesc><table coords="8,113.56,132.23,20.34,7.86"><row><cell>RUN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,90.00,582.00,423.00,86.74"><head>Table 2 .</head><label>2</label><figDesc>Modality Classification task: overview of the performances of our different runs. For reference we have included the best performing run from the other participants (fourth line).</figDesc><table coords="9,187.54,614.26,227.92,54.49"><row><cell>RUN</cell><cell>Modality</cell><cell>ACC</cell></row><row><cell cols="3">XRCE MODCLS COMB Mixed (textual + visual) 0.94</cell></row><row><cell>XRCE TXT</cell><cell>Textual</cell><cell>0.90</cell></row><row><cell>XRCE VIS</cell><cell>Visual</cell><cell>0.87</cell></row><row><cell>best non-XRCE run</cell><cell>Mixed</cell><cell>0.93</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,99.96,737.36,381.87,7.86"><p>Here we use Jelinek-Mercer interpolation but we can also use e.g. Dirichlet smoothing instead.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,99.96,726.40,413.03,7.86;3,99.96,737.36,228.49,7.86"><p>The Information Gain, aka Generalised (or average) Mutual Information<ref type="bibr" coords="3,388.40,726.40,9.22,7.86" target="#b5">[6]</ref>, is used for selecting features in text categorisation<ref type="bibr" coords="3,188.87,737.36,14.33,7.86" target="#b20">[21,</ref><ref type="bibr" coords="3,204.74,737.36,7.17,7.86" target="#b8">9]</ref> or detecting collocations<ref type="bibr" coords="3,316.17,737.36,9.22,7.86" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="8,99.96,737.36,410.87,8.12"><p>An implementation is available on Léon Bottou's web-page: http://leon.bottou.org/projects/sgd.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="9,99.96,715.44,184.21,7.86"><p>When it was possible to detect automatically.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="9,99.96,726.40,180.24,7.86"><p>For example references might be (a) or (a-c)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="9,99.96,738.00,207.12,9.21"><p>http://www.csie.ntu.edu.tw/ ~cjlin/liblinear/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="10,99.96,682.56,413.04,7.86;10,99.96,693.52,155.87,7.86"><p>Note that, due to the linearity property of the dot product, this is equivalent to the dot product of the concatenation of all three IFV vectors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7" coords="10,99.96,704.48,413.04,7.86;10,99.96,715.44,339.35,8.28"><p>Note that WIKI AX means that we combined the lexical entailment based scores with the scores obtained with Wikipedia based query expansion to obtain sT R(I ) for further fusion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8" coords="10,99.96,726.40,413.04,7.86;10,99.96,737.36,106.48,7.86"><p>This is the output of the LRC image modality classifier trained on the binarized bag-of-word feature vectors for the modality c.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">European</rs> Project <rs type="grantNumber">PinView FP7/2007-2013</rs> and the <rs type="funder">French National Project Fragrances</rs> <rs type="grantNumber">ANR-08-CORD-008</rs>. We would like also to acknowledge <rs type="person">Craig Saunders</rs> for his proofreading and useful comments on the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FhG22hn">
					<idno type="grant-number">PinView FP7/2007-2013</idno>
				</org>
				<org type="funding" xml:id="_fPSW5Xx">
					<idno type="grant-number">ANR-08-CORD-008</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,98.19,330.33,414.81,7.86;12,106.76,341.28,135.30,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,174.58,330.33,300.38,7.86">Data fusion in information retrieval using consensus aggregation operators</title>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,495.34,330.33,17.66,7.86;12,106.76,341.28,44.51,7.86">Web Intelligence</title>
		<imprint>
			<biblScope unit="page" from="662" to="668" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,352.35,414.81,7.86;12,106.76,363.31,406.24,7.86;12,106.76,374.27,58.87,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,246.80,352.35,185.38,7.86">Information retrieval as statistical translation</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,467.12,352.35,45.88,7.86;12,106.76,363.31,376.77,7.86">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,385.34,414.81,7.86;12,106.76,396.30,406.25,7.86;12,106.76,407.26,239.58,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,269.10,385.34,158.52,7.86">Information-based models for ad hoc ir</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,447.14,385.34,65.86,7.86;12,106.76,396.30,406.25,7.86;12,106.76,407.26,31.94,7.86">SIGIR &apos;10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,416.06,414.80,10.13;12,106.76,429.29,406.24,7.86;12,106.76,440.25,155.69,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,325.08,418.33,172.25,7.86">Lexical entailment for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,106.76,429.29,365.95,7.86">Advances in Information Retrieval, 28th European Conference on IR Research, ECIR 2006</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">April 10-12. 2006</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,451.32,414.81,7.86;12,106.76,462.28,406.24,7.86;12,106.76,473.23,406.24,7.86;12,106.76,484.19,315.23,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,292.90,451.32,220.11,7.86;12,106.76,462.28,169.02,7.86">Multi-language models and meta-dictionary adaptation for accessing multilingual digital libraries</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,298.70,462.28,214.30,7.86;12,106.76,473.23,366.63,7.86">Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">September 17-19, 2008. 2008</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="12,98.19,495.26,414.81,7.86;12,106.76,506.22,20.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,169.17,495.26,137.76,7.86">Information et analyse des données</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,314.40,495.26,85.64,7.86">Pub. Inst. Stat. Univ</title>
		<imprint>
			<biblScope unit="volume">XXXVII</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="43" to="60" />
			<date type="published" when="1993">1993</date>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,517.29,414.81,7.86;12,106.76,528.25,352.02,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,328.49,517.29,184.52,7.86;12,106.76,528.25,35.25,7.86">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.53,528.25,268.92,7.86">PASCAL Challenges Workshop for Recognizing Textual Entailment</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,539.32,414.81,7.86;12,106.76,550.28,108.48,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,168.11,539.32,255.59,7.86">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,433.17,539.32,79.83,7.86;12,106.76,550.28,28.62,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.19,561.35,414.81,7.86;12,106.76,572.31,236.43,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,181.09,561.35,328.36,7.86">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,106.76,572.31,153.92,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,583.38,415.15,7.86;12,106.76,594.34,406.24,7.86;12,106.76,605.30,70.93,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,312.53,583.38,200.47,7.86;12,106.76,594.34,73.46,7.86">A probabilistic classification approach for lexical textual entailment</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,203.94,594.34,309.06,7.86;12,106.76,605.30,42.17,7.86">Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05)</title>
		<meeting>the Twentieth National Conference on Artificial Intelligence (AAAI-05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,616.37,415.15,7.86;12,106.76,627.32,168.75,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,167.14,616.37,232.74,7.86">A probabilistic approach to automatic keyword indexing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,411.28,616.37,101.72,7.86;12,106.76,627.32,125.81,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,638.39,415.15,7.86;12,106.76,649.35,208.74,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,229.33,638.39,227.43,7.86">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,476.05,638.39,36.95,7.86;12,106.76,649.35,169.42,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,660.42,415.15,7.86;12,106.76,671.38,194.20,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,265.37,660.42,247.63,7.86;12,106.76,671.38,121.76,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.95,671.38,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,682.45,415.15,7.86;12,106.76,693.41,406.24,7.86;12,106.76,704.37,60.45,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,355.38,682.45,157.62,7.86;12,106.76,693.41,210.39,7.86">Leveraging image, text and corss-media similarities for diversity-focused multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Th</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,377.55,693.41,131.64,7.86">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.85,715.44,415.15,7.86;12,106.76,726.40,406.24,7.86;12,106.76,737.36,105.10,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,172.22,726.40,221.81,7.86">Overview of the clef 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,413.87,726.40,99.14,7.86;12,106.76,737.36,17.31,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,103.73,415.15,7.86;13,106.76,114.69,20.99,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,221.30,103.73,244.08,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,484.06,103.73,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,125.65,415.15,7.86;13,106.76,136.61,64.53,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,260.26,125.65,248.13,7.86">Large-scale image categorization with explicit data embedding</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,118.27,136.61,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,147.57,415.15,7.86;13,106.76,158.53,172.82,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,379.94,147.57,133.06,7.86;13,106.76,158.53,100.36,7.86">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,226.57,158.53,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,169.49,415.15,7.86;13,106.76,180.45,284.05,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,336.78,169.49,176.22,7.86;13,106.76,180.45,56.98,7.86">Overview of the wikipedia retrieval task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,183.55,180.45,119.47,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,191.40,415.15,7.86;13,106.76,202.36,406.24,7.86;13,106.76,213.32,292.35,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,309.00,191.40,158.23,7.86">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,486.90,191.40,26.11,7.86;13,106.76,202.36,406.24,7.86;13,106.76,213.32,93.93,7.86">SIGIR &apos;96: Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.85,224.28,415.15,7.86;13,106.76,235.24,402.26,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,249.12,224.28,248.49,7.86">A comparative study on feature selection in text categorization</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,106.76,235.24,310.99,7.86">Proceedings of ICML-97, 14th International Conference on Machine Learning</title>
		<meeting>ICML-97, 14th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
