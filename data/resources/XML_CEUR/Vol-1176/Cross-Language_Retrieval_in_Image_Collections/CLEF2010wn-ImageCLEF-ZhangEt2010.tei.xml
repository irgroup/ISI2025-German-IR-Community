<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.14,114.60,331.18,15.06;1,243.73,132.53,127.97,15.06">Random Sampling Image to Class Distance for Photo Annotation</title>
				<funder ref="#_nZPCZu2">
					<orgName type="full">Microsoft Fund Projects HIT</orgName>
				</funder>
				<funder ref="#_FGQBdPJ">
					<orgName type="full">Special Fund</orgName>
				</funder>
				<funder ref="#_mnAdCAE">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.81,170.89,61.59,10.46"><forename type="first">Deyuan</forename><surname>Zhang</surname></persName>
							<email>dyzhang@insun.hit.edu.cn</email>
						</author>
						<author>
							<persName coords="1,234.70,170.89,57.96,10.46"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,300.32,170.89,56.06,10.46"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
							<email>cjsun@insun.hit.edu.cn</email>
						</author>
						<author>
							<persName coords="1,383.93,170.89,66.72,10.46"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>wangxl@insun.hit.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">ITNLP Lab</orgName>
								<orgName type="institution">Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.14,114.60,331.18,15.06;1,243.73,132.53,127.97,15.06">Random Sampling Image to Class Distance for Photo Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A3623CA5F49112D0B07135867C9ED752</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Annotation</term>
					<term>Nearest Neighbor Classification</term>
					<term>Random Sampling</term>
					<term>ImageCLEF Photo Annotation Task</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image classification or annotation is proved difficult for the computer algorithms. The Naive-Bayes Nearest Neighbor method is proposed to tackle the problem, and achieved the state of the art results on Caltech-101 and Caltech-256 image databases. Although the method is simple and fast, for the real applications, it suffer from the imbalance of the training datasets. In this paper, we extend the image to class distance which is more general, and use the random sampling technique to alleviate the situation of the imbalance of the training datasets. We perform our method on the ImageCLEF 2010 Photo Annotation task, and the results (INSUNHIT)  showing that the algorithm is fast and stable. Although it does not achieving the state of the art performance, more image features can be used to improve the performance and dimension reduction techniques can be adopted to reduce the complexity of space and time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although human beings recognize the scenes or objects in an image easily, automatic image classification and annotation is a challenging task for computer programs. According to <ref type="bibr" coords="1,241.05,511.99,9.97,10.46" target="#b0">[1]</ref>, human can recognize about 30000 categories, while discriminating even two categories is difficult for computer vision systems <ref type="bibr" coords="1,452.24,523.94,12.80,10.46" target="#b1">[2]</ref>.</p><p>In recent thirty years researchers have proposed many image descriptors and learning algorithms to recognize objects and scenes. The image descriptors in the literature is roughly classified into five categories: global descriptors <ref type="bibr" coords="1,452.16,559.81,12.21,10.46" target="#b2">[3]</ref> <ref type="bibr" coords="1,464.36,559.81,16.28,10.46" target="#b10">[11]</ref> that represent the image with global attributes, block based descriptors <ref type="bibr" coords="1,440.33,571.76,12.37,10.46" target="#b4">[5]</ref> which represent the image using the image blocks, the region based features <ref type="bibr" coords="1,436.68,583.72,12.18,10.46" target="#b5">[6]</ref> that is generated by image segmentation algorithms, local patch features that represent images with descriptors find by the interest point or blob operators of the image, and some other features <ref type="bibr" coords="1,236.49,619.58,12.18,10.46" target="#b7">[8]</ref> such as the text labels tagged by the internet user on photo sharing communities. Although these image features have been proposed to tackle some specific image recognition tasks, they usually failed to succeed when applying to new image concepts and datasets.</p><p>The learning algorithms have been proposed with the development of image descriptors. Various kinds of learning algorithms are proposed or adopted in the literature, including Non-parametric methods <ref type="bibr" coords="2,343.37,142.24,13.68,10.46" target="#b6">[7]</ref>, kernel machines <ref type="bibr" coords="2,428.60,142.24,13.23,10.46" target="#b8">[9]</ref>, generative models <ref type="bibr" coords="2,179.77,154.19,13.05,10.46" target="#b3">[4]</ref>, multiple instance learning algorithms <ref type="bibr" coords="2,357.05,154.19,12.71,10.46" target="#b5">[6]</ref>, distance metric learning frameworks <ref type="bibr" coords="2,180.05,166.15,18.11,10.46" target="#b11">[12]</ref>, or biological inspired neural networks <ref type="bibr" coords="2,370.54,166.15,17.49,10.46" target="#b12">[13]</ref>. These models have applied to many different tasks, and achieved state of the art results.</p><p>Although these image recognition systems succeed in several tasks or benchmark databases, they remain naive for the real world applications. In the real world image databases, the visual concepts shares large intra-class and interclass variability, and the relation of the concepts is complicated. In addition, the images used for training is imbalanced, resulting in the failure of some learning algorithms. Finally, the databases contain large scale images, and the simplicity and the running time need to be considered when design image categorization systems.</p><p>In this paper we describe our system(INSUNHIT) for ImageCLEF2010 Photo Annotation task. We use the dense SIFT <ref type="bibr" coords="2,303.78,302.94,19.17,10.46" target="#b9">[10]</ref> image descriptor based on the observation that it performs well both in object categorization and scene classification. We extend Naive-Bayes Nearest Neighbor(NBNN) <ref type="bibr" coords="2,348.72,326.85,15.42,10.46" target="#b6">[7]</ref> classification method, and propose Random Sampling Image to Class Distance(RS-ICD) for image classification. RS-ICD can deal with imbalanced dataset while preserving the simplicity of NBNN method. The ImageCLEF2010 challenge results shows that the method is very stable and fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the ImageCLEF2010 Photo Annotation Dataset</head><p>In this section we briefly discuss some difficulties of the ImageCLEF Photo Annotation dataset. For a detail overview of the dataset please refer to <ref type="bibr" coords="2,462.36,482.79,14.62,10.46" target="#b14">[15]</ref>, the emphasis in this section is that why this dataset is difficult for computer vision algorithms. The focus of our system is image content, therefore we do not introduce the difficulties of text labels and EXIF information of the photos. First, compared to the benchmark datasets used in the literature, the visual concepts(annotation keywords) is more abstract and shares more variability. There exist 93 visual concepts, including objects(such as "trees" and "flowers"), scenes(such as "desert", "sky" and so on), abstract concepts(such as "macro", "spring" and "summer"). Some of the concepts have visual similar propertiesfor example, "child" and "baby"-, and the image features can not discriminate them perfectly. Image descriptors is difficult to choose.</p><p>Second, the image dataset is very imbalanced. The quantity of training images of each concept ranges from 12 training images("skateboard"), to 7484 images("Neutral Illumination"). This makes the classification system more difficult to train the concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section we introduce our learning algorithm in detail. Our algorithm extends the NBNN method, and the RS-ICD is stable for measuring the distance of the image to a query concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of NBNN</head><p>The NBNN method defines the distance of image to class to cope with the large intra-class variance of the class. The method is based on the Naive-Bayes and Kernel Density Estimation framework, and the image to class distance is the near optimal distance when training images is very large.</p><p>Here we only review the method of NBNN. The NBNN methods operate on the local patch based image features. For a given class C j , j ∈ (1, 2, ..., L), and query image Q and its corresponding image descriptors d 1 , d 2 , ..., d n , the distance of Q and class C j is defined as follows:</p><formula xml:id="formula_0" coords="3,253.09,313.52,227.50,31.41">d(Q, C j ) = n 1 N N Cj (d i )<label>(1)</label></formula><p>where N N Cj (d i ) is the nearest distance of the descriptor d i to class C j :</p><formula xml:id="formula_1" coords="3,208.48,368.68,272.11,11.36">N N Cj (d i ) = min(distance(d i , d Cj )), d Cj ∈ C j (2)</formula><p>The classification process is proceed:</p><formula xml:id="formula_2" coords="3,231.67,412.16,248.93,11.36">C opt = argmin C (d(Q, C j )), C j ∈ C (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image to Class Distance</head><p>Although the NBNN is effective for image classification that output a single label when decision, it can not be extend to image annotation(multi label) because the image to class distance of each image is not comparable. In order to deal with this problem, the distance should be normalized:</p><formula xml:id="formula_3" coords="3,238.90,513.59,241.69,31.41">dK(Q, C j ) = 1 n n 1 KN N Cj (d i )<label>(4)</label></formula><p>Where the KN N Cj (d i ) is the average distance of the top K nearest neighbor:</p><formula xml:id="formula_4" coords="3,223.73,569.55,256.86,31.41">KN N Cj (d i ) = 1 K K 1 distance(d i , d Cj )<label>(5)</label></formula><p>When K = 1, both the distance function and the decision function are the same as the NBNN method. Therefore the distance defined by NBNN is a special case of our Image to Class Distance(ICD). The most important is that ICD is comparable between images, and the distance can be used to do multi label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Random Sampling Generalized NBNN</head><p>The Image to Class Distance suffers from the imbalanced training datasets. We use random sampling technique to tackle the problem. Therefore the our algorithm described as follows:</p><p>The Random Sampling Image to Class Distance Based Annoation Begin For t = 1,2,...T Random sampling L images from each class Ci denoted as Ci(t); End Extract the descriptors of query image Q For t = 1,2,...T compute dK(Q, Ci(t)) of each class End the image to class distance dK(Q, Ci) = average(dK(Q, Ci)) compute the probability p(Q, Ci)=exp(-a*dK(Q, Ci)) End</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Challenge Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Here we describe the experimental setup of our algorithm. The images are transformed into gray image, and resized to 300 pixels while keep the aspect ratio if the image's length or width are larger than 300 pixels. 128 bin Dense SIFT image features with the step of 8 pixels are extracted. The parameter T of Random Sampling process is set to 10.</p><p>We run the algorithm on a computer with Intel Core 2 Duo Q9400 CPU, 4 GB Memory, 32 bit linux operating system. The SIFT extraction matlab program is provided by Lazbnik <ref type="bibr" coords="4,218.86,492.32,17.67,10.46" target="#b15">[16]</ref>, and the classification algorithm is coded by Python and Numpy toolkit. For Approximate Nearest Neighbor Search, FLANN[17] using randomized KD-Tree with Python interface is used.</p><p>To evaluate the performance of the system, Mean Average Precision results is performed as the main evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In this section we discuss the results of our system(INSUNHIT). All the results is showed on the website <ref type="bibr" coords="4,238.40,607.15,16.72,10.46" target="#b13">[14]</ref>. We submit 5 runs: the best MAP result is 23.71%, and the worst result is 22.51%. The detailed setups and the results are showed in Table1.</p><p>Our results achieved the centered of the overall results. We use different setup, and achieve similar results. This indicate that our algorithm is very stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Classifying the whole test image dataset takes about 12 hours. Although the performance of our algorithm is far behind the state of art performance, further improvements can be easily obtained. First, the 128-bin SIFT descriptor is too large, while for the image classification, we can try other image descriptors or dimension reduction techniques. Second, multiple image descriptors should be used to improve the annotation results. In recent years, most of the promising results on benchmark image datasets are performed by combining multiple image features or multiple classifiers that computed on these image features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,178.79,115.44,257.74,85.38"><head>Table 1 .</head><label>1</label><figDesc>The MAP results of different setups of our algorithms</figDesc><table coords="5,189.48,136.21,236.41,64.61"><row><cell cols="4">Runs Images per random sampling process KNN Accuracy</cell></row><row><cell>1</cell><cell>25</cell><cell>1</cell><cell>22.86%</cell></row><row><cell>2</cell><cell>15</cell><cell>1</cell><cell>23.71%</cell></row><row><cell>3</cell><cell>20</cell><cell>1</cell><cell>23.19%</cell></row><row><cell>4</cell><cell>50</cell><cell>1</cell><cell>22.89%</cell></row><row><cell>5</cell><cell>15</cell><cell>3</cell><cell>22.51%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This investigation was supported by the project of the <rs type="funder">National Natural Science Foundation of China</rs> (grants No. <rs type="grantNumber">60973076</rs>), <rs type="funder">Special Fund</rs> <rs type="programName">Projects for Harbin Science and Technology Innovation Talents</rs>(grants No. <rs type="grantNumber">2010RFXXG003</rs>) and <rs type="funder">Microsoft Fund Projects HIT</rs>.<rs type="grantNumber">KLOF.2009021</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mnAdCAE">
					<idno type="grant-number">60973076</idno>
				</org>
				<org type="funding" xml:id="_FGQBdPJ">
					<idno type="grant-number">2010RFXXG003</idno>
					<orgName type="program" subtype="full">Projects for Harbin Science and Technology Innovation Talents</orgName>
				</org>
				<org type="funding" xml:id="_nZPCZu2">
					<idno type="grant-number">KLOF.2009021</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,450.03,342.16,9.41;5,146.91,460.99,163.11,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,300.27,450.03,100.31,9.41">Visual object recognition</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Sheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,407.97,450.03,72.54,9.41;5,146.91,460.99,50.39,9.41">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="577" to="621" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,471.67,342.14,9.41;5,146.91,482.62,252.10,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,297.69,471.67,182.79,9.41;5,146.91,482.62,22.62,9.41">Why is Real-World Visual Object Recognition Hard?</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,177.13,482.62,116.41,9.41">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">e27</biblScope>
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,493.31,342.13,9.41;5,146.91,504.26,39.92,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,270.22,493.31,57.58,9.41">Color indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,334.91,493.31,89.99,9.41">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,514.94,342.18,9.41;5,146.91,525.90,333.62,9.41;5,146.91,536.86,333.61,9.41;5,146.91,547.82,30.73,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,237.11,514.94,243.42,9.41;5,146.91,525.90,40.55,9.41">A Bayesian Hierarchical Model for Learning Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,194.99,525.90,285.55,9.41;5,146.91,536.86,153.31,9.41">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,558.50,342.20,9.41;5,146.91,569.46,192.49,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,226.84,558.50,249.52,9.41">Incorporating multiple SVMs for automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,146.91,569.46,79.78,9.41">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="728" to="741" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,580.14,342.18,9.41;5,146.91,591.10,224.81,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,253.72,580.14,226.81,9.41;5,146.91,591.10,29.71,9.41">Image Categorization by Learning and Reasoning with Regions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,183.93,591.10,79.61,9.41">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="913" to="939" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,601.78,342.17,9.41;5,146.91,612.74,333.61,9.41;5,146.91,623.70,30.71,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,308.47,601.78,172.05,9.41;5,146.91,612.74,49.77,9.41">In defense of Nearest-Neighbor based image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,202.63,612.74,250.33,9.41">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,634.38,342.18,9.41;5,146.91,645.34,333.62,9.41;5,146.91,656.30,136.93,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,294.51,634.38,77.37,9.41">Web 2.0 dictionary</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,380.25,634.38,100.28,9.41;5,146.91,645.34,271.88,9.41">Proceedings of the 2008 international conference on Content-based image and video retrieval</title>
		<meeting>the 2008 international conference on Content-based image and video retrieval<address><addrLine>Niagara Falls, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,119.17,342.18,9.41;6,146.91,130.13,333.59,9.41;6,146.91,141.09,58.36,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,317.09,119.17,163.44,9.41;6,146.91,130.13,102.83,9.41">Support vector machines for histogrambased image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,256.76,130.13,162.15,9.41">Neural Networks, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1055" to="1064" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,152.05,337.90,9.41;6,146.91,163.01,333.62,9.41;6,146.91,173.97,319.56,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,316.21,152.05,164.30,9.41;6,146.91,163.01,227.56,9.41">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,382.03,163.01,98.51,9.41;6,146.91,173.97,229.88,9.41">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="2169" to="2178" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,184.93,337.91,9.41;6,146.91,195.89,333.61,9.41;6,146.91,206.84,95.72,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,259.07,184.93,221.46,9.41;6,146.91,195.89,120.81,9.41">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,275.49,195.89,168.11,9.41">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,217.80,337.88,9.41;6,146.91,228.76,333.62,9.41;6,146.91,239.72,244.81,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,335.64,217.80,144.86,9.41;6,146.91,228.76,284.05,9.41">Learning Globally-Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,437.58,228.76,42.95,9.41;6,146.91,239.72,182.97,9.41">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,250.68,337.92,9.41;6,146.91,261.64,333.62,9.41;6,146.91,272.60,291.91,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,388.12,250.68,92.41,9.41;6,146.91,261.64,303.03,9.41">Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fu Jie Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,458.41,261.64,22.13,9.41;6,146.91,272.60,229.88,9.41">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,283.56,337.98,9.41;6,146.91,294.52,207.11,9.41" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,151.52,283.56,250.64,9.41">IMAGECLEF Visual Concept Detection and Annotation Task</title>
		<ptr target="http://www.imageclef.org/2010/PhotoAnnotationMAPResults" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,305.47,337.92,9.41;6,146.91,316.43,333.65,9.41;6,146.91,327.39,103.51,9.41" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,291.06,305.47,189.47,9.41;6,146.91,316.43,205.61,9.41">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,366.89,316.43,113.67,9.41;6,146.91,327.39,16.79,9.41">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,338.35,337.98,11.15;6,146.91,349.31,112.97,9.41" xml:id="b15">
	<monogr>
		<ptr target="http://www.cs.unc.edu/~lazebnik/research/spatial_pyramid_code.zip" />
		<title level="m" coord="6,151.52,338.35,106.75,9.41">Spatial Pyramid Matching</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
