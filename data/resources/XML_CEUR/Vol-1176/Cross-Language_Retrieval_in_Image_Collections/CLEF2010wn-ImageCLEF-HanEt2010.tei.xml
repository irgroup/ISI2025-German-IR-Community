<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.48,116.10,322.38,12.64;1,168.73,134.03,277.88,12.64;1,224.57,151.96,166.21,12.64">ImageCLEF 2010 Modality Classification in Medical Image Retrieval: Multiple feature fusion with normalized kernel function</title>
				<funder ref="#_sHNpkZY #_P47Grjr">
					<orgName type="full">Japanese Ministry for Education, Science, Culture and Sports</orgName>
				</funder>
				<funder>
					<orgName type="full">Research</orgName>
				</funder>
				<funder>
					<orgName type="full">Ritsumeikan Global Innovation Research Organization</orgName>
					<orgName type="abbreviated">GIRO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.41,189.70,58.37,8.76"><forename type="first">Xian-Hua</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<addrLine>Kasatsu-shi</addrLine>
									<postCode>525-8577</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.23,189.70,58.24,8.76"><forename type="first">Yen-Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<addrLine>Kasatsu-shi</addrLine>
									<postCode>525-8577</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.48,116.10,322.38,12.64;1,168.73,134.03,277.88,12.64;1,224.57,151.96,166.21,12.64">ImageCLEF 2010 Modality Classification in Medical Image Retrieval: Multiple feature fusion with normalized kernel function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5CB728061CA3D0E590C8F25249BA8A18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe an approach for the automatic modality classification in medical image retrieval task of the 2010 CLEF cross-language image retrieval campaign (ImageCLEF). This work is focused on the process of feature extraction from medical images and fusion the different extracted visual feature and textual feature for modality classification. To extract visual features from the images, we used histogram descriptor of edge, gray or color intensity and block-based variation as global features and SIFT histogram as local feature, and the binary histogram of some predefined vocabulary words for image captions is used for textual feature. Then we combine the different features using normalized kernel functions for SVM classification. The proposed algorithm is evaluated by the provided modality dataset by ImageCLEF2010.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Imaging modality is an important aspect of the image for medical retrieval <ref type="bibr" coords="1,446.35,429.17,7.47,8.76" target="#b0">[1]</ref><ref type="bibr" coords="1,453.82,429.17,3.73,8.76" target="#b1">[2]</ref><ref type="bibr" coords="1,453.82,429.17,3.73,8.76" target="#b2">[3]</ref><ref type="bibr" coords="1,453.82,429.17,3.73,8.76" target="#b3">[4]</ref><ref type="bibr" coords="1,453.82,429.17,3.73,8.76" target="#b4">[5]</ref><ref type="bibr" coords="1,457.56,429.17,7.47,8.76" target="#b5">[6]</ref>. In user-studies, clinicians have indicated that modality is one of the most important filters that they would like to be able to limit their search by. Many image retrieval websites (Goldminer, Yottalook) allow users to limit the search results to a particular modality. However, this modality is typically extracted from the caption and is often not correct or present. Studies have shown that the modality can be extracted from the image itself using visual features <ref type="bibr" coords="1,217.90,500.91,10.96,8.76" target="#b6">[7,</ref><ref type="bibr" coords="1,228.86,500.91,7.30,8.76" target="#b7">8,</ref><ref type="bibr" coords="1,238.24,500.91,7.19,8.76" target="#b8">9]</ref>. Therefore, In this paper, we propose to use both visual and textual features for medical image representation, and combine the different features using normalized kernel function in SVM.</p><p>In computer vision, studies have shown that the simple global features such as histogram of edge, gray or color intensity and so on can represent images, and give the acceptable performance in image retrieval or recognition research fields. Based on the success of the above mentioned visual features for general image recognition, we also use them as medical image representation for modality classification. Recently, using local visual feature for image representation has been become very popular, and been proved to be very effective for image categorization or retrieval <ref type="bibr" coords="1,389.77,608.50,15.27,8.76" target="#b9">[10]</ref>. The most famous approach for image representation using local visual feature is bag of keypoints <ref type="bibr" coords="1,464.83,620.46,15.77,8.76" target="#b10">[11,</ref><ref type="bibr" coords="1,134.76,632.41,11.83,8.76">12]</ref>. The basic idea of bag of keypoints is that a set of local image patches is sampled using some method (e.g. densely, randomly, or using a keypoint detector) and a vector of visual descriptors is evaluated on each patch independently (e.g. SIFT descriptor, normalized pixel values). The resulting distribution of descriptors in descriptor space is then quantified in some way (e.g. by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for (i.e. patches assigned to) codebook centres) and the resulting global descriptor vector is used as a characterization of the image (e.g. as feature vector on which to learn an image classification rule based on an SVM classifier). Furthermore, according to the visual properties of medical images, we also calculate a histogram of small-block variance as visual feature for image representation. For textual feature, we pre-define 90 vocabulary words somewhat according to the statistical properties of training samples' captions and our (not radiologist) knowledge about medical modality, and calculate a binary histogram for any medical image using their captions. After obtain the different feature for image representation, we combine them together using kernel function for SVM classifier. Because different features maybe have deferent scale and dimension, in order to allow each individual feature to contribute equally for modality classification, we normalize the distance between two samples using mean distance of all training samples, and then, obtain the kernel function for each individual feature. The final kernel for SVM classification is the mean of individual kernel, which can be called Joint Kernel Equal Contribution (JKEC). The proposed algorithm is evaluated on the modality dataset of ImageCLEF2010, and the classification rate is almost approximated the classification goal of the modality classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature extraction for image representation</head><p>In this section we describe how we extract a feature representation which is somewhat robust to the high variability inherent in medical images and includes enough discriminative information for modality category. As we known that it is difficult to classify image categorization only with one type of image feature. So in this paper, we represent images with differen images features: including gray and color intensity histogram, block-based edge and variance histogram and popular bag-of-words model as visual feature, and a binary histogram of the predefined vocabulary words for image captions as textual feature. Then we merge them together for modality classification. Next, we simply introduce the used features for medical image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gray and Color intensity histogram:</head><p>Intensity histograms are widely used to capture the distribution information in an image. They are easy to compute and tend to be robust against small changes of camera viewpoints. For Gray intensity histogram, we can calculate the number of each intensity (0-255) for all image pixel, and normalize it using pixel number. Given an image I in some color space (e.g., red, green, blue), for calculate color histogram the color channels are quantized into a coarser space with k bins for red, m bins for green and l bins for blue. Therefore the color histogram is a vector</p><formula xml:id="formula_0" coords="2,162.54,630.64,94.58,11.23">h = (h 1 , h 2 , • • • , h n ) T</formula><p>, where n = k ml , and each element h i represents the number of pixels of the discretized color in the image. We assume that all images have been scaled to the same size. Otherwise, we normalize histogram elements as</p><formula xml:id="formula_1" coords="3,277.10,126.24,203.49,24.80">h ′ j = y j ∑ n j=0 y j (1)</formula><p>Blockbased edge histogram: We firstly segment the image into several blocks, and calculate edge histogram weighted by gradient intensity in each block. In experiment, we grid-segment an image into 4 by 4 block, and calculate a 20-bin edge histogram in each block. So we have 320 (20*16)-dimensional edge histogram feature for medical image representation.</p><p>Blockbased variace histogram: For each pixel in an image, a small patch centered by the specific pixel are used for calculating the local variation of the pixel. after obtaining the local variation of all pixels in the image, a histogram of variation intensity is calculated for the image representation.</p><p>bagofwords feature : In computer vision, local descriptors (i.e. features computed over limited spatial support) have proved well-adapted to matching and recognition tasks, as they are robust to partial visibility and clutter. In this paper, we use gridsampling patches, and then compute appearance-based descriptors on the patches. In contrast to the interest points from the detector, these points can also fall onto very homogeneous areas of the image. After the patches are extracted,the SIFT <ref type="bibr" coords="3,421.70,329.75,16.60,8.76" target="#b9">[10]</ref> descriptor is applied to represent the local features. The SIFT descriptor computes a gradient orientation histogram within the support region. For each of 8 orientation planes, the gradient image is sampled over a 4 by 4 grid of locations, thus resulting in a 128-dimensional feature vector for each region. A Gaussian window function is used to assign a weight to the magnitude of each sample point. This makes the descriptor less sensitive to small changes in the position of the support region and puts more emphasis on the gradients that are near the center of the region. To obtain robustness to illumination changes, the descriptors are made invariant to illumination transformations of the form aI(x) + b by scaling the norm of each descriptor to unity <ref type="bibr" coords="3,307.74,437.35,15.27,8.76" target="#b9">[10]</ref>. These SIFT features are then clustered with a k-means algorithm using the Euclidean distance. Then we discard all information for each patch except its corresponding closest cluster center identifier. For the test data, this identifier is determined by evaluating the Euclidean distance to all cluster centers for each patch. Thus, the clustering assigns a cluster c(x)21, ...C to each image patch x and allows us to create histograms of cluster frequencies by counting how many of the extracted patches belong to each of the clusters. The histogram representation h(X) with C bins is then determined by counting and normalization such that:</p><formula xml:id="formula_2" coords="3,251.12,544.29,229.48,30.67">h c (X) = 1 L X LX ∑ l=1 δ(c, c(x l ))<label>(2)</label></formula><p>where δ denotes the Kronecker delta function. Figure <ref type="figure" coords="3,361.25,584.59,4.98,8.76">2</ref> shows the procedure bag-ofwords(BoW) feature extraction and the extracted histogram feature of example images. Obviously, there exist alternatives to algorithmic choices made in the proposed method. For example, different interest point detectors can be used. However, it do not manifest obvious merit for different background cluster of images. Furthermore, the geometrical relation between the extracted patches is completely neglected in the approach presented here. While this relation could be used to improve classification accuracy, it remains difficult to achieve an effective reduction of the error rate in various situations by doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual features</head><p>According to the statistical properties of word occurrence in each training modality image's captions and our prior knowledge about the classifying modality, we select 90 key-words, such as CT, Curve, MR, urethrogram, PET and so on, as the vocabulary for forming a binary histogram for each medical image. The binary histogram for image representation is 90-dimension vector, where each dimension is correspond to one selected keyword. If one key-word is appeared one or more than one times in an image's caption, the value of the corresponding dimension in its represented binary histogram will be 1, otherwise it will be 0. Kernel methods make use of kernel functions defining a measure of similarity between pairs of instances. In the context of feature combination it is useful to associate a kernel to each image feature as the following Eq. 3, and combine the kernels of different features together. For a kernel function K of each feature between real vectors we define the short-hand notation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">feature fusion</head><formula xml:id="formula_3" coords="4,184.82,457.11,295.78,9.69">K m (I i , I j , ) = K(f m (I i ), f m (I j )) = K(S(f m (I i ), f m (I j )))<label>(3)</label></formula><p>where I i and I j are two samples, f m (I i ) is the m th extracted feature from the sample I i and S(f m (I i ), f m (I j )) is the similarity measure between the m th features of the samples I i and I j . Then the image kernel K m : χ × χ ∈ ℜ only considers similarity with respect to image feature f m . If the image feature is specific to a certain aspect, say, it only considers color information, then the kernel measures similarity only with regard to this aspect. The subscript m of the kernel can then be understood as indexing into the set of features. Because different features maybe have deferent scale and dimension, in order to allow each individual feature to contribute equally for modality classification, we normalize the distance between two samples using mean distance of all training samples, and then, obtain the kernel function for each individual feature f m . The final kernel for SVM classification is the mean of individual kernel, which can be called Joint Kernel Equal Contribution (JKEC). For the feature similarity calculation of two simples, we use χ 2 distance as the following:</p><formula xml:id="formula_4" coords="4,233.54,638.06,247.05,30.21">S(f m (I i ), f m (I j )) = L ∑ 1 (x l -y l ) 2 x l + y l (4)</formula><p>where x and y represent the m th features f m (I i ), f m (I j ) of samples i and j, respectively, and x l is the l th element of the vector x. Then, the RBF function is used for calculating the kernel:</p><formula xml:id="formula_5" coords="5,222.34,163.33,254.37,22.32">K m (I i , I j , ) = exp( -S(f m (I i ), f m (I j )) γ ) (<label>5</label></formula><formula xml:id="formula_6" coords="5,476.71,170.27,3.87,8.76">)</formula><p>where γ is the normalized item for Joint Equal Contribution of each feature. Here, we use the distance mean of all training samples as γ, which will lead to similar contribution of each feature to kernel. The proposed algorithm is evaluated on the modality dataset of ImageCLEF2010, and the classification rate is almost approximated the classification goal of the modality classification task.</p><p>4 Experimental setup A more detailed explanation of the database and the tasks can be found in <ref type="bibr" coords="5,236.03,385.31,15.27,8.76" target="#b12">[13]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We evaluate classification performance of different features with SVM using training dataset. From each modality set, 180 images are randomly selected for training, the remainder are for test. We did this procedure 5 times, and gave the average classification rate in Fig. <ref type="figure" coords="7,182.47,260.20,4.98,8.76">2</ref> (a) for all modality category. From Fig. <ref type="figure" coords="7,353.54,260.20,3.71,8.76">2</ref>(a), It is obvious that different features have different discriminant for each modality category. For an instance in all visual features, the BOF (Bag-of-Feature) has the best recognition rate for MR modality; however, the color histogram has the best result for PX modality. Therefore, after combining all visual features together, the recognition rate for most of modality can be greatly improved (Fig. <ref type="figure" coords="7,229.20,319.97,14.57,8.76">2(a)</ref>). The final results with combination of visual and textual feature are also have large improvement that those with only visual or textual feature. Figure . 2(b) gives the average classification rate for all modality. Based on the evaluated better performance with visual feature than textual feature, we also use large weight for visual feature when combine the visual and textual feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runs Submitted</head><p>As Medical Image Processing Group (MIPG) of our Intelligent Image Processing Laboratory (IIPL) In Ritsumeikan University, we prepared four runs for evaluation image set, which used combine visual feature, textual feature, both visual and textual features and weighted visual and textual features. The recognition results are shown in Table <ref type="table" coords="7,473.13,455.27,3.74,8.76" target="#tab_0">1</ref>.</p><p>We submitted two runs using textual, combined textual and visual features by on-linesystem, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed to extract different visual and textual features for medical image representation, and fusion the different extracted visual feature and textual feature for modality classification. To extract visual features from the images, we used histogram descriptor of edge, gray or color intensity and block-based variation as global features and SIFT histogram as local feature, and the binary histogram of some predefined vocabulary words for image captions is used for textual feature. Because different features maybe have deferent scale and dimension, in order to allow each individual feature to contribute equally for modality classification, we proposed to use Joint Kernel Equal Contribution (JKEC) for kernel fusion of different features. The proposed algorithm is evaluated by the provided modality dataset by ImageCLEF2010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.76,313.68,345.84,9.65;4,134.76,325.64,345.83,9.65;4,134.76,337.59,7.19,9.30;4,141.96,336.02,10.27,6.12;4,153.22,337.59,270.77,9.66;4,423.98,336.02,7.68,6.12;4,435.21,337.79,45.38,8.76;4,134.76,349.55,345.83,9.30;4,134.76,361.70,345.84,8.76;4,134.76,373.66,345.85,8.76;4,134.76,385.61,64.44,8.76"><head></head><label></label><figDesc>Given a training set (x i , y i ) i=1,2,••• ,N of N instances consisting of an image x i ∈ χ and a class label y i ∈ 1, 2, • • • , C, and given a set of F image features f m : χ → ℜ dm , m = 1, 2, • • • , F , where d m denotes the dimensionality of the m th feature, the problem of learning a classification function y : χ → 1, 2, • • • , C from the features and training set is called feature combination problem. In computer vision, the problem of learning a multi-class classifier from training data is often addressed by means of kernel methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.76,294.27,71.94,8.78;5,134.76,313.58,345.85,8.76;5,134.76,325.54,345.85,8.76;5,134.76,337.49,345.84,8.76;5,134.76,349.45,345.85,8.76;5,134.76,361.40,345.84,8.76;5,134.76,373.36,148.06,8.76"><head>4. 1</head><label>1</label><figDesc>Image DataThe database released for the ImageCLEF-2010 Medical modality classification in medical retrieval task includes 2390 annotated modality images (CT: 314; GX: 355; MR: 299; NM: 204; PET: 285; PX: 330; US: 307; XR:296) for training and a separate evaluated set consisting of 2620 images. The aim is to automatically classify the evaluated set using 8 different modality label sets including CT, MR, PET and so on. some example images are shown in Fig.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.76,621.65,345.81,8.01;5,134.76,632.73,209.42,7.88;5,134.76,415.93,374.09,190.86"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Sample images of 8 medical modalities. From left to right and upper to down, the images are CT, MR, NM, PET, GX, US, XR and PX, respectively.</figDesc><graphic coords="5,134.76,415.93,374.09,190.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.76,116.25,374.09,262.56"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.76,116.25,374.09,262.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.76,392.58,374.09,335.36"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.76,392.58,374.09,335.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.76,115.95,345.82,49.46"><head>Table 1 .</head><label>1</label><figDesc>Overall calssification rates on medical evaluated dataset using combination of different features.</figDesc><table coords="7,166.38,146.16,282.60,19.24"><row><cell>Features</cell><cell cols="3">Visual Textual Visual+Texture Weighted Visual+Textual</cell></row><row><cell cols="2">classification rate(%) 87.07 84.58</cell><cell>93.36</cell><cell>93.89</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work was supported in part by the <rs type="grantName">Grand-in Aid for Scientific Research</rs> from the <rs type="funder">Japanese Ministry for Education, Science, Culture and Sports</rs> under the Grand No. <rs type="grantNumber">21300070</rs> and <rs type="grantNumber">22103513</rs>, and in part by the <rs type="funder">Research</rs> fund from <rs type="funder">Ritsumeikan Global Innovation Research Organization (R-GIRO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sHNpkZY">
					<idno type="grant-number">21300070</idno>
					<orgName type="grant-name">Grand-in Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_P47Grjr">
					<idno type="grant-number">22103513</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.12,222.47,342.47,7.88;8,146.47,233.43,334.11,7.88;8,146.47,244.39,98.11,7.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,332.02,222.47,148.57,7.88;8,146.47,233.43,208.45,7.88">A review of content-based image retrieval systems in medicine clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,364.26,233.43,116.32,7.88;8,146.47,244.39,41.33,7.88">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,255.35,342.46,7.88;8,146.47,266.31,334.12,7.88;8,146.47,277.27,334.11,7.88;8,146.47,288.23,334.11,7.88;8,146.47,299.19,57.79,7.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,359.70,255.35,120.88,7.88;8,146.47,266.31,334.12,7.88;8,146.47,277.27,58.67,7.88">Overview of the ImageCLEFmed 2006 medical retrieval annotation tasks, Evaluation of Multilingual and Multimodal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,214.31,277.27,262.24,7.88">Seventh Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,310.14,342.46,7.88;8,146.47,321.10,334.11,7.88;8,146.47,332.06,45.33,7.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,301.73,310.14,178.84,7.88;8,146.47,321.10,118.91,7.88">Medical Image Retrieval and Automated Annotation: OHSU at ImageCLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,276.22,321.10,164.64,7.88">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,343.02,342.46,7.88;8,146.47,353.98,334.11,7.88;8,146.47,364.94,20.17,7.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,453.56,343.02,27.02,7.88;8,146.47,353.98,200.42,7.88">Quality of DICOM header information for image categorization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Guld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bredno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,356.44,353.98,62.81,7.88">Proceedings SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4685</biblScope>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,375.90,342.46,7.88;8,146.47,386.86,334.11,7.88;8,146.47,397.82,21.66,7.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,373.72,375.90,106.86,7.88;8,146.47,386.86,200.52,7.88">Advancing biomedical image retrieval: development and analysis of a test collection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,357.11,386.86,99.86,7.88">J Amer Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,408.77,342.46,7.88;8,146.47,419.73,334.11,7.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,265.79,408.77,214.79,7.88;8,146.47,419.73,138.85,7.88">Automatic image modality based classification and annotation to improve medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,293.56,419.73,97.66,7.88">Stud Health Technol Inform</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">Pt 2</biblScope>
			<biblScope unit="page" from="1334" to="1338" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,430.69,342.46,7.88;8,146.47,441.65,315.97,7.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,326.72,430.69,153.86,7.88;8,146.47,441.65,57.36,7.88">Photobook: Content-based manipulation of image databases</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stan</forename><surname>Sclaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,213.45,441.65,146.47,7.88">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,452.61,342.46,7.88;8,146.47,463.57,334.11,7.88;8,146.47,474.53,126.89,7.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,306.06,452.61,174.52,7.88;8,146.47,463.57,141.01,7.88">A New Content-Based Image Retrieval Approach Based on Pattern Orientation Histogram</title>
		<author>
			<persName coords=""><forename type="first">Abolfazl</forename><surname>Lakdashti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shahram Moin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,353.60,463.57,126.98,7.88;8,146.47,474.53,103.21,7.88">Computer Vision/Computer Graphics Collaboration Techniques</title>
		<imprint>
			<biblScope unit="volume">4418</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.12,485.49,342.47,7.88;8,146.47,496.45,114.81,7.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,245.96,485.49,133.67,7.88">Image retrieval using color and shape</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,389.73,485.49,69.99,7.88">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1233" to="1244" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.23,507.40,338.34,7.88;8,146.47,518.36,158.60,7.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,190.68,507.40,205.26,7.88">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,405.86,507.40,74.71,7.88;8,146.47,518.36,69.78,7.88">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.23,529.32,338.34,7.88;8,146.47,540.28,334.11,7.88" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,364.81,529.32,115.77,7.88;8,146.47,540.28,42.91,7.88">Visual categoraization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,208.28,540.28,234.75,7.88">Proc. ECCVWorkshop on Statistical Learning in Computer Vision</title>
		<meeting>ECCVWorkshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.95,551.24,329.63,7.88;8,146.47,562.20,284.75,7.88" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,278.20,551.24,202.37,7.88;8,146.47,562.20,137.38,7.88">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,303.20,562.20,41.45,7.88">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.23,573.16,338.34,7.88;8,146.47,584.12,334.11,7.88;8,146.47,595.08,183.74,7.88" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,237.88,584.12,211.88,7.88">Overview of the CLEF 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.62,584.12,10.96,7.88;8,146.47,595.08,106.57,7.88">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
