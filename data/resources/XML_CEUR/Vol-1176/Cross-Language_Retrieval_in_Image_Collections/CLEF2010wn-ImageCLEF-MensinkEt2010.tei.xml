<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.60,116.90,316.16,12.90;1,201.95,134.83,211.46,12.90">LEAR and XRCE&apos;s participation to Visual Concept Detection Task -ImageCLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.06,172.87,69.46,8.64"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LEAR</orgName>
								<orgName type="institution" key="instit2">INRIA Rhône-Alpes</orgName>
								<address>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.31,172.87,65.02,8.64"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.77,172.87,70.56,8.64"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.60,184.83,57.09,8.64"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.01,184.83,58.27,8.64"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LEAR</orgName>
								<orgName type="institution" key="instit2">INRIA Rhône-Alpes</orgName>
								<address>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.60,116.90,316.16,12.90;1,201.95,134.83,211.46,12.90">LEAR and XRCE&apos;s participation to Visual Concept Detection Task -ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01A148A5231F7EFBA363D8252A56423F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Classification</term>
					<term>Auto Annotation</term>
					<term>Multi-Modal</term>
					<term>Linear SVM</term>
					<term>Fisher Vectors</term>
					<term>TagProp Flickr-tags Boardwalk</term>
					<term>Sunset</term>
					<term>Wilsonsprom</term>
					<term>Wilsonspromontory</term>
					<term>Victoria</term>
					<term>Australia</term>
					<term>3kmseoftidalriver</term>
					<term>Explore Annotation-concepts Landscape Nature</term>
					<term>No Visual Season</term>
					<term>Outdoor</term>
					<term>Plants</term>
					<term>Trees</term>
					<term>Sky</term>
					<term>Clouds</term>
					<term>Day</term>
					<term>Neutral Illumination</term>
					<term>No Blur</term>
					<term>No Persons</term>
					<term>Overall Quality</term>
					<term>Park Garden</term>
					<term>Visual Arts</term>
					<term>Natural</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the common effort of Lear and XRCE for the ImageCLEF Visual Concept Detection and Annotation Task. We first sought to combine our individual state-of-the-art approaches: the Fisher vector image representation, with the TagProp method for image auto-annotation. Our second motivation was to investigate the annotation performance by using extra information in the form of provided Flickr-tags. The results show that using the Flickr-tags in combination with visual features improves the results of any method using only visual features. Our winning system, an early-fusion linear-SVM classifier, trained on visual and Flickr-tags features, obtains 45.5% in mean Average Precision (mAP), almost a 5% absolute improvement compared to the best visual-only system. Our best visual-only system obtains 39.0% mAP, and is close to the best visual-only system. It is a late-fusion linear-SVM classifier, trained on two types of visual features (SIFT and colour). The performance of TagProp is close to our SVM classifiers. The methods presented in this paper, are all scalable to large datasets and/or many concepts. This is due to the fast FK framework for image representation, and due to the classifiers. The linear SVM classifier has proven to scale well for large datasets. The k-NN approach of TagProp, is interesting in this respect since it requires only 2 parameters per concept.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our participation to the ImageCLEF Visual Concept Detection and Annotation Task (VCDT) we focused on two main aspects. First, we wanted to investigate the effect of using the available modalities, visual (image) and textual (Flickr-tags), both at train and test time. Our second goal was to compare some of our recent techniques that potentially scale to large data sets with many concepts on the proposed task.</p><p>The VCDT is a multi-label classification challenge on the MIR Flickr dataset <ref type="bibr" coords="1,466.48,621.57,10.58,8.64" target="#b4">[5]</ref>. It aims at automatic annotation of 10, 000 test images with multiple concepts, learned from 8, 000 train images. The 93 concepts include abstract categories (like Partylife), the time of day (like day or night), persons (like no person visible, small or big group) and quality (like blurred or underexposed). For a complete overview of the challenge see <ref type="bibr" coords="2,149.98,132.26,15.27,8.64" target="#b11">[12]</ref>. This year's challenge allowed the use of 'multi-modal approaches that consider visual information and/or Flickr user tags and/or EXIF information'. For all images in the train and test set the original tag data of the Flickr users (further denoted as Flickr-tags) was provided. The set of Flickr-tags contains over 53, 000 different tags, from which we use a subset of most occurring tags. Also, for most of the photos the EXIF data was provided, however in our experiments we did not use this information.</p><p>In Fig. <ref type="figure" coords="2,178.49,218.35,4.98,8.64">1</ref> an image from the database is shown, together with the Flickr-tags and the annotation concepts. We see that the tags and annotation concepts are quite complementary. While the Flickr-tags of an image corresponds to concepts which are not necessary visually perceptible (e.g. Australia), the image annotation system is interested in the visual concepts (e.g. sky and clouds).</p><p>Although the objective of a user tagging his images is different from a (visual) keyword based retrieval system, the Flickr-tags might offer useful information in the annotation task. To analyse our first aspect we have used the Flickr-tags as textual representation of an image, and conducted experiments with systems using either both modalities, or using only the visual modality. The results (see <ref type="bibr" coords="2,384.63,327.14,40.93,8.64">Section 4)</ref> show that indeed the Flickr-tags are complementary to the visual information. All our systems using both modalities outperform any of the visual only systems.</p><p>Concerning the second aspect, in spite of the fact that the task was relatively small especially in the number of images, we tested methods that potentially scale to large annotated data sets, e.g. up to hundreds of thousands of labelled images, and/or many concepts. Hence, we used image representations and classifiers which are efficient both in learning and in classifying. Efficiency includes (1) the cost of computing the representations, (2) the cost of learning classifiers on these representations, and (3) the cost of classifying a new image.</p><p>As our image representation we use the Improved Fisher vectors <ref type="bibr" coords="2,406.38,449.09,15.77,8.64" target="#b12">[13,</ref><ref type="bibr" coords="2,423.81,449.09,11.83,8.64" target="#b13">14]</ref>, which are based on the Fisher Kernel (FK) framework <ref type="bibr" coords="2,312.58,461.05,10.58,8.64" target="#b5">[6]</ref>. The Fisher vector extends the popular bag-of-visual-words (BOV) histograms <ref type="bibr" coords="2,293.06,473.01,10.58,8.64" target="#b1">[2]</ref>, by not only including word counts, but also additional information about the distribution of the descriptors. Due to the use of this additional information the visual code book in a FK approach could be much smaller than in the BOV approach. We use a code book of only 256 words, while a size of several thousands is common in BOV approaches. Since the size of the visual code book determines largely the computational cost for the descriptor, this makes the FK a very fast descriptor.</p><p>On the classifier part, we compare a per-keyword-trained linear Support-Vector-Machine (SVM) <ref type="bibr" coords="3,201.66,204.11,16.60,8.64" target="#b15">[16]</ref> to TagProp, a k-NN classifier with learned neighbourhood weights <ref type="bibr" coords="3,134.77,216.07,10.58,8.64" target="#b3">[4]</ref>. The training cost of a linear SVM is linear in the number of images <ref type="bibr" coords="3,414.43,216.07,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="3,426.88,216.07,11.83,8.64" target="#b14">15]</ref>, therefore they can be efficiently learned with large quantities of images <ref type="bibr" coords="3,388.25,228.02,15.27,8.64" target="#b9">[10]</ref>. The advantage of the k-NN classifier is that it requires only 2 parameters per keyword, additional training for a new keyword is therefore very fast. For both classifiers we have used the same image and text representations, therefore we can fairly compare the results of the two methods.</p><p>Note that these representations and methods have shown state-of-the-art performances <ref type="bibr" coords="3,167.61,299.87,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="3,180.06,299.87,12.45,8.64" target="#b12">13,</ref><ref type="bibr" coords="3,194.17,299.87,13.28,8.64" target="#b13">14]</ref> on different tasks on several publicly available databases. However they were not necessarily compared or combined. The ImageCLEF VCDT challenge gave us a good opportunity to do this.</p><p>The rest of the paper is organized as follows. In Section 2 we describe the FK framework and the recent improvements on Fisher vectors. In Section 3 we give an overview of our TagProp method. Then in Section 4 we present in more detail the experiments we did, the submitted runs and the obtained results. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Features -the Improved Fisher vector</head><p>As image representation, we use the Improved Fisher vector <ref type="bibr" coords="3,375.25,440.64,15.77,8.64" target="#b12">[13,</ref><ref type="bibr" coords="3,392.69,440.64,11.83,8.64" target="#b13">14]</ref>. The Fisher vector is an extension of the bag-of-visual-words (BOV) representation, instead of characterizing an image with the number of occurrences of each visual word, it characterizes the image with a gradient vector derived from a generative probabilistic model. The gradient of the log-likelihood describes the contribution of the parameters to the generation process.</p><p>We assume that the local descriptors X = {x t , t = 1 . . . T } of an image are generated by a Gaussian mixture model (GMM) u λ with parameters λ. X can be described by the gradient vector <ref type="bibr" coords="3,224.26,536.40,10.79,8.64" target="#b5">[6]</ref>:</p><formula xml:id="formula_0" coords="3,258.44,555.85,222.15,22.31">G X λ = 1 T ∇ λ log u λ (X).<label>(1)</label></formula><p>A natural kernel on these gradients is using the Fisher information matrix <ref type="bibr" coords="3,429.64,587.73,10.79,8.64" target="#b5">[6]</ref>:</p><formula xml:id="formula_1" coords="3,158.98,608.83,321.61,13.38">K(X, Y ) = G X λ F -1 λ G Y λ , F λ = E x∼u λ [∇ λ log u λ (x)∇ λ log u λ (x) ] .<label>(2)</label></formula><p>As F λ is symmetric and positive definite, F -1 λ has a Cholesky decomposition F -1 λ = L λ L λ . Therefore K(X, Y ) can be rewritten as a dot-product between normalized vectors G λ with: G X λ = L λ G X λ . We will refer to G X λ as the Fisher vector of X.</p><p>As generative model we use a GMM: u λ (x) = M i=1 w i u i (x), with parameters λ = {w i , µ i , Σ i , i = 1 . . . M }. Gaussian u i has mixture weight w i , mean vector µ i , and covariance matrix Σ i . We assume diagonal covariance matrix Σ i and denote the variance vector by σ 2 i . Let G X µ,i (resp. G σ,i ) be the normalized gradient vectors with respect to the µ i (resp. σ i ) of Gaussian i. The final gradient vector G X λ is the concatenation of the G X µ,i and G X σ,i vectors for i = 1 . . . M , and is therefore 2M D-dimensional. The Improved Fisher vector <ref type="bibr" coords="4,267.18,193.41,15.27,8.64" target="#b13">[14]</ref>, takes the Fisher vector as described above and adds L2 normalization and Power normalization, both described in details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">L2 normalization</head><p>It has been shown that the Fisher vector approximately discards image-independent (i.e. background) information <ref type="bibr" coords="4,236.47,265.47,15.27,8.64" target="#b13">[14]</ref>. However the vector depends on the proportion of imagespecific information w.r.t. to the proportion of background information. We use the L2 norm to cancel this effect.</p><p>According to the law of large numbers Eq. 1 can be approximated as: G X λ ≈ ∇ λ x p(x) log u λ (x)dx. Assume that p is a mixture containing a background component (u λ ) and an image-specific component (with image-specific distribution q), and let ω denote the mixing weight:</p><formula xml:id="formula_2" coords="4,166.71,360.44,313.89,19.31">G X λ ≈ ω∇ λ x q(x) log u λ (x)dx + (1 -ω)∇ λ x u λ (x) log u λ (x)dx.<label>(3)</label></formula><p>Since the parameters λ are estimated with a Maximum Likelihood approach (i.e. to maximize E x∼u λ log u λ (x)), the derivative of the background component approximates zero. Consequently, the FV equals G X λ ≈ ω∇ λ x q(x) log u λ (x)dx, it focuses on the image-specific content, but depends on the proportion of image specific component ω.</p><p>Therefore, two images containing the same object but at different scales will have different signatures. To remove the dependence on ω, we L2-normalize the vector G X λ or equivalently G X λ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Power normalization</head><p>The Power normalization is motivated by an empirical observation: Fisher vectors become sparser as the number of Gaussians increases. Because fewer descriptors x t are assigned (with a significant probability) to each Gaussian, and the derivative of a Gaussian without assigned descriptors is zero. Hence, the distribution of features in a given dimension becomes more peaky around zero, as shown in Fig <ref type="figure" coords="4,382.56,556.44,3.74,8.64" target="#fig_0">2</ref>.</p><p>Linear classification requires a dot-product kernel, however the L2 distance is a poor measure of similarity on sparse vectors. Therefore we "unsparsify" the vector z by using:</p><formula xml:id="formula_3" coords="4,267.41,601.87,213.18,11.03">f (z) = sign(z)|z| α ,<label>(4)</label></formula><p>where 0 ≤ α ≤ 1 is a parameter of the normalization. The optimal value of α may vary with the number M of Gaussians in the GMM. Earlier experiments have shown that α = 0.5 is a reasonable value for 16 ≤ M ≤ 512, so this value is used throughout the experiments. In When combining the power normalization and the L2 normalization, we apply the power normalization first and then the L2 normalization. We note that this does not affect the analysis of the previous section: the L2 normalization on the power-normalized vectors sill removes the influence of the mixing coefficient ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial Pyramids</head><p>Spatial pyramid matching was introduced by Lazebnik et al . to take into account the rough geometry of a scene <ref type="bibr" coords="5,242.52,372.87,10.58,8.64" target="#b8">[9]</ref>. It consists in repeatedly subdividing an image and computing histograms of local features at increasingly fine resolutions by pooling descriptorlevel statistics. We follow the splitting strategy adopted by the winning systems of PAS-CAL VOC 2008 <ref type="bibr" coords="5,203.69,408.74,10.58,8.64" target="#b2">[3]</ref>, and extract 8 Fisher vectors per image: one for the whole image, three for the top, middle and bottom regions and four for each of the four quadrants.</p><p>In the case where Fisher vectors are extracted from sub-regions, the "peakiness" effect will be even more exaggerated as fewer descriptors are pooled at a region-level compared to the image-level. Hence, the power normalization is likely to be even more beneficial in this case. When combining power normalization and L2 normalization with spatial pyramids, we normalize each of the 8 Fisher vectors independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Annotation with TagProp</head><p>In this section we present TagProp <ref type="bibr" coords="5,277.21,536.07,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="5,289.65,536.07,11.83,8.64" target="#b16">17]</ref>, our weighted nearest neighbour annotation model. We assume that some visual similarity or distance measures between images are given, abstracting away from their precise definition. We proceed by discussing how to use rank based weights with multiple distances in Section 3.2 and we extend the model by adding a per-word sigmoid function that can compensate for the different frequencies of annotation terms in the database, in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Weighted Nearest Neighbour Model</head><p>In the following we use y iw ∈ {-1, +1} to denote whether concept w is relevant for image i or not. The probability that concept w is relevant for image i, i.e. p(y iw = +1), is obtained by taking a weighted sum of the relevance values for w of neighbouring training images j. Formally, we define:</p><formula xml:id="formula_4" coords="6,229.14,151.58,251.45,19.91">p(y iw = +1) = j π ij p(y iw = +1|j),<label>(5)</label></formula><formula xml:id="formula_5" coords="6,221.70,180.31,258.89,23.30">p(y iw = +1|j) = 1 - for y jw = +1, otherwise.<label>(6)</label></formula><p>The π ij denote the weight of training image j when predicting the annotation for image i. To ensure proper distributions, we require that π ij ≥ 0, and j π ij = 1. The introduction of is a technicality to avoid zero prediction probabilities when none of the neighbours j have the correct relevance value. In practice we fix = 10 -5 , although the exact value has little impact on performance.</p><p>The parameters of the model control the weights π ij . To estimate these parameters we maximize the log-likelihood of predicting the correct annotations for training images in a leave-one-out manner. Taking care to exclude each training image as a neighbour of itself, i.e. by setting π ii = 0, our objective is to maximize the log-likelihood:</p><formula xml:id="formula_6" coords="6,265.77,328.63,214.82,19.91">L = i,w ln p(y iw ).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rank-based weighting</head><p>In our experiments we use rank-based TagProp, which has shown good performance on the MIR Flickr database <ref type="bibr" coords="6,235.31,394.42,15.27,8.64" target="#b16">[17]</ref>. When using rank-based weights we set π ij = γ k if j is the k-th nearest neighbour of i. This directly generalizes a simple K nearest neighbour approach, where the K nearest neighbours receive an equal weight of 1/K. The data log-likelihood <ref type="bibr" coords="6,193.41,430.29,11.62,8.64" target="#b6">(7)</ref> is concave in the parameters γ k , and can be maximised using an EMalgorithm or a projected-gradient algorithm. In our implementation we use the latter because of its speed. To limit the computational cost of the learning algorithm we only allow non-zero weights for the first K neighbours, typically K is in the order of 100 to 1000. The number of parameters of the model then equals K. By pre-computing the K nearest neighbours of each training image the run-time of the learning algorithm is O(N K) with N the number of training images.</p><p>In order to make use of several different distance measures between images we can extend the model by introducing a weight for each combination of rank and distance measure. For each distance measure d we define a weight π d ij that is equal to γ dk if j is the k-th neighbour of i according to the d-th distance measure. The total weight for an image j is then given by the sum of weights π ij = d π d ij obtained using different distance measures. Again we require all weights to be non-negative and to sum to unity: j,d π d ij = 1. In this manner we effectively learn rank-based weights per distance measure, and at the same time learn how much to rely on the rank-based weights provided by each distance measure.</p><p>In the experiments we use a fixed K = 1000 independently from the number of distance measures used. So the effective number of k-NN per distance measures varies. E.g. when two distance measures are used, we take the 500 NN per distance measure. An image might occur twice, as neighbour according to both distance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-specific Logistic Discriminants</head><p>The weighted nearest neighbour model introduced above tends to have relatively low recall scores for rare annotation terms. This effect is easy to understand as in order to receive a high probability for the presence of a term, it needs to be present among most neighbours with a significant weight. This, however, is unlikely to be the case for rare annotation terms.</p><p>To overcome this, we introduce word-specific logistic discriminant model that can boost the probability for rare terms and possibly decrease it for frequent ones. The logistic model uses weighted neighbour predictions by defining:</p><formula xml:id="formula_7" coords="7,226.17,243.63,254.42,9.65">p(y iw = +1) = σ(α w x iw + β w ),<label>(8)</label></formula><formula xml:id="formula_8" coords="7,264.13,260.57,216.46,19.91">x iw = j π ij p(y iw = +1|j),<label>(9)</label></formula><p>where σ(z) = (1+exp(-z)) -1 is the sigmoid function, and x iw is the weighted nearest neighbour prediction for term w and image i c.f. Eq. 5. The word-specific models adds two parameters per annotation term.</p><p>In practice we estimate the parameters {α w , β w } and π ij in an alternating fashion. For fixed π ij the model is a logistic discriminant model, and the log-likelihood is concave in {α w , β w }, and can be trained per term. In the other step we optimize the parameters that control the weights π ij using gradient descent. We observe rapid convergence, typically after alternating the optimization three times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ImageCLEF Experiments</head><p>In this section we describe the experiments for the VCDT. We evaluate the performance of systems using the textual and visual modality and compare them to visual-only systems. Also, we investigate the performance of per-keyword-trained SVMs compared to TagProp. See Table <ref type="table" coords="7,214.52,466.68,4.98,8.64" target="#tab_0">1</ref> for an overview of our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Features</head><p>The dataset of this year's ImageCLEF VCDT was the MIRFlickr dataset <ref type="bibr" coords="7,440.58,514.81,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="7,453.03,514.81,11.83,8.64" target="#b11">12]</ref>. In contrast to last year, there were more concept classes (93) and the training set was extended to 8, 000 images. Also, in the 'multi-modal' approach it was allowed to use the provided textual 'Flickr-tag' information during both the training phase and test phase.</p><p>Features We extract our low level visual features from 32 × 32 pixel patches on regular grids (every 16 pixels) at five scales. Besides using 128-D SIFT-like Orientation Histograms (ORH) descriptors <ref type="bibr" coords="7,259.62,609.61,15.27,8.64" target="#b10">[11]</ref>, we also use simple 96-D colour features (COL) in the experiments. To obtain the latter, a patch is subdivided into 4 × 4 sub-regions (as for the SIFT descriptor) and in each sub-region the mean and standard deviation for the three R, G and B channels are computed. Both SIFT and colour features are reduced to 64 dimensions using Principal Component Analysis (PCA). In all our experiments, we use GMMs with M = 256 Gaussians to compute the Fisher vectors (referred also to as FV in which follows). The GMMs are trained using the Maximum Likelihood (ML) criterion and a standard Expectation-Maximization (EM) algorithm.</p><p>We extracted visual features using three spatial layouts (1 × 1, 2 × 2, and 1 × 3) as described in Section 2.3. The dimensionality of each FV is M × (2 * 64), since we take the derivative w.r.t. to mean and (diagonal) covariance. For each layout the component Fisher vectors were simply concatenated (e.g. 3 FVs in the 1 × 3 layout).</p><p>In some of the experiments we also use textual information (here the Flickr-tags). As textual representation for an image we use the binary absence/presence vector of the 698 most common tags among the over 53.000 provided Flickr-tags. We required each tag to be present in both the train-set and test-set, and for each tag to occur at least 25 times. This binary feature vector for each image i, is L2 normalized (denoted by t i ). The tag-similarity s T ij between the tags of image i and image j is the dot-product:</p><formula xml:id="formula_9" coords="8,134.77,482.03,48.81,12.32">s T ij = t i • t j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SVM Experiments</head><p>In these experiments we wanted to investigate on one hand the effect of using both visual and textual modalities, and on the other hand the different fusion techniques (early and late) in this context. Since we use the FV representation, with the corresponding dot-product similarities, we use linear SVM's for all experiments. In all our experiments, we used the LIBSVM package <ref type="bibr" coords="8,315.12,584.32,11.62,8.64" target="#b0">[1]</ref> with C = 1 (some preliminary crossvalidation results have shown this is a reasonable choice for this task).</p><p>Late Fusion For the late fusion experiments we have learned for each concept a classifier per low level feature (FV-ORH, FV-COL) and per spatial-layout (1x1, 2x2, 1x3) leading to 6 visual classifiers per concept. In additional, we trained a classifier per concept on the textual features (t i ). The scores of the Late Fusion SVM are obtained by averaging the scores of the individual classifiers with equal weights. For the mixed modality we average over 7 scores, and for the visual-only over 6 scores.</p><p>We have also included a visual-only late fusion experiment using Linear Sparse Logistic Regression (SLR) <ref type="bibr" coords="9,245.09,156.17,10.58,8.64" target="#b7">[8]</ref>, instead of SVM. SLR is a logistic regression classifier with a Laplacian prior. It uses the log-loss (instead of the hinge loss), and the probabilistic output might be more interpretable. Nevertheless, on all the measurements the corresponding SVM outperformed the SLR run (see Table <ref type="table" coords="9,369.16,192.04,3.60,8.64" target="#tab_1">2</ref>).</p><p>Early Fusion For the early fusion experiments we have to concatenate the feature vectors. Since we use the dot-product kernel K d (i, j), concatenation of feature vectors is equivalent to the Early Fusion kernel: K EF (i, j) = d K d (i, j). We learn one SVM per concept using this kernel. We have experimented with visual-only (d = {1, . . . , 6}) and mixed modality (d = {1, . . . , 7}) classifiers.</p><p>Scoring Note that only the final scores (after either late or early fusion) were normalized to be between 0 and 1, as required. We defined our confidence score as: x = (xmin(X))/(max(X) -min(X)). This normalization does not affect the ordering, and therefore does not influence the per concept evaluation. The threshold for the binary decision (for per image evaluation) was set to 0 on the original scoring function x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TagProp Experiments</head><p>Concerning TagProp we wanted to investigate on one hand the performance improvement by using the textual modality, and on the other hand the performance difference between SVM and TagProp using the FV representation. Therefore we have used exactly the same features, and distance measures, in these experiments as in the previous section. We have followed the word-specific rank-based TagProp, as described in Section 3. For all experiments we have used K = 1000, which is a good choice on this dataset as shown in <ref type="bibr" coords="9,214.21,453.82,15.27,8.64" target="#b16">[17]</ref>.</p><p>We have ran two different sets of experiments using TagProp, one with and one without combining the spatial-layouts. When combining the different spatial-layouts we sum over the kernels K d (i, j) of the three spatial layouts to compute a single FV-ORH and a single FV-COL kernel. This is equivalent to early fusion of the spatial layout vectors. Using these combined visual kernels reduces the number of similarities used in TagProp, therefore effectively more neighbours per similarity are used, which might result in a better set of nearest neighbours. The 3 rd (resp 7 th ) feature (see Table <ref type="table" coords="9,462.61,537.50,4.15,8.64" target="#tab_0">1</ref>) is the textual kernel based on t i . To obtain K = 1000 nearest neighbours from D different similarity measures, we select from each similarity measure the K d = ceil(K/D) neighbours, and concatenate those into K = {K 1 , . . . , K D }.</p><p>The output of TagProp is a probability value, therefore we use it directly as the confidence score. For the binary decision scores we use a threshold of .5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of the Results</head><p>Performance evaluation To determine the quality of the annotations five measures were used, three for the evaluation per concept and two for the evaluation per photo. For the evaluation per concept the mean Average Precision (mAP), the equal-error-rate (EER), and the area-under the curve (AUC) are used, using the confidence scores. For the evaluation per photo the example-based F-Measure (F-ex) and the Ontology Score with Flickr Context Similarity cost map (OS) are used, which uses the binary annotation scores. More details on these measures can be found in <ref type="bibr" coords="10,356.38,428.42,15.27,8.64" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Results</head><p>In Table <ref type="table" coords="10,262.02,450.30,4.98,8.64" target="#tab_1">2</ref> we list the performance of our submitted runs and the highest scoring competitors, sorted on the mAP value. In Fig. <ref type="figure" coords="10,395.85,462.25,4.98,8.64" target="#fig_1">3</ref> we show individual concept-based comparison of different algorithms, see also the caption for more details.</p><p>From these results we can deduce that:</p><p>-All our approaches using visual and tag features outperform any of the visual-only approach. The performance is increased in the order of 5 -8% in mAP. -While early fusion outperforms late fusion when we use the textual feature, there is no clear winner for the visual-only classifiers. The reason might be that the textual information is more complementary, while there is more redundancy between the different visual features. -Combining the spatial-layout features into a single similarity (used in TagProp)</p><p>gives slightly better results. This might be due to the fact that effectively more neighbours per similarity measure are used. -While linear-SVM classifiers outperform TagProp, the performance is quite similar, especially for the mixed modality approach. The latter might be due to the weights TagProp learns for the two modalities, while the SVM uses an equal weighting. This conclusion confirms the observations made in <ref type="bibr" coords="10,365.61,645.48,16.60,8.64" target="#b16">[17]</ref> using a different set of features. -Finally, the performance of our best visual-only classifier is close to the best scoring visual-only UvA-MKL classifier, and we are using a fast image representation with linear-SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our goal for the ImageCLEF VCDT 2010 challenge was to take advantage of the available textual information. The experiments have shown that all our methods combining visual and textual modalities outperform the best visual only classifiers. Our best scoring classifier obtains 45.5 % in mAP, about 5% higher than the best visual-only system. Besides we have compared two different approaches, linear SVM classifiers versus TagProp (a k-NN classifier). The results show that the SVM approach is superior to TagProp, but TagProp is able to compete. We believe that both these methods allow for learning from datasets with large number of images and/or concepts. Linear-SVMs have proven to scale to very large quantities images. TagProp is especially interesting for cases with many concepts and partially labelled datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,208.36,345.83,8.12;5,134.77,219.66,345.82,7.77;5,134.77,230.34,345.83,8.06;5,134.77,241.58,306.07,7.77"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of the values in the first dimension of the L2-normalized Fisher vector. (a), (b) and (c): resp. 16 Gaussians, 64 Gaussians and 256 Gaussians with no power normalization. (d): 256 Gaussians with power normalization (α = 0.5). Note the different scales. All the histograms have been estimated on the 5,011 training images of the PASCAL VOC 2007 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,134.77,361.26,345.83,8.12;11,134.77,372.57,345.82,7.77;11,134.77,383.53,345.82,7.77;11,134.77,394.49,345.82,7.77;11,134.77,405.44,198.11,7.77"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparisons of different submissions, in each figure the AP of each concept is plotted. Plot (a) shows the performance of the best scoring SVM classifier (V&amp;T) versus the visual only SVM. Plot (b) and (c) compares the early version late fusion SVM's. Plot (d) and (e) compares TagProp versus the early fusion SVM's. Plot (f) shows the performance of the best visual submission (UvA-MKL) versus our best visual only SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,161.44,116.82,292.48,158.72"><head>Table 1 .</head><label>1</label><figDesc>Overview of the submitted runs</figDesc><table coords="8,161.44,147.82,292.48,127.72"><row><cell cols="4">Name Modality Nr Features Remark</cell></row><row><cell>SVM</cell><cell>Mixed</cell><cell>7</cell><cell>Equally weighed late fusion</cell></row><row><cell>SVM</cell><cell>Mixed</cell><cell>7</cell><cell>Equally weighed early fusion</cell></row><row><cell>SVM</cell><cell>Visual</cell><cell>6</cell><cell>Equally weighed late fusion</cell></row><row><cell>SVM</cell><cell>Visual</cell><cell>6</cell><cell>Equally weighed early fusion</cell></row><row><cell>SLR</cell><cell>Visual</cell><cell>6</cell><cell>Equally weighed late fusion</cell></row><row><cell cols="2">TagProp Mixed</cell><cell>7</cell><cell></cell></row><row><cell cols="2">TagProp Mixed</cell><cell>3</cell><cell>Visual distance summed over three spatial layouts</cell></row><row><cell cols="2">TagProp Visual</cell><cell>6</cell><cell></cell></row><row><cell cols="2">TagProp Visual</cell><cell>2</cell><cell>Visual distance summed over three spatial layouts</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,116.82,345.82,227.29"><head>Table 2 .</head><label>2</label><figDesc>Overview of the performance of the different submissions. For reference we have included the best scoring (according to mAP) results of several competitors.</figDesc><table coords="10,175.62,150.64,261.87,193.48"><row><cell>Run</cell><cell cols="2">Modality mAP EER AUC</cell><cell>F-ex OS</cell></row><row><cell>SVM Early Fusion</cell><cell cols="2">V&amp;T 45.5 23.9 82.9</cell><cell>65.5 65.6</cell></row><row><cell>SVM Late Fusion</cell><cell cols="2">V&amp;T 43.7 24.3 82.6</cell><cell>62.4 63.7</cell></row><row><cell>TagProp CVD D3</cell><cell cols="2">V&amp;T 43.7 24.5 82.4</cell><cell>60.1 41.1</cell></row><row><cell>TagProp D7</cell><cell cols="2">V&amp;T 43.5 24.6 82.1</cell><cell>60.2 41.1</cell></row><row><cell>UvA MKL Mixed Mixed</cell><cell>V</cell><cell>40.7 24.4 82.6</cell><cell>68.0 59.1</cell></row><row><cell>SVM Late Fusion</cell><cell>V</cell><cell>39.0 25.8 80.9</cell><cell>62.7 63.8</cell></row><row><cell>SVM Early Fusion</cell><cell>V</cell><cell>38.9 26.3 80.5</cell><cell>63.9 64.5</cell></row><row><cell>SLR Late Fusion</cell><cell>V</cell><cell>37.1 26.1 80.6</cell><cell>60.0 58.2</cell></row><row><cell>TagProp CVD D2</cell><cell>V</cell><cell>36.4 27.3 79.3</cell><cell>58.0 38.5</cell></row><row><cell>TagProp D6</cell><cell>V</cell><cell>36.2 27.5 78.7</cell><cell>58.2 38.7</cell></row><row><cell>HHI S-IQ</cell><cell>V</cell><cell>34.9 28.6 78.2</cell><cell>62.8 63.6</cell></row><row><cell>IJS run1</cell><cell>V</cell><cell>33.4 28.1 78.8</cell><cell>59.6 59.5</cell></row><row><cell cols="3">MEIJI text and visual words V&amp;T 32.6 35.9 63.7</cell><cell>57.2 36.6</cell></row><row><cell>CNRS Mean Score 50</cell><cell cols="2">V&amp;T 29.6 35.2 70.2</cell><cell>35.1 39.1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,143.87,337.98,7.77;12,150.95,154.83,258.11,8.87" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,239.59,143.87,177.49,7.77">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,165.79,337.98,7.77;12,150.95,176.75,165.43,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,358.99,165.79,121.60,7.77;12,150.95,176.75,33.20,7.77">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,202.29,176.75,87.95,7.77">ECCV SLCV Workshop</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,187.71,337.98,7.77;12,150.95,198.67,211.33,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,403.55,187.71,77.04,7.77;12,150.95,198.67,91.64,7.77">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,269.73,198.67,66.40,7.77">VOC2008) Results</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,209.62,337.98,7.77;12,150.95,220.58,270.20,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,345.60,209.62,134.99,7.77;12,150.95,220.58,204.53,7.77">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,373.59,220.58,21.42,7.77">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,231.54,300.55,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,233.48,231.54,126.28,7.77">The MIR Flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM MIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,242.50,337.98,7.77;12,150.95,253.46,45.58,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,252.68,242.50,211.21,7.77">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,253.46,19.43,7.77">NIPS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,264.42,244.20,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,200.60,264.42,122.57,7.77">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,341.24,264.42,19.42,7.77">KDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,275.38,337.98,7.77;12,150.95,286.34,302.98,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,379.64,275.38,100.95,7.77;12,150.95,286.34,191.46,7.77">Sparse multinomial logistic regression: Fast algorithms and generalization bounds</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,348.67,286.34,21.59,7.77">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="957" to="968" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,297.30,337.98,7.77;12,150.95,308.25,196.34,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,282.83,297.30,197.76,7.77;12,150.95,308.25,128.83,7.77">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.73,308.25,23.42,7.77">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,319.21,338.35,7.77;12,150.95,330.17,81.69,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,291.32,319.21,189.27,7.77;12,150.95,330.17,16.40,7.77">Landmark classification in large-scale image collections</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,185.08,330.17,21.42,7.77">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,341.13,322.83,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,189.07,341.13,203.33,7.77">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,398.33,341.13,18.93,7.77">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,352.09,338.35,7.77;12,150.95,363.05,243.60,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,241.59,352.09,239.00,7.77;12,150.95,363.05,110.78,7.77">New strategies for image annotation: Overview of the photo annotation task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,280.21,363.05,88.19,7.77">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,374.01,338.35,7.77;12,150.95,384.97,49.57,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,242.71,374.01,222.01,7.77">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,384.97,23.42,7.77">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,395.93,338.35,7.77;12,150.95,406.89,113.57,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,299.60,395.93,180.99,7.77;12,150.95,406.89,45.79,7.77">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,214.46,406.89,23.91,7.77">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,417.84,338.35,7.77;12,150.95,428.80,84.68,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,306.28,417.84,174.31,7.77;12,150.95,428.80,16.25,7.77">Pegasos: Primal estimate sub-gradient solver for SVM</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,187.07,428.80,22.42,7.77">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,439.76,288.11,7.77" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="12,193.13,439.76,148.38,7.77">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,450.72,338.35,7.77;12,150.95,461.68,246.49,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,346.42,450.72,134.17,7.77;12,150.95,461.68,42.78,7.77">Image annotation with tagprop on the mirflickr set</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,211.23,461.68,143.89,7.77">ACM Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2010-03">mar 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
