<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,86.66,75.59,421.87,12.54;1,200.33,93.59,194.66,12.54">Meiji University at the ImageCLEF2010 Visual Concept Detection and Annotation Task: Working notes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.70,117.80,67.59,9.05"><forename type="first">Naoki</forename><surname>Motohashi</surname></persName>
							<email>motohashi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.37,117.80,40.69,9.05"><forename type="first">Ryo</forename><surname>Izawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.78,117.80,68.60,9.05"><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
							<email>takagi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,86.66,75.59,421.87,12.54;1,200.33,93.59,194.66,12.54">Meiji University at the ImageCLEF2010 Visual Concept Detection and Annotation Task: Working notes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E94BAF81F662937123E350379A5C07F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>Annotation, Bag of Visual words, Conceptual Fuzzy Sets, Visual Words Combination, ImageCLEF, Flickr</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Web Science Laboratory of Meiji University in the ImageCLEF2010 photo annotation task. We devised a system that combines conceptual fuzzy sets with visual words that have become popular in the fields of image retrieval and recognition. The utility of the system was verified by comparing it with the ordinary Bag of Visual words technique. In addition, we constructed a system using Flickr User Tags that was based on the same idea. Because there are many images without tags, the system integrates visual word and tag-based methods. The utility of the integrated system was verified in an experiment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The recent ImageCLEF2010 ran a Visual Concept Detection and Annotation Task <ref type="bibr" coords="1,412.70,494.05,25.14,9.05">[1] [2]</ref>. This task follows on from the one in 2009 and involved teams competing to create the most accurate automatic annotations of test images. The test images were annotated with words belonging to the 93 concepts of ImageCLEF. The concepts ranged from vague ones such as "outdoor" to rather concrete ones such as "dog", and they formed an ontology with a tree structure. The MIR Flickr 25.000 image dataset <ref type="bibr" coords="1,318.21,540.13,11.94,9.05" target="#b2">[3]</ref> was used as the data collection. 8,000 training images and 10,000 test images were chosen from this dataset. A big change from last year was that the participants could use Flickr User Tags. Hence, this task could be solved by using three approaches: visual only, tag only, and a mixed approach. Seventeen groups participated in this year's task, and altogether, they submitted 63 runs.</p><p>Our system combined conceptual fuzzy sets with visual words, which have recently become popular in the fields of image retrieval and recognition. We verified the utility of the system by comparing it with the ordinary Bag of Visual words technique.</p><p>This paper is organized as follows. Section 2 describes conceptual fuzzy sets. Section 3 describes the visual words approach. Section 4 describes the technique of applying conceptual fuzzy sets to visual words. Section 5 describes our overall system. Section 6 describes the five variants that we submitted and the results of our experiments. In Section 7, we discuss the results and their implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CONCEPTUAL FUZZY SETS</head><p>A Conceptual Fuzzy Set (CFS) <ref type="bibr" coords="1,203.93,709.36,11.69,9.05" target="#b3">[4]</ref>  <ref type="bibr" coords="1,219.49,709.36,11.69,9.05" target="#b4">[5]</ref> is a theory to solve the problem of situated cognition of the meaning representation for the concept, and it is based on the theory of meaning propounded by Wittgenstein. Simply put, it is a theory that aims to make a computer understand the meanings of words that change from situation to situation. For example, a dictionary gives the word "Java" three meanings: the "name of an island and region", a "brand or type of coffee", and a "programming language". Thus, a computer cannot understand which meaning to use unless it is supplied with a sufficient context. The vagueness can be resolved by considering the words appearing around Java. For instance, if "Java" co-occurs with the word "computer", Java likely refers to the programming language. A CFS for "Java" is thus the word plus links to other words related to or co-occurring with it that distinguish its meanings. We devised a technique for applying CFS to visual words that are made from clustered keypoints of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUAL WORDS APPROACH</head><p>First, the keypoints are extracted from an image by using SIFT <ref type="bibr" coords="2,336.19,179.84,10.77,9.05" target="#b5">[6]</ref>, Second, they are clustered using k-means clustering, and the center of each cluster is taken to be a visual word of that cluster. Reference <ref type="bibr" coords="2,456.40,191.36,11.69,9.05" target="#b6">[7]</ref> indicates that image recognition and retrieval using visual words has been actively researched in recent years. The Bag of Visual words <ref type="bibr" coords="2,127.05,214.28,11.77,9.05" target="#b7">[8]</ref> is a popular technique. In this technique, the keypoints are replaced with visual words, and the image is represented by a frequency histogram of visual words. The images are retrieved on the basis of the similarity of this histogram. However, visual words have a drawback in that they are vaguer than actual words. For instance, a word like "Dog" may refer to an object in an image, but it is difficult for visual words to be used to recognize such an object because they are only for one point in the image. Some research <ref type="bibr" coords="2,442.83,260.27,11.76,9.05" target="#b8">[9]</ref> has been done in an attempt to resolve this vagueness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL WORDS COMBINATION APPROACH</head><p>The Visual Words Combination Approach (VWCA) is a model that the CFS applies to visual words. Several visual words are combined in order to decrease their vagueness. The Bag of Visual words technique was originally proposed as an analogy of the Bag of Words approach, and visual words have the same meaning as words in sentences. Thus, CFS, which is useful for sentence retrieval, might also be useful for visual words. Moreover, this idea considers human psychology; someone visually recognizes an object by seeing many points at the same time. For instance, we recognize someone's face by seeing the eyes, nose, and mouth at the same time. The Visual Words Combination (VWC) is a word set generated by combining several visual words. For example, ten pairs (combinations) can be generated from five visual words. Because we cannot judge which visual words are related to each other, we generated various combinations by using this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM DESCRIPTION</head><p>This section explains the system that we made for this task. First, we will explain the system using only visual information. After that, we will explain the system that uses Flickr User Tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Dictionary</head><p>The Visual Dictionary (VD) was our dictionary that contains visual words. Visual words were generated by quantizing keypoints extracted from images. We randomly chose 1,200 images from the 8,000 training images, and made 4,000 visual words from about 830,000 keypoints <ref type="bibr" coords="2,320.22,550.45,15.94,9.05" target="#b9">[10]</ref> <ref type="bibr" coords="2,336.16,550.45,15.94,9.05" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CFS System (VisualOnly_CFS_of_meiji)</head><p>The CFS system was constructed by using VWCs. The procedure, from preprocessing to annotation, is as follows.</p><p>1 ) The test images are converted into a frequency histogram of visual words.</p><p>2 ) The VWCs are generated from the result of step 1).</p><p>3 ) Compare the test image's VWCs with the visual words in the casebase made from the training images, and identify similar images. 4 ) The test images are annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Frequency histogram</head><p>The test images were converted into a frequency histogram of visual words. The procedure is as follows and is illustrated in Figure <ref type="figure" coords="2,168.47,722.10,3.78,9.05" target="#fig_0">1</ref>.</p><p>1 ) The keypoints are extracted from the test image by using SIFT.</p><p>2 ) Each keypoint is assigned into a visual word in the Visual Dictionary by using the nearest neighbor algorithm <ref type="bibr" coords="3,140.54,73.02,15.43,9.05" target="#b11">[12]</ref>. 3 ) A histogram is made from the visual words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Generation of VWC</head><p>Next, the VWCs of the test image were generated. We needed to use a small number of visual words. For instance, the VWCs generated from 300 visual words in combinations of 3 would amount to 3 300C . As a result, 4,455,100 combinations would be generated. The calculation cost increases when using a lot of visual words. Hence, we used only 20 visual words that had high frequencies in the histogram and made 1,140 VWCs from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Construction of the Casebase</head><p>The casebase for the CFS system to annotate the 10,000 test images was made from 8,000 training images. Figure <ref type="figure" coords="3,107.90,408.49,4.98,9.05" target="#fig_1">2</ref> shows an example of the casebase. The casebase stored relations such as "If this image has these visual words, then this image has these concepts". The training images were also converted into histograms in the same way and stored as combinations of 20 visual words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Retrieval</head><p>We retrieved similar images in the casebase by matching VWCs generated from the test image and the training images. If a training image had a VWC of the test image, the matching produced a hit. If it didn't have a VWC, the matching was considered to be a miss. If the number of hits for a certain training image exceeded ten, the images were considered to be similar. An example of matching is shown in Fig. <ref type="figure" coords="3,430.07,720.30,3.76,9.05" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Annotation</head><p>The confidence was the word frequency in the retrieval results divided by the number of retrieval results. In addition, when confidence was converted into a binary value of either 0 or 1, if the concept had a confidence of 0.2 or more, the concept's binary value was taken to be 1. If the value was less than 0.2, the binary value was 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Bag of Visual words system (VisualOnly_BagOfVisual wordss_of_meiji)</head><p>The Bag of Visual words system was used as a baseline for evaluating the CFS system. The procedure to annotate the test images is almost the same as in the CFS system. The difference is that left side of the casebase of the Bag of Visual Words system in Fig. <ref type="figure" coords="4,262.08,399.49,4.98,9.05" target="#fig_1">2</ref> is not 20 visual words but the frequency histogram in 5.2.1. Twenty training images were retrieved from the casebase, and each test image was annotated in the same way as described in section 5.2.5. The cosine measure was used to determine the degree of similarity between the test image and the training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Flickr User Tag System</head><p>We describe about a system using Flickr User Tag. The procedure from preprocessing to annotation is as follows.</p><p>1 ) The casebase is constructed in advance.</p><p>2 ) The confidence between the test image and concept is calculated.</p><p>3 ) The test images are annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Construction of the Casebase</head><p>The casebase here is different from the casebase explained in 5.2.3. It was constructed as follows.</p><p>1 ) The training images for each concept are collected.</p><p>2 ) The tags of the collected images are collected.</p><p>3 ) The term frequency (TF) and the document frequency (DF) of each tag are calculated, and the tags are weighted by using the TF-IDF method. 4 ) The casebase (Fig. <ref type="figure" coords="4,183.58,637.48,4.17,9.05" target="#fig_3">4</ref>) paraphrasing concepts with tags is constructed by using tags having the calculated TF-IDF values. In the figure, the numerical values in parentheses are the TF-IDFs.</p><p>We shall illustrate this procedure by using the concept "Dog" as an example. The training images come annotated with concepts from ImageCLEF. Of the 8000 images, there were 211 images with the concept "dog" attached to them. Moreover, the training images had tags plus the concept. The tags of the 211 images were collected. The TF-IDF value of each tag was then calculated and the tag and its value were stored.</p><p>The casebase described in sec. 5.2.3 stored the 8,000 cases, but the casebase here stored 93 cases. The idea behind this casebase comes from the theory of meaning propounded by Wittgenstein. According to the theory of meaning, the meaning of a word can be represented by another word. In the example of Figure <ref type="figure" coords="4,471.58,740.94,3.76,9.05" target="#fig_3">4</ref>, "partylife" is paraphrased by "concert" and "fireworks". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Confidence Calculation</head><p>Next, the confidence between the test image and each concept is calculated. The procedure is as follows (an example is shown in Fig. <ref type="figure" coords="5,180.62,323.87,3.67,9.05" target="#fig_4">5</ref>).</p><p>1 ) The tags of the concept and the test image are matched, and the TF-IDFs of the tags that become hits are added together. The total TF-TDF is stored in the concept. 2 ) Step 1) is repeated for all concepts.</p><p>3 ) All concepts are regularized by the maximum value of each concept, and this value is assumed to be the confidence. 4 ) The steps from 1) to 3) are repeated for all test images.</p><p>When the tag of the test image and concept were compared, combinations like those in section 5.2.2 were not always generated because there were many unsuitable tags. For instance, there were spelling mistakes like "Mweeting" and sentences like "Girlecstaticallydancingondnbtuneswhileholdingaglass ofbeerinherhand".</p><p>Moreover, about 1,000 images of the 10,000 test images didn't have any tags; i.e., only about 9,000 images were annotated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Conversion to Binary</head><p>Every concept, not every image, was considered when the confidence was converted into a binary value. For every concept, the confidences of all test images were sorted in descending order. The binaries of the test images that had the top n confidences became 1 because these images may have had a strong relationship with each concept. The binaries of the other test images were 0.</p><p>The value of n was determined stochastically. For instance, in the example described in 5.4.1, the concept "Dog" was annotated on 211 of the 8000 training images. Note that when calculating the number of test images that are annotated to 9,000 images stochastically, the concept "Dog" is sure to be annotated to 237 test images in 9000 training images. Figure <ref type="figure" coords="6,114.02,107.48,4.98,9.05">6</ref> shows an example of converting the concept "1: partylife". In this case, n is 2.</p><p>Fig. <ref type="figure" coords="6,187.20,464.05,4.98,9.05">6</ref> Example of converting a confidence score into a binary value</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">AND RESULTS</head><p>Below, we describe the five systems that we submitted and their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Submitted Systems</head><p>1. VisualOnly_CFS_of_meiji This system is the CFS system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VisualOnly_BagOfVisual words_of_meiji</head><p>This system is the Bag of Visual words system. 3. Mixed_CFS_and_Tags_of_meiji 9,000 images that have tags attached to them are annotated by using the Flickr User Tag System. The remaining 1,000 images are annotated with the CFS system. This system integrated the results of systems 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mixed_Tag_Based_CFS_of_meiji.txt</head><p>This system attempts to improve the result of system 1 by using the Flickr User Tag System.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Mixed_Text_Based_BagOfVisual_words_of_meiji</head><p>This system attempts to improve on the result of system 2 by using the Flickr User Tag System.</p><p>The improvements to system's 4 and 5 were to refine the annotation results of systems 1 and 2 by using the results of the Flickr User Tag System. The confidence of each concept was calculated by using the Flickr User Tag System on all test images. Seven concepts that had high confidence were employed to improve the results of systems 1 and 2. The confidences of these seven concepts of systems 1 and 2 added up to 0.5. They were converted into a binary value of 1 because they exceeded 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>First, we can see that system 2, which uses the original Bag of Visual words, is more accurate than system 1 that uses CFS. The CFS result was not as good as we expected because the number of visual words was limited in order to reduce the processing cost. In short, the image was represented by only 20 visual words, and this is not enough information. Moreover, there is a research result showing that visual words whose frequencies are high are not so important <ref type="bibr" coords="7,251.33,137.00,15.39,9.05" target="#b12">[13]</ref>. Thus, the visual words were improperly selected. On the other hand, the accuracy went up when the tags were used. However, the improvement was small. The reason is that the seven concepts were compulsorily annotated when the results of system 1 and 2 improved and this led to the possibility that noise concepts were annotated. System 5 was the most accurate of the systems that we submitted. We ranked 23rd out of 63 runs and 6th among 17 teams. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>The system that improved the results of the Bag of Visual words method by using Flickr User Tags was the most accurate one that we submitted. This means that annotations using text information like tags are effective. Unfortunately, the system did not have as good a result as we expected. We think the problem was that the selection and the number of visual words were not suitable (see sec. 6.2). It will be necessary to verify which visual words to select in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP</head><p>Average F-ex OS with FCS </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,248.33,246.23,112.74,9.05;3,131.90,93.85,345.60,150.25"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Frequency Histogram</figDesc><graphic coords="3,131.90,93.85,345.60,150.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,212.33,628.60,170.41,9.05;3,182.05,446.30,245.25,174.70"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Example of Casebase (CFS System)</figDesc><graphic coords="3,182.05,446.30,245.25,174.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,244.25,256.44,127.88,9.31;4,139.55,73.45,337.35,174.95"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Example of Matching</figDesc><graphic coords="4,139.55,73.45,337.35,174.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,197.09,260.51,215.03,9.05;5,148.55,82.45,312.35,175.90"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Example of Casebase (Flickr User Tag System)</figDesc><graphic coords="5,148.55,82.45,312.35,175.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,212.93,645.16,169.11,9.05;5,71.30,471.10,452.75,171.75"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Example of Calculation Confidence</figDesc><graphic coords="5,71.30,471.10,452.75,171.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,93.40,132.25,418.50,330.00"><head></head><label></label><figDesc></figDesc><graphic coords="6,93.40,132.25,418.50,330.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,242.33,206.00,134.96,9.05"><head>Table 1 :</head><label>1</label><figDesc>Results of Meiji Systems</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,92.30,473.05,338.39,9.05" xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://www.imageclef.org/2010/PhotoAnnotation" />
	</analytic>
	<monogr>
		<title level="j" coord="7,92.30,473.05,134.98,9.05">ImageCLEFphotoAnnotation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,496.09,431.94,9.05;7,92.30,507.49,387.75,9.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,257.27,496.09,266.98,9.05;7,92.30,507.49,147.82,9.05">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.87,507.49,133.21,9.05">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,530.53,254.00,9.05" xml:id="b2">
	<monogr>
		<idno>25.000</idno>
		<ptr target="http://press.liacs.nl/mirflickr/" />
		<title level="m" coord="7,92.30,530.53,45.17,9.05;7,170.07,530.53,53.31,9.05">image dataset</title>
		<imprint/>
	</monogr>
	<note>MIR Flickr</note>
</biblStruct>

<biblStruct coords="7,92.30,553.57,431.97,9.05;7,92.30,565.12,408.81,9.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,304.82,553.57,219.45,9.05;7,92.30,565.12,108.61,9.05">Conceptual fuzzy sets as a meaning representation and their inductive construction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,211.50,565.12,171.31,9.05">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="929" to="945" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,588.04,432.25,9.05;7,92.30,599.56,307.77,9.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,306.99,588.04,217.56,9.05;7,92.30,599.56,14.55,9.05">Multilayered reasoning by means of conceptual fuzzy sets</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,116.66,599.56,171.15,9.05">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,622.60,432.16,9.05;7,92.30,634.12,204.72,9.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,152.60,622.60,254.92,9.05">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,422.74,622.60,101.72,9.05;7,92.30,634.12,66.64,9.05">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,657.03,432.24,9.06;7,92.30,668.55,137.73,9.06" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,204.50,657.04,286.86,9.05">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,503.02,657.03,21.52,9.06;7,92.30,668.55,108.64,9.06">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.30,691.60,432.07,9.05;7,92.30,703.12,315.78,9.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,280.18,691.60,185.35,9.05">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,490.76,691.60,33.62,9.05;7,92.30,703.12,240.45,9.05">Proc. of ECVWCorkshop on Statistical Learning in Computer Vision</title>
		<meeting>of ECVWCorkshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="59" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.30,73.02,432.19,9.05;8,92.30,84.56,432.01,9.05;8,92.30,95.96,214.71,9.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,469.80,73.02,54.70,9.05;8,92.30,84.56,40.72,9.05">Visual words Ambiguity</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cor</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.132</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,148.10,84.56,282.55,9.05">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.94,119.00,411.20,9.05;8,92.30,130.52,406.15,9.05" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,399.53,119.00,124.61,9.05;8,92.30,130.52,188.50,9.05">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,303.03,130.52,104.29,9.05">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.26,153.56,424.85,9.05;8,99.26,164.96,424.74,9.05;8,99.26,176.48,22.65,9.05" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,282.85,153.56,241.26,9.05;8,99.26,164.96,225.44,9.05">Fraunhofer FIRST&apos;s Submission to ImageCLEF2009 Photo Annotation Task: Non-sparse Multiple Kernel Learning</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,348.58,164.96,107.39,9.05">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.26,199.52,348.94,9.05" xml:id="b11">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Nearest_neighbor_algorithm" />
		<title level="m" coord="8,99.26,199.52,113.21,9.05">Nearest neighbour algorithm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.26,222.56,424.90,9.05;8,99.26,233.96,149.28,9.05" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,210.60,222.56,227.48,9.05">Creating Efficient Codebooks for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.41,222.56,71.75,9.05;8,99.26,233.96,66.64,9.05">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="604" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
