<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.28,115.96,330.79,12.62;1,162.06,133.32,291.25,12.62;1,158.74,150.67,297.89,12.62">Text-and Content-based Approaches to Image Modality Detection and Retrieval for the ImageCLEF 2010 Medical Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.59,187.96,99.69,8.74"><roleName>Md</roleName><forename type="first">Matthew</forename><surname>Simpson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.60,187.96,85.30,8.74"><forename type="first">Mahmudur</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.86,187.96,62.33,8.74"><forename type="first">Sachin</forename><surname>Singhal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,430.87,187.96,20.89,8.74;1,188.58,199.53,74.47,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.68,199.53,62.89,8.74"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.93,199.53,64.86,8.74"><forename type="first">George</forename><surname>Thoma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.28,115.96,330.79,12.62;1,162.06,133.32,291.25,12.62;1,158.74,150.67,297.89,12.62">Text-and Content-based Approaches to Image Modality Detection and Retrieval for the ImageCLEF 2010 Medical Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C3D9B9F23C1C3BECDDF2DD0A732CC622</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Retrieval</term>
					<term>Case-based Retreival</term>
					<term>Image Modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation of the Image and Text Integration (ITI) group from the U.S. National Library of Medicine (NLM) in the ImageCLEF 2010 medical retrieval track. Our methods encompass a variety of techniques relating to document summarization and text-and content-based image retrieval. Our text-based approaches utilize the Unified Medical Language System (UMLS) synonymy to identify concepts in information requests and image-related text in order to retrieve semantically relevant images. Our image content-based approaches utilize similarity metrics based on computed "visual concepts" and lowlevel image features to identify visually similar images. In this article we present an overview of the application of our methods to the modality detection, ad-hoc image retrieval, and case-based retrieval tasks and describe our submitted runs and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of the Image and Text Integration (ITI) group from the U.S. National Library of Medicine (NLM) in the ImageCLEF 2010 medical retrieval track.</p><p>ImgeCLEFmed <ref type="bibr" coords="1,211.79,494.10,35.54,8.74">'10 [12]</ref> consists of an image modality detection task and two medical retrieval tasks. For the modality detection task, the goal is to automatically classify given medical images according to eight modalities (e.g., CT or MRI). In the first retrieval task, a set of ad-hoc information requests is given, and the goal is to retrieve the most relevant images for each topic. Finally, in the second retrieval task, a set of case-based information requests is given, and the goal is to retrieve the most relevant articles describing similar cases.</p><p>In the following sections, we describe the text-and content-based features that comprise our image and case representation (Sections 2-3) and our methods for the modality detection (Section 4) and medical retrieval tasks (Sections 5-6). Our text-based retrieval approach relies on mapping information requests and image-related text to concepts in the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,134.77,632.97,10.52,8.74" target="#b7">[8]</ref> Metathesaurus, and our modality detection and content-based retrieval approaches analogously rely on mapping the content of medical images to "visual concepts" using supervised machine learning techniques.</p><p>In Section 7, we describe our submitted runs, and in Section 8 we present our results. For the modality detection task, our best submission achieved a classification accuracy of 92% which was the 2nd ranked submission overall. For the retrieval tasks, our results were lower than expected yet reveal new insights which we anticipate will improve future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Representation</head><p>Images contained in biomedical articles can be represented using both textand content-based features. Text-based features include text that pertains to an image, such as in captions and "mentions" (snippets of text within the body of an article that discuss an image), and content-based features include information derived from the image itself, such as shapes, colors and textures. We describe our text-and content-based image representations below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-Based Features</head><p>We represent each image in the ImageCLEFmed'10 collection <ref type="bibr" coords="2,399.07,320.84,15.50,8.74" target="#b11">[12]</ref> as a structured document of image-related text. Our representation includes the title, abstract, and MeSH terms<ref type="foot" coords="2,206.92,342.41,3.97,6.12" target="#foot_0">1</ref> of the article in which the image appears as well as the image's caption and mention.</p><p>We organize the content of an image's caption into the well-formed clinical question framework following the method described by Demner-Fushman and Lin <ref type="bibr" coords="2,153.07,390.28,9.96,8.74" target="#b2">[3]</ref>. Extractors identify UMLS concepts related to problems, interventions, age, anatomy, drugs, and image modality. We assign one of the eight modality classes to an image according to the extracted modality terms. Additionally, we extract textual Regions of Interest (ROIs) from image captions. A textual ROI is a noun phrase describing the content of an interesting region of an image which is identified within a caption by a pointer. For example, in the caption "MR image reveals hypointense indeterminate nodule (arrow)," the word arrow points to the ROI containing a hypointense indeterminate nodule.</p><p>The above structured documents can be indexed and searched with a traditional search engine or the extracted concepts may be combined with additional features (discussed below) for use in a multimodal representation. For the latter approach, "keywords" in a structured document D j can be represented as an N -dimensional feature vector</p><formula xml:id="formula_0" coords="2,239.77,544.59,240.82,15.05">f keyword j = [w j1 , w j2 , • • • , w jN ] T<label>(1)</label></formula><p>where w jk denotes the weight (typically tf-idf ) of keyword t k in document D j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Content-Based Features</head><p>In addition to the above textual features, we also represent the visual content of images using various low-level global image features and several derived features intended to capture high-level semantic content. For concept model generation, we utilize a multi-class SVM composed of binary SVM classifiers combined using the one-against-one strategy <ref type="bibr" coords="3,427.82,321.54,9.96,8.74" target="#b4">[5]</ref>. To train the SVM, a set of L labels are assigned as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level Global Features</head><formula xml:id="formula_1" coords="3,134.77,333.11,345.83,21.22">C = {c 1 , • • • , c i , • • • , c L }, where each c i ∈ C characterizes a visual concept.</formula><p>The training set consists of local patches generated by a fixed-partition and represented by a combination of color and texture moment-based features. The input to the system is the feature vectors for patches along with their manually assigned concept labels. Concept labels are assigned by fixed partitioning each image</p><formula xml:id="formula_2" coords="3,302.76,390.95,177.84,9.68">I j into l regions as {x 1j , • • • , x kj , • • • , x lj },</formula><p>where each x kj ∈ d is a combined color and texture feature vector. For each x kj , its category c m is determined by the prediction of the multi-class SVM. Hence, instead of the low-level feature-based representation, an entire image is represented as a two-dimensional index linked to visual concepts. Based on this encoding scheme, an image I j is represented as a vector of concepts</p><formula xml:id="formula_3" coords="3,233.88,470.11,246.71,15.05">f concept j = [w 1j , • • • , w ij , • • • w Lj ] T<label>(2)</label></formula><p>where each w ij denotes the "tf-idf " weight of a concept c i , 1 ≤ i ≤ L in image I j , depending on its information content.</p><p>"Bag of Keypoints" Feature We also extract robust and invariant image features that are commonly termed affine region detectors <ref type="bibr" coords="3,397.97,547.17,14.61,8.74" target="#b10">[11]</ref>. These regions simply refer to a set of pixels or interest points, which are invariant to affine transformations as well as occlusion, lighting, and intra-class variations. We use the Harris-affine detector to locate interest points <ref type="bibr" coords="3,350.95,581.89,15.50,8.74" target="#b9">[10]</ref> as a large number of overlapping regions. We then associate with each interest point a vector descriptor invariant to viewpoint changes and, to some extent, illumination changes computed from the intensity pattern within the point. We use a local descriptor developed by Lowe <ref type="bibr" coords="3,220.94,628.18,10.52,8.74" target="#b8">[9]</ref> based on the Scale-Invariant Feature Transform (SIFT), to describe the information in a set of scale-invariant coordinates. The SIFT descriptor is chosen to be invariant to viewpoint changes and, to some extent, illumination changes, and to discriminate between the regions. The above features are vector quantized by a self-organizing map (SOM)-based clustering. Finally, images are represented by a bag of these quantized features (i.e., a bag of keypoints). Hence, the model is applied to images by using a visual analogue of the bag of words model used in text retrieval <ref type="bibr" coords="4,316.31,441.65,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Case Representation</head><p>We represent an article describing a patient's case by combining the textual features of each image contained in the article into a single surrogate document. Thus, each case representation consists of the article's title, abstract, and MeSH terms as well as the caption, mention and textual ROIs of each image contained in the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modality Detection Task</head><p>Owing to their empirical success, we utilize multi-class SVMs to classifying images into eight image modalities <ref type="bibr" coords="4,279.05,609.81,15.50,8.74" target="#b11">[12]</ref> based on the above features. We compose multi-class SVMs by using the one-against-one method [5] for combining the pairwise classifications of each binary SVM.</p><p>Figure <ref type="figure" coords="4,182.14,644.55,4.98,8.74" target="#fig_0">1</ref> shows the overall modality detection process. Textual and visual features can be used individually or combined to form a single feature vector, and the output of the multi-class SVMs can be used as separate predictions or "fused" to form a single classifier. We use the popular classifier combination techniques derived from Bayes' theory (product, sum, maximum and mean rules) <ref type="bibr" coords="5,134.77,153.71,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="5,146.95,153.71,7.75,8.74" target="#b6">7]</ref> for fusing separate classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ad-Hoc Image Retrieval Task</head><p>In this section we describe our text-and content-based approaches to image retrieval. The methods may be combined (e.g., by re-ranking retrieved images) to form multimodal approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text-Based Approach</head><p>We use the NLM-developed Essie <ref type="bibr" coords="5,289.08,282.77,10.52,8.74" target="#b5">[6]</ref> search engine to index our collection of structured image documents and retrieve relevant images. Key features of Essie that make it particularly well-suited to the medical retrieval track include its automatic expansion of query terms along synonymy relationships in the UMLS Metathesaurus and its ability to weight term occurrences according the location of the document in which they occur. For example, term occurrences in an image caption can be given a higher weight than those in the abstract of the article in which the image appears.</p><p>To construct queries for each topic, we organize each information request according to the well-formed clinical question framework, extracting UMLS concepts relating to problems, interventions, age, anatomy, drugs, and image modality. This procedure is identical to that described in Section 2. <ref type="bibr" coords="5,403.64,410.16,4.24,8.74" target="#b0">1</ref> We use three methods of varying specificity for combining the extracted terms to form queries. First, the term-based method produces the OR of each extracted term. Second, the type-based method first applies the term-based method for each type (problem, intervention, etc.) and then ANDs the result for each type group. Finally, the sentence-based method first applies the type-based method for each topic sentence and then ANDs the result for each sentence.</p><p>Additionally, we may expand each query to include concepts taken from the definition of problems extracted from the original topic. Query expansion using problem definitions applies to each query construction method described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Content-Based Approach</head><p>Our content-based image retrieval approach is based on retrieving images that are visually similar to the given topic images. The similarity between a query image I q and target image I j is defined by</p><formula xml:id="formula_4" coords="5,235.39,609.81,245.21,22.37">Sim(I q , I j ) = F α F Sim F (I q , I j )<label>(3)</label></formula><p>where F ∈ {Concepts, Keypoints, EHD, CLD, CEDD, FCTH} and α F are the weights within the different image representations.</p><p>The feature weights are determined based on the 5-fold cross-validation (CV) accuracies of retrieval on the training set of images. The weights are normalized to 0 ≤ α F ≤ 1 and α F = 1 for F ∈ {Concept, Keypoint, EHD, CLD, CEDD, FCTH}. In addition, based on the online category prediction of a query image, precomputed category-specific feature weights (e.g., α F ) are utilized in the above linear combination of the similarity matching function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case-Based Retrieval Task</head><p>Our method for performing case-based retrieval is analogous to our text-based approach for ad-hoc image retrieval. Here, we use the Essie <ref type="bibr" coords="6,406.77,243.36,10.52,8.74" target="#b5">[6]</ref> search engine to index the structured case documents and construct queries for each case descriptions as described in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Submitted Runs</head><p>In this section we describe each of our submitted runs for the modality detection, ad-hoc image retrieval, and case-based retrieval runs. Each run is identified by its (abbreviated) ID used with the trec_eval program and followed by a submission mode (textual, visual or mixed). All submitted runs are automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Modality Detection Task</head><p>We submitted the following 10 runs for the modality detection task:</p><p>1. result image combined (visual): SVM classification combining an image's visual features (Concept, Keypoint, CLD, EHD, CEDD and FCTH) in a single feature vector. 2. result image comb cv (visual): Classifier combination weighting the underlying classifiers according to their normalized cross validation accuracies. Visual features are each considered individually for SVM classification. 3. result image comb sum (visual): Classifier combination using the "Sum" method of Bayes' theorem where an image's visual features are each considered individually for SVM classification. 4. result image comb max (visual): Classifier combination using the "Maximum" method of Bayes' theorem where an images' visual features are each considered individually for SVM classification. 5. result text title caption mod mesh (textual): SVM classification combining an image's textual features (tf-idf of keywords extracted from the title, caption, modality, and MeSH fields of an image's textual representation) as a single feature vector. 6. result text image combined (mixed): SVM classification combining an image's textual and visual features as a single feature vector. 7. result text image comb sum (mixed): Classifier combination using the "Sum" method of Bayes' theorem where an image's textual and visual features are each considered individually for SVM classification.</p><p>8. result text image comb prod (mixed): Classifier combination using the "Product" method of Bayes' theorem where an image's textual and visual features are each considered individually for SVM classification. 9. result text image comb max (mixed): Classifier combination using the "Maximum" method of Bayes' theorem where an image's textual and visual features are each considered individually for SVM classification. 10. result text image comb cv (mixed): Classifier combination weighting classifiers according to their normalized cross validation accuracies. Textual and visual features are each considered individually for SVM classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ad-hoc Image Retrieval Task</head><p>We submitted the following 10 runs for the ad-hoc image retrieval task:</p><p>1. queries terms (textual): Essie search using term-based query construction. 2. expanded queries terms (textual): Essie search like run (1) but with query expansion using problem definitions. 3. queries terms modalities (mixed): Re-ranking of (1) according to topic image modality (determined by our modality detection approach) applied to the retrieved images' text-based modality class. 4. fusion cv merge max (visual): Similarity matching using visual features (Concept, Keypoint, CLD, EHD, CEDD and FCTH) that are each weighted according to their normalized cross validation accuracy (from the modality detection task). All topic images produce individual result lists that are then merged based on the maximum score of each retrieved image. 5. fusion cv merge mean (visual): Similarity matching according to run (4). All topic images produce individual result lists that are then merged based on the mean score of each retrieved image. 6. fusion cat merge max (visual): Similarity matching using visual features that are each weighted according to online modality classification. All topic images produce individual result lists that are then merged based on the maximum score of each retrieved image. 7. adhoc queries citations cbir cv merge max (mixed): Re-ranking of run <ref type="bibr" coords="7,467.86,490.28,12.73,8.74" target="#b0">(1)</ref> according to run (4). 8. adhoc exp queries citations cbir cv merge max (mixed): Re-ranking of run</p><p>(2) according to run (4). 9. adhoc exp queries citations cbir cat merge max (mixed): Re-ranking of run</p><p>(2) according to run (6). 10. multimodal rerank roi qe merge (mixed): Re-ranking of run (1) according to visual Region of Interest (ROI) detection. Concept features from the ROIs of retrieved images are extracted and added to the Concept features of the original topic image (a form of query expansion). Similarity matching is then performed in the Concept space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Case-based Retrieval Task</head><p>We submitted the following 10 runs for the case-based retrieval task:  <ref type="bibr" coords="8,370.62,466.04,12.73,8.74" target="#b6">(7)</ref> but only considering terms related to image modality and anatomy. 9. queries cbir without case backoff (mixed): Essie search like run (3) but forming the query using the captions of the top 3 images retrieved using a contentbased approach. 10. queries cbir with case backoff (mixed): Essie search like run <ref type="bibr" coords="8,415.17,523.88,12.73,8.74" target="#b8">(9)</ref> but also including the original topic case in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>Table <ref type="table" coords="8,161.65,598.25,4.98,8.74" target="#tab_1">1</ref> presents the classification accuracy of our submitted runs for the modality detection task. result text image combined, a multimodal approach, achieved the highest accuracy (92%) of our submitted runs and was ranked 2nd overall. Additionally, our text-based approach performed surprising well, achieving a classification accuracy of 89%, whereas our content-based approaches alone performed poorest. This result indicates that the combination of textual and visual features can be leveraged to significantly improve the automatic modality classification of images found in biomedical articles. Table <ref type="table" coords="9,177.43,489.17,4.98,8.74" target="#tab_2">2</ref> presents the results of our submitted runs for the ad-hoc image retrieval task. While our text-based submissions performed better than either the multimodal or content-based submissions, the Mean Average Precision (MAP) was much lower than expected given our prior experience <ref type="bibr" coords="9,392.08,523.89,14.61,8.74" target="#b13">[14]</ref>. We have determined that this discrepancy is likely due to noise in our text-based image representation, specifically concerning the extraction of image mentions and ROIs.</p><p>Our text-based submissions that expand queries to include concepts extracted from problem definitions show an improved MAP compared to submissions that do not perform query expansion in this way. This improvement is a promising result for use in future work.</p><p>Finally, Table <ref type="table" coords="9,212.21,609.83,4.98,8.74" target="#tab_3">3</ref> presents the results of our submitted runs for the case-based retrieval task. Given that our case representation is derived from our text-based image representation, the MAP of our case-based retrieval runs are also lower than expected. However, the submissions utilizing query expansion again show an improved in MAP, providing further evidence of its benefit. This article describes the methods and results of the ITI group at the Communications Engineering Branch, NLM, for the ImageCLEF 2010 medical retrieval track. We submitted 10 runs each for the modality detection task and the ad-hoc and case-based retrieval tasks. For the modality detection task, our multimodal approach achieved a classification accuracy of 92%, which was the 2nd ranked submission overall. For the retrieval tasks, our results demonstrate that query expansion using the definitions of extracted terms is a promising direction for improving retrieval. While our results show no benefit in combining textual and visual features for the retrieval tasks, modality detection is improved when utilizing both text-and content-based approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,177.24,350.01,260.88,7.89;4,134.77,115.83,345.83,219.76"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Process flow diagram of the modality detection approach</figDesc><graphic coords="4,134.77,115.83,345.83,219.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,138.97,122.89,341.62,351.89"><head>Table 1 .</head><label>1</label><figDesc>Accuracy results for the modality detection task.</figDesc><table coords="8,138.97,122.89,341.62,351.89"><row><cell>ID</cell><cell>Mode</cell><cell>Accuracy (%)</cell></row><row><cell>result text image combined</cell><cell>Mixed</cell><cell>92.00</cell></row><row><cell>result text image comb max</cell><cell>Mixed</cell><cell>91.00</cell></row><row><cell>result text image comb prod</cell><cell>Mixed</cell><cell>91.00</cell></row><row><cell>result text image comb cv</cell><cell>Mixed</cell><cell>89.00</cell></row><row><cell>result text title caption mod mesh</cell><cell>Textual</cell><cell>89.00</cell></row><row><cell>result text image comb sum</cell><cell>Mixed</cell><cell>87.00</cell></row><row><cell>result image comb cv</cell><cell>Visual</cell><cell>80.00</cell></row><row><cell>result image comb sum</cell><cell>Visual</cell><cell>80.00</cell></row><row><cell>result image combined</cell><cell>Visual</cell><cell>79.00</cell></row><row><cell>result image comb max</cell><cell>Visual</cell><cell>76.00</cell></row><row><cell cols="3">1. queries terms (textual): Essie search using term-based query construction.</cell></row><row><cell cols="3">2. queries types (textual): Essie search using type-based query construction.</cell></row><row><cell cols="3">3. queries backoff (textual): Essie search using sentence-based query construc-</cell></row><row><cell cols="3">tion. If a query retrieves no results, it is sequentially relaxed using the type-</cell></row><row><cell>based and term-based methods.</cell><cell></cell><cell></cell></row><row><cell cols="3">4. expanded queries terms (textual): Essie search like run (1) but with query</cell></row><row><cell>expansion using problem definitions.</cell><cell></cell><cell></cell></row><row><cell cols="3">5. expanded queries types (textual): Essie search like run (2) but with query</cell></row><row><cell>expansion using problem definitions.</cell><cell></cell><cell></cell></row><row><cell cols="3">6. expanded queries backoff (textual): Essie search like run (3) but with query</cell></row><row><cell>expansion using problem definitions.</cell><cell></cell><cell></cell></row><row><cell cols="3">7. queries pico backoff (textual): Essie search matching terms extracted from</cell></row><row><cell cols="3">the topic case with the structured captions in case representations. If a query</cell></row><row><cell cols="3">retrieves no results, it is sequentially relaxed by removing all terms of a</cell></row><row><cell cols="2">particular type (i.e., problem, intervention, etc.).</cell><cell></cell></row><row><cell cols="2">8. queries pico ma (textual): Essie search like run</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,153.07,122.89,309.22,286.77"><head>Table 2 .</head><label>2</label><figDesc>Retrieval results for the ad-hoc image retrieval task</figDesc><table coords="9,153.07,122.89,309.22,286.77"><row><cell>ID</cell><cell></cell><cell cols="2">Mode</cell><cell>Type</cell><cell>MAP</cell></row><row><cell>expanded queries terms</cell><cell></cell><cell cols="3">Textual Automatic</cell><cell>0.19</cell></row><row><cell>queries terms</cell><cell></cell><cell cols="3">Textual Automatic</cell><cell>0.16</cell></row><row><cell>queries terms modalities</cell><cell></cell><cell cols="2">Mixed</cell><cell>Automatic</cell><cell>0.11</cell></row><row><cell cols="2">expanded queries terms cbir cv merge max</cell><cell cols="2">Mixed</cell><cell>Automatic</cell><cell>0.06</cell></row><row><cell cols="2">expanded queries terms cbir cat merge max</cell><cell cols="2">Mixed</cell><cell>Automatic</cell><cell>0.06</cell></row><row><cell>queries terms cbir cv merge max</cell><cell></cell><cell cols="2">Mixed</cell><cell>Automatic</cell><cell>0.06</cell></row><row><cell>multimodal rerank roi qe merge</cell><cell></cell><cell cols="2">Mixed</cell><cell>Automatic</cell><cell>0.05</cell></row><row><cell>fusion cv merge mean</cell><cell></cell><cell cols="2">Visual</cell><cell>Automatic</cell><cell>0.01</cell></row><row><cell>fusion cv merge max</cell><cell></cell><cell cols="2">Visual</cell><cell>Automatic</cell><cell>0.00</cell></row><row><cell>fusion cat merge max</cell><cell></cell><cell cols="2">Visual</cell><cell>Automatic</cell><cell>0.00</cell></row><row><cell>ID</cell><cell cols="2">Mode</cell><cell></cell><cell>Type</cell><cell>MAP</cell></row><row><cell>expanded queries backoff</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.15</cell></row><row><cell>queries pico backoff</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.14</cell></row><row><cell>queries backoff</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.13</cell></row><row><cell>expanded queries types</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.12</cell></row><row><cell>queries pico MA</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.11</cell></row><row><cell>queries types</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.10</cell></row><row><cell>expanded queries terms</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.06</cell></row><row><cell>queries terms</cell><cell cols="2">Textual</cell><cell cols="2">Automatic</cell><cell>0.05</cell></row><row><cell>queries cbir with case backoff</cell><cell cols="2">Mixed</cell><cell cols="2">Automatic</cell><cell>0.04</cell></row><row><cell>queries cbir without case backoff</cell><cell cols="2">Mixed</cell><cell cols="2">Automatic</cell><cell>0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,187.99,416.68,239.38,7.89"><head>Table 3 .</head><label>3</label><figDesc>Retrieval results for the case-based retrieval task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,317.24,7.86"><p>MeSH is a controlled vocabulary created by NLM to index biomedical articles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.44,183.59,7.47"><p>http://freshmeat.net/projects/lirecbir/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,301.35,337.63,7.86;10,151.52,311.96,25.60,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,291.37,301.35,119.07,7.86">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribiero-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,322.57,337.64,7.86;10,151.52,333.18,308.97,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,285.69,322.57,137.58,7.86">Overview of the MPEG-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,430.01,322.57,50.58,7.86;10,151.52,333.18,218.35,7.86">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,343.79,337.64,7.86;10,151.52,354.39,320.13,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,275.72,343.79,204.87,7.86;10,151.52,354.39,100.54,7.86">Answering clinical questions with knowledge-based and statistical techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,259.19,354.39,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="103" />
			<date type="published" when="2007-03">Mar 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,365.00,337.64,7.86;10,151.52,375.61,45.69,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,306.79,365.00,86.49,7.86">Pattern Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,386.22,337.64,7.86;10,151.52,396.83,104.49,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,255.86,386.22,134.68,7.86">Classification by pairwise coupling</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,396.72,386.22,83.87,7.86;10,151.52,396.83,13.87,7.86">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,407.44,337.64,7.86;10,151.52,418.04,329.07,7.86;10,151.52,428.65,132.19,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,342.84,407.44,137.76,7.86;10,151.52,418.04,139.75,7.86">Essie: A concept-based search engine for structured biomedical text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,298.21,418.04,182.38,7.86;10,151.52,428.65,46.17,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,439.26,337.64,7.86;10,151.52,449.87,229.74,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,352.87,439.26,98.25,7.86">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,458.45,439.26,22.14,7.86;10,151.52,449.87,134.51,7.86">IEEE Transactions on Pattern Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="2329" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,460.48,337.63,7.86;10,151.52,471.08,236.61,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,328.36,460.48,147.94,7.86">The unified medical language system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,471.08,145.99,7.86">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,481.69,337.64,7.86;10,151.52,492.30,224.98,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,205.63,481.69,234.11,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.31,481.69,32.28,7.86;10,151.52,492.30,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,502.91,337.98,7.86;10,151.52,513.52,316.21,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,272.50,502.91,168.74,7.86">An affine invariant interest point detector</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.04,502.91,17.56,7.86;10,151.52,513.52,232.34,7.86">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,524.13,337.98,7.86;10,151.52,534.73,329.07,7.86;10,151.52,545.34,229.59,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,283.62,534.73,157.52,7.86">A comparison of affine region detectors</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.31,534.73,32.28,7.86;10,151.52,545.34,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,555.95,337.98,7.86;10,151.52,566.56,329.07,7.86;10,151.52,577.17,111.27,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,201.37,566.56,223.54,7.86">Overview of the clef 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.53,566.56,34.06,7.86;10,151.52,577.17,82.61,7.86">Working Notes of CLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,587.78,337.98,7.86;10,151.52,598.38,329.07,7.86;10,151.52,608.99,309.48,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,318.29,587.78,162.30,7.86;10,151.52,598.38,204.41,7.86">A medical image retrieval framework in correlation enhanced visual concept feature space</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,379.71,598.38,100.89,7.86;10,151.52,608.99,280.82,7.86">Proceedings of the 22nd IEEE International Symposium on Computer-Based Medical Systems</title>
		<meeting>the 22nd IEEE International Symposium on Computer-Based Medical Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,619.60,337.98,7.86;10,151.52,630.21,329.07,7.86;10,151.52,640.82,102.21,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,151.52,630.21,329.07,7.86;10,151.52,640.82,73.53,7.86">Text-and content-based approaches to image retrieval for the imageclef 2009 medical retrieval track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
