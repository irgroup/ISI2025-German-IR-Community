<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.85,115.96,291.66,12.62;1,204.82,133.89,205.71,12.62">A Multi Cue Discriminative Approach to Semantic Place Classification</title>
				<funder ref="#_PztVPgq">
					<orgName type="full">SNSF</orgName>
				</funder>
				<funder ref="#_R9sXks7">
					<orgName type="full">Spanish &quot;Junta de Comunidades de Castilla-La Mancha</orgName>
				</funder>
				<funder ref="#_dBuWsYe">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.84,171.69,62.37,8.74"><forename type="first">Marco</forename><surname>Fornoni</surname></persName>
							<email>mfornoni@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P.O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.04,171.69,96.35,8.74"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P.O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.09,171.69,70.76,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>bcaputo@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P.O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.85,115.96,291.66,12.62;1,204.82,133.89,205.71,12.62">A Multi Cue Discriminative Approach to Semantic Place Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CEC5725F9D5464776FED0B73CECA0B65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Idiap-MULTI to the Robot Vision Task at imageCLEF 2010. Our approach was based on a discriminative classification algorithm using multiple cues. Specifically, we used an SVM and combined up to four different histogram-based features with the kernel averaging method. We considered as output of the classifier, for each frame, the label and its associated margin, which we took as a measure of the confidence of the decision. If the margin value is below a threshold, determined via cross-validation during training, the classifier abstains from assigning a label to the incoming frame. This method was submitted to the obligatory task, obtaining a maximum score of up to 662, which ranked second in the overall competition. We then extended this algorithm for the optional task, where it is possible to exploit the temporal continuity of the sequence. We implemented a door detector so to infer when the robot has entered a new room. Then, we designed a stability estimation algorithm for determining the label of the room where the robot has entered, and we used this knowledge as a prior for the upcoming frames. Our approach obtained a score of up to 2052 in the obligatory task, ranking first.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the algorithms used by the Idiap-MULTI team at the third edition of the Robot Vision task, held under the umbrella of the ImageCLEF 2010 evaluation challenge. The focus of the Robot Vision task has been, since its first edition in 2009, semantic place localization for mobile robots, using visual information. This year, the task posed two distinctive research questions to participants: (1) can we design visual recognition algorithms able to recognize room categories, and (2) can we equip robots with methods for detecting unknown rooms?</p><p>The Idiap-MULTI team took a multi cue discriminative approach for addressing both issues. The core of our methods, both in the obligatory and optional tasks, is an SVm classifier, trained on a large number of visual features, combined together via a flat average of kernels [Gheler09]. This outputs, for each frame of the testing sequence, a classification label and a measure of the confidence in the decision. These two informations are then used to evaluate if the perceived room is one of those already seen, or if it is unknown to the system. Figure <ref type="figure" coords="2,475.61,154.86,4.98,8.74" target="#fig_0">1</ref> gives an overall overview of the training and classification steps.</p><p>In the rest of the paper we provide a detailed description of each step outlined in the diagrams: section 2 gives an overview of the oversampling strategy, devised to increase robustness. Section 3 describes the features used, and section 4 the cue integration approach. Section 5 and 6 describes into details the algorithms used for the obligatory and optional tasks. We report the experimental results in section 7. The paper concludes with an overall discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Process</head><p>Classification Process The capability to recognize room categories implies robustness to slight variations in the rooms' appearance. To achieve this, we propose, as a pre-processing step, to increase the number of training frames by applying simulated illumination changes to the original training frames. We generate new frames using the original training frames as templates. We apply colour modifications to that templates, trying to emulate the effect of extreme low/high lighting environments (figure <ref type="figure" coords="2,197.85,620.53,3.87,8.74" target="#fig_1">2</ref>). We increased the original training set adding an additional sequence (the size was 30% of the original training sequence) with images generated increasing or decreasing the luminance component for all the pixels. Even though in principle categorical variations are different from lighting variations, preliminary experiments indicate that this pre-processing step is beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Extraction</head><p>As features, we chose a variety of global descriptors representing different features of the images. We opted for histogram-based global features, mostly in the spatial-pyramid scheme introduced in <ref type="bibr" coords="3,322.97,346.14,9.96,8.74" target="#b6">[7]</ref>. This representation scheme was chosen because it combines the structural and statistical approaches: it takes into account the spatial distribution of features over an image, while the local distribution is in turn estimated by mean of histograms; moreover it has proven to be more versatile and to achieve higher accuracies in our experiments.</p><p>The descriptors we have opted to extract belong to five different families: Pyramid Histogram of Orientated Gradients (PHOG) <ref type="bibr" coords="3,378.22,418.58,9.96,8.74" target="#b2">[3]</ref>, Sift-based Pyramid Histogram Of visual Words (PHOW) <ref type="bibr" coords="3,304.40,430.53,9.96,8.74" target="#b1">[2]</ref>, Pyramid histogram of Local Binary Patterns (PLBP) <ref type="bibr" coords="3,213.20,442.49,9.96,8.74" target="#b8">[9]</ref>, Self-Similarity-based PHOW (SS-PHOW) <ref type="bibr" coords="3,415.65,442.49,14.61,8.74" target="#b9">[10]</ref>, and Compose Receptive Field Histogram (CRFH) <ref type="bibr" coords="3,312.20,454.44,9.96,8.74" target="#b7">[8]</ref>. Among all these descriptors, CRFH is the only one which is not computed pyramidly. For the remaining families we have extracted an image descriptor for every value of L = {0, 1, 2, 3}, so that the total number of descriptors extracted per image is equal to 25 (4+4 PHOG, 4+4 PHOW, 4 PLBP, 4 SS-PHOW, 1 CRFH). The exact settings for each descriptor are summarized in Table <ref type="table" coords="3,245.85,514.22,3.87,8.74" target="#tab_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cue Integration</head><p>Categorization is a difficult task and this is particularly true for indoor visual place categorization. Indoor environments are indeed characterized by an high variability in the visual appearance within each category, mainly due to clutter, occlusion, partial visibility of the rooms and local illumination changes. To further complicate matters, a robot is supposed to interact responsively with its environment and therefore a strong requirement is efficiency. We decided to combine these two issues by using a very efficient cue integration scheme, namely kernel averaging <ref type="bibr" coords="4,208.71,220.60,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="4,220.89,220.60,7.01,8.74" target="#b4">5]</ref>. Our approach consists of two steps:</p><p>1. pre-select the visual cues which are found to maximize the performances, when integrated together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">compute the average-kernel over the preselected features as an effective and efficient cue-integration method</head><p>In order to select the best visual cues to be combined together we have performed a pre-selection of the corresponding pre-computed kernel matrices, by using a simple depth-first search on the tree of all possible combinations of features. Since the number of all possible combinations of n features is 2 n -1, we have adopted the following efficiency measures to make the computation feasible:</p><p>estimate the accuracy of a given combination of kernels using only a subsample of the training and validation set (10 and 30 percent) prune down the tree of all possible combinations, by imposing a condition on the improvement which has to be satisfied in order to explore a branch: if the improvement achieved by averaging the kernel k 2 with the kernel k 1 is less then or equal to a threshold (ratio * accuracy of k 1 ), the branch is not further explored. However if averaging k 3 with k 1 does satisfy the condition, the branch k 1 -k 3 -k 2 is explored. Finally if the branch k 1 -k 2 has already been explored, obviously the branch k 1 -k 3 -k 2 is not explored again.</p><p>An example exploration is shown in figure <ref type="figure" coords="4,322.00,483.15,3.87,8.74" target="#fig_3">3</ref>. If this accuracy is not greater then the accuracy on the ancestor arc, the combination is discarded. The best combination retrieved in this example is PHOG+CRFH Best performing combinations selected are shown in figure <ref type="figure" coords="5,406.63,291.99,3.87,8.74">4</ref>, where we have sorted them with respect to the number of image descriptors used and we have taken into account only the best combinations with a maximum of four cues. For our final runs we used the following combinations of visual cues:</p><formula xml:id="formula_0" coords="5,149.86,118.32,135.80,107.45">PHOG360_ L 3 CRFH PHOG180_ L 3 CRFH PHOG360_ L 2 CRFH PHOG180_L2 CRFH PHOG360_ L 0 PHOG180_ L 3 CRFH PHOG180_ L 0 PHOG360_ L 3 CRFH PHOG180_ L 1 PHOG180_ L 3 CRFH PLBP_L0 PHOG180_L2 CRFH PLBP_ L 0 PHOG180_ L 2 PHOG360_ L</formula><formula xml:id="formula_1" coords="5,140.99,348.22,203.31,70.04">-PHOG 360 L3 , CRHF -PHOG 180 L3 , CRFH -PHOG 360 L0 , PHOG 180 L3 , CRFH -PHOG 180 L0 , PHOG 360 L3 , CRFH -PHOG 180 L1 , PHOG 180 L3 , CRFH -PLBP L0 , PHOG 180 L2 , PHOG 180 L1 , CRFH</formula><p>which correspond to the two best combinations with 2 cues, the three best combinations with 3 cues and the best combination with 4 cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Obligatory Task: The Algorithm</head><p>For the obligatory task, each test frame has to be classified without taking into account the continuity of the test sequence. Each test frame will be classified just using the SVM after the feature extraction step with the cue integration. Our first approach was just label each test frame with the room (class) that obtained the highest output value, but wrong classifications obtain negative values for the task score (as will be observed in section 7).</p><p>Our algorithm post-process the output obtained by the SVM to avoid classifying a test frame if it is not very confidence with the correct class. We normalize the output obtained with the SVM classifier for the test sequence, obtaining (for each test frame) 8 numeric values between -1.0 and +1.0 corresponding to each one of the training rooms. A test frame f i will be labelled with class C j only when the normalized output value for that class O i,j is above a threshold value and O i,j clearly overcomes all the others output values.</p><p>All thresholds were obtained with the preliminary experiments using the validation sequence provided by the task organizers. For these preliminary experiments, we observed that for a big percentage of the validation sequence, just a class obtained a positive output value. Moreover, large and small office presented as most problematic rooms and Printer Area, Recycle Area and Toiled obtained best accuracy.</p><p>For the parameter tuning, we used a classical Hill Climbing algorithm for all thresholds (we have 8 thresholds for each feature combination). A threshold value of 0.0 means that none of the test frames will be classified using that class and 1.0 will be used if we highly trust the classification algorithm for a selected class.</p><p>For the Hill Climbing algorithm, we tested positive and negative variations for the threshold values. These variations will be performed if the score obtained for the obtained run with the selected threshold (and using the validation sequence as test sequence) does not decrease. This greedy method has high risks of failing into local optima, and so we perform three executions using 0.25, 0.5 and 0.75 as initial values, selecting as final threshold value that achieving the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Optional Task: The Algorithm</head><p>For the optional task, we are allowed to exploit the temporal continuity in the sequence. We therefore implemented a door detector for estimating when the robot moves into a new room. This information, coupled with a stability estimation algorithm, can be useful for classifying a sequence of consecutive test frames. We estimate the stability of the classification process using the last n frames and their associated labels, obtained with the classification algorithm used for the obligatory task. A room is selected as the most probable label for the incoming data if at least the last n frames were classified with that label. This method is used for labeling frames for which the classification algorithm has a low level of confidence and therefore abstains to take a decision. The process is initiated every time the door detector signals that the robot has entered a new room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Door detection algorithm</head><p>We developed a basic door detection algorithm for indoor office environments as those used for the Robot Vision task. When the robot moves from a room to a new one, acquired images show two vertical rectangles with the same colour. The width of both rectangles increases when the robot gets closer to the door. The image processing algorithm consists of a Canny filter <ref type="bibr" coords="6,360.80,511.83,10.52,8.74" target="#b3">[4]</ref> to extract all the edges of the images. After this step, we use the Hough transform <ref type="bibr" coords="6,373.48,523.78,10.52,8.74" target="#b0">[1]</ref> for lines detection and we discard all the non vertical lines. Finally, we measure the average colour value between each two vertical lines, removing non homogeneous colour distributions (blobs). All these process can be observed in Fig. <ref type="figure" coords="6,347.18,559.65,3.87,8.74" target="#fig_4">5</ref>, where we detect three colour homogeneous blobs (two of them can be used to detect the door crossing)</p><p>Once we have extracted all the key blobs from a frame, we have to study the time correspondence for these blobs between this frame and the last frames. If two blobs with the same average colour are increasing for new frames we are reaching a door and both blobs are marked as candidates. Preliminary candidates will be selected as definitive ones if one of the two blobs starts decreasing after reaching the largest size at the left (right) of the image. Figure <ref type="figure" coords="6,427.55,644.16,4.98,8.74" target="#fig_5">6</ref> shows four consecutive training frames, where white rectangles represent blobs, preliminary  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Processing</head><p>For the additional processing we use the output obtained with the algorithm for the obligatory task. For each test frame, that algorithm obtains a class value or leaves it without classifying. Our processing tries to detect two situations: stable process and unstable process. All the internal threshold values were obtained from preliminary experimented developed with the validation sequence as testing set:</p><p>-Stable estimation The process is stable if, after crossing the last door, most of the frames were preliminary labelled with the same class C i . In such situation we will use C i to label test frames not labelled by the classification algorithm. The process will be considered stable when at least the last 18 frames have been classified with the same label. -Unstable estimation Instability will appear when preliminary class values for the last frames is not the same or they were not labelled. If this situation continues for a large number of frames, the process will not be able to achieve a stable situation. We assume that the process is unstable when the number of frames since we achieved a stable situation is greater than 50. Facing an unstable situation, the additional processing will label with the special label "Unknown" all the frames not labelled previously. This label is used for classifying new rooms not imaged in the training/validation sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>Our algorithms were evaluated following the procedure proposed by the organizers of the RobotVision@ImageCLEF 2010 competition. A training set containing 2741 frames had to be classified using a room label, marked as unknown or not classified. Performance was evaluated according to a pre-defined score function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Obligatory Task: Results</head><p>We submitted a total of twelve runs to the obligatory task. These runs were divided into two sets, one with parameters determined via cross validation, one without, as described in section 5. Each of the two sets comprises six experiments using the same exact feature combinations. Our best score ranked second in the competition, with a difference with respect to the winner of only -2.22%. It was obtained using the cue combination: PHOG 180 L0 PHOG 360 L3 CRFH, with γ and C estimated via cross-validation. Also our second best score (ranked third) was obtained with a combination of the type: PHOG L0 PHOG L3 CRFH, but the quantization of the orientation space was in this case swapped: 360 for the PHOG L0 and 180 for the PHOG L3 .</p><p>As shown in figure <ref type="figure" coords="8,231.47,632.21,3.87,8.74" target="#fig_6">8</ref>, most of the experiments where the cross-validation step was performed obtained a higher performance.This improvement is confirmed also by computing the average score in the two sets. can be explained using the parameters values obtained: the γ parameters estimated via cross-validation (0 &lt; γ &lt; 4) were in general much lower than the corresponding values obtained as the average pairwise χ 2 distance between histograms (1.5 &lt; γ &lt; 16). The SVM C parameter obtained by the crossvalidation (5 &lt; C &lt; 35) was also not far from the default value of 10, which is an unusually low value for a classification task (common values are often between 100 and 1000). The low value of the C parameter enforced a stronger regularization of the solution, thus improving the generalization capability of the classifier.</p><p>It is important to say, however, that our second best performance on this task was obtained without executing any cross-validation step and nonetheless turned out to outperform the corresponding cross-validated one. Also in this case one explanation for the failure of the cross-validation could be found by looking again at the γ values obtained: one of the three kernels averaged (PHOG 360 L0 ) obtained a very low value (0.000031). With this setting the kernel matrix is almost 1 for most of the couples and when computing the average kernel it only plays the role of a (smoothing) constant. The cross-validation algorithm in this case got stuck in a local minima in which the information added by the PHOG 360 L0 kernel to the combination was very limited and the final performance was not improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Optional Task: Results</head><p>For the optional task we submitted twelve runs, using the same combination of features and cross-validations that for the obligatory task. Fig. <ref type="figure" coords="9,415.01,632.21,4.98,8.74">9</ref> shows all the results, where it can be observed that all our runs achieved first positions for this task. Only other two groups (CAOR and DYNILSIS) submitted runs for this task, and their best scores were consistently smaller than all our runs (62.0 and -67.0 respectively). Therefore, our group was the winner of the optional task. If we compare figures 9 and 7, our algorithm for the optional task allows us to increase the final score for all the feature combinations. This increase proves the goodness of our proposal for exploiting the continuity of the test sequence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper describes the participation of Idiap-MULTI to the Robot Vision task at ImageCLEF 2010. We participated to both the obligatory and optional tracks with algorithms based on an SVM cue integration approach. Our best runs in the two tracks ranked respectively second (obligatory track) and first (optional track), showing the effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,166.61,491.24,279.06,7.89;2,150.66,289.71,155.50,187.21"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall overview of the training and the classification process</figDesc><graphic coords="2,150.66,289.71,155.50,187.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,255.78,345.82,7.89;3,134.77,266.76,151.66,7.86;3,221.22,115.94,172.80,125.07"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the oversampling process, where two new training frames are generated using a original training image</figDesc><graphic coords="3,221.22,115.94,172.80,125.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,136.16,547.06,62.07,7.86;3,300.40,547.06,47.29,7.86;3,466.96,547.06,5.76,7.86;3,147.68,558.42,38.54,7.86;3,268.11,558.42,111.88,7.86;3,449.87,558.42,39.93,7.86;3,147.68,569.37,38.54,7.86;3,268.11,569.37,111.88,7.86;3,449.87,569.37,39.93,7.86;3,145.98,580.33,42.43,7.86;3,245.36,580.33,157.38,7.86;3,449.86,580.33,39.93,7.86;3,145.15,591.29,44.09,7.86;3,245.36,591.29,157.38,7.86;3,449.86,591.29,39.93,7.86;3,147.34,602.35,24.83,7.86;3,172.17,600.58,14.38,5.24;3,172.17,605.97,9.57,5.24;3,223.91,602.35,200.29,7.86;3,449.87,602.35,39.93,7.86;3,145.76,613.31,344.04,7.86;3,154.01,624.27,26.36,7.86;3,204.25,624.27,239.60,7.86"><head>= 10 , 1 P = 8 ,</head><label>1018</label><figDesc>V = 300 and r = {4, 8, 12, 16} {0, 1, 2, 3} PHOWcolor M = 10, V = 300 and r = {4, 8, 12, 16} {0, R = 1, RotationInvariantUniform2 version {0, 1, 2, 3} SS-PHOW M = 5, V = 300, S = 5 ,R = 40, nRad = 4 and nT heta = 20 {0, 1, 2, 3} CRFH Gaussian-Derivatives={Lx, Ly}, K = 14 and s = {1, 2, 4, 8}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,134.77,600.23,345.83,7.89;4,134.77,611.22,345.83,7.86;4,134.77,622.18,345.83,7.86;4,134.77,633.14,345.83,7.86;4,134.77,644.10,345.83,7.86;4,134.77,655.05,317.69,7.86;4,202.39,497.93,207.56,96.03"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Search example. Each node represents the average kernel computed with all the features of the path starting from the root node. The number beside the feature name in each node represents the order in which the tree is visited, while the arc's weight represents the accuracy obtained by averaging the successor kernel, to the ancestor one. If this accuracy is not greater then the accuracy on the ancestor arc, the combination is discarded. The best combination retrieved in this example is PHOG+CRFH</figDesc><graphic coords="4,202.39,497.93,207.56,96.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,134.77,196.45,345.83,7.89;7,134.77,207.43,314.00,7.86;7,150.52,115.84,311.13,74.34"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Complete process to extract blobs. Left: original image. Centre: Vertical lines detection. Right: Homogeneous colour distributions between two vertical lines</figDesc><graphic coords="7,150.52,115.84,311.13,74.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,134.77,413.31,345.83,7.89;7,134.77,424.29,345.82,7.86;7,134.77,435.25,165.03,7.86;7,150.52,292.93,311.15,114.11"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Door detection for four consecutive frames. Top images are the original frames using P for preliminary candidates and D for definitive ones. Bottom images show the blobs extracted and time correspondence</figDesc><graphic coords="7,150.52,292.93,311.15,114.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,134.77,285.73,345.83,7.89;9,134.77,296.71,332.69,7.86"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Score obtained by the our submitted runs. The same experiments have been performed using the parameters obtained by cross-validation and parameters fixed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,134.77,595.85,345.82,7.89;10,134.77,606.84,332.69,7.86;10,134.77,426.73,345.67,151.89"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Score obtained by the our submitted runs. The same experiments have been performed using the parameters obtained by cross-validation and parameters fixed</figDesc><graphic coords="10,134.77,426.73,345.67,151.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,149.71,644.16,330.88,8.74;10,134.77,656.12,345.83,8.74"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 shows a complete comparison for the score obtained for each feature combination, with and without using a cross-validation step. It is worth to note</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,221.55,635.15,172.26,7.89"><head>Table 1 .</head><label>1</label><figDesc>Settings of the image descriptors</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,168.89,345.83,96.52"><head></head><label></label><figDesc>Performance of best combinations returned by the algorithm (ordered with respect to the number of kernels used), as measured by the accuracy on validation set</figDesc><table coords="5,134.77,168.89,327.19,85.57"><row><cell>Accuracy</cell></row><row><cell>1 CRFH</cell></row><row><cell>54 55 56 57 58 59 60 61 62 63</cell></row><row><cell>Fig. 4.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,117.78,345.83,182.56"><head></head><label></label><figDesc>Fig.9. Ranking of our submitted runs, combination features employed, score and checkmark if the result has been obtained using parameters estimated via crossvalidation</figDesc><table coords="10,162.20,117.78,290.95,140.19"><row><cell>Rank Feature Combination</cell><cell>Score Cross-Validation</cell></row><row><cell cols="2">1 PLBP L0 PHOG180 L2 PHOG360 L1 CRFH 2052</cell></row><row><cell>2 PHOG180 L3 CRFH</cell><cell>1770</cell></row><row><cell>3 PHOG180 L0 PHOG360 L3 CRFH</cell><cell>1361</cell></row><row><cell>4 PHOG180 L1 PHOG180 L3 CRFH</cell><cell>1284</cell></row><row><cell>5 PHOG360 L0 PHOG180 L3 CRFH</cell><cell>1262</cell></row><row><cell>6 PHOG180 L1 PHOG180 L3 CRFH</cell><cell>1090</cell></row><row><cell>7 PHOG180 L0 PHOG360 L3 CRFH</cell><cell>1028</cell></row><row><cell>8 PHOG360 L0 PHOG180 L3 CRFH</cell><cell>1019</cell></row><row><cell>9 PHOG360 L3 CRFH</cell><cell>963</cell></row><row><cell>10 PHOG360 L3 CRFH</cell><cell>916</cell></row><row><cell cols="2">11 PLBP L0 PHOG180 L2 PHOG360 L1 CRFH 886</cell></row><row><cell>12 PHOG180 L3 CRFH</cell><cell>682</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">SNSF</rs> project <rs type="projectName">MULTI</rs> (M. F.) and by the <rs type="funder">Spanish "Junta de Comunidades de Castilla-La Mancha</rs>" (<rs type="grantNumber">PCI08-0048-8577</rs> and <rs type="grantNumber">PBI-0210-7127</rs> Projects, J. M.-G.).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PztVPgq">
					<orgName type="project" subtype="full">MULTI</orgName>
				</org>
				<org type="funding" xml:id="_R9sXks7">
					<idno type="grant-number">PCI08-0048-8577</idno>
				</org>
				<org type="funding" xml:id="_dBuWsYe">
					<idno type="grant-number">PBI-0210-7127</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,268.78,337.64,7.86;11,151.52,279.73,150.87,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,210.23,268.78,244.59,7.86">Generalizing the Hough transform to detect arbitrary shapes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,463.29,268.78,17.30,7.86;11,151.52,279.73,61.56,7.86">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,290.69,337.64,7.86;11,151.52,301.65,329.07,7.86;11,151.52,312.61,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,315.86,290.69,164.74,7.86;11,151.52,301.65,36.36,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.09,301.65,185.11,7.86">International Conference on Computer Vision</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,323.57,337.64,7.86;11,151.52,334.53,329.07,7.86;11,151.52,345.49,127.90,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,311.26,323.57,169.34,7.86;11,151.52,334.53,22.84,7.86">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.05,334.53,288.54,7.86;11,151.52,345.49,31.94,7.86">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">408</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,356.45,337.63,7.86;11,151.52,367.41,248.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,195.96,356.45,184.18,7.86">A computational approach to edge detection</title>
		<author>
			<persName coords=""><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,391.64,356.45,88.95,7.86;11,151.52,367.41,201.18,7.86">Readings in computer vision: issues, problems, principles, and paradigms</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page">184</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,378.36,337.63,7.86;11,151.52,389.32,159.91,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,196.10,378.36,158.82,7.86">Can learning kernels help performance?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,369.99,378.36,110.60,7.86;11,151.52,389.32,131.67,7.86">invited talk at International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,400.28,337.63,7.86;11,151.52,411.24,182.19,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,260.40,400.28,220.19,7.86;11,151.52,411.24,14.75,7.86">On feature combination for multiclass object classification</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,185.56,411.24,45.14,7.86">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,422.20,337.63,7.86;11,151.52,433.16,329.07,7.86;11,151.52,444.12,299.29,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,312.81,422.20,167.78,7.86;11,151.52,433.16,194.05,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,384.97,433.16,95.63,7.86;11,151.52,444.12,229.20,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,455.08,337.64,7.86;11,151.52,466.04,273.89,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,274.81,455.08,205.78,7.86;11,151.52,466.04,142.30,7.86">Object recognition using composed receptive field histograms of higher dimensionality</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.49,466.04,83.98,7.86">Proc. ICPR. Citeseer</title>
		<meeting>ICPR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,476.99,337.64,7.86;11,151.52,487.95,329.07,7.86;11,151.52,498.91,84.02,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,341.38,476.99,139.21,7.86;11,151.52,487.95,193.68,7.86">Gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,355.77,487.95,99.38,7.86">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="404" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,509.87,337.97,7.86;11,151.52,520.83,329.07,7.86;11,151.52,531.79,109.84,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,276.68,509.87,203.91,7.86;11,151.52,520.83,23.52,7.86">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,195.28,520.83,256.15,7.86;11,151.52,531.79,36.03,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR&apos;07</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
