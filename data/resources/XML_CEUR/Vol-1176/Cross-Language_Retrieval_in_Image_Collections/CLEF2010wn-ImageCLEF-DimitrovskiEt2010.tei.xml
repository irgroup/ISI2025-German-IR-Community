<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.84,116.75,341.67,12.93;1,162.65,134.69,290.04,12.93">Detection of Visual Concepts and Annotation of Images using Predictive Clustering Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.63,172.23,75.47,9.62"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
							<email>ivicad@feit.ukim.edu.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute Jamova</orgName>
								<address>
									<addrLine>cesta 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering and Information Technology Karpoš bb</orgName>
								<address>
									<postCode>1000</postCode>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.02,172.23,54.68,9.62"><forename type="first">Dragi</forename><surname>Kocev</surname></persName>
							<email>dragi.kocev@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute Jamova</orgName>
								<address>
									<addrLine>cesta 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.26,172.23,78.01,9.62"><forename type="first">Suzana</forename><surname>Loskovska</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering and Information Technology Karpoš bb</orgName>
								<address>
									<postCode>1000</postCode>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.20,172.23,60.04,9.62"><forename type="first">Sašo</forename><surname>Džeroski</surname></persName>
							<email>saso.dzeroski@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute Jamova</orgName>
								<address>
									<addrLine>cesta 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.84,116.75,341.67,12.93;1,162.65,134.69,290.04,12.93">Detection of Visual Concepts and Annotation of Images using Predictive Clustering Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B07283CF7617260262174D520A920E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a multiple targets classification system for visual concepts detection and image annotation. Multiple targets classification (MTC) is a variant of classification where an instance may belong to multiple classes at the same time. The system is composed of two parts: feature extraction and classification/annotation. The feature extraction part provides global and local descriptions of the images. These descriptions are then used to learn a classifier and to annotate an image with the corresponding concepts. To this end, we use predictive clustering trees (PCTs), which are capable to classify an instance to multiple classes at once, thus exploit the interactions that may occur among the different visual concepts (classes). Moreover, we constructed ensembles (random forests) of PCTs, to improve the predictive performance. We tested our system on the image database from the visual concept detection and annotation task part of ImageCLEF 2010. The extensive experiments conducted on the benchmark database show that our system has very high predictive performance and can be easily scaled to large number of images and visual concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An ever increasing amount of visual information is becoming available in digital form in various digital archives. The value of the information obtained from an image depends on how easily it can be found, retrieved, accessed, filtered and managed. Therefore, tools for efficient archiving, browsing, searching and annotation of images are a necessity.</p><p>A straightforward approach, used in some existing information retrieval tools for visual materials, is to manually annotate the images by keywords and then to apply text-based query for retrieval. However, manual image annotation is an expensive and time-consuming task, especially given the large and constantly growing size of image databases.</p><p>The image search provided by major search engines, such as Google, Bing, Yahoo! and AltaVista, relies on textual or metadata descriptions of images found on the web pages containing the images and the file names of the images. The results from these search engines are very disappointing when the visual content of the images is not mentioned, or properly reflected, in the associated text.</p><p>A more sophisticated approach to image retrieval is automatic image annotation: a computer system assigns metadata in the form of captions or keywords to a digital image <ref type="bibr" coords="2,200.93,179.80,9.95,9.62" target="#b5">[6]</ref>. These annotations reflect the visual concepts that are present in the image. This approach begins with the extraction of feature vectors (descriptions) from the images. A machine learning algorithm is then used to learn a classifier, which will then classify/annotate new and unseen images.</p><p>Most of the systems for detection of visual concepts learn a separate model for each visual concept <ref type="bibr" coords="2,236.55,239.93,9.95,9.62" target="#b7">[8]</ref>. However, the number of visual concepts can be large and there can be mutual connections between the concepts that can be exploited. An image may have different meanings or contain different concepts, multiple targets classification (MTC) can be used for obtaining annotations (i.e., labels for the multiple visual concepts present in the image) <ref type="bibr" coords="2,367.78,287.74,9.95,9.62" target="#b7">[8]</ref>. The goal of MTC is to assign to each image multiple labels, which are a subset of a previously defined set of labels.</p><p>In this paper, we present a system for detection of visual concepts and annotation of images. For the annotation of the images, we propose to exploit the interactions between the target visual concepts (inter-class relationships among the image labels) by using predictive clustering trees (PCTs) for MTC. PCTs are able to handle multiple target concepts, i.e., perform MTC. To improve the predictive performance, we use ensembles (random forests) of PCTs for MTC. For the extraction of features, we use several techniques that are recommended as most suitable for the type of images at hand <ref type="bibr" coords="2,344.50,407.65,9.95,9.62" target="#b7">[8]</ref>.</p><p>We tested the proposed approaches on the image database from the visual concept detection and annotation task part of ImageCLEF 2010 <ref type="bibr" coords="2,414.16,431.91,14.59,9.62">[10]</ref>. The visual concept detection and annotation task is a multiple labels (targets) classification challenge. It aims at the automatic annotation of a large number of consumer photos with multiple annotations. The concepts used in this annotation task are for example abstract categories like Family/Friends or Partylife, the time of day (day, night, sunny, ...), Persons (no, single, small or big group), Quality (blurred, underexposed, ...) and etc.</p><p>The remainder of this paper is organized as follows. Section 2 presents the proposed large scale visual concept detection system. Section 3 explains the experimental design. Section 4 reports the obtained results. Conclusions and a summary are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System for Detection of Visual Concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall architecture</head><p>Fig. <ref type="figure" coords="2,155.50,632.87,4.98,9.62" target="#fig_0">1</ref> presents the architecture of the proposed system for visual concepts detection and image annotation. The system is composed of a feature extraction part and a classification/annotation part. We use two different sets of features to describe the images: global and local features extracted from the image pixel values. We employ different sampling strategies and different spatial pyramids to extract the visual features (both global and local) <ref type="bibr" coords="3,367.67,143.57,9.95,9.62" target="#b4">[5]</ref>. As an output of the feature extraction part, we obtain several sets of descriptors of the image content that can be used to learn a classifier to annotate the images with the visual concepts. Tommassi et al. <ref type="bibr" coords="3,348.21,398.28,15.48,9.62" target="#b13">[14]</ref> show that usage of various visual features that bring different information about the visual content of the images clearly outperform single feature approaches. Following these findings, in our research we use 'high level' feature fusion scheme.</p><p>The high level fusion scheme (depicted in Fig. <ref type="figure" coords="3,362.21,446.10,4.42,9.62" target="#fig_1">2</ref>) is performed as follows. First, we learn a classifier for each set of descriptors separately. The classifier outputs the probabilities with which an image is annotated with the given visual concepts. To obtain a final prediction, we combine the probabilities output from the classifiers for the different descriptors by averaging them. Depending on the domain, different weights can be used for the predictions of the different descriptors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple Targets Classification</head><p>Following the reccomendations from <ref type="bibr" coords="4,293.45,137.13,9.95,9.62" target="#b2">[3]</ref>, we formally describe the machine learning task that we consider here -multiple targets classification.</p><p>We define the task of multiple targets prediction as follows: Given:</p><p>-A description space X that consists of tuples of primitives (boolean, discrete or continuous variables), i.e. ∀X i ∈ X, X i = (x i1 , x i2 , ..., x iD ), where D is the size of a tuple (or number of descriptive variables), -a target space Y , where each tuple consists of several variables that can be either continuous or discerete, i.e., ∀Y i ∈ Y, Y i = (y i1 , y i2 , ..., y iT ), where T is the size of a tuple (or number of target variables),</p><formula xml:id="formula_0" coords="4,140.99,260.01,339.59,29.00">-a set of examples/instances E, where E = {(X i , Y i )|X i ∈ X, Y i ∈ Y, 1 ≤ i ≤ N } and N is the number of examples of E (N = |E|)</formula><p>, and a quality criterion q (which rewards models with high predictive accuracy and low complexity).</p><p>Find: a function f : X → Y such that f maximizes q. Here, the function f is presented with decision trees, i.e., predictive clustering trees.</p><p>If the tuples from Y (the target space) consist of continuous/numeric variables then the task at hand is multiple targets regression. Likewise, if the tuples from Y consist of discrete/nominal variables then the task is called multiple targets classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ensembles of PCTs for MTC</head><p>In the PCT framework <ref type="bibr" coords="4,233.97,417.74,9.95,9.62" target="#b0">[1]</ref>, a tree is viewed as a hierarchy of clusters: the top-node corresponds to one cluster containing all data, which is recursively partitioned into smaller clusters while moving down the tree.</p><p>PCTs are constructed with a standard "top-down induction of decision trees" (TDIDT) algorithm. The heuristic for selecting the tests is the reduction in variance caused by partitioning the instances, where the variance V ar(S) is defined by equation (1) below. Maximizing the variance reduction maximizes cluster homogeneity and improves predictive performance.</p><p>A leaf of a PCT is labeled with/predicts the prototype of the set of examples belonging to it. With appropriate variance and prototype functions, PCTs can handle different types of data, e.g., multiple targets <ref type="bibr" coords="4,363.32,537.29,9.95,9.62" target="#b3">[4]</ref>, hierarchical multi-label classification <ref type="bibr" coords="4,191.94,549.25,15.48,9.62" target="#b14">[15]</ref> or time series <ref type="bibr" coords="4,268.18,549.25,14.59,9.62" target="#b11">[12]</ref>. A detailed description of the PCT framework can be found in <ref type="bibr" coords="4,206.61,561.20,9.95,9.62" target="#b0">[1]</ref>. The PCT framework is implemented in the CLUS system, which is available for download at http://www.cs.kuleuven.be/ ~dtai/clus.</p><p>The prototype function returns a vector containing the probabilities that an example belongs to a given class for each target attribute. This afterwards can be used to calculate the majority class for each target attribute. The variance function is computed as the sum of the entropies of class variables:</p><formula xml:id="formula_1" coords="4,229.06,638.98,251.52,30.27">Var (E) = T i=1 GiniCoefficient(E , y i ) (1)</formula><p>For a detailed description of PCTs for MTC the reader is referred to <ref type="bibr" coords="5,457.90,119.66,10.50,9.62" target="#b0">[1,</ref><ref type="bibr" coords="5,470.08,119.66,7.00,9.62" target="#b3">4]</ref>. Next, we explain how PCTs are used in the context of an ensemble classifier, namely ensembles further improve the performance of PCTs.</p><p>Random Forests of PCTs To improve the predictive performance of PCTs, we use ensemble methods. An ensemble classifier is a set of classifiers. Each new example is classified by combining the predictions of each classifier from the ensemble. These predictions can be combined by taking the average (for regression tasks) or the majority vote (for classification tasks) <ref type="bibr" coords="5,412.00,222.11,9.95,9.62" target="#b1">[2]</ref>. In our case, the predictions in a leaf are the proportions of examples of different classes that belong to it. We use averaging to combine the predictions of the different trees. As for the base classifiers, a threshold should be specified to make a prediction.</p><p>We use random forests as an ensemble learning technique. A random forest <ref type="bibr" coords="5,470.08,270.08,10.50,9.62" target="#b1">[2]</ref> is an ensemble of trees, obtained both by bootstrap sampling, and by randomly changing the feature set during learning. More precisely, at each node in the decision tree, a random subset of the input attributes is taken, and the best feature is selected from this subset (instead of the set of all attributes). The number of attributes that are retained is given by a function f of the total number of input attributes x (e.g.,</p><formula xml:id="formula_2" coords="5,288.90,334.30,183.90,24.21">f (x) = x, f (x) = √ x, f (x) = ⌊log 2 x⌋ + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Extraction</head><p>We use different commonly used types of techniques for feature extraction from images. We employ two types of global image descriptors: gist features <ref type="bibr" coords="5,445.74,405.23,15.48,9.62" target="#b10">[11]</ref> and a RGB color histogram, with 8 bins in each color channel for the RGB color space.</p><p>Local features include scale-invariant feature transforms (SIFT) extracted densely on a multi-scale grid <ref type="bibr" coords="5,265.27,453.22,9.95,9.62" target="#b6">[7]</ref>. The dense sampling gives an equal weight to all key-points, independent of their spatial location in the image. To overcome this limitation, one can use spatial pyramids of 1x1, 2x2 and 1x3 regions <ref type="bibr" coords="5,455.01,477.13,14.59,9.62" target="#b12">[13]</ref>.</p><p>We computed four different sets of SIFT descriptors over the following color spaces: RGB, opponent, normalized opponent and gray. For each set of SIFT descriptors, we use the codebook approach to avoid using all visual features of an image <ref type="bibr" coords="5,177.38,525.12,14.59,9.62" target="#b12">[13]</ref>.</p><p>The generation of the codebook begins by randomly sampling 50 key-points from each image and extracting SIFT descriptors in each key-point (i.e., each key-point is described by a vector of numerical values). Then, to create the codewords, we employ k-means clustering on the set of all key-points. We set the number of clusters to 4000, thus we define a codebook with 4000 codewords (a codeword corresponds to a single cluster and a codebook to the set of all clusters). Afterwards, we assign the key-points to the discrete codewords predefined in the codebook and obtain a histogram of the occurring visual features. This histogram will contain 4000 bins, one for each codeword. To be independent of the total number of key-points in an image, the histogram bins are normalized to sum up to 1.</p><p>The number of key-points and codewords (clusters) are user defined parameters for the system. The values used above (50 key-points and 4000 codewords) are recommended for general images <ref type="bibr" coords="6,296.82,143.57,14.59,9.62" target="#b12">[13]</ref>.</p><p>3 Experimental Design  <ref type="table" coords="6,419.08,255.52,3.87,9.62" target="#tab_1">2</ref>. The goal of the task is to predict which of the visual concepts are present in each of the testing images.</p><p>We generated six sets of visual descriptors for the images: four sets of SIFT descriptors (one detector, dense sampling, over four different color spaces) with 32000 bins for each set (8 sub-images, from the spatial pyramids: 1x1, 2x2 and 1x3, 4000 bins each). We also generated two sets of global descriptors (gist features with 960 bins and RGB color histogram with 512 bins).</p><p>The parameter values for the random forests were as follows: we used 100 base classifiers and the size of the feature subset was set to 10% of the number of descriptive attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance measures</head><p>The evaluation of the results is done using three measures of performance suggested by the organizers of the challenge: mean average precision (MAP), Fmeasure and average ontology score (AOS) <ref type="bibr" coords="6,331.39,449.10,14.59,9.62">[10]</ref>. The first score evaluates the performance for each visual concept (concept-based evaluation), while the second and the third evaluate the performance for each testing image (example-based evaluation).</p><p>The mean average precision is widely used evaluation measure. For a given target concept, the average precision can be calculated as the area under the precision-recall curve for that target. Hence, it combines both precision and recall into a single performance value. The average precision is calculated for each visual concept separately and the obtained values are then averaged to obtain the mean average precision.</p><p>The F-measure is also widely used measure and it is well known. F-measure is calculated as the weighted harmonic mean of precision and recall:</p><formula xml:id="formula_3" coords="6,222.44,600.18,258.13,23.78">F -measure = 2 • P recision • Recall P recision + Recall<label>(2)</label></formula><p>The AOS measure calculates the misclassification cost for each missing or wrongly annotated concept per image. The AOS score is based on structure information (distance between concepts in the provided ontology of concepts), relationships from the ontology and the agreement between annotators for a concept extend with misclassification cost that incorporates the Flickr context similarity costs map <ref type="bibr" coords="7,225.73,143.57,9.95,9.62" target="#b8">[9]</ref>.</p><p>Additionally, we report Equal Error Rate (EER) and Area Under the ROC curve (AUC). However, we do not discuss these evaluation measures because they were not used for the evaluation of the submitted runs by the organizers of the competition [10]. This was the case because precision/recall analysis gives more intuitive and sensitive evaluation than the ROC analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted runs</head><p>We have submitted four different runs (see Table <ref type="table" coords="7,355.43,254.97,3.87,9.62" target="#tab_0">1</ref>). We do not use the EXIF metadata and the Flickr user tags provided for the photos. This means that all our runs consider automatic annotation using only visual information.</p><p>The runs can be divided using the following criteria: used descriptors and rescaling of the outputs. We used two different sets of descriptors: only SIFT (local descriptors) and SIFT combined with global descriptors (RGBHist and Gist). Since the AOS measure uses threshold 0.5 to determine wheter an image is annotated with a concept, we lineraly scale the probabilities to cope with the skewed distribution of the visual concepts. The linear scaling can be done on low level or high level. With the low level approach, we linearly scale the outputs from each classifier (obtained from the separate descriptors) and then average these values. For the high level approach, we linearly scale the averaged output of the classifiers.</p><p>The runs can be summarized as follows:</p><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" coords="7,162.71,549.02,4.98,9.62" target="#tab_0">1</ref> presents the results on the testing set of images from the 4 runs. The results show that by using both local and global descriptors we get better predictive performance. The incorporation of global descriptors in the learning phase lifts the predictive performance of AP by 6% on average for the following visual concepts: Flowers, Baby, Fancy, Summer, Sunset-Sunrise, Park-Garden, Plants, Still-Life and Food. We can note decrease in the predictive performance (by 7% for AP) only for the visual concept Travel. Considering the linear scaling, we obtain better results by applying high level scaling.</p><p>In the following we focus on the prediction scores for the individual visual concepts as evaluated by average precision score. We can note the higher AP values for the following visual concepts: Neutral-Illumination, No-Visual-Season, No-Blur, No-Persons, Outdoor, Sky, Day, Landscape-Nature, No-Visual-Time, Clouds, Natural, Plants. We obtain lower AP values for the concepts that are less represented in the training set of images (e.g., rain, horse, skateboard, graffiti...) and the 'difficult' concepts (e.g., abstract, technical, boring). The agreement of human annotators on the 'difficult' concepts is ∼ 75% <ref type="bibr" coords="8,373.23,179.44,9.95,9.62" target="#b8">[9]</ref>. Further improvements can be expected if different weighting schemes are used (to combine the predictions of the various descriptors). The weight of the descriptors can be adapted for each visual concept separately. For instance, the SIFT descriptors are invariant to color changes, and they do not predict well concepts where illumination is important. Thus, the weight of the SIFT descriptors in the combined predictions for those concepts should be decreased. Also we should find better descriptors for these concepts, such as estimating the color temperature and overall light intensity.</p><p>Another approach is to tackle the problem with the skewed distribution of concepts over the images. On approach can be generation of virtual images containing the under-represented visual concepts. These virtual images can be obtained with re-scaling, translation, rotation, changing the brightness of the images from the under-represented concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Multiple targets classification (MTC) problems are encountered increasingly often in image annotation. However, flat classification machine learning approaches are predominantly applied in this area. In this paper, we propose to exploit the dependencies between the different target attributes by using ensembles of trees for MTC. Our approach to MTC builds a single classifier that simultaneously predicts all of the visual concepts present in the images at once. This means adding new visual concepts will just slighly decrease the computational efficiency. While, for the other approaches that create a classifier for each visual concept separately this means learning an additional classifier.</p><p>Applied on the image database from the visual concept detection and annotation task part of ImageCLEF 2010 our approach was ranked fourth for the example-based performance measures (Ontology Score with FCS and Average F-measure) and fifth for the concept-based evaluation (Mean Average Precision), out of 17 competing groups.</p><p>The system we presented is general. It can be easily extended with new feature extraction methods, and it can thus be easily applied to other domains, types of images and other classification schemes. In addition, it can handle arbitrarily sized hierarchies organized as trees or directed acyclic graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,331.17,345.82,8.66;3,134.77,342.13,67.37,8.66"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Architecture of the proposed system for detection of visual concepts and annotation of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,205.02,634.01,205.31,8.66"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. High level fusion of the various descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,199.80,198.44,8.95;6,134.77,219.66,345.82,9.62;6,134.77,231.62,345.82,9.62;6,134.77,243.57,345.81,9.62;6,134.77,255.52,280.68,9.62"><head>3. 1</head><label>1</label><figDesc>Definition and Parameter Settings We evaluated our system on the image database from the visual concept detection and annotation task part of ImageCLEF 2010. The image database consists of training (8000) and test (10000) images. The images are labeled with 93 visual concepts [10]. A list of the visual concepts is presented in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,151.70,431.31,328.87,9.62;7,151.70,443.26,270.01,9.62;7,140.99,455.39,321.85,9.62;7,140.99,467.51,339.58,9.62;7,151.70,479.46,265.87,9.62;7,140.99,491.58,316.45,9.62"><head></head><label></label><figDesc>SIFT + RGBHist + Gist (HLScale): local descriptors (SIFT) and global descriptors (RGBHist and Gist) with high level linear scaling. -SIFT (HLScale): local descriptors (SIFT) with high level linear scaling. -SIFT + RGBHist + Gist (LLScale): local descriptors (SIFT) and global descriptors (RGBHist and Gist) with low level linear scaling. -SIFT (LLScale): local descriptors (SIFT) with low level linear scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,211.00,345.83,90.97"><head>Table 1 .</head><label>1</label><figDesc>Results of the experiments evaluated using Mean Average Precision, Average F-measure and Ontology Score incorporating the Flickr Context Similarity costs.</figDesc><table coords="8,172.22,243.50,270.91,58.47"><row><cell>Run name</cell><cell cols="3">MAP F-measure OS with FCS</cell></row><row><cell cols="2">SIFT + RGBHist + Gist (HLScale) 0.334</cell><cell>0.596</cell><cell>0.595</cell></row><row><cell>SIFT (HLScale)</cell><cell>0.318</cell><cell>0.574</cell><cell>0.570</cell></row><row><cell cols="2">SIFT + RGBHist + Gist (LLScale) 0.334</cell><cell>0.556</cell><cell>0.541</cell></row><row><cell>SIFT (LLScale)</cell><cell>0.317</cell><cell>0.545</cell><cell>0.543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,116.64,345.83,30.58"><head>Table 2 .</head><label>2</label><figDesc>Results per concept for our best run in the Large-Scale Visual Concept Detection Task using the Average Precision, Equal Error Rate and Area Under the Curve. The concepts are presented in descending order by Average Precision.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,234.94,342.24,8.66;10,146.92,245.90,153.35,8.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,308.96,234.94,156.11,8.66">Top-down induction of clustering trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.92,245.90,93.22,8.66">Proc. of the 15th ICML</title>
		<meeting>of the 15th ICML</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,256.87,273.29,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,200.69,256.87,64.09,8.66">Random Forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,271.73,256.87,70.67,8.66">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,267.82,342.23,8.66;10,146.92,278.78,160.11,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,201.44,267.82,193.30,8.66">Towards a General Framework for Data Mining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,414.84,267.82,65.74,8.66;10,146.92,278.78,54.64,8.66">Proc. of the 5th KDID, LNCS</title>
		<meeting>of the 5th KDID, LNCS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4747</biblScope>
			<biblScope unit="page" from="259" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,289.74,342.22,8.66;10,146.92,300.70,230.98,8.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,325.68,289.74,154.89,8.66;10,146.92,300.70,19.90,8.66">Ensembles of Multi-Objective Decision Trees</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,173.87,300.70,51.59,8.66">Proc. ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">4701</biblScope>
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,311.66,342.24,8.66;10,146.92,322.62,281.52,8.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,288.85,311.66,191.74,8.66;10,146.92,322.62,172.23,8.66">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,326.05,322.62,23.35,8.66">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2169" to="2178" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,333.57,342.25,8.66;10,146.92,344.54,281.58,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,226.98,333.57,195.82,8.66">Real-Time Computerized Annotation of Pictures</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,429.90,333.57,50.70,8.66;10,146.92,344.54,182.64,8.66">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="985" to="1002" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,355.50,342.25,8.66;10,146.92,366.45,227.53,8.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,202.90,355.50,237.45,8.66">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.32,355.50,32.28,8.66;10,146.92,366.45,137.52,8.66">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,377.41,342.24,8.66;10,146.92,388.37,333.67,8.66;10,146.92,399.33,252.43,8.66;10,146.92,410.29,84.33,8.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,246.00,377.41,234.59,8.66;10,146.92,388.37,127.91,8.66">Overview of the CLEF 2009 Large-Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,282.87,388.37,178.09,8.66;10,146.92,399.33,248.23,8.66">II Multimedia Experiments: 10th Workshop of the CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Multilingual Information Access Evaluation</note>
</biblStruct>

<biblStruct coords="10,138.35,421.24,342.23,8.66;10,146.92,432.20,240.57,8.66;10,134.77,443.17,7.85,8.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,264.07,421.24,216.51,8.66;10,146.92,432.20,37.57,8.66">Multilabel classification evaluation using ontology information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.73,432.20,87.07,8.66">Workshop on IRMLeS</title>
		<meeting><address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,443.17,337.98,8.66;10,146.92,454.12,201.60,8.66" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,151.52,443.17,298.86,8.66">Visual Concept Detection and Annotation Task at ImageCLEF</title>
		<ptr target="http://www.imageclef.org/2010/PhotoAnnotation" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,465.08,337.97,8.66;10,146.92,476.04,333.67,8.66;10,146.92,487.00,25.59,8.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,245.36,465.08,235.22,8.66;10,146.92,476.04,92.68,8.66">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,247.14,476.04,167.25,8.66">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,497.96,337.98,8.66;10,146.92,508.92,333.66,8.66;10,146.92,519.87,159.96,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,375.08,497.96,105.51,8.66;10,146.92,508.92,286.55,8.66">Finding explained groups of time-course gene expression profiles with predictive clustering trees</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Slavkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gjorgjioski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,440.89,508.92,39.69,8.66;10,146.92,519.87,44.41,8.66">Molecular BioSystems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="740" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,530.83,337.97,8.66;10,146.92,541.80,180.44,8.66" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,319.30,530.83,161.28,8.66;10,146.92,541.80,83.57,8.66">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,237.12,541.80,20.99,8.66">CIVR</title>
		<imprint>
			<biblScope unit="page" from="141" to="150" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,552.75,337.98,8.66;10,146.92,563.71,329.42,8.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,340.91,552.75,139.68,8.66;10,146.92,563.71,102.30,8.66">Discriminative cue integration for medical image annotation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,256.48,563.71,111.55,8.66">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1996" to="2002" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,574.67,337.98,8.66;10,146.92,585.63,315.41,8.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,408.68,574.67,71.91,8.66;10,146.92,585.63,145.82,8.66">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,299.38,585.63,72.33,8.66">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
