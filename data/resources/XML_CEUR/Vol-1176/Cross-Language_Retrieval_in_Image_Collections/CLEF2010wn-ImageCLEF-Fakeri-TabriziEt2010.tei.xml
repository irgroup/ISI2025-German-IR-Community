<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.12,66.80,315.16,14.35">UPMC/LIP6 at ImageCLEFannotation 2010</title>
				<funder ref="#_mVuKMDN">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.80,104.89,75.92,9.96"><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
						</author>
						<author>
							<persName coords="1,222.97,104.89,64.14,9.96"><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
						</author>
						<author>
							<persName coords="1,294.28,104.89,66.38,9.96"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
						</author>
						<author>
							<persName coords="1,368.57,104.89,82.63,9.96"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
						</author>
						<author>
							<persName coords="1,271.20,116.89,73.12,9.96"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université Pierre et Marie Curie -Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR CNRS 7606</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75252</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.12,66.80,315.16,14.35">UPMC/LIP6 at ImageCLEFannotation 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FCFFC1F2B5CDAE59918252C8C124FD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SVM</term>
					<term>Multi-Class Multi-Label Image Classification</term>
					<term>Imbalanced Class Problem</term>
					<term>Semi-Supervised Learning</term>
					<term>Transductive Learning</term>
					<term>Visual Concepts</term>
					<term>Ranking SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the LIP6 annotation models for the ImageCLEFannotation 2010 task. We study two methods to train and merge the results of different classifiers in order to improve annotation. In particular, we propose a multiview learning model based on a RankingSVM. We also consider the use of the tags matching the visual concept names to improve the scores predicted by the models. The experiments show the difficulty of merging several classifiers and also the interest to have a robust model able to merge relevant information. Our method using tags always improves the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Last year, in ImageCLEFannotation 2009, we focused on how to deal with imbalanced data <ref type="bibr" coords="1,198.61,414.85,10.00,9.96" target="#b1">[2]</ref>. Instead of training a standard SVM, we have used a Ranking SVM in which the chosen loss function is helpful in the case of imbalanced data. This year, in ImageCLEFannotation 2010 <ref type="bibr" coords="1,321.98,438.85,10.00,9.96" target="#b4">[5]</ref>, we additionally focus on how to use different visual feature spaces in the same model using supervised and semisupervised learning. We also consider to use the tags associated to the images.</p><p>In this work, we consider two models that merge the predictions of several classifiers. The first model takes the mean of the predicted score of the classifier, where each classifier is trained on a specific visual feature space using the labeled data provided for the competition. The second model makes use of additional unlabeled data to train the classifier in the semi-supervised, multiview paradigm <ref type="bibr" coords="1,134.76,534.61,10.00,9.96" target="#b0">[1]</ref>. In our case, the representation of an image in a given visual feature space is a view of the image. Semi-sueprvised learning is carried out by first learning classifiers on each view with the labeled data, and then enforcing these classifiers to make similar predictions on the unlabeled data. As in our first model, the final prediction is the mean of the scores predicted by the different classifiers.</p><p>In addition to visual descriptors, the text associated to an image is often relevant to improve image retrieval. There are only a few works <ref type="bibr" coords="1,412.68,606.49,10.57,9.96" target="#b6">[7]</ref> studying the The remainder of the paper is organized as follows. In Section 2, we propose our models for image annotation using multiple views. Section 3 describes our method which uses tags to improve annotation. The experiences are illustrated in Section 4. The conclusion and perspectives are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using RankingSVM in Imbalanced Dataset Case</head><p>The data for image annotation is often highly imbalanced: for many classes, there are only very few positive (or negative) examples. As we showed in <ref type="bibr" coords="2,467.28,366.13,10.00,9.96" target="#b1">[2]</ref>, standard learning algorithms like SVMs may be biased towards the majority class in such cases, but more involved algorithms like RankingSVM may help to overcome this problem. The Ranking SVM does not strive to separate the two classes, but rather learns a score function that gives greater scores to positive examples than to negative ones. We choose to optimize the Area under the ROC Curve (AUC) as in <ref type="bibr" coords="2,220.69,437.89,10.00,9.96" target="#b3">[4]</ref>. The AUC is the probability that a positive example has a greater score than a negative one.</p><p>Strictly speaking, a Ranking SVM does not learn a classifier. A classifier can however be obtained by comparing the scores with an appropriate threshold. In the following, the classifier is obtained by comparing the score to 0: if an observation x is such that w, φ(x) &gt; 0, then we predict that x is in the positive class, otherwise we predict that it is in the negative class. Although this choice may not be optimal, it is a simple decision rule that gives good results in practice.</p><p>Last year <ref type="bibr" coords="2,194.88,534.37,10.00,9.96" target="#b1">[2]</ref>, we focused on how to deal with imbalanced data. This year, in the ImageCLEFannotation 2010, we focus on how to train and merge the results of different classifiers to improve annotation, and specially on how to use different visual feature spaces in the same model.</p><p>To obtain a baseline, we perform several RankingSVM on different visual features, then we create a fusion of the outputs using an arithmetic mean. Figure <ref type="figure" coords="2,475.57,594.49,4.98,9.96" target="#fig_0">1</ref> describes the fusion model we use in the ImageCLEFannotation2010 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiview Learning Model</head><p>Images can be described in several visual feature spaces (like SIFT, HSV, ...). Each of these representations, or views of an image, is very informative about the label of the image, but the different views provide rather independent information. Semi-supervised multiview learning aims at using these characteristics of the different views in order to improve the accuracy of the classifiers. The main principle is as follows: after training classifiers on each view independently (using standard supervised learning, in our case RankingSVMs), these classifiers are modified so that they predict, as much as possible, the same class labels on the unlabeled data. In our multiview learning model, we get the different views</p><p>The algorithm we use for the semi-supervised procedure is an iterative procedure. After the initial training step of the different classifiers on each view, each iteration of the algorithm consists of (1) predicting on all available unlabeled examples, then (2) adding to the training set (and removing from the unlabeled set) all examples for which all classifiers agree on a given class label. Equivalently, the unlabeled examples are given predicted labels based on the unanimous vote of the different classifiers. After step (2), the classifiers are re-trained on the new training set, and steps ( <ref type="formula" coords="3,280.74,280.33,4.25,9.96">1</ref>) and ( <ref type="formula" coords="3,317.48,280.33,4.25,9.96">2</ref>) are repeated. The algorithm stops when there are no more unlabeled examples on which all the classifiers agree. The process is described in Figure <ref type="figure" coords="3,287.16,304.21,3.90,9.96" target="#fig_2">2</ref>.</p><p>The rationale of this procedure is that when all the classifiers agree on a class label, the label is likely to be correct. In practice, the semi-supervised learning procedure improves performance when the classifiers are sufficiently accurate so that most training examples added at each iteration are correctly classified. If this is not the case, multiview learning may actually decrease performance, since we train new classifiers on more noisy training sets.</p><p>In the ImageCLEFannotation 2010 task, the observations are composed of the multiple views of image: each view can be considered as a feature type. We perform RankingSVM on each feature type using the training dataset. The training dataset is the labeled dataset and the test dataset is the unlabeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Using Tags to Improve Visual Concept Annotation</head><p>To improve image annotation, the image associated text can be used. Free texts, tags and visual concepts are three kinds of text we can use for image annotation, but there is a big difference between them.</p><p>1. Free text is based on a very large vocabulary, it can be composed of a description of the image -this type of free text is often relevant to improve the image classification -or it can be composed of the associated text (like web pages) -this type of free text is often fewly relevant. 2. Tags are a little bit more specific then free text. When an image is associated with a tag, this tag is most of the time relevant for this image, but when a tag is not associated with an image, it should not mean that this tag is not relevant for the image. 3. Finally, visual concepts come from a very specific vocabulary, and contrary to tags, when a visual concept is not associated with an image, it means that this visual concept is not relevant for this image.</p><p>We then deduce that if the name of a visual concept tagged an image, then this image should be annotated with this concept, but the contrary is false. Our method is based on this idea. If a tag maches the name of a visual concept, then the images associated with this tag will have their prediction scores for this concept set to a given value we called U P . In order to improve the matching between the name of the visual concepts and tags, we first apply -on both concept names and tags -standard text processing, such as porter stemming algorithm <ref type="bibr" coords="4,180.01,424.69,10.00,9.96" target="#b5">[6]</ref>. For example, if a test image has a prediction score of 0.4 with the classifier of the concept Sunny and if its associated tags contain the text sunni (which is the stemmed word for Sunny), then its prediction score is altered to the value of U P , which is estimated using the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The corpus is composed of a training set of 8000 Flickr images and a test set of 10000 Flickr images. We split the training set in a training set of 5000 images and a validation set of 3000 images 1 . Each image is annotated in average by 12 visual concepts chosen among the 93 hierarchical visual concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Features</head><p>We extract three different types of visual features from each image.</p><p>HSV First, we segment images into 3 horizontal regions and extract HSV features. For each region, we compute a color histogram in the HSV space. We believe that these visual descriptors are particularly interesting for general concepts (i.e. not objects), such as: sky, sunny, vegetation, sea and etc.</p><p>SIFT Second, we extract SIFT keypoints, and then we cluster them to obtain a visual dictionary: we extract the SIFT keypoints of each image. To reduce the size of the dictionary and avoid duplicate keypoints, the keypoints are clustered with a nearest-neighbors algorithm, to obtain 1024 clusters. Then, each cluster represents a visual word in the dictionary, and each image is indexed using this dictionary.</p><p>Mixed+PCA Third, we use a concatenation of various visual features from 3 labs proposed by the AVEIR consortium <ref type="bibr" coords="5,322.46,217.33,10.57,9.96" target="#b2">[3]</ref> reduced using a PCA (Mixed + PCA): this space is composed of the concatenation of the visual features from 3 labs: 51 dimensions HSV histograms from LIP6 lab, 150 dimensions of entropic features from LSIS and 120 dimensions features composed of a combination of color, texture and shape from PTECH lab. This space is transformed using a PCA. Then we keep the first 180 dimensions which correspond to 98% of the cumulative variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Features</head><p>We first apply -on both concept names and tags -standard text processing, principally stemming using Porter stemming algorithm <ref type="bibr" coords="5,371.79,343.45,10.00,9.96" target="#b5">[6]</ref>. Table <ref type="table" coords="5,413.88,343.45,4.98,9.96" target="#tab_0">1</ref> gives different information on the sets in function of the use (or not) of stemming. For example, in the training set, without stemming, there are 39 (out of 93) concepts matching at least one tag in the documents, whereas there are 69 (out of 93) concepts matching using stemming. We can see that it is difficult to match tags and concept names without stemming and also with stemming, maybe because the names of the concepts are chosen by a specialist of visual concept detection whereas tags are chosen by Flickr users. For example, a user will never tag an image with "No Visual Season", "Out of focus" or "Neutral Illumination".</p><p>Another information is the number of predicted scores modified by the tags. For example, in the validation set, there are 3000 images and 93 concepts, so there are 279000 predicted values. Among those, only 1829 are modified when we use stemming. On average over the different concepts, 19.7 predicted scores are modified. Even though the tags affect rather few values perc concept, we will see in the next section that those modifications are most often relevant.</p><p>Figure <ref type="figure" coords="5,181.21,522.73,4.98,9.96" target="#fig_3">3</ref> compares the number of images which contain a tag corresponding to a given concept (for example, if we consider the concept "Sunny", it is the number of images which are tagged with "Sunny"), and the number of images relevant for this concept in the training set (for example, if we consider the concept "Sunny", it is the number of images which are labelled are positive for the concept "Sunny" in the ground truth of the training set). This figure shows that, contrarily to the intuition, the number of images tagged with a given concept is not correlated to the number of relevant images for this concept. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on the Validation Set</head><p>As we said in the previous sections, we use RankingSVM as the basic learner.</p><p>We have trained RankingSVMs on all three types of features in a fully supervised setting (the fusion model) or in the semi-supervised setting (the multiview model). For both types of training, the different classifiers are merged in a single model by taking the mean of the predicted scores. These scores are finally normalized using a gamma distribution. We optionally use tags.  <ref type="table" coords="6,177.84,451.09,4.98,9.96" target="#tab_1">2</ref> compares the results of the different models on the validation set. The fusion model gives better results than the SIFT and multiview models. Our method using tags always improves significantly the scores.</p><p>Figure <ref type="figure" coords="6,180.37,486.85,4.98,9.96">4</ref> compares the AUC scores between fusion and fusion+tags on validation set for each concept. We can notice that the use of the tags never decreases the AUC of a concept. This means that when a tag is put on an image, it is most often relevant for this image.</p><p>Figure <ref type="figure" coords="6,180.61,534.73,4.98,9.96">5</ref> compares the AUC obtained with and without stemming depending on the value of U P . This figure shows that using Porter stemming algorithm significantly improves the results. We can notice that the best scores are obtained for U P = 1. This confirms that if a tag corresponding to a concept is associated to an image, than this image should be labeled by this concept.</p><p>Figure <ref type="figure" coords="6,181.33,594.49,4.98,9.96" target="#fig_4">6</ref> compares the behaviour of the different models depending on U P . We remark that the fusion model always gives the best results. We also note that the multiview model is less accurate than the Fusion or Sift models. This may be due to the use of unanimous vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Submitted Runs and Official Results</head><p>In ImageCLEFannotation 2010, we submitted the following 5 runs :</p><p>Run1 We perform a RankingSVM using only SIFT features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run2</head><p>We perform a RankingSVM for each type of features (HSV, SIFT, Mixed + PCA). Then we merge the prediction scores using an arithmetic mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run3</head><p>We perform the multiview learning by using three views (HSV, SIFT, Mixed + PCA); each view represents a type of visual features.</p><p>Run4 Same as Run2, but here we also consider tags. The prediction score given by the classifier for a given concept is increase up to 1, if the image is tagged with the name of the visual concept.  Run5 Same as Run3, but here we also consider tags. The prediction score given by the classifier for a given concept is increase up to 1, if the image is tagged with the name of the visual concept. Table <ref type="table" coords="9,176.52,400.69,4.98,9.96" target="#tab_3">3</ref> gives the official results on test set. We note that the results are close to the results obtain by a random run. We can conclude that there might be some mistakes in our process (maybe in the final step where we have to make the prediction scores in a given format), but not in our methods because the results on validation set are reasonable (see Table <ref type="table" coords="9,355.44,448.45,3.88,9.96" target="#tab_1">2</ref>). In Table <ref type="table" coords="9,410.40,448.45,3.90,9.96" target="#tab_3">3</ref>, we can notice that the use of the tags improves significantly the results. We proposed two models to train and merge the results of different classifiers in order to improve annotation. We described a multiview learning method for image annotation in which each view is a visual feature type. We also merged the predicted scores for all feature types. The experiments showed that our multiview model is close to, but less effective than, the fusion model. Maybe the voting part should be modified to obtain higher performances. We also considered the use of the tags matching the visual concept names to improve the scores predicted by the models. Our method using tags always improved the results of the classifiers. Our study of the links between visual concept names and tags showed that when an image is tagged with a concept name this image should be labeled with this concept.</p><p>As perspectives, we can modify the voting part in the multiview model to avoid adding noisy examples to the training set. Moreover, we intend to try our models with other feature types, such as text, to study the evolution of the performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,236.64,166.18,141.98,8.27;2,137.72,66.90,339.91,74.78"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schema of the fusion model</figDesc><graphic coords="2,137.72,66.90,339.91,74.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,292.70,328.21,187.98,9.96;3,134.76,340.09,345.86,9.96;3,134.76,352.09,345.83,9.96;3,134.76,363.97,345.85,9.96;3,134.76,375.97,132.76,9.96"><head></head><label></label><figDesc>Then, adding those examples to the training set gives us more training examples. Moreover, because we systematically add to the training set new examples on which the classifiers already agree, the procedure tends to increase the agreement between the different classifiers on the original unlabeled dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,228.60,200.02,158.17,8.27;4,137.70,67.16,339.77,108.38"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schema of our multiview model</figDesc><graphic coords="4,137.70,67.16,339.77,108.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.76,329.38,345.80,8.27;7,134.76,340.42,345.78,8.27;7,134.76,351.34,257.98,8.27"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of the number of images in the training set relevant for a given concept, and of the number of images which contain a tag corresponding to this given concept in the training set. Each cross corresponds to a concept</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,134.76,309.58,345.82,8.27;9,134.76,320.50,94.09,8.27"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of AUC scores in function of the value of UP for the different models using stemming</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,150.96,66.58,313.42,105.23"><head>Table 1 .</head><label>1</label><figDesc>Information on the different sets</figDesc><table coords="6,318.48,85.66,145.90,8.27"><row><cell>Training Set Validation Set Test Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,149.76,320.14,227.60,140.91"><head>Table 2 .</head><label>2</label><figDesc>Results on validation set</figDesc><table coords="6,149.76,339.22,225.99,121.83"><row><cell>Run</cell><cell>EER AUC MAP</cell></row><row><cell>SIFT</cell><cell>0.326 0.731 0.251</cell></row><row><cell>Fusion</cell><cell>0.305 0.754 0.278</cell></row><row><cell>Multiview</cell><cell>0.321 0.732 0.260</cell></row><row><cell>SIFT+tags</cell><cell>0.309 0.753 0.312</cell></row><row><cell>Fusion+tags</cell><cell>0.287 0.776 0.330</cell></row><row><cell cols="2">Multiview+tags 0.301 0.760 0.310</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.76,88.60,345.85,522.88"><head></head><label></label><figDesc>Comparison of AUC scores with and without stemming in function of the value of U P for the Fusion model</figDesc><table coords="8,134.76,88.60,345.75,511.92"><row><cell></cell><cell>0.78</cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell></cell></row><row><cell>AUC AUC Fusion+tags (validation set)</cell><cell>0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.68 0.7 0.72 0.74 0.66</cell><cell>SIFT Fusion Multiview</cell></row><row><cell></cell><cell></cell><cell>SIFT+Tags</cell></row><row><cell></cell><cell>0.55</cell><cell>Fusion+Tags Multiview+Tags</cell></row><row><cell cols="3">0.55 Fig. 4. Comparison of AUC scores between Fusion and Fusion+tags on validation set. 0.6 0.65 0.7 0.75 0.8 0.85 0.9 AUC Fusion (validation set) Each cross represents the scores for a concept 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0 0.2 0.4 0.6 0.8 1 AUC (validation set) UP Fusion Fusion+Tags with stemming Fusion+Tags without stemming 0 0.2 0.4 0.6 0.8 1 Fig. 5. 0.64 UP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,169.44,499.66,276.29,82.43"><head>Table 3 .</head><label>3</label><figDesc>Official results</figDesc><table coords="9,169.44,518.62,276.29,63.47"><row><cell>Run</cell><cell cols="4">MAP Average F-ex Ontology score EER AUC</cell></row><row><cell>run1 SIFT</cell><cell>0.145</cell><cell>0.127</cell><cell>0.328</cell><cell>0.502 0.497</cell></row><row><cell>run2 Fusion</cell><cell>0.146</cell><cell>0.174</cell><cell>0.348</cell><cell>0.497 0.504</cell></row><row><cell>run3 Multiview</cell><cell>0.148</cell><cell>0.173</cell><cell>0.348</cell><cell>0.498 0.502</cell></row><row><cell>run4 Fusion+tags</cell><cell>0.182</cell><cell>0.184</cell><cell>0.351</cell><cell>0.463 0.559</cell></row><row><cell cols="2">run5 Multiview+tags 0.180</cell><cell>0.186</cell><cell>0.351</cell><cell>0.464 0.557</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002 AVEIR</rs> project).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mVuKMDN">
					<idno type="grant-number">ANR-06-MDCA-002 AVEIR</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.32,384.22,342.28,8.27;10,146.88,394.84,333.51,8.96;10,146.88,406.06,204.00,8.27" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,373.02,384.22,107.58,8.27;10,146.88,395.14,120.70,8.27">Multi-view learning in the presence of view disagreement</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,289.32,394.84,186.64,8.96">Uncertainty in Artificial Intelligence (UAI-08)</title>
		<meeting><address><addrLine>Corvallis, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="88" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,417.10,342.21,8.27;10,146.88,427.72,333.78,8.96;10,146.88,438.64,333.64,8.96;10,146.88,449.68,314.08,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,394.91,417.10,85.62,8.27;10,146.88,428.02,262.70,8.27">Improving image annotation in imbalanced classification problems with ranking svm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.60,427.72,48.06,8.96;10,146.88,438.64,123.63,8.96;10,293.54,438.64,186.98,8.96;10,146.88,449.68,230.05,8.96">II Multimedia Experiments: Proceedings of the 10th Workshop of the Cross-Language Evaluation Forum</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2010</date>
		</imprint>
	</monogr>
	<note>Multilingual Information Access Evaluation</note>
</biblStruct>

<biblStruct coords="10,138.32,460.90,342.05,8.27;10,146.88,471.82,333.71,8.27;10,146.88,482.56,333.52,8.96;10,146.88,493.78,20.80,8.27" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,365.57,471.82,115.01,8.27;10,146.88,482.86,220.36,8.27">Comparison of various aveir visual concept detectors with an index of carefulness</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quenot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.28,482.56,84.23,8.96">CLEF working notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,504.70,342.23,8.27;10,146.88,515.43,288.88,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,232.54,504.70,248.01,8.27;10,146.88,515.74,18.82,8.27">A support vector method for multivariate performance measures</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,185.16,515.43,221.82,8.96">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,526.66,342.28,8.27;10,146.88,537.28,327.99,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,260.97,526.66,219.63,8.27;10,146.88,537.58,160.64,8.27">New strategies for image annotation: Overview of the photo annotation task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.48,537.28,119.14,8.96">Working Notes of CLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,548.31,342.08,8.96;10,146.88,559.54,83.80,8.27" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,205.30,548.62,130.57,8.27">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,344.16,548.31,132.71,8.96">Readings in information retrieval</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,570.46,342.06,8.27;10,146.88,581.19,333.50,8.96;10,146.88,592.11,208.35,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,168.83,581.50,256.28,8.27">Exploiting visual concepts to improve text-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,443.40,581.19,36.98,8.96;10,146.88,592.11,179.74,8.96">European Conference on Information Retrieval (ECIR)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
