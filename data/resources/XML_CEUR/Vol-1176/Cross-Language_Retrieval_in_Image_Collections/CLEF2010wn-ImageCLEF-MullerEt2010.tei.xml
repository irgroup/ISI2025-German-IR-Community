<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.00,115.72,303.37,12.93;1,257.64,133.72,99.97,12.93">Overview of the CLEF 2010 medical image retrieval track</title>
				<funder ref="#_FDsfQPa">
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_3aEma7p">
					<orgName type="full">American National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_SuFgDHF">
					<orgName type="full">Swiss National Science Foundation (FNS)</orgName>
				</funder>
				<funder ref="#_z6Wk3Z6">
					<orgName type="full">National Library of MEdicine</orgName>
				</funder>
				<funder>
					<orgName type="full">EU FP7 projects Khresmoi and Promise</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,155.52,171.31,68.00,9.62"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Geneva University Hospitals</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences Western Switzerland</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.48,171.31,123.79,9.62"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.52,171.31,46.22,9.62"><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Geneva University Hospitals</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.04,171.31,28.82,9.62;1,168.96,183.31,33.22,9.62"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.64,183.31,57.05,9.62"><forename type="first">Joe</forename><surname>Reisetter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.32,183.31,84.03,9.62"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Medical College of Wisconsin</orgName>
								<address>
									<settlement>Milwaukee</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.72,183.31,63.26,9.62"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.00,115.72,303.37,12.93;1,257.64,133.72,99.97,12.93">Overview of the CLEF 2010 medical image retrieval track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9DD513737793DED3BDE088CCBC15C2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The seventh edition of the ImageCLEF medical retrieval task was organized in 2010. As in 2008 and 2009, the collection in 2010 uses images and captions from the Radiology and Radiographics journals published by RSNA (Radiological Society of North America). Three subtasks were conducted within the auspices of the medical task: modality detection, image-based retrieval and case-based retrieval. The goal of the modality detection task was to detect the acquisition modality of the images in the collection using visual, textual or mixed methods. The goal of the image-based retrieval task was to retrieve an ordered set of images from the collection that best met the information need specified as a textual statement and a set of sample images, while the goal of the case-based retrieval task was to return an ordered set of articles (rather than images) that best met the information need provided as a description of a "case". The number of registrations to the medical task increased to 51 research groups. However, groups submitting runs have remained stable at 16, with the number of submitted runs increasing to 155. Of these, 61 were ad-hoc runs, 48 were case-based runs while the remaining 46 were modality classification runs. The best results for the ad-hoc retrieval topics were obtained using mixed methods with textual methods also performing well. Textual methods were clearly superior for the case-based topics. For the modality detection task, although textual and visual methods alone were relatively successful, combining these techniques proved most effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF<ref type="foot" coords="1,187.92,599.83,3.97,6.71" target="#foot_0">1</ref>  <ref type="bibr" coords="1,196.56,601.27,8.26,9.62" target="#b0">[1]</ref><ref type="bibr" coords="1,204.82,601.27,4.13,9.62" target="#b1">[2]</ref><ref type="bibr" coords="1,208.95,601.27,8.26,9.62" target="#b2">[3]</ref> started in 2003 as part of the Cross Language Evaluation Forum (CLEF<ref type="foot" coords="1,197.04,611.71,3.97,6.71" target="#foot_1">2</ref> , <ref type="bibr" coords="1,207.48,613.27,10.31,9.62" target="#b3">[4]</ref>). A medical image retrieval task was added in 2004 and has been held every year since <ref type="bibr" coords="1,252.96,625.15,10.57,9.62" target="#b2">[3,</ref><ref type="bibr" coords="1,265.20,625.15,7.05,9.62" target="#b4">5]</ref>. The main goal of ImageCLEF continues to be promoting multi-modal information retrieval by combining a variety of media including text and images for more effective information retrieval.</p><p>In 2010, the format of CLEF was changed from a workshop at the European Conference on Digital Libraries (ECDL) to an independent conference on multilingual and multimedia retrieval evaluation <ref type="foot" coords="2,330.60,165.07,3.97,6.71" target="#foot_2">3</ref> which includes several organized evaluation tasks now called labs.</p><p>2 Participation, Data Sets, Tasks, Ground Truth This section describes the details concerning the set-up and the participation in the medical retrieval task in 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Participation</head><p>In 2010, a new record of 112 research groups registered for the four sub-tasks of ImageCLEF down from seven sub tasks in 2009. For the medical retrieval task the number of registrations also reached a new maximum with 51. 16 of the participants submitted results to the tasks, essentially the same number as in previous years. The following groups submitted at least one run:</p><p>-AUEB (Greece); -Bioingenium (Columbia) * ; -Computer Aided Medical Diagnoses (Edu??), * ; -Gigabioinforamtics (Belgium) * ; -IRIT (France); -ISSR (Egypt); -ITI, NIH (USA); -MedGIFT (Switzerland); -OHSU (USA); -RitsMIP (Japan) * ; -Sierre, HES-SO (Switzerland); -SINAI (Spain); -UAIC (Romania) * ; -UESTC (China) * ; -UIUC-IBM (USA) * ; -Xerox (France) * .</p><p>Participants marked with a star had never before participated in the medical retrieval task, indicating that the number of first-time participants was fairly high with eight among the 16 participants.</p><p>A total of 155 valid runs were submitted, 46 of which were submitted for modality detection, 61 for the image-based topics and 48 for the case-based topics. The number of runs per group was limited to ten per subtask and casebased and image-based topics were seen as separate subtasks in this view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>The database used in 2009 was again made accessible by the Radiological Society of North America (RSNA <ref type="foot" coords="3,247.92,150.19,3.97,6.71" target="#foot_3">4</ref> ). The database contained a total of 77,506 images, and was the largest collection to ever have been used for ImageCLEFmed. All images in the collection originated from the journals Radiology and Radiographics, published by the RSNA. A similar database is also available via the Goldminer<ref type="foot" coords="3,476.16,186.07,3.97,6.71" target="#foot_4">5</ref> interface. This collection constitutes an important body of medical knowledge from the peer-reviewed scientific literature including high quality images with textual annotations. Images are associated with journal articles, and can also be part of a larger figure. Figure captions were made available to participants, as well as the sub-caption concerning a particular subfigure (if available). This high-quality set of textual annotations enabled textual searching in addition to content-based retrieval. Furthermore, the PubMed IDs of each figure's originating article were also made available, allowing participants to access the MeSH (Medical Subject Headings) index terms assigned by the National Library of Medicine for MEDLINE<ref type="foot" coords="3,240.00,305.59,3.97,6.71" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Modality Classification</head><p>Previous research <ref type="bibr" coords="3,212.28,359.11,10.57,9.62" target="#b5">[6]</ref> has demonstrated the utility of classifying images by modality in order to improve the precision of the search. The modality classification task was conceived as the first step for the medical image retrieval task whereby participants use the modality classifier created in this step to improve their performance for the retrieval task. For this task, 2390 images were provided as a training set where each image was classified as belonging to one of 8 classes (CT, GX, MR, NM, PET, PX, US, XR). One of the authors (JKC) had manually, but somewhat cursorily, verified the assigned modality of all images. 2620 test images were provided for the task. Each of these images were to be assigned a modality using visual, textual or mixed techniques. Participants were also requested to provide a classification for all images in the collection. A majority vote classification for all images in the collection was made available upon request to participants of the task after the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Image-Based Topics</head><p>The topics for the image-based retrieval task were created using methods similar to previous years where realistic search topics were identified by surveying actual user needs. The starting point for the 2010 topics was a user study <ref type="bibr" coords="3,423.01,578.59,10.57,9.62" target="#b6">[7]</ref> conducted at Oregon Health &amp; Science University (OHSU) in early 2009. Using qualitative methods, this study was conducted with medical practitioners and was focused on understanding their needs, both met and unmet, regarding medical image retrieval. The first part of the study was dedicated to the investigation of the demographics and characteristics of participants, a population served by medical image retrieval systems (e.g., their background, searching habits, etc.). After a demonstration of state-of-the-art image retrieval systems, the second part of the study was devoted to learning about the motivation and tasks for which the intended audience uses medical image retrieval systems (e.g., contexts in which they seek medical images, types of useful images, numbers of desired answers, etc.). In the third and last part, the participants were asked to use the demonstrated systems to try to solve challenging queries, and provide responses to questions investigating how likely they would be to use such systems, aspects they did and did not like, and missing features they would like to see added. In total, the 37 participants utilized the demonstrated systems to perform a total of 95 searches using textual queries in English. We randomly selected 25 candidate queries from the 95 searches to create the topics for ImageCLEFmed 2009. Similarly, this year, we randomly selected another 25 queries from the remaining queries. From these, using the OHSU image retrieval system which was indexed using the 2009 ImageCLEF collection, we finally selected 16 topics for which at least one relevant image was retrieved by the system.</p><p>We added 2 to 4 sample images to each query from the previous collections of ImageCLEFmed. Then, for each topic, we provided a French and a German translation of the original textual description provided by the participants. Finally, the resulting set of topics was categorized into three groups: 3 visual topics, 9 mixed topics, and 4 semantic topics. This categorization of topics was based on the organizers' prior experience with how amenable certain types of search topics are to visual, textual or mixed search techniques. However, this is not an exact science and was merely provided for guidance. The entire set of topics was finally approved by a physician.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Case-Based Topics</head><p>Case-based topics were made available for the first time in 2009, and in 2010 the number of case-based topics was increased from 5 to 14, roughly half of all topics. The goal was to move image retrieval potentially closer to clinical routine by simulating the use case of a clinician who is in the process of diagnosing a difficult case. Providing this clinician with articles from the literature that discuss cases similar <ref type="foot" coords="4,189.36,540.67,3.97,6.71" target="#foot_6">7</ref> to the case (s)he is working on can be a valuable aid to choosing a good diagnosis or treatment.</p><p>The topics were created based on cases from the teaching file Casimage <ref type="bibr" coords="4,467.27,566.35,10.00,9.62" target="#b7">[8]</ref>. This teaching file contains cases (including images) from radiological practice that clinicians document mainly for using them in teaching. 20 cases were preselected and a search with the diagnosis was performed in the ImageCLEF data set to make sure that there were at least a few matching articles. Fourteen topics were finally chosen. The diagnosis and all information on the chosen treatment was then removed from the cases so as to simulate the situation of the clinician who has to diagnose the patient. In order to make the judging more consistent, the relevance judges were provided with the original diagnosis for each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Relevance Judgements</head><p>The relevance judgements were performed with the same on-line system as in 2008 and 2009 for the image-based topics as well as case-based topics. The system had been adapted in 2009 for the case-based topics, displaying the article title and several images appearing in the text (currently the first six, but this can be configured). Judges were provided with a protocol for the process with specific details on what should be regarded as relevant versus non-relevant. A ternary judgement scheme was used again, wherein each image in each pool was judged to be "relevant", "partly relevant", or "non-relevant". Images clearly corresponding to all criteria were judged as "relevant", images whose relevance could not be accurately confirmed but could still be possible were marked as "partly relevant", and images for which one or more criteria of the topic were not met were marked as "non-relevant". Judges were instructed in these criteria and results were manually verified during the judgement process. As in previous years, judges were recruited by sending out an e-mail to current and former students at OHSU's Department of Medical Informatics and Clinical Epidemiology. Judges, primarily clinicians, were paid a small stipend for their services. Many topics were judged by two or more judges to explore inter-rater agreements and its effects on the robustness of the rankings of the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section describes the results of ImageCLEF 2010. Runs are ordered based on the techniques used (visual, textual, mixed) and the interaction used (automatic, manual). Case-based topics and image-based topics are separated but compared in the same sections. Trec eval was used for the evaluation process, and we made use of most of its performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submissions</head><p>The numbers of submitting teams was slightly lower in 2010 than in 2009 with 16 instead of 17. The numbers of runs increased from 124 to 155. The distribution among the three run types of modality detection, image-based retrieval and casebased retrieval showed that all three types reached almost the same number of submissions.</p><p>Groups subsequently had the chance to evaluate additional runs themselves as the qrels were made available to participants two weeks ahead of the submission deadline for the working notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modality Detection Results</head><p>A variety of commonly used image processing techniques were explored by the participants. Features used included local binary patterns (LBP) <ref type="bibr" coords="6,429.12,150.79,10.00,9.62" target="#b8">[9]</ref>, Tamura texture features <ref type="bibr" coords="6,205.80,162.67,14.70,9.62" target="#b9">[10]</ref>, Gabor features <ref type="bibr" coords="6,294.02,162.67,14.70,9.62" target="#b10">[11]</ref>, the GIFT (GNU Image Finding Tool), the Color Layout Descriptor (CLD) and Edge Histogram Descriptor (EHD) from MPEG-7, Color and Edge Directivity Descriptor (CEDD) and Fuzzy Color and Texture Histogram (FCTH) using the Lucene image retrieval (LIRE) library, Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="6,323.43,210.55,15.61,9.62" target="#b11">[12]</ref> as well as various combinations of these. Classifiers ranged from simple k-nearest neighbors (kNN) to Ada-Boost, multilayer perceptrons and Support Vector Machines (SVMs) as well as a variety of techniques to combine the output from multiple classifiers including those derived from Bayes theory such as product, sum, maximum and mean rules</p><p>The results of the modality detection tasks are given in Table <ref type="table" coords="6,432.36,282.31,28.09,9.62" target="#tab_0">1below</ref>. As seen in the table, the best results were obtained using mixed methods (94%) for the modality classification task. The best run using textual methods (90%) had a slightly better accuracy than the best run using visual methods (87%). However, for groups that submitted runs using different methods, the best results were obtained when they combined visual and textual methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image-Based Retrieval Results</head><p>The best results for the ad-hoc retrieval topics were obtained using mixed methods. Textual methods, as in previous years also performed well. However, visual methods by themselves, were not very effective for this collection.</p><p>Visual Retrieval As in previous years, only 8 of the 61 submitted runs used purely visual techniques. As discussed previously, this collection, with extremely well annotated textual captions and images that are primarily from radiology, does not lend itself to purely visual techniques. However, as seen from the results of the mixed runs, the use of the visual information contained in the image can improve the search performance over that of a purely textual system.</p><p>An analysis of the results shows that most techniques are in a very similar range and only a single run had a significantly better result in terms of MAP. The baseline system GIFT (GNU Image Finding Tool) is in the upper half of the performance.</p><p>Textual Retrieval Participants explored a variety of information retrieval techniques from the use of stop word removal and stemming to utilizing Lucene or Lemur, commonly used toolkits to techniques using Latent Semantic Indexing, database searches using full-text Boolean queries, query expansion with external sources such as MeSH terms (manually or automatically assigned), UMLS concepts (using MetaMap) or wikipedia, to modality filtration to more complex language models that incorporate phrases (not just words) or paragraphs, sentence selection and query translation, as well as techniques such as pseudo relevance feedback. Many participants found the use of the manually assigned MeSH terms to be most useful. Modality filtration, using either text-based or image-based modality detection techniques was found to be useful by some participants while others found only minimal benefit using the modality. Best results were obtained by the University of Athens and Xerox. The baseline runs using Lucene with the captions and full text and without any optimization performed in the lower half.</p><p>Multimodal Retrieval This year, the run with the highest MAP utilized a multimodal approach to retrieval. However, many groups that performed a pure fusion of the text-based and image-based runs found a significant deterioration in performance as the visual runs had very poor performance. This year's results again emphasize the previously noted observations that although the use of visual information can improve the search results over purely textual methods, the process of effectively combining the information from the captions and image itself can be quite complex and are often not robust. Simple approaches of fusing visual and textual runs rarely lead to optimized performance. Interactive Retrieval This year, as in previous years, interactive retrieval was only used by a very small number of participants. The results were not substantially better than automatic runs. This continues to be an area where we would like to see improved participation but little success in doing so. For this reason the manual and interactive runs are not shown in separate tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case-based Retrieval Results</head><p>In terms of case-based retrieval almost all groups focused on using textual retrieval techniques as combining visual retrieval on a case basis is a difficult approach. Best results were obtained with a textual retrieval approach when using relevance feedback.</p><p>Visual Retrieval The performance of the single visual run submitted (see Table <ref type="table" coords="9,149.76,566.47,4.46,9.62" target="#tab_4">5</ref>) shows that the results are much lower than the text-based techniques. Still, compared with the image-based retrieval only a single image-based run had a higher MAP, meaning that also case-based retrieval is possible with purely visual retrieval techniques and can be used as a complement to the text approaches.</p><p>Textual Retrieval The vast majority of submissions was in the category of textual retrieval (see Table6). Best results were obtained by a collaboration of IBM and UIUC in the textual part. Surprisingly the baseline text result of using Lucene with the full text articles and with absolutely no optimization has the third best result and is within the limit of statistical significance of the best run.</p><p>The first three runs are basically very close and then the performance slowly drops of. In general results are slightly lower than for the image-based topics.</p><p>The baseline run using the image captions and then combining results of the single images obtains a much lower performance.</p><p>For the first time in several years there was actually a substantial number of feedback runs, although only two groups submitted feedback runs (see Table <ref type="table" coords="10,151.32,302.83,3.88,9.62" target="#tab_6">7</ref>). These runs show that relevance feedback can improve results, although the improvement is fairly low compared with the automatic run. All but one of the feedback runs has very good results, showing that the techniques work in a stable manner.</p><p>Multimodal Retrieval Only two participants actually submitted a mixed case-based result, and the performance of these runs is fairly low highlighting the difficulty in combining the textual and visual results properly. Much more research on the visual and combined retrieval seems necessary as the current techniques in this field do not seem to work in a satisfying way. For this reason an information fusion task using ImageCLEF 2009 data was organized at ICPR 2010, showing an enormous increase in performance when good fusion techniques are applied even when the base results have very strong variations in performance <ref type="bibr" coords="10,192.36,463.63,14.70,9.62" target="#b12">[13]</ref>. Very few of these runs using more sophisticated fusion techniques had a degradation in performance over the best single run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relevance Judgement Analysis</head><p>A number of topics, both image-based and case-based, were judged by two or even three judges. Seven topics were judged by two judges while two additional topics were judged by three judges. There were significant variations in the kappa metric used to evaluate the inter-rater agreement. Kappa for these topics ranged from 0 to 1. The average kappa was 0.47. However, there were 4 topics where the kappa was zero as one judge had assessed no images as being relevant while the other had said that 1-11 images were relevant. On the other hand, there was a topic where both judges agreed that only a single image was relevant. Topics with low number of relevant images (Â¡10) can cause difficulties in evaluation as difference in opinions between judges one a single image can result in large differences in performance metrics for that topic. Without these topics, the average kappa was 0.657, a more acceptable figure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Robustness of Rankings</head><p>We briefly explored the variability in the rankings of the various runs caused by using different judges for a topic, especially on topics that had very few relevant images. Topics 2 and 8 had a kappa of zero as one judge had not found any relevant images in the pool with the other found 1 and 9 relevant images respectively. Both judges had found one relevant image for topic 7. We explored the changes in ranking caused by eliminating these topics from the evaluation. Most runs had a none to substantial improvement in bpref with three runs demonstrating a substantial improvement in rankings without these topics. However, four runs had a drop in bpref as these runs had performed quite well on topic 7 and extremely well on topic 8. The relative rankings of the groups were vastly unchanged with using the assessment of different judges aside from topics with low number of relevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>As in 2009, the largest number of runs for the image-based and case-based tasks used textual techniques. The semantic topics combined with a database containing high-quality annotations lend themselves to textual methods. However, unlike in 2009, the best runs were those that effectively combined visual and textual methods. Visual runs continue to be rare and generally poor in performance.</p><p>Case-based topics had an increased participation over last year. As may be expected based on the nature of the task, case-based retrieval is more easily accomplished using textual techniques. Unlike in the ad-hoc runs, combining visual image severely degraded the performance for case-based topics, meaning that much more care needs to be taken with these combinations. More focus has to be put on the combinations to increase performance. Maybe a pure fusion task of results could be an additional challenge for the coming years.</p><p>A kappa analysis between several relevance judgements for the same topics shows that, although there are differences between judges, there was moderate agreement on topics that have more than 10 relevant images. As a result topics with very few relevant images could be removed or a more thorough testing could already remove them during the topic creation process.</p><p>For future campaign it seems important to explore how to effectively combine visual techniques with the text-based methods. As has been stated at previous ImageCLEFs, we strongly believe that interactive and manual retrieval are important and we strive to improve participation in these. This year's results show that even simple feedback can significantly improve results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,136.20,121.26,356.47,403.92"><head>Table 1 .</head><label>1</label><figDesc>Results of the runs of modality classification task.</figDesc><table coords="7,136.20,150.67,356.47,374.51"><row><cell>Run</cell><cell>Group</cell><cell cols="2">Run Type Classification Accuracy</cell></row><row><cell>XRCE MODCLS COMB testset.txt</cell><cell>XRCE</cell><cell>Mixed</cell><cell>0.94</cell></row><row><cell>XRCE MODCLS COMB allset.txt</cell><cell>XRCE</cell><cell>Mixed</cell><cell>0.94</cell></row><row><cell>Modality combined.txt</cell><cell>RitsMIP</cell><cell>Mixed</cell><cell>0.93</cell></row><row><cell>result text image combined.dat</cell><cell>ITI</cell><cell>Mixed</cell><cell>0.92</cell></row><row><cell>result text image comb Max.dat</cell><cell>ITI</cell><cell>Mixed</cell><cell>0.91</cell></row><row><cell>result text image comb Prod.dat</cell><cell>ITI</cell><cell>Mixed</cell><cell>0.91</cell></row><row><cell>gigabioinformatics-both.txt</cell><cell cols="2">GIGABIOINFORMATICS Mixed</cell><cell>0.90</cell></row><row><cell>result text image comb CV.dat</cell><cell>ITI</cell><cell>Mixed</cell><cell>0.89</cell></row><row><cell>result text image comb Sum.dat</cell><cell>ITI</cell><cell>Mixed</cell><cell>0.87</cell></row><row><cell>Modalityall Mix.txt</cell><cell>RitsMIP</cell><cell>Mixed</cell><cell>0.78</cell></row><row><cell>XRCE MODCLS TXT allset.txt</cell><cell>XRCE</cell><cell>Textual</cell><cell>0.90</cell></row><row><cell>result text titile caption mod Mesh.dat</cell><cell>ITI</cell><cell>Textual</cell><cell>0.89</cell></row><row><cell>entire text based modality class.dat</cell><cell>ITI</cell><cell>Textual</cell><cell>0.86</cell></row><row><cell>gigabioinformatics-text.txt</cell><cell cols="2">GIGABIOINFORMATICS Textual</cell><cell>0.85</cell></row><row><cell>Modality text.txt</cell><cell>RitsMIP</cell><cell>Textual</cell><cell>0.85</cell></row><row><cell>Modalityall Text.txt</cell><cell>RitsMIP</cell><cell>Textual</cell><cell>0.85</cell></row><row><cell>ipl aueb rhcpp full CT.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.74</cell></row><row><cell>ipl aueb rhcpp full CTM.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.71</cell></row><row><cell>ipl aueb rhcpp full CTMA.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.53</cell></row><row><cell>ipl aueb svm full CT.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.53</cell></row><row><cell>ipl aueb svm full CTM.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.49</cell></row><row><cell>ipl aueb svm full CTMA.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.32</cell></row><row><cell>ipl aueb rhcpp test only CTM.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.13</cell></row><row><cell>ipl aueb rhcpp test only CT.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.13</cell></row><row><cell>ipl aueb rhcpp test only CTMA.txt</cell><cell>AUEB</cell><cell>Textual</cell><cell>0.12</cell></row><row><cell>XRCE MODCLS IMG allset.txt</cell><cell>XRCE</cell><cell>Visual</cell><cell>0.87</cell></row><row><cell>UESTC modality boosting</cell><cell>UESTC</cell><cell>Visual</cell><cell>0.82</cell></row><row><cell>UESTC modality svm</cell><cell>UESTC</cell><cell>Visual</cell><cell>0.80</cell></row><row><cell>result image comb sum.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.80</cell></row><row><cell>result image comb CV.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.80</cell></row><row><cell>entire result image comb CV.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.80</cell></row><row><cell>entire result image comb CV.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.80</cell></row><row><cell>result image combined.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.79</cell></row><row><cell>entire result image combined.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.79</cell></row><row><cell>result image comb Max.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.76</cell></row><row><cell>entire result image comb max.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.76</cell></row><row><cell>gigabioinformatics-visual.txt</cell><cell cols="2">GIGABIOINFORMATICS Visual</cell><cell>0.75</cell></row><row><cell>result8.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.33</cell></row><row><cell>result7.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.32</cell></row><row><cell>result1.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.31</cell></row><row><cell>result2.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.30</cell></row><row><cell>result3.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.30</cell></row><row><cell>result4.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.30</cell></row><row><cell>result5.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.29</cell></row><row><cell>result6.txt</cell><cell>CAMD</cell><cell>Visual</cell><cell>0.29</cell></row><row><cell>entire result image comb sum.dat</cell><cell>ITI</cell><cell>Visual</cell><cell>0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,140.28,555.30,334.81,90.96"><head>Table 2 .</head><label>2</label><figDesc>Results of the visual runs for the medical image retrieval task.</figDesc><table coords="7,140.28,575.35,334.81,70.91"><row><cell>Run Name</cell><cell cols="2">Retrieval Type Run Type</cell><cell>Group</cell><cell>MAP bPref P10</cell></row><row><cell>fusion cv merge mean.dat</cell><cell>Visual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0091 0.0179 0.0125</cell></row><row><cell>XRCE IMG max.trec</cell><cell>Visual</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.0029 0.0069 0.0063</cell></row><row><cell>fusion cv merge max.dat</cell><cell>Visual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0026 0.0075 0.0063</cell></row><row><cell>GE GIFT8.treceval</cell><cell>Visual</cell><cell cols="3">Automatic medGIFT 0.0023 0.006 0.0125</cell></row><row><cell>NMFAsymmetricDCT5000 k2 7</cell><cell>Visual</cell><cell cols="3">Automatic Bioingenium 0.0018 0.011 0.0063</cell></row><row><cell>fusion cat merge max.dat</cell><cell>Visual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0018 0.0057 0.0063</cell></row><row><cell>NMFAsymmetricDCT2000 k2 5</cell><cell>Visual</cell><cell cols="3">Automatic Bioingenium 0.0015 0.0079 0.0063</cell></row><row><cell>NMFAsymmetricDCT5000 k2 5</cell><cell>Visual</cell><cell cols="3">Automatic Bioingenium 0.0014 0.0076 0.0063</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,136.20,198.06,348.53,307.44"><head>Table 3 .</head><label>3</label><figDesc>Results of the textual runs for the medical image retrieval task.</figDesc><table coords="8,136.20,227.47,348.53,278.03"><row><cell>Run Name</cell><cell cols="2">Retrieval Type</cell><cell>Run Type</cell><cell>Group</cell><cell>MAP bPref P10</cell></row><row><cell>WIKI AX MOD late.trec</cell><cell></cell><cell>Textual</cell><cell>Not applicable</cell><cell>XRCE</cell><cell>0.338 0.3828 0.5062</cell></row><row><cell>ipl aueb AdHoc default TC.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3235 0.3109 0.4687</cell></row><row><cell>ipl aueb adhoq default TCg.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3225 0.3087 0.4562</cell></row><row><cell>ipl aueb adhoq default TCM.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3209 0.3063 0.4687</cell></row><row><cell>ipl aueb AdHoc pivoting TC.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3155 0.2998 0.45</cell></row><row><cell>ipl aueb adhoq Pivoting TCg.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3145 0.2993 0.45</cell></row><row><cell>ipl aueb adhoq Pivoting TCM.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.3102 0.3005 0.4375</cell></row><row><cell>OHSU pm all all mod.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.3029 0.344 0.4313</cell></row><row><cell>OHSU pm major all mod.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.3004 0.3404 0.4375</cell></row><row><cell cols="2">OHSU all mh major jaykc mod.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.2983 0.3428 0.4188</cell></row><row><cell>XRCE CHI2AX MOD late.trec</cell><cell></cell><cell>Textual</cell><cell>Feedback</cell><cell>XRCE</cell><cell>0.2925 0.3027 0.4125</cell></row><row><cell>ipl aueb AdHoc Modality Filtering</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>AUEB</cell><cell>0.2787 0.296 0.375</cell></row><row><cell>I CT T.lst</cell><cell></cell><cell>Textual</cell><cell>Not applicable</cell><cell>SINAI</cell><cell>0.2764 0.2693 0.425</cell></row><row><cell>UESTC image pNw.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>UESTC</cell><cell>0.2751 0.3028 0.3438</cell></row><row><cell>XRCE DIR TXT.trec</cell><cell></cell><cell>Textual</cell><cell>Feedback</cell><cell>XRCE</cell><cell>0.2722 0.2837 0.4</cell></row><row><cell>UESTC image pBasic.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>UESTC</cell><cell>0.2713 0.2963 0.3438</cell></row><row><cell>I CT TMDM.lst</cell><cell></cell><cell>Textual</cell><cell>Feedback</cell><cell>SINAI</cell><cell>0.2672 0.2683 0.4125</cell></row><row><cell cols="3">OHSU high recall with tit mod reord Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.2623 0.2754 0.3875</cell></row><row><cell>I CT TM.lst</cell><cell></cell><cell>Textual</cell><cell>Feedback</cell><cell>SINAI</cell><cell>0.2616 0.2638 0.4062</cell></row><row><cell>OHSU high recall with titles.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.2592 0.2714 0.3875</cell></row><row><cell>issr CT.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.2583 0.2667 0.3187</cell></row><row><cell>hes-so-vs image-based captions</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">HES-SO VS 0.2568 0.278 0.35</cell></row><row><cell cols="3">OHSU all mh major jaykc mod reord Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.256 0.2533 0.3813</cell></row><row><cell>OHSU mm all mod.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.2476 0.2594 0.4125</cell></row><row><cell>WIKI LGD IMG MOD late.trec</cell><cell></cell><cell>Textual</cell><cell>Not applicable</cell><cell>XRCE</cell><cell>0.2343 0.2463 0.3937</cell></row><row><cell>issr CTS.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.231 0.2367 0.2812</cell></row><row><cell>issr CTP.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.2199 0.2424 0.325</cell></row><row><cell cols="3">ad hoc QE 0.1 Citations All Im Txt Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.188 0.2158 0.375</cell></row><row><cell cols="3">ad hoc queries terms 0.1 Cit All Txt Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.1589 0.1785 0.325</cell></row><row><cell>issr CT T MT.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.1472 0.1516 0.2571</cell></row><row><cell>issr CT T dic.txt</cell><cell></cell><cell>Textual</cell><cell>Manual</cell><cell>ISSR</cell><cell>0.1394 0.1117 0.1929</cell></row><row><cell>hes-so-vs image fulltext</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">HES-SO VS 0.1312 0.1684 0.1813</cell></row><row><cell>NMFText k2 11</cell><cell></cell><cell>Textual</cell><cell cols="3">Automatic Bioingenium 0.1005 0.1289 0.1875</cell></row><row><cell>issr CT T MeSh.txt</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.0985 0.1205 0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,136.20,173.94,354.66,164.04"><head>Table 4 .</head><label>4</label><figDesc>Results of the multimodal runs for the medical image retrieval task.</figDesc><table coords="9,136.20,203.35,354.66,134.63"><row><cell>Run Name</cell><cell cols="3">Retrieval Type Run Type</cell><cell>Group</cell><cell>MAP bPref P10</cell></row><row><cell>XRCE AX rerank comb.trec</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.3572 0.3841 0.4375</cell></row><row><cell cols="2">XRCE CHI2 LOGIT IMG MOD late.trec</cell><cell>Mixed</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.3167 0.361 0.3812</cell></row><row><cell>XRCE AF LGD IMG late.trec</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.3119 0.3201 0.4375</cell></row><row><cell>WIKI AX IMG MOD late.trec</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.2818 0.3279 0.3875</cell></row><row><cell>OHSU all mh major all mod reorder.txt</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.256 0.2533 0.3813</cell></row><row><cell>OHSU high recall.txt</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>OHSU</cell><cell>0.2386 0.2533 0.3625</cell></row><row><cell>queries terms 0.1 Modalities.trec</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.1067 0.1376 0.2812</cell></row><row><cell>XRCE AX rerank.trec</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>XRCE</cell><cell>0.0732 0.1025 0.1063</cell></row><row><cell cols="3">Exp Queries Cit CBIR CV MERGE MAXt Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0641 0.0962 0.1438</cell></row><row><cell>runMixt.txt</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic UAIC2010 0.0623 0.0666 0.1313</cell></row><row><cell cols="3">Exp Queries Cit CBIR CAT MERGE MAX Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0616 0.0975 0.1375</cell></row><row><cell cols="3">Queries Citations CBIR CV MERGE MAX Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0583 0.0783 0.125</cell></row><row><cell>Multimodal-Rerank-ROI-QE-Merge</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0486 0.0803 0.1</cell></row><row><cell>NMFAsymmetricMixed k2 11</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic Bioingenium 0.0395 0.047 0.0438</cell></row><row><cell>GE Fusion img fulltext Vis0.2.run</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic medGIFT 0.0245 0.0718 0.0375</cell></row><row><cell>GE Fusion img captions Vis0.2.run</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic medGIFT 0.0208 0.0753 0.0375</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.76,116.10,346.85,55.08"><head>Table 5 .</head><label>5</label><figDesc>Results of the visual runs for the medical image retrieval task (case-based topics).</figDesc><table coords="10,136.20,156.07,345.41,15.11"><row><cell>Run Name</cell><cell cols="2">Retrieval Type Run Type Group</cell><cell>MAP bPref P10</cell></row><row><cell>1276253404955 GE GIFT8 case</cell><cell>Visual</cell><cell cols="2">Automatic medGIFT 0.0358 0.0612 0.0929</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,136.20,128.23,366.73,361.79"><head>Table 6 .</head><label>6</label><figDesc>Results of the textual runs for the medical image retrieval task (Case-Based Topics).</figDesc><table coords="11,136.20,156.07,366.73,333.95"><row><cell>Run</cell><cell>Retrieval Type</cell><cell>Run Type</cell><cell>Group</cell><cell>MAP bPref P10</cell></row><row><cell>baselinefbWMR 10 0.2sub</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UIUCIBM 0.2902 0.3049 0.4429</cell></row><row><cell>baselinefbWsub</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UIUCIBM 0.2808 0.2816 0.4429</cell></row><row><cell>hes-so-vs case-based fulltext.txt</cell><cell>Textual</cell><cell cols="3">Automatic HES-SO VS 0.2796 0.2699 0.4214</cell></row><row><cell>baselinefbsub</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UIUCIBM 0.2754 0.2856 0.4286</cell></row><row><cell>baselinefbWMD 25 0.2sub</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UIUCIBM 0.2626 0.2731 0.4</cell></row><row><cell>C TA T.lst</cell><cell>Textual</cell><cell>Not applicable</cell><cell>SINAI</cell><cell>0.2555 0.2518 0.3714</cell></row><row><cell>IRIT SemAnnotator-2.0 BM25 N28.res</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2265 0.2351 0.3429</cell></row><row><cell>C TA TM.lst</cell><cell>Textual</cell><cell>Not applicable</cell><cell>SINAI</cell><cell>0.2201 0.2307 0.3643</cell></row><row><cell>IRIT SemAnnotator-</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2193 0.2139 0.3286</cell></row><row><cell>2.0 BM25 N28 1.res</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRIT SemAnnotator-</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2182 0.2267 0.3571</cell></row><row><cell>1.5.2 BM25 N34.res</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRIT-run-bl.res</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2103 0.1885 0.2786</cell></row><row><cell>IRIT SemAnnotator-</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2085 0.2083 0.3143</cell></row><row><cell>1.5.2 BM25 N34 1.res</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRIT SemAnnotator-</cell><cell>Textual</cell><cell>Automatic</cell><cell>IRIT</cell><cell>0.2085 0.2083 0.3143</cell></row><row><cell>2.0 BM25 N34 1.res</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ISSR cb cts.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.1986 0.1883 0.3071</cell></row><row><cell>ISSR cp ctp.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.1977 0.1873 0.3</cell></row><row><cell>ISSR CB CT.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>ISSR</cell><cell>0.1977 0.1873 0.3</cell></row><row><cell>ipl aueb CaseBased CTM 0.2.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1874 0.1927 0.3214</cell></row><row><cell>ipl aueb CaseBased CTM 0.1.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.186 0.1897 0.3214</cell></row><row><cell>ipl aueb CaseBased CT.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1841 0.1803 0.3143</cell></row><row><cell>ipl aueb CaseBased CTM 0.3.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1833 0.1919 0.3143</cell></row><row><cell>ipl aueb CaseBased CTM 0.4.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1809 0.1895 0.3143</cell></row><row><cell>ipl aueb CaseBased CTM 0.4.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1809 0.1895 0.3143</cell></row><row><cell>ipl aueb CaseBased CTM 0.5.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell>IPL</cell><cell>0.1716 0.1811 0.3429</cell></row><row><cell>UESTC case pBasic.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UESTC 0.1692 0.184 0.2643</cell></row><row><cell>UESTC case pQE.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UESTC 0.1677 0.1852 0.2786</cell></row><row><cell>UESTC case pNw.txt</cell><cell>Textual</cell><cell>Automatic</cell><cell cols="2">UESTC 0.1522 0.1725 0.2714</cell></row><row><cell cols="2">case based expanded queries backoff 0.1.trecTextual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.1501 0.1749 0.2929</cell></row><row><cell>case based queries backoff 0.1.trec</cell><cell>Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.128 0.1525 0.2357</cell></row><row><cell>hes-so-vs case-</cell><cell>Textual</cell><cell cols="3">Automatic HES-SO VS 0.1273 0.1375 0.25</cell></row><row><cell>based nodoubles captions.txt</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">case based expanded queries types 0.1.trec Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.1217 0.1502 0.2929</cell></row><row><cell>C TAbs TM.lst</cell><cell>Textual</cell><cell>Not applicable</cell><cell>SINAI</cell><cell>0.1146 0.1661 0.2643</cell></row><row><cell>case based queries pico MA 0.1.trec</cell><cell>Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.1145 0.1439 0.2</cell></row><row><cell>C TAbs T.lst</cell><cell>Textual</cell><cell>Not applicable</cell><cell>SINAI</cell><cell>0.1076 0.166 0.2571</cell></row><row><cell>case based queries types 0.1.trec</cell><cell>Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0996 0.1346 0.2286</cell></row><row><cell>case based queries terms 0.1.trec</cell><cell>Textual</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0522 0.07 0.0857</cell></row><row><cell>GE GIFT8 case.treceval</cell><cell>Visual</cell><cell>Automatic</cell><cell cols="2">medGIFT 0.0358 0.0612 0.0929</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,134.76,533.11,346.80,106.67"><head>Table 7 .</head><label>7</label><figDesc>Results of the textual interactive and feedback runs for the medical image retrieval task (Case-Based Topics).</figDesc><table coords="11,136.20,568.87,345.36,70.91"><row><cell>Run</cell><cell cols="3">Retrieval Type Run Type Group</cell><cell>MAP bPref P10</cell></row><row><cell>PhybaselineRelfbWMR 10 0.2sub</cell><cell>Textual</cell><cell cols="3">Feedback UIUCIBM 0.3059 0.3348 0.4571</cell></row><row><cell>PhybaselineRelfbWMD 25 0.2sub</cell><cell>Textual</cell><cell cols="3">Feedback UIUCIBM 0.2837 0.3127 0.4571</cell></row><row><cell>PhybaselineRelFbWMR 10 0.2 top20sub</cell><cell>Textual</cell><cell cols="3">Feedback UIUCIBM 0.2713 0.2897 0.4286</cell></row><row><cell>case based queries pico backoff 0.1.trec</cell><cell>Textual</cell><cell>Feedback</cell><cell>ITI</cell><cell>0.1386 0.1666 0.2</cell></row><row><cell>PhybaselinefbWMR 10 0.2sub</cell><cell>Textual</cell><cell cols="3">Manual UIUCIBM 0.3551 0.3714 0.4714</cell></row><row><cell>PhybaselinefbWsub</cell><cell>Textual</cell><cell cols="3">Manual UIUCIBM 0.3441 0.348 0.4714</cell></row><row><cell>PhybaselinefbWMD 25 0.2sub</cell><cell>Textual</cell><cell cols="3">Manual UIUCIBM 0.3441 0.348 0.4714</cell></row><row><cell cols="2">case based expanded queries terms 0.1.trec Textual</cell><cell>Manual</cell><cell>ITI</cell><cell>0.0601 0.0825 0.0857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,134.76,116.10,346.85,79.32"><head>Table 8 .</head><label>8</label><figDesc>Results of the multimodal runs for the medical image retrieval task (Case-Based Topics).</figDesc><table coords="12,136.20,156.43,345.41,38.99"><row><cell>Run</cell><cell cols="4">Retrieval Type Run Type Group</cell><cell>MAP bPref P10</cell></row><row><cell cols="2">case based queries cbir with case backoff</cell><cell>Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0353 0.0509 0.0429</cell></row><row><cell cols="3">case based queries cbir without case backoff Mixed</cell><cell>Automatic</cell><cell>ITI</cell><cell>0.0308 0.0506 0.0214</cell></row><row><cell>GE Fusion case captions Vis0.2</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic medGIFT 0.0143 0.0657 0.0357</cell></row><row><cell>GE Fusion case fulltext Vis0.2</cell><cell></cell><cell>Mixed</cell><cell cols="3">Automatic medGIFT 0.0115 0.0786 0.0357</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,645.74,117.03,8.27"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.72,656.66,135.75,8.27"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.72,656.66,112.35,8.27"><p>http://www.clef2010.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.72,634.82,93.63,8.27"><p>http://www.rsna.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.72,645.74,121.71,8.27"><p>http://goldminer.arrs.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,144.72,656.66,102.99,8.27"><p>http://www.pubmed.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,144.72,656.46,272.07,8.65"><p>"Similar" in terms of images and other clinical data on the patient.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. This work was partially funded by the <rs type="funder">Swiss National Science Foundation (FNS)</rs> under contracts <rs type="grantNumber">205321-109304/1</rs> and <rs type="grantNumber">PBGE22-121204</rs>, the <rs type="funder">American National Science Foundation (NSF)</rs> with grant <rs type="grantNumber">ITR-0325160</rs>, <rs type="funder">Google</rs>, the <rs type="funder">National Library of MEdicine</rs> grant <rs type="grantNumber">K99LM009889</rs>, and the <rs type="funder">EU FP7 projects Khresmoi and Promise</rs>. We would like to thank the <rs type="institution">RSNA</rs> for supplying the images of their journals Radiology and Radiographics for the ImageCLEF campaign.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SuFgDHF">
					<idno type="grant-number">205321-109304/1</idno>
				</org>
				<org type="funding" xml:id="_3aEma7p">
					<idno type="grant-number">PBGE22-121204</idno>
				</org>
				<org type="funding" xml:id="_FDsfQPa">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
				<org type="funding" xml:id="_z6Wk3Z6">
					<idno type="grant-number">K99LM009889</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.88,353.10,337.61,8.65;13,151.56,364.02,328.82,8.65;13,151.56,375.06,329.03,8.65;13,151.56,385.98,138.42,8.65" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,215.98,364.02,217.38,8.65">The CLEF 2005 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,458.40,364.02,21.99,8.65;13,151.56,375.06,147.24,8.65">Cross Language Evaluation Forum (CLEF</title>
		<title level="s" coord="13,330.83,375.06,149.77,8.65;13,151.56,385.98,29.18,8.65">Springer Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2005-09">2005. September 2006</date>
			<biblScope unit="page" from="535" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,396.42,337.62,8.65;13,151.56,407.46,328.80,8.65;13,151.56,418.38,328.98,8.65;13,151.56,429.30,329.06,8.65;13,151.56,440.22,294.90,8.65" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,311.34,396.42,169.16,8.65;13,151.56,407.46,100.76,8.65">The CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,268.27,418.38,212.28,8.65;13,151.56,429.30,222.38,8.65">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="13,450.25,429.30,30.38,8.65;13,151.56,440.22,135.40,8.65">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,450.78,337.60,8.65;13,151.56,461.70,328.99,8.65;13,151.56,472.62,329.00,8.65;13,151.56,483.66,296.46,8.65" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,216.45,461.70,264.09,8.65;13,151.56,472.62,57.07,8.65">Overview of the ImageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,235.55,472.62,97.94,8.65">CLEF 2007 Proceedings</title>
		<title level="s" coord="13,411.09,472.62,69.47,8.65;13,151.56,483.66,98.68,8.65">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="473" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,494.10,337.61,8.65;13,151.56,505.02,328.99,8.65;13,151.56,516.06,73.03,8.65" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,191.74,494.10,139.28,8.65">Report on CLEF-2001 experiments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,351.55,494.10,128.94,8.65;13,151.56,505.02,167.59,8.65">Report on the CLEF Conference 2001 (Cross Language Evaluation Forum)</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,526.50,337.75,8.65;13,151.56,537.42,328.95,8.65;13,151.56,548.34,178.63,8.65" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,413.74,526.50,66.89,8.65;13,151.56,537.42,228.22,8.65">A reference data set for the evaluation of medical image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>VallÃ©e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Terrier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,389.22,537.42,91.29,8.65;13,151.56,548.34,89.42,8.65">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,558.90,337.66,8.65;13,151.56,569.82,328.99,8.65;13,151.56,580.74,329.08,8.65;13,151.56,591.78,60.66,8.65" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,288.92,558.90,191.62,8.65;13,151.56,569.82,156.55,8.65">Multimodal medical image retrieval: image categorization to improve search precision</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,331.17,569.82,149.38,8.65;13,151.56,580.74,212.93,8.65">MIR &apos;10: Proceedings of the international conference on Multimedia information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,602.22,337.67,8.65;13,151.56,613.14,329.06,8.65;13,151.56,624.06,69.70,8.65" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="13,403.19,602.22,77.36,8.65;13,151.56,613.14,146.50,8.65">Understanding and improving image retrieval in medicine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Oregon Health and Science University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="13,142.88,634.62,337.32,8.65;13,151.56,645.54,328.81,8.65;13,151.56,656.46,103.51,8.65" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,442.04,634.62,38.17,8.65;13,151.56,645.54,234.68,8.65">Casimage project -a digital teaching files authoring environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dfouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>VallÃ©e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ratib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,398.62,645.54,81.76,8.65;13,151.56,656.46,32.44,8.65">Journal of Thoracic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.88,119.34,337.56,8.65;14,151.56,130.38,328.98,8.65;14,151.56,141.30,280.50,8.65" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,320.69,119.34,159.75,8.65;14,151.56,130.38,230.16,8.65">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,390.83,130.38,89.70,8.65;14,151.56,141.30,170.87,8.65">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.55,152.22,337.96,8.65;14,151.56,163.26,328.92,8.65;14,151.56,174.18,31.97,8.65" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,313.26,152.22,167.24,8.65;14,151.56,163.26,40.72,8.65">Texture features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,204.23,163.26,224.91,8.65">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="472" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.55,185.10,337.87,8.65;14,151.56,196.14,328.87,8.65;14,151.56,207.06,237.88,8.65" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,258.70,185.10,161.56,8.65">Texture features and learning similarity</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,445.30,185.10,35.11,8.65;14,151.56,196.14,328.87,8.65;14,151.56,207.06,43.23,8.65">Proceedings of the 1996 IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;96)</title>
		<meeting>the 1996 IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;96)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-06">June 1996</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.55,217.98,338.12,8.65;14,151.56,229.02,223.51,8.65" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,206.85,217.98,231.05,8.65">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,448.30,217.98,32.36,8.65;14,151.56,229.02,138.78,8.65">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.55,239.94,338.04,8.65;14,151.56,250.86,328.86,8.65;14,151.56,261.90,329.06,8.65;14,151.56,272.82,329.01,8.65;14,151.56,283.74,22.59,8.65" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,290.96,239.94,189.63,8.65;14,151.56,250.86,273.01,8.65">The ImageCLEF medical retrieval task at icpr 2010 -information fusion to combine viusal and textual information</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,445.31,250.86,35.11,8.65;14,151.56,261.90,291.77,8.65">Proceedings of the International Conference on Pattern Recognition (ICPR 2010)</title>
		<title level="s" coord="14,450.24,261.90,30.38,8.65;14,151.56,272.82,137.85,8.65">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the International Conference on Pattern Recognition (ICPR 2010)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
