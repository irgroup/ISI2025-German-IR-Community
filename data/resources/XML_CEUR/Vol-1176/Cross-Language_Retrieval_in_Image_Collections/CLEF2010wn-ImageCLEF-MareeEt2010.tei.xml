<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.84,66.80,307.56,12.93;1,146.04,84.68,323.23,12.93;1,201.00,102.68,213.30,12.93;1,162.48,120.56,290.34,12.93">Biomedical Imaging Modality Classification Using Bags of Visual and Textual Terms with Extremely Randomized Trees: Report of ImageCLEF 2010 Experiments</title>
				<funder>
					<orgName type="full">European Network of Excellence, PASCAL2</orgName>
				</funder>
				<funder>
					<orgName type="full">GIGA (University of Liège)</orgName>
				</funder>
				<funder>
					<orgName type="full">Walloon Region</orgName>
				</funder>
				<funder>
					<orgName type="full">Belgian State, Science Policy Office</orgName>
				</funder>
				<funder>
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
				<funder ref="#_jqyUjgV">
					<orgName type="full">F.R.S.-FNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,201.36,157.93,63.79,9.96"><forename type="first">Raphaël</forename><surname>Marée</surname></persName>
							<email>raphael.maree@ulg.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">GIGA Bioinformatics Avenue de l&apos;Hopital 1</orgName>
								<orgName type="institution">Tilman University of Liège</orgName>
								<address>
									<addrLine>Sart</addrLine>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.25,157.93,54.73,9.96"><forename type="first">Olivier</forename><surname>Stern</surname></persName>
							<email>olivier.stern@ulg.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">GIGA Bioinformatics Avenue de l&apos;Hopital 1</orgName>
								<orgName type="institution">Tilman University of Liège</orgName>
								<address>
									<addrLine>Sart</addrLine>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.00,157.93,59.02,9.96"><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
							<email>pierre.geurts@ulg.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">GIGA Bioinformatics Avenue de l&apos;Hopital 1</orgName>
								<orgName type="institution">Tilman University of Liège</orgName>
								<address>
									<addrLine>Sart</addrLine>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.84,66.80,307.56,12.93;1,146.04,84.68,323.23,12.93;1,201.00,102.68,213.30,12.93;1,162.48,120.56,290.34,12.93">Biomedical Imaging Modality Classification Using Bags of Visual and Textual Terms with Extremely Randomized Trees: Report of ImageCLEF 2010 Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DBF46EDE730C161BC5527EAD08D26E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>extremely randomized trees</term>
					<term>random subwindows</term>
					<term>bag-offeatures</term>
					<term>image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our experiments related to the ImageCLEF 2010 medical modality classification task using extremely randomized trees. Our best run combines bags of textual and visual features. It yields 90% recognition rate and ranks 6th among 45 runs (ranging from 94% downto 12%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Task description</head><p>We participated in the ImageCLEF 2010 task related to imaging modality classification<ref type="foot" coords="1,173.64,412.10,3.97,5.52" target="#foot_0">1</ref> . A set of 2390 (in color or greylevels) training images were provided. These were extracted by the organizers of the challenge from articles published in scientific journals (Radiology and Radiographics) together with the text of the figure captions and the title of articles. Images have been classified by experts into the following 8 classes that are illustrated by Figure <ref type="figure" coords="1,385.48,460.81,7.77,9.96" target="#fig_0">1:</ref> -CT: Computerized tomography (314 images) -GX: Graphics, typically drawing and graphs, (355 images) -MR: Magnetic resonance imaging (299 images) -NM: Nuclear Medicine (204 images) -PET: Positron emission tomography including PET/CT (285 images) -PX: optical imaging including photographs, micrographs, gross pathology etc (330 images) -US: ultrasound including (color) Doppler (307 images) -XR: x-ray including x-ray angiography (296 images) The goal of the task is to build classification models able to recognize the imaging modality, using visual information only, textual information only, or both. Such models are expected to improve further content-based image retrieval. Participants submitted their predictions on a independent test set of 2620 images (for which modality classifications were not available) and classification results were later evaluated by organizers. A total of 45 runs were submitted by 7 research teams. In this paper, we report our results exploiting visual and textual information independently or combined using extremely randomized trees in a straightforward fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generation of bags of textual terms</head><p>We adopted the bag of words model that is widely used in natural language processing and information retrieval. We processed the provided XML file using a Python script to build a dictionary of all unique term (words) from the training set of image captions and article titles. More precisely, the XML file is cleaned (by removing XML tags and unuseful characters, and lowering characters) and a dictionary is built which finally consists of 13553 unique terms. Each image was then described by 13553 features where each feature is simply the term frequency ie. the number of times a given term appears for that image (in its caption and title) normalized by the number of terms for that image (in its caption and title), so that each textual feature is comprised in the [0, 1] interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generation of bags of visual terms</head><p>Despite many years of research in feature extraction, decribing image content is not a trivial task when facing a new image classification problem. We adopt here a generic approach that learns a global, bag of visual words, image representation by using dense random subwindows extraction <ref type="bibr" coords="3,391.23,126.61,51.08,9.96" target="#b6">[MGPW05,</ref><ref type="bibr" coords="3,442.32,126.61,38.31,9.96" target="#b7">MGW07]</ref> and extremely randomized trees [GEW06,MNJ08,MGW09]. More precisely, from each training image, we extracted 2000 small subwindows (which sizes were randomized between 10% and 25% of image sizes) at random positions in images, we resized them to 16 × 16 patches and described them by 768 HSV raw values, as illustrated by Figure <ref type="figure" coords="3,240.02,186.49,3.90,9.96" target="#fig_1">2</ref>. We then built T = 10 extremely randomized trees using K = 28 random tests in each tree node and a minimum node sample size of 4000 (see Section 2.3 for algorithm details). Subwindow sizes and minimum node sample size were optimized on the training set by cross-validation. Other parameters were set to default values but further optimization might improve results. These parameter values yield 52660 terminal nodes in the ensemble of trees. Each terminal node (or leaf) of a tree is then used as visual feature (also known as "codebook" or "visual word"). By propagating image subwindows downto trees, each image is thus described by a global feature vector which dimensionality equals the number of terminal nodes in the ensemble of trees. For a given image, the value encoded for a given feature was equal to the visual term frequency, ie. the number of image subwindows that reach this terminal node divided by the total number of subwindows extracted in the image (so a leaf value is included in [0, 1], and the sum over all terminal nodes equals to 1 in a given tree for a given image), as illustrated by Figure <ref type="figure" coords="4,227.79,188.89,3.90,9.96" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extremely Randomized Trees Classifier</head><p>Once features are built, they are concatenated and fed into a machine learning algorithm to build a classifier. We used the extremely randomized trees algorithm <ref type="bibr" coords="4,180.97,594.49,40.34,9.96" target="#b4">[GEW06]</ref> that was successfully used in various application domains (e.g. [GdF + 05,HTIWG10]) and more particularly in the context of various im-age classification tasks where it was already combined with random subwindows extraction <ref type="bibr" coords="5,182.17,81.25,49.11,9.96" target="#b6">[MGPW05,</ref><ref type="bibr" coords="5,231.27,81.25,36.83,9.96" target="#b7">MGW07]</ref>.</p><p>Starting with the whole training set at the root node, the Extra-Trees algorithm builds an ensemble of decision trees according to the classical top-down decision tree induction procedure <ref type="bibr" coords="5,286.10,117.13,42.02,9.96" target="#b0">[BFOS84]</ref> that uses tests on input variables to progressively partitions the input space into hyperrectangular regions so as to yield regions where the output is constant. The two main differences between this algorithm and other tree-based ensemble methods are that it splits nodes by choosing both attributes and cut-points at random (rather than choosing the best cut-point that optimizes a score measure like in Tree Bagging <ref type="bibr" coords="5,436.69,176.89,30.97,9.96" target="#b1">[Bre96]</ref> or Random Forests <ref type="bibr" coords="5,207.36,188.89,30.38,9.96" target="#b2">[Bre01]</ref>) and that it uses the whole learning sample (rather than a bootstrap replica in Tree Bagging and Random Forests) to grow the trees.</p><p>In our case, we use extremely randomized trees to build visual features from images as already mentionned in previous section, but also as final classifier either on textual or visual features or both.</p><p>In the case of construction of visual features where subwindows are described by raw pixel values, a test associated to an internal node of a tree simply compares the value of a pixel (intensity of a grey level or of a certain color component) at a fixed location within a subwindow to a cut-point value. In the case of its use as final classifier, a internal test compares the value of a (textual or visual) feature to a cut-point value.</p><p>In order to filter irrelevant attributes, the filtering parameter K corresponds to the number of input features chosen at random at each node, where K can take all possible values from 1 to the number of variables describing the training objects (e.g. 768 when building visual features from HSV subwindows; 13553 when building trees on textual features only). For each of these K attributes, a numerical threshold is randomly choosen within the range of variations of that attribute in the subset of objects available in the the current tree node. The score of each binary test is then computed on the current training subset according to an information criterion (the score measure is defined in <ref type="bibr" coords="5,445.70,416.05,34.93,9.96" target="#b10">[Weh97]</ref> and corresponds to a measure of impurity), and the best test among the K tests is chosen to split the current node into two nodes. Objects that fulfill the choosen test are propagated to the left child node, and others to the right node, and the process is repeated recursively on both child nodes. The development of a node is stopped as soon as either all input variables or the output variable are constant in the local subset of the leaf (in which cases impurity can not be further reduced), or the number of objects in the leaf is smaller than a predefined value (the minimum node sample size, n min ). A number T of such trees are grown from the training sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual features only</head><p>Using T = 10000 trees and K = √ 13453 = 116 random tests in each tree node, we obtain 85% recognition rate on the independent test set using textual features only. This result is comparable with results we obtained by cross-validation on the training set (86%). Table <ref type="table" coords="6,269.05,81.25,4.26,9.96" target="#tab_2">3</ref>.1 gives the first 50 textual terms used by the ensemble of trees and their relative importance for each class. Among the 13553 terms, the model is able to select very relevant ones to discriminate imaging modalities (e.g. the active molecule FDG for PET, the term photograph for PX, the scintigraphy test for NM, . . . ) but it does not filter out several artefacts neither rather neutral words (A, B, BB, BBB, IN, . . . ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual features only</head><p>Using T = 10000 trees and K = √ 52660 = 230 random tests in each tree node of the final classifier, we obtained 75% recognition rate on the independant test set. This result is slightly better than results we obtained by cross-validation on the training set (71.75%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combination of textual and visual features</head><p>Building an ensemble of T = 10000 trees using both textual and visual features (ie. 62213 input features) raised results up to 90% recognition rate. This final result is comparable with results we obtained by cross-validation on the training set (90.377% recognition rate using both feature types, see the confusion matrix in Table <ref type="table" coords="6,174.25,323.05,3.88,9.96" target="#tab_1">2</ref>). To reduce computing times and give less importance to visual features than textual features, we introduced a new parameter, p, that indicates the proportion of visual features (randomly choosen) that each tree will use as input features. The value of K was fixed to its default value K = √ 13453 + p × 52660. The submitted result was obtained with p = 0.05 so that each tree was able to use 13553 textual features and 2633 (52660 × 0.05) visual features. This value yielded the best results by cross-validation on the training set.</p><p>Table <ref type="table" coords="6,177.48,406.81,4.98,9.96" target="#tab_2">3</ref> shows contribution of both feature types for each class. Visual features are the most useful for the GX (Graphics, typically drawing and graphs) imaging modality while textual features are the most useful for the PET imaging modality. Table <ref type="table" coords="6,204.49,442.69,4.98,9.96">4</ref> illustrates some test images and their classification confidences using individual feature types of a combination of them.</p><p>It has to be noted that we also experimented with LIBLINEAR<ref type="foot" coords="6,421.68,465.74,3.97,5.52" target="#foot_1">2</ref> as final classifier on both feature types but results were slightly inferior (86.42% recognition rate by cross-validation on the training set) to those obtained with extremely randomized trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computing times</head><p>Our approach consists in rather simple operations, especially for the prediction phase. First, the training of 10 trees for the bag of visual terms construction takes about 20 hours when using about 4.6 million subwindows (2000 subwindows for each of the 2320 training images) on a single processor. This training phase is only performed once using all training images. Building the 10000 trees of the final classifier requires about 4 hours. Once trees have been built, the bag of visual terms of one test image is constructed on average in 0.83s (0.65s for the extraction and resizing of 2000 subwindows, 0.18s for their propagation in the ensemble of 10 trees). Computing the bag of textual term frequencies for test images requires 0.03s per image on average. Propagating an image (described by both textual and visual features) into the final classifier requires 0.01s per image. In total, a new image could then be described and classified roughly in less than 1s using both feature types on a single computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We obtained 90% recognition rate on the ImageCLEF 2010 modality classification task using a rather straightfoward and fast approach that combine textual and visual features. Optimization of parameters and other combination mechanisms might further improve results.  <ref type="table" coords="9,165.60,596.80,4.12,8.97">4</ref>. Examples of image predictions using textual, visual or both feature types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.76,274.84,345.65,8.97;2,134.76,285.76,307.47,8.97;2,137.64,66.84,340.10,193.72"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images for each of the 8 classes of the ImageCLEF 2010 imaging modality classification task (from articles published in Radiology and Radiographics).</figDesc><graphic coords="2,137.64,66.84,340.10,193.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,152.52,526.36,307.23,8.97;3,137.64,219.13,340.17,292.95"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Random subwindows extraction and description by raw pixel values.</figDesc><graphic coords="3,137.64,219.13,340.17,292.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.76,471.88,345.74,8.97;4,134.76,482.92,25.11,8.97;4,137.64,222.42,340.10,235.30"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training an ensemble of trees from random subwindows to build bags of visual terms.</figDesc><graphic coords="4,137.64,222.42,340.10,235.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,185.64,68.20,24.26,8.97;9,265.92,68.20,103.23,8.97;9,387.12,68.20,25.35,8.97;9,439.92,68.20,19.76,8.97;9,259.56,163.00,22.85,8.97;9,302.64,163.00,13.15,8.97;9,333.84,163.00,139.26,8.97;9,257.28,307.48,27.41,8.97;9,301.56,307.48,15.18,8.97;9,333.36,307.48,140.82,8.97;9,259.56,365.56,22.85,8.97;9,302.64,365.56,13.15,8.97;9,333.84,365.56,138.89,8.97;9,257.28,494.08,27.41,8.97;9,302.16,494.08,14.11,8.97;9,333.60,494.08,139.74,8.97;9,259.56,585.88,22.85,8.97;9,302.28,585.88,13.74,8.97;9,333.84,585.88,139.26,8.97;9,136.68,596.80,25.50,8.97"><head></head><label></label><figDesc>81) XR (0.43) XR (0.47) 128482 MR GX (0.41) MR (0.38) MR (0.50) 36398 PX PX (0.80) CT (0.26) PX (0.32) 223797 GX XR (0.42) GX (0.98) GX (0.86) 63382 XR PX (0.29) PET (0.36) XR (0.45) Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.76,69.16,347.80,598.77"><head>Table 1 .</head><label>1</label><figDesc>The top 50 textual terms according to their importance given by 10000 extremely randomized trees.</figDesc><table coords="7,136.20,69.16,346.36,576.81"><row><cell>Term</cell><cell cols="2">Importance CT GX MR</cell><cell>NM PET PX US XR</cell></row><row><cell>US</cell><cell>100.00</cell><cell cols="2">1.77 0.64 1.47 0.35 0.44 0.54 17.32 1.49</cell></row><row><cell>CT</cell><cell>98.05</cell><cell cols="2">15.73 0.44 1.56 0.61 0.54 0.76 1.96 1.80</cell></row><row><cell>MR</cell><cell>65.64</cell><cell cols="2">1.22 0.52 10.74 0.41 0.56 0.48 1.05 1.06</cell></row><row><cell>PET</cell><cell>63.86</cell><cell cols="2">0.87 0.31 0.41 0.59 12.96 0.24 0.39 0.45</cell></row><row><cell>FDG</cell><cell>62.09</cell><cell cols="2">0.79 0.30 0.37 0.61 12.69 0.25 0.37 0.41</cell></row><row><cell>T-WEIGHTED</cell><cell>53.56</cell><cell cols="2">0.65 0.72 9.66 0.22 0.30 0.34 0.56 0.59</cell></row><row><cell>PHOTOGRAPH</cell><cell>52.03</cell><cell cols="2">0.64 0.68 0.42 0.17 0.14 8.67 0.44 0.76</cell></row><row><cell>SCAN</cell><cell>43.08</cell><cell cols="2">4.57 1.18 0.86 0.40 0.85 0.47 0.78 1.16</cell></row><row><cell>RADIOGRAPH</cell><cell>42.44</cell><cell cols="2">0.77 0.63 0.51 0.23 0.12 0.28 0.46 7.37</cell></row><row><cell>UPTAKE</cell><cell>36.59</cell><cell cols="2">0.65 0.24 0.36 3.11 4.47 0.29 0.36 0.47</cell></row><row><cell>GRAPH</cell><cell>35.56</cell><cell cols="2">0.27 6.15 0.28 0.09 0.08 0.21 0.32 0.35</cell></row><row><cell>IMAGE</cell><cell>32.86</cell><cell cols="2">0.49 1.32 2.42 0.30 0.19 0.69 1.51 0.90</cell></row><row><cell>STAIN</cell><cell>26.87</cell><cell cols="2">0.20 0.31 0.24 0.08 0.09 4.79 0.21 0.22</cell></row><row><cell>ORIGINAL</cell><cell>24.51</cell><cell cols="2">0.18 0.25 0.25 0.08 0.09 4.38 0.18 0.20</cell></row><row><cell>FOR</cell><cell>23.46</cell><cell cols="2">0.43 3.09 0.32 0.19 0.20 0.23 0.33 0.50</cell></row><row><cell>MAGNIFICATION</cell><cell>22.58</cell><cell cols="2">0.18 0.31 0.23 0.08 0.09 3.91 0.19 0.17</cell></row><row><cell>-YEAR-OLD</cell><cell>22.44</cell><cell cols="2">0.97 1.88 0.47 0.26 0.29 0.41 0.29 0.65</cell></row><row><cell>PHOTOMICROGRAPH</cell><cell>22.34</cell><cell cols="2">0.18 0.21 0.19 0.07 0.07 4.01 0.20 0.17</cell></row><row><cell>SCINTIGRAPHY</cell><cell>19.38</cell><cell cols="2">0.15 0.12 0.10 5.34 0.20 0.08 0.11 0.16</cell></row><row><cell>SCINTIGRAM</cell><cell>19.34</cell><cell cols="2">0.14 0.12 0.08 5.42 0.14 0.08 0.12 0.17</cell></row><row><cell>ARROW</cell><cell>18.96</cell><cell cols="2">0.82 1.76 0.33 0.21 0.27 0.23 0.31 0.45</cell></row><row><cell>PET-CT</cell><cell>18.94</cell><cell cols="2">0.23 0.09 0.09 0.15 3.96 0.08 0.10 0.11</cell></row><row><cell>IMAGES</cell><cell>16.81</cell><cell cols="2">0.29 0.54 1.38 0.22 0.47 0.43 0.33 0.41</cell></row><row><cell>ACTIVITY</cell><cell>16.00</cell><cell cols="2">0.23 0.13 0.15 2.65 1.03 0.09 0.16 0.24</cell></row><row><cell>FONT</cell><cell>15.77</cell><cell cols="2">0.16 0.16 0.16 0.09 0.10 2.62 0.16 0.19</cell></row><row><cell>SPECIMEN</cell><cell>14.96</cell><cell cols="2">0.23 0.19 0.18 0.07 0.07 2.28 0.17 0.27</cell></row><row><cell>AXIAL</cell><cell>14.80</cell><cell cols="2">0.67 0.35 0.96 0.18 0.54 0.20 0.22 0.48</cell></row><row><cell>PETCT</cell><cell>14.10</cell><cell cols="2">0.15 0.08 0.09 0.06 2.95 0.05 0.09 0.09</cell></row><row><cell>A</cell><cell>13.93</cell><cell cols="2">0.40 1.02 0.29 0.21 0.12 0.42 0.33 0.48</cell></row><row><cell>ARROWS</cell><cell>13.87</cell><cell cols="2">0.36 1.11 0.34 0.16 0.11 0.23 0.50 0.42</cell></row><row><cell>RADIONUCLIDE</cell><cell>13.58</cell><cell cols="2">0.12 0.09 0.07 3.69 0.12 0.09 0.06 0.13</cell></row><row><cell>HYPOECHOIC</cell><cell>13.54</cell><cell cols="2">0.19 0.10 0.19 0.04 0.05 0.11 2.46 0.11</cell></row><row><cell>AB</cell><cell>13.01</cell><cell cols="2">0.19 0.21 0.26 0.38 0.04 0.81 0.70 0.58</cell></row><row><cell>IMAGING</cell><cell>12.95</cell><cell cols="2">0.36 0.20 1.12 0.44 0.15 0.22 0.31 0.42</cell></row><row><cell>FUSED</cell><cell>12.65</cell><cell cols="2">0.16 0.07 0.07 0.09 2.62 0.05 0.08 0.07</cell></row><row><cell>LONGITUDINAL</cell><cell>12.40</cell><cell cols="2">0.23 0.09 0.09 0.06 0.05 0.08 2.22 0.16</cell></row><row><cell>DOPPLER</cell><cell>12.14</cell><cell cols="2">0.14 0.07 0.15 0.04 0.05 0.07 2.27 0.12</cell></row><row><cell>HEMATOXYLIN-EOSIN</cell><cell>11.79</cell><cell cols="2">0.09 0.13 0.10 0.04 0.05 2.09 0.09 0.09</cell></row><row><cell>SHOWS</cell><cell>11.58</cell><cell cols="2">0.37 0.54 0.32 0.22 0.13 0.40 0.39 0.40</cell></row><row><cell>OBTAINED</cell><cell>11.16</cell><cell cols="2">0.53 0.62 0.23 0.27 0.15 0.23 0.26 0.38</cell></row><row><cell>TRANSVERSE</cell><cell>11.00</cell><cell cols="2">0.52 0.41 0.22 0.13 0.10 0.15 0.71 0.38</cell></row><row><cell>IN</cell><cell>10.93</cell><cell cols="2">0.36 0.28 0.26 0.25 0.55 0.33 0.30 0.36</cell></row><row><cell>TC-M</cell><cell>10.68</cell><cell cols="2">0.09 0.06 0.05 2.97 0.06 0.04 0.09 0.09</cell></row><row><cell>SAGITTAL</cell><cell>10.63</cell><cell cols="2">0.20 0.26 1.05 0.08 0.10 0.16 0.46 0.24</cell></row><row><cell>CORONAL</cell><cell>10.46</cell><cell cols="2">0.22 0.30 0.44 0.15 0.95 0.12 0.14 0.24</cell></row><row><cell>BAB</cell><cell>10.39</cell><cell cols="2">0.31 0.64 0.29 0.25 0.20 0.23 0.24 0.32</cell></row><row><cell>B</cell><cell>10.23</cell><cell cols="2">0.27 0.75 0.22 0.29 0.17 0.17 0.23 0.34</cell></row><row><cell>BBB</cell><cell>10.00</cell><cell cols="2">0.31 0.60 0.26 0.21 0.18 0.26 0.24 0.31</cell></row><row><cell>BB</cell><cell>9.89</cell><cell cols="2">0.31 0.25 0.18 0.44 0.07 0.37 0.40 0.45</cell></row><row><cell>SPECT</cell><cell>9.81</cell><cell cols="2">0.07 0.03 0.07 2.73 0.10 0.04 0.06 0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.76,169.96,345.79,75.33"><head>Table 2 .</head><label>2</label><figDesc>Confusion matrix obtained by cross-validation on the training set using both feature types (90.377% recognition rate).</figDesc><table coords="8,202.80,213.64,209.69,31.65"><row><cell cols="2">Feature type CT GX MR NM PET PX US XR</cell></row><row><cell cols="2">Textual 0.07 0.02 0.08 0.06 0.09 0.07 0.07 0.06</cell></row><row><cell>Visual</cell><cell>0.06 0.12 0.05 0.03 0.03 0.07 0.06 0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.76,247.24,345.75,19.89"><head>Table 3 .</head><label>3</label><figDesc>Class importance of feature types given by 10000 extremely randomized trees on the training set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,607.74,173.43,8.27"><p>http://www.imageclef.org/2010/medical</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.72,607.74,206.43,8.27"><p>http://www.csie.ntu.edu.tw/~cjlin/liblinear/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. RM is supported by the <rs type="funder">GIGA (University of Liège)</rs> with the help of the <rs type="funder">Walloon Region</rs> and the <rs type="funder">European Regional Development Fund</rs>. PG is Research Associate of the <rs type="funder">F.R.S.-FNRS</rs>. This paper presents research results of the <rs type="projectName">Belgian Network BIOMAGNET (Bioinformatics and Modeling</rs>: from Genomes to Networks), funded by the <rs type="programName">Interuniversity Attraction Poles Programme</rs>, initiated by the <rs type="funder">Belgian State, Science Policy Office</rs>. This work is supported by the <rs type="funder">European Network of Excellence, PASCAL2</rs>. The scientific responsibility rests with the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jqyUjgV">
					<orgName type="project" subtype="full">Belgian Network BIOMAGNET (Bioinformatics and Modeling</orgName>
					<orgName type="program" subtype="full">Interuniversity Attraction Poles Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,184.79,93.04,295.72,8.97;10,184.80,103.96,268.10,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,427.20,93.04,53.31,8.96;10,184.80,103.96,82.89,8.96">Classification and Regression Trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth International</publisher>
			<pubPlace>California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,114.88,295.01,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,237.09,114.88,74.29,8.97">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,319.44,114.88,70.75,8.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,125.92,265.49,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,237.09,125.92,61.76,8.97">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,306.72,125.92,68.19,8.96">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.80,136.84,295.58,8.97;10,184.80,147.76,295.57,8.97;10,184.80,158.80,295.71,8.97;10,184.80,169.71,161.33,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,439.44,147.76,40.93,8.97;10,184.80,158.80,291.13,8.97">Proteomic mass spectra classification using decision tree based ensemble methods</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Deseny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianne</forename><surname>Fillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Alice</forename><surname>Meuwis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michel</forename><surname>Malaise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Paule</forename><surname>Merville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,184.80,169.71,57.94,8.96">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3138" to="3145" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,180.64,295.87,8.97;10,184.80,191.56,133.60,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,344.09,180.64,112.43,8.97">Extremely randomized trees</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,464.40,180.64,16.25,8.96;10,184.80,191.56,57.79,8.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,187.43,202.59,293.05,8.97;10,184.80,213.51,295.75,8.97;10,184.80,224.44,283.48,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,222.11,213.51,258.44,8.97;10,184.80,224.44,57.73,8.97">Inferring regulatory networks from expression data using treebased methods</title>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Irrthum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,305.78,224.44,44.88,8.96">PLoS ONE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>To appear in. DREAM4 collection</note>
</biblStruct>

<biblStruct coords="10,185.51,235.47,294.96,8.97;10,184.80,246.39,295.58,8.97;10,184.80,257.31,48.63,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,382.72,235.47,97.75,8.97;10,184.80,246.39,104.36,8.97">Random subwindows for robust image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.80,246.39,72.36,8.96">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,268.35,295.75,8.97;10,184.80,279.27,295.74,8.97;10,184.80,290.19,295.75,8.97;10,184.80,301.23,230.91,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,411.24,268.35,69.30,8.97;10,184.80,279.27,295.74,8.97;10,184.80,290.19,11.91,8.97">Random subwindows and extremely randomized trees for image classification in cell biology</title>
		<author>
			<persName coords=""><forename type="first">Raphaël</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,205.44,290.19,275.11,8.96;10,184.80,301.23,155.45,8.96">BMC Cell Biology supplement on Workshop of Multiscale Biological Imaging, Data Mining and Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,312.15,295.70,8.97;10,184.80,323.07,295.83,8.97;10,184.80,334.11,295.60,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,396.48,312.15,84.01,8.97;10,184.80,323.07,265.60,8.97">Content-based image retrieval by indexing random subwindows with randomized trees</title>
		<author>
			<persName coords=""><forename type="first">Raphaël</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,460.80,323.07,19.83,8.96;10,184.80,334.11,205.09,8.96">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2009-01">jan 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.80,345.03,295.72,8.97;10,184.80,355.95,295.49,8.97;10,184.80,366.99,44.92,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,389.24,345.03,91.27,8.97;10,184.80,355.95,118.88,8.97">Randomized clustering forests for image classification</title>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,311.40,355.95,114.44,8.96">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1632" to="1646" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.79,377.91,295.80,8.97;10,184.80,388.83,187.68,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,241.92,377.91,200.94,8.96">Automatic Learning Techniques in Power Systems</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-11">November 1997</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
