<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.63,99.89,397.36,12.90;1,231.65,117.83,160.96,12.90;1,222.74,138.21,178.79,10.75">FASTDOCODE: Finding Approximated Segments of N-Grams for Document Copy Detection Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.54,175.14,73.50,8.64"><forename type="first">Gabriel</forename><surname>Oberreuter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.64,175.14,68.51,8.64"><forename type="first">Gaston</forename><surname>L'huillier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.42,175.14,69.46,8.64"><forename type="first">Sebastián</forename><forename type="middle">A</forename><surname>Ríos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.40,175.14,73.31,8.64"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Velásquez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">University of Chile</orgName>
								<address>
									<settlement>Santiago</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.63,99.89,397.36,12.90;1,231.65,117.83,160.96,12.90;1,222.74,138.21,178.79,10.75">FASTDOCODE: Finding Approximated Segments of N-Grams for Document Copy Detection Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C9C6B58F08C9DB3ED575CDEE4CDBDE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, plagiarism has been presented as one of the main distresses that the information technology revolution has lead into our society for which using pattern matching algorithms and intelligent data analysis approaches, these practices could be identified. Furthermore, a fast document copy detection algorithm could be used in large scale applications for plagiarism detection in academia, scientific research, patents, knowledge management, among others. Notwithstanding the fact that plagiarism detection has been tackled by exhaustive comparison of source and suspicious documents, approximated algorithms could lead to interesting results. In this paper, an approach for plagiarism detection is presented. Results in a learning dataset of plagiarized documents from the PAN'09, and its further evaluation in the PAN'10 plagiarism detection challenge, showed that the trade-off between speed and performance could improve other plagiarism detection algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plagiarism in academia is rising and multiple authors have worked to describe this phenomena <ref type="bibr" coords="1,476.28,413.19,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="1,494.44,413.19,12.45,8.64" target="#b11">12,</ref><ref type="bibr" coords="1,509.27,413.19,11.83,8.64" target="#b19">20]</ref>. As commented by Hunt in <ref type="bibr" coords="1,207.86,425.15,15.27,8.64" target="#b10">[11]</ref>, "Internet Plagiarism" is referred sometimes as a cataclysmic consequence of the "Information Technology revolution", as it proves to be a big problem in academia. In <ref type="bibr" coords="1,461.89,437.10,15.27,8.64" target="#b19">[20]</ref>, plagiarism is analyzed from various perspectives and considered as a problem that is growing bigger over time. In <ref type="bibr" coords="1,505.96,449.06,15.27,8.64" target="#b11">[12]</ref>, the author analyzes different statistical data and the implications of the "IT age".</p><p>To tackle this problem, one approach is to try to detect plagiarism. Different methods involving computer aided plagiarism detection have been under research <ref type="bibr" coords="1,310.32,485.46,10.79,8.64" target="#b5">[6,</ref><ref type="bibr" coords="1,323.65,485.46,7.47,8.64" target="#b7">8,</ref><ref type="bibr" coords="1,333.67,485.46,12.45,8.64" target="#b9">10,</ref><ref type="bibr" coords="1,348.67,485.46,12.45,8.64" target="#b13">14,</ref><ref type="bibr" coords="1,363.66,485.46,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="1,378.66,485.46,12.45,8.64" target="#b23">24,</ref><ref type="bibr" coords="1,393.66,485.46,11.83,8.64" target="#b24">25]</ref>, from which different system for automatic plagiarism detection have been developed. However, different ways for neutralizing such detection systems have been presented. Such methods usually involve modifying the text in such way that the presentation of the document remains the same, but the underlying code is different and normally this differentiation render the detection systems useless <ref type="bibr" coords="1,304.82,533.28,15.27,8.64" target="#b18">[19]</ref>.</p><p>Plagiarism detection for document sources can be classified into several categories <ref type="bibr" coords="1,460.71,545.77,10.58,8.64" target="#b6">[7]</ref>. From exact document copy, to paraphrasing, different levels of plagiarism techniques can been used in several contexts <ref type="bibr" coords="1,99.21,569.68,15.77,8.64" target="#b13">[14,</ref><ref type="bibr" coords="1,117.18,569.68,11.83,8.64" target="#b27">28]</ref>. Likewise, pairs of documents can be described into different categories as unrelated, related, partly overlapped, subset, and copied.</p><p>When comparing a suspicious document against a collection of possible sources, it is tried to identify the sentences, paragraph or ideas that have been copied. This is called external plagiarism detection <ref type="bibr" coords="1,488.70,606.07,15.27,8.64" target="#b21">[22]</ref>, and multiple efforts are being oriented in this area. Another approach, is to determine within features extracted from just one given document. However, this work is mainly focused on external plagiarism detection, without considering elements from the intrinsic plagiarism detection case.</p><p>The main contribution of this work is a technique for plagiarism detection based on a two step evaluation. First, a filter evaluation which considers a fast generation of segments of n-grams for an approximated decision. And second, an obfuscation and exhaustive search process for the offset and length of the plagiarized extraction between two previously classified documents is performed. This two-step algorithm is based on different document pre-processing strategies and decision thresholds which gives a large number of parameters or degrees of freedom to be determined. This paper is structured as follows: In Section 2 an overview of plagiarism detection algorithms and related work is presented. Then, in Section 3 the proposed FAST Document Copy Detection (FASTDOCODE) method is introduced. Afterwards, in Section 4, the experimental setup and evaluation performance criteria are described. In Section 5 results are discussed. Finally, in Section 6 the main conclusions are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>According to Schleimer et al. <ref type="bibr" coords="2,196.84,126.76,15.27,8.64" target="#b22">[23]</ref>, copy prevention and detection methods can be combined to reduce plagiarism. While copy detection methods can only minimize it, prevention methods can fully eliminate it and decrease it. Notwithstanding this fact, prevention methods need the whole society to take part, thus its solution is non trivial. Copy or plagiarism detection methods tackle different levels, from simple manual comparison to complex automatic algorithms <ref type="bibr" coords="2,252.30,174.58,15.27,8.64" target="#b21">[22]</ref>. Among these techniques, document similarity detection, writing style detection, document content similarity, content translation, multi-site source plagiarism, and multi-lingual plagiarism detection methods have been previously proposed <ref type="bibr" coords="2,370.78,198.49,10.79,8.64" target="#b5">[6,</ref><ref type="bibr" coords="2,384.06,198.49,7.47,8.64" target="#b6">7,</ref><ref type="bibr" coords="2,394.02,198.49,12.45,8.64" target="#b13">14,</ref><ref type="bibr" coords="2,408.97,198.49,12.45,8.64" target="#b17">18,</ref><ref type="bibr" coords="2,423.91,198.49,12.45,8.64" target="#b21">22,</ref><ref type="bibr" coords="2,438.85,198.49,12.45,8.64" target="#b22">23,</ref><ref type="bibr" coords="2,453.80,198.49,11.83,8.64" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intrinsic Plagiarism Detection</head><p>When comparing texts against a reference set of possible sources, comes the complication of choosing the right set. And now more than ever, with the possibilities the internet bring to plagiarists, this task becomes more complicated to achieve. For this, intrinsic plagiarism detection algorithms have been developed <ref type="bibr" coords="2,474.94,271.31,15.27,8.64" target="#b27">[28]</ref>.</p><p>The writer style can be analyzed within the document and an examination for incongruities can be done. The complexity and style of each text can be analyzed based on certain parameters such as text statistics, syntactic features, part-of-speech features, closed-class word sets, and structural features <ref type="bibr" coords="2,425.25,307.17,15.27,8.64" target="#b27">[28]</ref>. Whose main idea is to define a criterium to determine if the style has changed enough to indicate plagiarism.</p><p>It is important to note that using intrinsic plagiarism detection for both, automated and manual, it is not demonstrated that a paragraph or a part of the document is being copied, because there is no reference to compare to. Therefore this kind of plagiarism detection category is only indicative and should be used in conjunction with human supervision. Nevertheless, intrinsic plagiarism is useful when trying to discover originality and authorship of a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">External Plagiarism</head><p>Before the comparison between each possible source and the suspicious document can be executed, an important obstacle is to be resolved. This task consist in defining and gathering the possible sources, and this is becoming more and more complex as the technology becomes more available. In <ref type="bibr" coords="2,423.68,451.72,10.58,8.64" target="#b4">[5]</ref>, the suspicious document is chopped into queries and web search engines are used to obtain a set of candidates sources. This approach helps tackle this problem but more work is needed.</p><p>Another issue to be considered, is when the collection of possible sources become too large. The size of the set of possible sources can be thousands of documents. In <ref type="bibr" coords="2,338.04,499.54,15.27,8.64" target="#b21">[22]</ref>, PAN Competition and Workshop, the external part of the competition, and now merged with the training corpus, considers a set of sources of 14,428 documents or possible sources. In this case solutions do exist, as reducing the search space using different data mining techniques.</p><p>In <ref type="bibr" coords="2,96.05,547.36,15.27,8.64" target="#b16">[17]</ref>, the use of n-grams for plagiarism detection is explored. The use of n-grams gives some flexibility to the detection, as reworded fragments could still be detected. In particular, in <ref type="bibr" coords="2,388.16,559.32,11.62,8.64" target="#b2">[3]</ref> the tri-gram structure is found to be the most effective in this task. This method is possible because the common n-grams between two documents are usually a low percentage of the total number of n-grams of both text. Due to this, n-grams could probe promissory for plagiarism detection techniques. Furthremore, in <ref type="bibr" coords="2,389.49,595.18,15.27,8.64" target="#b15">[16]</ref>, Lyon et al. extended their work and the Ferret system was implemented, which uses this approximation to detect plagiarism. A distance is calculated between the documents, based on the n-grams found in common. The results indicate that this structure is useful and provides flexibility at detecting plagiarism with modifications of words.</p><p>Other approaches focus on solving the plagiarism detection problem as a traditional classification problem from the machine learning community <ref type="bibr" coords="2,248.73,654.96,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,262.79,654.96,7.47,8.64" target="#b8">9,</ref><ref type="bibr" coords="2,273.53,654.96,11.83,8.64" target="#b12">13]</ref>. Bao et al. in <ref type="bibr" coords="2,346.29,654.96,16.60,8.64" target="#b12">[13]</ref> and then in <ref type="bibr" coords="2,415.24,654.96,10.58,8.64" target="#b0">[1]</ref>, proposed to use a Semantic Sequence Kernel (SSK), and then using it into a traditional Support Vector Machines (SVMs) formulation based on the Structural Risk Minimization (SRM) <ref type="bibr" coords="2,330.12,678.87,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="2,344.40,678.87,13.28,8.64" target="#b26">27]</ref> principle from statistical learning theory, where the general objective is finding out the optimal classification hyperplane for the binary classification problem (plagiarized, not plagiarized). Likewise, other approaches solves the same classification problem by using Self Organizing Feature Maps (SOFM) <ref type="bibr" coords="2,304.61,714.73,15.27,8.64" target="#b14">[15]</ref>, with promising results in the classification performance.</p><p>An interesting issue is the multi-lingual and cross-language detection of plagiarism. This topic is currently under research <ref type="bibr" coords="2,156.93,750.60,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="2,170.04,750.60,11.83,8.64" target="#b20">21]</ref>, where promising results for plagiarism and cross-lingual information retrieval have been presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reducing Search Space</head><p>One of the issues to be resolved in external plagiarism analysis and detection is the number of source document candidates. When the task is to detect plagiarism between a small set of suspicious against a small set of source documents, it is simple to search for plagiarism in every pair of documents. The problem is presented when the universe of possible sources is not well defined, or the set of documents is too large. In this case the approach need to be modified, and those changes usually consists in adding a step in the process of plagiarism discovery: the search space reduction.</p><p>The aim of this step is to effectively and efficiently identify which texts are possible sources of plagiarism, if any. Usually multiple statistical tools are used in order to reduce the computational time required for computing a large corpus of documents while trying to maintain accuracy at determining which sources need to be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, the main contribution of our work is described. In the first place, the overall FASTDOCODE algorithm is presented in terms of previously introduced notation. Then, the two steps that defines FAST-DOCODE, that is, the approximated segment finding algorithm and the exhaustive offset and length search, are presented in subsections 3.2 and 3.3 respectively. In this section all algorithms are presented as pseudocode, together with a brief description on how different parameters could be used.</p><p>Let us introduce some concepts. In the following, let V a vector of words that defines the vocabulary to be used. We will refer to a word w, as a basic unit of discrete data, indexed by {1, ..., |V|}. A document d is a sequence of S words (|d| = S) defined by w = (w 1 , ..., w S ), where w s represents the s th word in the message. Finally, a corpus is defined by a collection of D documents denoted by C = (w 1 , ..., w |D| ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FASTDOCODE</head><p>Given a wide set of parameters D source , D suspicious , n, k, m, SORTSTRATEGY, θ 1 , θ 2 , τ min , St, P e, the algorithm tries to find for a corpus C = {D source , D suspicious } all plagiarized documents in the suspicious partition, using as search space the source partition. This algorithm is based on external plagiarism detection, and does not include intrinsic plagiarism nor multi-lingual evaluation. In general terms, the algorithm first reduces the search space by using an approximated search of segments of n-grams, and then within selected pairs of documents, using an exhaustive search algorithm, finds the offset and its length for both exact and obfuscated copy.  In algorithm 3.1, the general evaluation of a corpus is presented. In particular, different procedures are used within the code which helps in the preprocessing of documents. The method PREPROCESSDOCU-MENT, presented in algorithm 3.2, takes as input a given document, and returns a set of n-grams or segments of n-grams given the case. If n = 2, only a set of bi-grams will be computed, and if n = 3 a process of finding segments of n-grams will be performed. Segments of n-grams will be intensively used in the approximated search for reducing the search space, whether the bi-grams will be used in finding all offsets and their lengths.</p><formula xml:id="formula_0" coords="3,105.69,582.10,262.29,68.87">1 foreach d i ∈ D suspicious do 2 (κ i , t i ) ← PREPROCESSDOCUMENT(d i , n = 3, k, m); 3 foreach d j ∈ D source do 4 (κ j , t j ) ← PREPROCESSDOCUMENT(d j , n = 3, k, m); 5 if APPROXIMATECOMPARISON(κ i , κ j , t i , t j ,</formula><formula xml:id="formula_1" coords="4,73.86,191.37,194.35,146.83">Algorithm 3.2: PREPROCESSDOCUMENT Data: d i , n, k, m if n = 3 then 1 REMOVESTOPWORDS(d i ); 2 t i ← GENERATENGRAMS(d i , n) ; 3 k i ← GENERATEKNGRAMS(t i , k); 4 k * i ← SORT(k i , SORTSTRATEGY) ; 5 κ i ← SELECTMLASTNGRAMS(k * i , m) ; 6 return (κ i , t i ); 7 else 8 t i ← GENERATENGRAMS(d i , n) ; 9 return t i ;<label>10</label></formula><p>As presented in algorithm 3.2, new methods are introduced for the processing, such as the GENERATEN-GRAMS function that takes a given document d i and returns a set of n-grams with the structure (w i , w i+1 . . . , w i+n ), ∀i ≥ 1, n ≤ S. Function GENERATEKNGRAMS, generates groups of length k using all n-grams. Then, a SORT algorithm is used within segments, with a specific sorting strategy. In this research, an alphabet sorting strategy and a Term Frequency sorting strategy where used as a variation on the proposed algorithm. Finally, a SELECTMLASTNGRAMS function, as specified in its name definition, selects only the last m n-grams within the segment. This approach can be considered as an analogy to a sampling strategy for each segment, thus contributing to minimize the number of comparisons to be executed and enhancing the runtime of the algorithm. Once documents d i and d j are processed in n-grams and segments of n-grams, t i , t j and κ i , κ j respectively, a set of conditions are evaluated in order to set the relation that document d i has with document d j , that is, if they are somehow related (algorithm 3.3 returns true), or if it is not worthy to keep finding further relationships (algorithm 3.3 returns false). In this sense, this is an approximated finding procedure that considers both n-grams and their k segments to decide if there is enough information to classify as plagiarism or not. Algorithm 3.3 first evaluates an SMATCH(t i , t j , s ≥ 1) algorithm which returns true whether at least one n-gram from t i matches one n-gram from t j . Also a variation of previous matching method is used within the segments of n-grams. Condition SMATCH(κ i , κ j , s ≥ θ 1 ) states that at least θ 1 n-grams must match in between segments κ i and κ j . If this is hold, the next condition SMATCH(t i , t j , s ≥ θ 2 ) is associated to find whether at least θ 2 n-grams matches between t i and t j . In general terms, this procedure helps on reducing the search space, and improving the algorithm in both execution time and hardware requirements. By using these constraints, it is possible now to go into a further algorithm for finding the needed offset and its length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding Segments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Find the Offset and its Length</head><p>Algorithm 3.4 describes roughly how to find the offset and its length within two documents.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, the experimental setup and the evaluation criteria is presented. First, the selected partition of a plagiarism detection corpus from the PAN'09 <ref type="bibr" coords="5,304.48,670.93,16.60,8.64" target="#b21">[22]</ref> is discussed together with some of the parameters selected to evaluate different benchmark plagiarism detection algorithms. Then, the evaluation criteria and performance measures used for the training step of the algorithm are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The PAN'09 plagiarism detection corpus <ref type="bibr" coords="5,270.00,750.60,16.60,8.64" target="#b21">[22]</ref> was used as a seed to train different plagiarism detection algorithms. For the experiment, a small sample of the PAN'09 corpus is chosen. This sample considers only external English plagiarism cases. It is constructed as Figure <ref type="figure" coords="6,282.88,303.54,4.98,8.64" target="#fig_2">1</ref> suggests: maintaining the number of references per suspicious document from the original corpus.</p><p>Four algorithms are used for experimental and testing purposes. Three of the selected algorithms are based on the previous approach presented in section 3 and a variation of the unix diff command used to detect changes between two documents was used as benchmark. Due to the lack of space, only a brief description of each of the selected algorithms is presented. Further information was intentionally discarded by authors.</p><p>The first, named "SimParalell" is an iteration where the pair of documents is compared exhaustively. The parameters used are n the parameter of the gram structure, sw is the size of an n-gram sliding window to be considered. Parameter K represents the minimum number of common n-grams to increase a counter indicator. Finally, parameter C is the number of cores used in a parallelized implementation of the algorithm.</p><p>The second algorithm, "SimTF", is equivalent to algorithm 3.3, but the sorting strategy is based on term frequency. In this case it is expected a faster running time than the latter, at a cost of a possibly loss of recall because of the approximated nature of the approach. Then, "SimVP" is the algorithm 3.3 whose pseudocode is presented in section 3. In this case, as well as "SimTF", it is expected a faster running time than "SimParalell" at a cost of a possibly loss of recall.</p><p>Finally, the "Diff" approach is a basic algorithm based on the unix command diff. This approach is based on the move, delete and add characteristics presented by the command, where each one of these outputs is used to determine the scoring function for plagiarism detection.</p><p>All of these algorithms outputs are considered as an approximation of the plagiarism detection problem, for which further analysis needs to be taken into consideration for a given pair of documents. They do not offer the offset nor the length of the plagiarism passages, however they determine how close a pair of documents are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Criteria</head><p>The resulting confusion matrix of this binary classification task can be described using four possible outcomes: Correctly classified plagiarized documents or True Positives (TP), correctly classified non plagiarized documents or True Negative (TN), wrong classified non plagiarized documents as plagiarized or False Positive (FP), and wrong classified plagiarized documents as non-plagiarized or False Negative (FN).</p><p>The evaluation criteria considered are common information retrieval measures, which are constructed using the before mentioned classification outcomes.</p><p>-Precision, that states the degree in which a pair of documents identified as a plagiarism case have indeed copy between them, and Recall, that states the percentage of plagiarized documents that the classifier manages to classify correctly. Can be interpreted as the classifier's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision = T P T P + F P</head><p>, Recall = T P T P + F N</p><p>Table <ref type="table" coords="7,184.20,99.82,3.74,8.96">1</ref>. Algorithms and their parameters used for the conducted experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussions</head><p>Previous algorithms were evaluated using the evaluation criteria on the selected corpus from the PAN'09 dataset. All results are presented in table <ref type="table" coords="7,263.85,401.23,3.74,8.64" target="#tab_3">2</ref>, where the accuracy, precision, recall, F-measure and the evaluation runtime are listed. The overall evaluation was performed for each plagiarized case, where for a given suspicious document, the confusion matrix was determined and their performance measures were evaluated. Then, after all suspicious documents were evaluated, the mean performance was recorded and listed in table <ref type="table" coords="7,131.31,449.05,3.74,8.64" target="#tab_3">2</ref>. The results for the experiment are listed in Table <ref type="table" coords="7,311.17,690.82,3.74,8.64" target="#tab_3">2</ref>. As the numbers indicate, the best results in term of F-measure are obtained with "SimParalell". This comes to no surprise, as the algorithm exhaustively checks the documents. The cost of such results, however, is the worst running time of the group. Alternatively, the "SimTF" and "SimVP" both get acceptable results but much better running times than "SimParalell". This factor is important as the number of pair of documents to compare becomes increasingly high. The "Diff" variant gets an overall worse result; based on the diff unix command entirely this approach does not take into account different obfuscation levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suspicious document i</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline FastDocode</head><p>Number of sources for each suspicious document In Figure <ref type="figure" coords="8,127.24,364.44,3.74,8.64" target="#fig_4">2</ref>, results for the SimVP0 algorithm, where the expected curve for source-suspicious relationship is presented together with the source-suspicious relationship that was retrieved with the proposed algorithm. These results show that in the overall evaluation of the selected corpus, our proposal was robust in different number of sources for each suspicious evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work we have presented a method for uncovering external plagiarism cases. The strategy proposed is based on word tri-grams and word bi-grams, and consists basically on two phases. The first is aimed at reducing the search space for possible sources, and the second is aimed at exhaustively search a pair of document for plagiarized passages, where the offset and its length are computed.</p><p>While reducing the search space, we proposed a method that uses a statistical approach; removing stopwords and selecting samples based on alphabetic order, which helps to reduce considerably the running time of the algorithm. This proved to be empirically successful but further analysis must be taken into consideration.</p><p>Second, all algorithms parameters used were not selected using an extensive analysis on the algorithms performance; due to the size of the corpus it was difficult to run an optimization or grid search strategy over these parameters. We did, however, approximate them by iterating and trying on our sample, thus obtaining acceptable results.</p><p>As future work, it could be interesting to experiment the proposed approach with char n-grams instead of word n-grams. This could help FASTDOCODE to include an intrinsic evaluation of a given document, or help the algorithm to detect plagiarized passages with high obfuscation levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgment</head><p>Authors would like to thank continuous support of "Instituto Sistemas Complejos de Ingeniería" (ICM: P-05-004-F, CONICYT: FBO16; www.sistemasdeingenieria.cl); FONDEF project (DO8I-1015) entitled, DOCODE: Document Copy Detection (www.docode.cl); and the Web Intelligence Research Group (wi.dii.uchile.cl).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,106.69,541.01,126.74,9.03;3,114.16,554.14,113.55,12.95;3,228.20,554.14,115.80,9.96;3,344.00,558.34,3.97,6.12;3,348.46,554.14,9.11,9.96;3,357.57,558.34,3.97,6.12;3,362.04,554.14,8.79,9.96;3,370.82,558.34,14.81,6.12;3,386.13,554.14,31.56,9.96;3,114.16,567.48,361.38,9.96;3,114.16,579.44,222.47,9.96"><head>Algorithm 3 . 1 :</head><label>31</label><figDesc>FAST-DOCODE Data: D source , D suspicious , n, k, m, SORTSTRATEGY, θ 1 , θ 2 , τ min , St, P e Result: Vector OL with all Offsets and their lengths for the complete corpus of documents Initialize Vector pair ← {} and Of f setLenght ← {};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,105.69,382.13,3.49,6.27;5,144.83,392.02,74.41,9.72;5,219.74,391.43,8.03,9.96;5,227.77,395.63,3.30,6.12;5,231.97,391.43,82.44,9.96;5,105.69,394.09,3.49,6.27;5,144.83,403.38,56.50,9.96;5,201.34,402.57,8.91,6.01;5,212.96,403.38,30.43,9.96;5,243.39,402.57,13.56,6.01;5,257.46,404.05,31.67,8.74;5,289.13,402.57,8.91,6.01;5,300.75,403.38,31.67,9.96;5,332.42,402.57,13.56,6.01;5,346.49,403.38,25.39,9.96;5,371.87,403.98,36.62,9.72;5,102.20,406.04,6.97,6.27;5,160.17,415.34,116.93,9.96;5,277.10,414.65,9.30,6.05;5,286.90,415.34,27.66,9.96;5,314.56,414.65,9.30,6.05;5,326.57,415.34,30.43,9.96;5,357.00,414.65,13.17,6.05;5,370.67,416.00,11.90,8.96;5,102.20,418.00,6.97,6.27;5,160.17,427.29,119.41,9.96;5,279.58,426.61,9.30,6.05;5,289.38,427.29,28.90,9.96;5,318.28,426.61,9.30,6.05;5,330.29,427.29,31.67,9.96;5,361.96,426.61,13.17,6.05;5,375.64,427.96,11.90,8.96;5,102.20,429.95,6.97,6.27;5,144.83,440.84,15.50,8.96;5,102.20,442.90,6.97,6.27;5,144.83,452.20,298.09,9.96;5,442.92,452.20,38.39,10.32;5,481.31,456.40,3.30,6.12;5,145.08,465.14,126.53,9.33;5,272.12,464.15,24.90,9.96;5,297.01,463.47,9.30,6.05;5,306.81,464.15,24.90,9.96;5,331.70,463.47,13.17,6.05;5,345.38,464.15,9.13,9.96;5,102.20,466.81,6.97,6.27;5,145.08,477.10,127.01,9.33;5,273.00,476.11,26.14,9.96;5,299.13,475.43,9.30,6.05;5,308.93,476.11,26.14,9.96;5,335.07,475.43,13.17,6.05;5,348.74,476.11,9.13,9.96;5,102.20,478.77,6.97,6.27;5,145.08,490.35,45.98,8.03;5,191.57,488.07,6.64,9.96;5,102.20,490.73,6.97,6.27;5,145.08,502.30,46.46,8.03;5,192.45,500.02,6.64,9.96;5,102.20,502.68,6.97,6.27;5,129.50,513.57,15.50,8.96;5,102.20,515.63,6.97,6.27;5,114.16,526.52,15.50,8.96;5,102.20,528.58,6.97,6.27;5,114.16,537.88,96.00,9.96;5,102.20,540.54,6.97,6.27;5,114.16,560.39,318.68,9.96;5,432.84,560.39,29.57,10.32;5,462.41,561.38,62.64,9.33;5,99.21,573.33,425.83,8.64;5,99.21,585.29,425.83,8.64;5,99.21,597.24,22.42,8.64"><head>8 until 13 / 15 UPDATE 19 return</head><label>8131519</label><figDesc>!XMATCH(t i , t j , s = 1, →, St, P e) ;9 if max{|bp(i) leftbp(i) right |, |bp(j) leftbp(j) right |} &gt; τ min then 10 Of f setLength(i).add(bp(i) left , |bp(i) leftbp(i) right |) ; 11Of f setLength(j).add(bp(j) left , |bp(j) leftbp(j) right |) ; 12 end / Remove all words inside break points for both d i and d j REMOVEINCLUDEDWORDS(t i , bp(i) left , bp(i) right ) ; 14 REMOVEINCLUDEDWORDS(t j , bp(j) left , bp(j) right ) ; Of f setlenght ; 20 Previous algorithm 3.4, finds obfuscated and textual copy within documents d i and d j , then a match strategy is moved both left and right side of the document, adding to the offset array the matching segments. Finally, to avoid the search over detected plagiarism passages, the break points are saved and used to remove them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,70.87,243.70,425.83,9.03;6,70.87,256.04,168.42,8.64;6,283.92,104.93,198.30,123.73"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Dataset distribution in terms of the construction of the number of source documents that each suspicious document was originated from.</figDesc><graphic coords="6,283.92,104.93,198.30,123.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,226.38,121.70,49.26,8.96;7,371.09,121.70,49.14,8.96;7,141.82,134.44,158.31,8.64;7,330.35,133.46,130.61,9.96;7,141.82,146.40,164.26,8.64;7,330.35,145.41,130.61,9.96;7,141.82,158.35,164.26,8.64;7,334.23,157.37,122.86,9.96;7,151.15,170.31,32.67,8.64;7,211.01,170.31,79.99,8.64;7,311.36,169.32,71.99,9.96;7,383.35,173.53,3.97,6.12;7,390.58,169.32,24.61,9.96;7,415.19,173.53,3.97,6.12;7,422.42,169.32,57.53,9.96;7,151.15,182.26,32.67,8.64;7,205.06,182.26,91.89,8.64;7,313.85,181.28,71.99,9.96;7,385.84,185.48,3.97,6.12;7,393.08,181.28,24.60,9.96;7,417.68,185.48,3.97,6.12;7,424.91,181.28,52.55,9.96;7,151.15,194.22,32.67,8.64;7,205.06,193.23,178.29,9.96;7,383.35,197.44,3.97,6.12;7,390.58,193.23,24.61,9.96;7,415.19,197.44,3.97,6.12;7,422.42,193.23,57.53,9.96;7,150.60,206.18,33.77,8.64;7,209.91,206.18,82.20,8.64;7,308.87,205.19,71.99,9.96;7,380.86,209.39,3.97,6.12;7,388.09,205.19,29.59,9.96;7,417.68,209.39,3.97,6.12;7,424.91,205.19,57.53,9.96;7,150.60,218.13,33.77,8.64;7,203.96,217.14,179.12,9.96;7,383.07,221.35,3.97,6.12;7,390.31,217.14,25.15,9.96;7,415.46,221.35,3.97,6.12;7,422.70,217.14,57.53,9.96;7,150.60,230.09,33.77,8.64;7,203.96,229.10,176.90,9.96;7,380.86,233.30,3.97,6.12;7,388.09,229.10,29.59,9.96;7,417.68,233.30,3.97,6.12;7,424.91,229.10,57.53,9.96;7,156.82,242.04,21.33,8.64;7,217.93,242.04,66.16,8.64;7,322.63,242.04,146.04,8.64;7,156.82,254.00,21.33,8.64;7,211.98,254.00,78.07,8.64;7,320.15,254.00,151.02,8.64;7,156.82,265.95,21.33,8.64;7,211.98,265.95,78.07,8.64;7,321.39,265.95,148.53,8.64;7,106.19,300.55,418.86,8.64;7,116.15,312.51,126.17,8.64;7,169.10,333.60,52.57,9.96;7,225.63,326.86,86.33,9.96;7,231.44,340.43,74.71,9.96;7,313.16,333.60,53.51,9.96;7,399.04,326.86,42.37,9.96;7,370.64,340.43,99.18,9.96;7,513.43,334.58,11.62,8.64"><head></head><label></label><figDesc>Description Parameters SimParalell0 CD Sim paralell original (n = 3, sw = 5, K = 3, c = 16) SimParalell1 CD Sim paralell modified 1 (n = 2, sw = 6, K = 3, c = 16) SimParalell2 CD Sim paralell modified 2 n = 4, sw = 8, K = 3, c = 16 SimTF0 CD Sim TF original (n = 3, sw = 5, θ 1 = 7, θ 2 = 2, k = 150) SimTF1 CD Sim TF modified 1 (n = 2, sw = 6, θ 1 = 7, θ 2 = 2, k = 50) SimTF2 CD Sim TF modified 2 (n = 4, sw = 8, θ 1 = 7, θ 2 = 2, k = 150) SimVP0 CD Sim AR original (n = 3, sw = 5, θ 1 = 18, θ 2 = 5, k = 250) SimVP1 CD Sim AR modified 1 (n = 2, sw = 6, θ 1 = 18θ 2 = 5, k = 250) SimVP2 CD Sim AR modified 2 (n = 4, sw = 8, θ 1 = 18, θ 2 = 5, k = 250) Diff0 CD Diff original (Add = -1 , Move = 10 , Delete = -1) Diff1 CD Diff modified 1 (Add = -10 , Move = 0 , Delete = -10) Diff2 CD Diff modified 2 (Add = -5 , Move = 0 , Delete = -10 ) -F-measure, the harmonic mean between the precision and recall, and Accuracy, the overall percentage of correct classified documents. F-measure = 2 * Precision * Recall Precision + Recall , Accuracy = T P + T N T P + T N + F P + F N (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,70.87,317.30,425.83,9.03;8,70.87,329.65,123.14,8.64;8,113.70,99.83,340.18,202.43"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Results comparing the baseline sources for suspicious documents (blue line), and those retrieved by FASTDOCODE (green line).</figDesc><graphic coords="8,113.70,99.83,340.18,202.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,102.20,640.65,309.84,120.92"><head></head><label></label><figDesc>θ 1 , θ 2 ) then</figDesc><table coords="3,102.20,655.26,107.75,105.27"><row><cell>7</cell></row><row><cell>pair.add(p);</cell></row><row><cell>14</cell></row></table><note coords="3,105.69,643.31,3.49,6.27;3,160.17,652.60,51.03,9.96;3,211.20,656.80,2.82,6.12;3,214.52,652.60,14.09,10.32;3,229.51,652.60,9.13,9.96;3,105.69,667.22,3.49,6.27;3,114.16,688.00,82.83,9.96;3,105.69,690.66,3.49,6.27;3,129.50,700.59,148.69,9.68;3,278.68,699.96,131.60,9.96;3,102.20,702.62,6.97,6.27;3,129.50,712.55,150.05,9.68;3,280.44,711.91,131.60,9.96;3,102.20,714.57,6.97,6.27;3,129.50,723.87,138.50,9.96;3,268.00,728.07,2.82,6.12;3,271.32,723.87,12.18,10.32;3,284.40,723.87,8.79,9.96;3,293.18,728.07,14.81,6.12;3,308.49,723.87,40.96,9.96;3,102.20,726.53,6.97,6.27;3,129.50,735.82,138.50,9.96;3,268.00,740.02,2.82,6.12;3,271.32,735.82,12.18,10.32;3,284.40,735.82,8.79,9.96;3,293.18,740.02,14.81,6.12;3,308.49,735.82,40.96,9.96;3,102.20,738.48,6.97,6.27;3,114.16,751.61,49.74,9.96"><p>6 p(i, j) ← (d i , d j ) ; 8 foreach p ∈ pair do 9 t i ← PREPROCESSDOCUMENT(p.d i , n = 2, k, m, SORTSTRATEGY); 10 t j ← PREPROCESSDOCUMENT(p.d j , n = 2, k, m, SORTSTRATEGY); 11 OL.add(FINDOFFSETLENGHT1(t i , t j , τ min , St, P e)); 12 OL.add(FINDOFFSETLENGHT2(t i , t j , τ min , St, P e)); 13 return OL ;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,105.69,222.88,399.79,166.91"><head></head><label></label><figDesc>Algorithm 3.4: FINDOFFSETLENGTH Data: d i , d j , t i , t j , τ min , St, P e // Find obfuscated and textual copy within documents d i and d j foreach w i ∈ d i do right .add(t i ) and bp(j) right .add(t j )</figDesc><table /><note coords="5,105.69,262.58,3.49,6.27;5,129.50,271.88,41.28,9.96;5,170.78,272.47,39.95,9.72;5,105.69,274.54,3.49,6.27;5,144.83,283.83,86.87,9.96;5,231.70,283.15,9.30,6.05;5,244.27,283.83,49.38,9.96;5,293.65,283.15,13.17,6.05;5,310.09,283.83,49.38,9.96;5,359.47,283.15,9.30,6.05;5,372.03,283.83,48.14,9.96;5,420.18,283.15,13.17,6.05;5,436.61,284.50,25.46,8.74;5,105.69,286.49,3.49,6.27;5,144.83,296.45,360.65,8.74;5,162.77,307.74,339.51,9.96;5,162.77,321.62,89.66,7.01;5,144.83,332.25,26.93,8.96;5,105.69,334.31,3.49,6.27;5,160.17,343.61,20.47,9.96;5,105.69,346.27,3.49,6.27;5,144.83,356.16,74.41,9.72;5,219.74,355.56,8.03,9.96;5,227.77,359.76,3.30,6.12;5,231.97,355.56,82.44,9.96;5,105.69,358.22,3.49,6.27;5,144.83,368.11,26.93,8.96;5,105.69,370.18,3.49,6.27;5,160.17,379.47,20.47,9.96;5,180.64,378.79,9.30,6.05;5,190.44,379.47,25.88,9.96;5,216.31,383.67,2.82,6.12;5,219.63,379.47,44.95,9.96;5,264.58,378.79,9.30,6.05;5,274.38,379.47,25.88,9.96;5,300.25,383.67,3.30,6.12;5,304.45,379.47,3.87,9.96"><p>1 foreach w j ∈ d j do 2 Initialize Vector bp(i) left ← {}, bp(j) right ← {}, bp(j) left ← {}, bp(i) right ← {}; 3 // Move the XMA T C H in both ← and → sides of the document in steps St and checking that P e percentage of similar words within the step repeat 4 bp(i) 5 until !XMATCH(t i , t j , s = 1, ←, St, P e) ; 6 repeat 7 bp(i) left .add(t i ) and bp(j) left .add(t j )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,99.21,477.29,425.84,184.67"><head>Table 2 .</head><label>2</label><figDesc>Results for Accuracy, Precision, Recall, F-measure and runtime for each algorithm presented in section 4</figDesc><table coords="7,174.63,509.07,274.99,152.89"><row><cell cols="4">Copy Detector Accuracy Precision Recall F-measure runtime (s)</cell></row><row><cell>SimParalell0 0.999</cell><cell>0.895 0.914</cell><cell>0.904</cell><cell>20,568</cell></row><row><cell>SimParalell1 0.990</cell><cell>0.616 0.958</cell><cell>0.750</cell><cell>21103</cell></row><row><cell>SimParalell2 0.961</cell><cell>0.882 0.916</cell><cell>0.899</cell><cell>29,655</cell></row><row><cell>SimTF0 0.874</cell><cell>0.824 0.821</cell><cell>0.823</cell><cell>6,959</cell></row><row><cell>SimTF1 0.923</cell><cell>0.766 0.800</cell><cell>0.783</cell><cell>7,451</cell></row><row><cell>SimTF2 0.874</cell><cell>0.836 0.818</cell><cell>0.827</cell><cell>6,615</cell></row><row><cell>SimVP0 0.887</cell><cell>0.865 0.857</cell><cell>0.861</cell><cell>5,393</cell></row><row><cell>SimVP1 0.899</cell><cell>0.859 0.852</cell><cell>0.855</cell><cell>5,596</cell></row><row><cell>SimVP2 0.849</cell><cell>0.828 0.868</cell><cell>0.847</cell><cell>5,231</cell></row><row><cell>Diff0 0.584</cell><cell>0.005 0.349</cell><cell>0.010</cell><cell>6,617</cell></row><row><cell>Diff1 0.007</cell><cell>0.007 1.000</cell><cell>0.014</cell><cell>6,529</cell></row><row><cell>Diff2 0.584</cell><cell>0.005 0.349</cell><cell>0.010</cell><cell>6,179</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,119.14,154.76,405.91,7.77;9,119.13,165.56,405.92,7.93;9,119.13,176.52,375.38,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,403.41,154.76,121.64,7.77;9,119.13,165.72,102.17,7.77">Semantic sequence kin: A method of document copy detection</title>
		<author>
			<persName coords=""><forename type="first">Jun-Peng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun-Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao-Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hai-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao-Di</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,177.98,176.52,125.21,7.73">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Honghua</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3056</biblScope>
			<biblScope unit="page" from="529" to="538" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin / Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,187.22,405.91,7.93;9,119.13,198.18,405.92,7.73;9,119.13,209.13,217.70,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,214.10,187.38,267.37,7.77">On the mono-and cross-language detection of text reuse and plagiarism</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,502.64,187.22,22.41,7.73;9,119.13,198.18,405.92,7.73;9,119.13,209.13,29.39,7.73">SIGIR &apos;10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="914" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,219.99,405.91,7.77;9,119.13,230.79,405.91,7.93;9,119.13,241.75,364.71,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,407.33,219.99,117.72,7.77;9,119.13,230.95,46.20,7.77">Word length n-grams for text reuse detection</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chiara</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirko</forename><surname>Degli Esposti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,183.49,230.79,341.56,7.73;9,119.13,241.75,109.27,7.73">CICLing &apos;10: Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<meeting><address><addrLine>Berlin, Germany; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="687" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,252.60,405.91,7.77;9,119.13,263.40,405.92,7.93;9,119.13,274.52,124.28,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,355.20,252.60,169.84,7.77;9,119.13,263.56,14.73,7.77">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,149.88,263.40,312.98,7.73">COLT &apos;92: Proceedings of the fifth annual workshop on Computational learning theory</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,285.22,405.91,7.77;9,119.13,296.02,405.91,7.93;9,119.13,306.98,405.91,7.93;9,119.13,318.10,195.66,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,494.17,285.22,30.87,7.77;9,119.13,296.18,217.87,7.77">Docodelite: A meta-search engine for document similarity retrieval</title>
		<author>
			<persName coords=""><forename type="first">Felipe</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L'</forename><surname>Gastón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastián</forename><forename type="middle">A</forename><surname>Huillier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Ríos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><forename type="middle">A</forename><surname>Velásquez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,442.87,296.02,82.18,7.73;9,119.13,306.98,345.26,7.73">KES&apos;2010: 14th International Conference on Knowledge-Based and Intelligent Information and Engineering Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Setchi</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,328.79,405.91,7.77;9,119.13,339.59,405.91,7.93;9,119.13,350.71,160.15,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,324.47,328.79,182.79,7.77">Copy detection mechanisms for digital documents</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Héctor</forename><surname>García-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,119.13,339.59,379.27,7.73">SIGMOD &apos;95: Proceedings of the 1995 ACM SIGMOD international conference on Management of data</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="398" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,361.24,405.91,7.93;9,119.13,372.20,328.96,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,175.37,361.40,140.71,7.77">The future of copy detection techniques</title>
		<author>
			<persName coords=""><forename type="first">Zdenek</forename><surname>Ceska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,332.44,361.24,192.61,7.73;9,119.13,372.20,125.39,7.73">Proceedings of the 1st Young Researchers Conference on Applied Sciences (YRCAS 2007)</title>
		<meeting>the 1st Young Researchers Conference on Applied Sciences (YRCAS 2007)<address><addrLine>Pilsen, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
			<biblScope unit="page" from="5" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,382.90,405.91,7.93;9,119.13,393.86,405.92,7.93;9,119.13,404.98,126.85,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,177.26,383.06,215.57,7.77">Plagiarism detection based on singular value decomposition</title>
		<author>
			<persName coords=""><forename type="first">Zdenek</forename><surname>Ceska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.30,382.90,113.75,7.73;9,119.13,393.86,270.65,7.73">GoTAL &apos;08: Proceedings of the 6th international conference on Advances in Natural Language Processing</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="108" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,415.67,405.91,7.77;9,119.13,426.47,282.91,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,291.52,415.67,233.52,7.77;9,119.13,426.63,118.56,7.77">Multilayer som with tree-structured data for efficient document retrieval and plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Tommy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,244.43,426.47,64.75,7.73">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1385" to="1402" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,437.33,405.91,7.77;9,119.13,448.13,405.92,7.93;9,119.13,459.08,405.92,7.93;9,119.13,470.20,119.63,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,256.91,437.33,268.13,7.77;9,119.13,448.29,31.60,7.77">Encoplot: Pairwise sequence matching in linear time applied to plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,498.64,448.13,26.41,7.73;9,119.13,459.08,350.18,7.73">SEPLN 2009 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="9,119.14,480.74,340.37,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,171.18,480.90,123.86,7.77">Let&apos;s hear it for internet plagiarism</title>
		<author>
			<persName coords=""><forename type="first">Russell</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,301.91,480.74,96.07,7.73">Teaching Learning Bridges</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="5" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,491.44,405.92,7.93;9,119.13,502.39,312.00,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,274.26,491.60,161.88,7.77">Student plagiarism and cheating in an it age</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">O</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M V</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,456.45,491.44,68.60,7.73;9,119.13,502.39,227.56,7.73">Proceedings of the International Conference on Computer Systems and Technology</title>
		<meeting>the International Conference on Computer Systems and Technology</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,513.25,405.91,7.77;9,119.13,524.05,405.91,7.93;9,119.13,535.01,172.12,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,409.56,513.25,115.49,7.77;9,119.13,524.21,62.19,7.77">Document copy detection based on kernel method</title>
		<author>
			<persName coords=""><forename type="first">Bao</forename><surname>Jun-Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shen</forename><surname>Jun-Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Xiao-Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Hai-Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhang</forename><surname>Xiao-Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.47,524.05,323.58,7.73;9,119.13,535.01,83.69,7.73">Proceedings of the 2003 International Conference on Natural Language Processing and Knowledge Engineering</title>
		<meeting>the 2003 International Conference on Natural Language Processing and Knowledge Engineering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,545.86,405.91,7.77;9,119.13,556.66,405.92,7.93;9,119.13,567.62,316.53,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,321.92,545.86,203.12,7.77;9,119.13,556.82,31.60,7.77">Ppchecker: Plagiarism pattern checker in document copy detection</title>
		<author>
			<persName coords=""><forename type="first">Namoh</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sangyong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,362.05,556.66,96.95,7.73">Text, Speech and Dialogue</title>
		<title level="s" coord="9,119.13,567.62,125.21,7.73">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ivan</forename><surname>Kopecek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karel</forename><surname>Pala</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4188</biblScope>
			<biblScope unit="page" from="661" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,578.32,405.92,7.93;9,119.13,589.44,94.88,7.77" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,323.12,578.32,77.09,7.73">Self-Organizing Maps</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,600.13,405.91,7.77;9,119.13,610.93,405.91,7.93;9,119.13,621.89,291.64,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,311.60,600.13,213.45,7.77;9,119.13,611.09,334.94,7.77">A theoretical basis to the automated detection of copying between texts, and its practical implementation in the ferret plagiarism and collusion detector</title>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruth</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,471.22,610.93,53.82,7.73;9,119.13,621.89,206.26,7.73">Proceedings of Plagiarism: Prevention, Practice and Policies Conference</title>
		<meeting>Plagiarism: Prevention, Practice and Policies Conference<address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,632.75,405.91,7.77;9,119.13,643.54,405.91,7.93;9,119.13,654.66,72.25,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,313.00,632.75,207.87,7.77">Detecting short passages of similar text in large document</title>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Dickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,129.12,643.54,333.05,7.73">Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2001 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,665.20,405.91,7.93;9,119.13,676.32,242.51,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,249.55,665.36,74.80,7.77">Plagiarism -a survey</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kappe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zaka</surname></persName>
		</author>
		<ptr target="//www.jucs.org/jucs_12_8/plagiarism_a_survey|" />
	</analytic>
	<monogr>
		<title level="j" coord="9,332.33,665.20,142.09,7.73">Journal of Universal Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1050" to="1084" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,687.02,405.91,7.77;9,119.13,697.81,405.92,7.93;9,119.13,708.77,405.91,7.93;9,119.13,719.89,92.73,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,189.12,687.02,296.69,7.77">counter plagiarism detection software&quot; and &quot;counter counter plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">Yurii</forename><surname>Palkovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,477.33,697.81,47.72,7.73;9,119.13,708.77,321.80,7.73">SEPLN 2009 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="9,119.14,730.43,405.91,7.93;9,119.13,741.39,306.50,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,150.38,730.59,300.14,7.77">In other (people&apos;s) words: plagiarism by university students -literature and lessons</title>
		<author>
			<persName coords=""><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,468.22,730.43,56.83,7.73;9,119.13,741.39,113.65,7.73">Assessment and Evaluation in Higher Education</title>
		<imprint>
			<publisher>Carfax Publishing</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="471" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,119.14,752.24,405.91,7.77;9,119.13,763.04,201.46,7.93" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,390.37,752.24,131.16,7.77">Cross-language plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,119.13,763.04,130.19,7.73">Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,103.95,405.91,7.77;10,90.79,114.91,405.92,7.77;10,90.79,125.71,405.92,7.93;10,90.79,136.67,259.45,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,423.28,103.95,73.42,7.77;10,90.79,114.91,178.39,7.77">Overview of the 1st international competition on plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.07,125.71,274.63,7.73;10,90.79,136.67,93.44,7.73">SEPLN 2009 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="10,90.79,147.79,405.91,7.77;10,90.79,158.59,405.91,7.93;10,90.79,169.71,151.18,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,288.22,147.79,205.04,7.77">Winnowing: local algorithms for document fingerprinting</title>
		<author>
			<persName coords=""><forename type="first">Saul</forename><surname>Schleimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,100.18,158.59,370.55,7.73">SIGMOD &apos;03: Proceedings of the 2003 ACM SIGMOD international conference on Management of data</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,180.67,405.91,7.77;10,90.79,191.46,405.91,7.93;10,90.79,202.58,83.41,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,278.89,180.67,213.50,7.77">Building a scalable and accurate copy detection mechanism</title>
		<author>
			<persName coords=""><forename type="first">Narayanan</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,100.35,191.46,294.24,7.73">DL &apos;96: Proceedings of the first ACM international conference on Digital libraries</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="160" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,213.38,405.91,7.93;10,90.79,224.34,405.92,7.93;10,90.79,235.46,22.31,7.77" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,289.40,213.54,172.59,7.77">Check: a document plagiarism detection system</title>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Va Leong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,481.03,213.38,15.67,7.73;10,90.79,224.34,248.83,7.73">SAC &apos;97: Proceedings of the 1997 ACM symposium on Applied computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,246.42,405.91,7.77;10,90.79,257.22,273.87,7.93" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,260.99,246.42,235.71,7.77;10,90.79,257.38,38.50,7.77">College cheating: A twenty-year follow-up and the addition of an honor code</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Labeff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">E</forename><surname>Vandehey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Diekhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,136.34,257.22,143.25,7.73">Journal of College Student Development</title>
		<imprint>
			<biblScope unit="page" from="468" to="480" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,268.18,405.91,7.93;10,90.79,279.30,93.73,7.77" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="10,170.75,268.18,286.55,7.73">The Nature of Statistical Learning Theory (Information Science and Statistics)</title>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.79,290.26,405.91,7.77;10,90.79,301.05,405.91,7.93;10,90.79,312.17,236.46,7.77" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,299.72,290.26,180.97,7.77">Plagiarism detection without reference collections</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Meyer Zu Eissen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marion</forename><surname>Kulig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,273.19,301.05,223.51,7.93;10,90.79,312.17,45.09,7.77">GfKl, Studies in Classification, Data Analysis, and Knowledge Organization</title>
		<editor>
			<persName><forename type="first">Reinhold</forename><surname>Decker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hans-Joachim</forename><surname>Lenz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
