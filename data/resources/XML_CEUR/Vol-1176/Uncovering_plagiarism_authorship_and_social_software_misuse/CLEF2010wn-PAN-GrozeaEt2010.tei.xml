<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.31,115.90,318.73,12.90;1,211.25,133.83,192.86,12.90;1,218.28,154.13,178.79,10.75">Encoplot -Performance in the Second International Plagiarism Detection Challenge Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.87,190.98,62.80,8.64"><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute FIRST</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.09,190.98,63.92,8.64"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.31,115.90,318.73,12.90;1,211.25,133.83,192.86,12.90;1,218.28,154.13,178.79,10.75">Encoplot -Performance in the Second International Plagiarism Detection Challenge Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB636FE162E413809C8593ED2DB35AAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our submission this year is generated by the same method Encoplot that we have developed for the last year competition. There is a single improvement, we compare in addition each suspicious document with each other and flag the passages most probably in correspondence as intrinsic plagiarism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our method Encoplot <ref type="bibr" coords="1,221.06,361.93,12.98,8.64" target="#b2">[3]</ref> has won the last year competition in the external plagiarism detection task and in the overall ranking. Since PAN'09 <ref type="bibr" coords="1,357.78,373.89,14.64,8.64">[5]</ref> we have tested it on some other tasks and it proved good even for the detection of the direction of the plagiarism: on the PAN'09 corpus it was able to indicate the source for each plagiarism instance, with 75% accuracy <ref type="bibr" coords="1,208.63,409.75,12.34,8.64" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">External Plagiarism Detection</head><p>The method consists in two stages. In a first stage the values from a string kernel matrix are computed, that give a rough approximation of the similarity between each two documents (a source and a suspicious one). The string kernel used is the normalization of the kernel that counts for each two strings how many character-based N -grams types of a fixed length N they share.</p><p>In the second stage the most promising pairs are examined in detail by creating the encoplot data for them and the passages in correspondence are extracted based on some simple heuristics. The encoplot data is a subset of the dotplot. It consists of a subset of the set of indexes on which the two documents have the same N -gram, thus: the first occurrence of an N -gram in one document is paired with the first occurrence of the same N -gram in the other document, the second occurrence with the second one and so on. Computationally, it is worth mentioning that computing the encoplot data is done in linear time, with the algorithm we have published in <ref type="bibr" coords="1,363.09,613.98,10.58,8.64" target="#b2">[3]</ref>. On the other hand, since our method is based on pairwise comparisons of documents, its runtime is of quadratic order in the number of documents. corresponding author -cristian.grozea@first.fraunhofer.de A critical choice is how to rank the similarity measures, how to prune the huge list of document pairs without loosing too much from performance. We had noticed in the previous competition that ranking all possible target documents for each fixed source leads to higher recall than ranking for each suspicious document (target) the sources. This can be seen in Figure <ref type="figure" coords="2,240.29,167.13,3.71,8.64" target="#fig_0">1</ref>(a), ranking the targets offers a consistent about 10% higher recall, therefore this is what we have used. Based on those graphs, we have decided for looking this time at the first 400 most likely targets for each source, up from the 50 we considered in the previous competition. We have kept the same method without any extra tuning -all hyperparameters but this one were kept to the values given in the Encoplot paper <ref type="bibr" coords="2,389.70,608.62,10.58,8.64" target="#b2">[3]</ref>. As a result of the increased size of the dataset -12000 * 16000 document pairs instead of 7000 * 7000 -computing the kernel matrix took this time 40 hours on a similar 8-core machine. Leveraging the speed of Encoplot, we have been able to extend the detailed analysis to the first (closest) 400 suspicious documents to each source document (4.8 million document pairs, about 2.5% of the total number before ranking and pruning). This took about a week to process on the available machine.</p><p>It turns out that for this dataset, for low number of neighbors, it is much better to rank the possible sources for each target than to rank the possible targets for each source, as in Figure <ref type="figure" coords="3,215.91,167.43,3.82,8.64" target="#fig_0">1</ref>(b). The difference is as extreme as between 38% and 64% when the first 20 neighbors are used. For the number of neighbors we have chosen, 400, our choice looks on the first glance close to ideal -more about that in the Evaluation and Discussion section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Internal Plagiarism Detection</head><p>Our position is that given the weaknesses of the intrinsic plagiarism detection and the higher probability that the source is anyway available to the investigator, it is better to attempt external plagiarism detection with an enlarged set of sources instead of risking with the low-performing intrinsic plagiarism detection methods. It was unclear whether this would be allowed by the rules -as a side note, in other data competitions such as VOC <ref type="bibr" coords="3,150.89,321.82,16.12,8.64" target="#b0">[1]</ref> there are separate tracks for learning only from the data explicitly provided and for the methods/submissions using supplementary data. Therefore we didn't test the suspicious documents against the Gutenberg archive -the most probable source. Instead we have performed a natural test that is typical done in a way or another by any professor suspecting the homeworks received of potential plagiarism <ref type="bibr" coords="3,410.40,369.65,10.79,8.64" target="#b1">[2]</ref>: we compared the suspicious documents (restricted on purpose only to the ones for which we had not found any external plagiarism) to each other. This comparison was done again using the method Encoplot with standard hyper-parameters. We have filtered the resulting passages in order to retain only the ones with very high probability of being copies to their found corresponding passage. The criteria used were: the length of each of the two passages in correspondence being over 1500 and having the matching score (roughly the percentage of N-grams that are matched ) at least 0.95.</p><p>Those passages have been reported as intrinsic plagiarism, as they have been identified as plagiarism by this document-set method but the source is not known. It may well be that in fact the two passages are copied from a third source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Discussion</head><p>Since our method was already proven in the last year competition (on which PAN-PC-09 is based), we did not evaluate it again on this dataset.</p><p>Our results in the competition are given in the Table <ref type="table" coords="3,359.36,572.45,3.74,8.64" target="#tab_0">1</ref>.</p><p>As far as the current competition is concerned, we have the confirmation that our decision to add cross-suspicious-documents detected passages led to an increase in recall that eventually gave a similar increase in the overall score. The precision is also slightly higher, as a result of the strong filtering of the cross-suspicious-documents detections. Hadn't we filter those, we would have obtained an higher recall (2.5% higher) at the expense of a lower precision (2% lower). Overall this would have improved further (but only slightly -1%) the overall score. The most interesting question is why is our score lower this year? Could it be that we didn't detect the non-artificial plagiarism as good as the artificial one? The score went down mostly as a result of the decrease in recall. We suggest as a possible explanation that the Figures <ref type="figure" coords="4,201.90,251.09,12.03,8.64" target="#fig_0">1(a</ref>) and 1(b) are over-optimistic for high number of neighbors. The actual recall we obtained in these two competitions corresponds to the ideal values for 10 to 20 neighbors. A likely explanation is that, as the kernel and the encoplot agree on their conclusions, encoplot will fail to notice common passages in the documents where the kernel failed to notice similarity. If this is true, then, according to Figure <ref type="figure" coords="4,134.76,310.87,4.15,8.64" target="#fig_0">1</ref>(b) we have used the lower performance ranking and better should have been to rank the sources for each target, that would have brought us a substantial performance boost without any change in the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Although three groups have succeeded to outperform our method in the challenge, we have argued here that our results could have been improved through pruning by ranking the source documents for each suspicious one, instead of ranking the suspicious documents for each source document. Given the consistent performance and the versatility of Encoplot, we plan to apply it to new fields in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.76,548.40,345.83,8.12;2,134.76,559.71,66.64,7.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Pruning: Is it better to rank the possible sources for each suspicious document or the other way around?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,150.55,115.83,312.02,71.49"><head>Table 1 .</head><label>1</label><figDesc>Results on various datasets</figDesc><table coords="4,150.55,134.91,312.02,52.41"><row><cell>Dataset</cell><cell cols="2">Overall Score Recall Precision Granularity</cell></row><row><cell>PAN'09</cell><cell>0.6957 0.6585 0.7418</cell><cell>1.0038</cell></row><row><cell>PAN'10</cell><cell>0.6154 0.4742 0.9073</cell><cell>1.0168</cell></row><row><cell>PAN'10 external detections only</cell><cell>0.6048 0.4602 0.9016</cell><cell>1.0106</cell></row><row><cell>PAN'10 with unfiltered internal detections</cell><cell>0.6252 0.5042 0.8852</cell><cell>1.0387</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.13,493.83,303.56,7.77" xml:id="b0">
	<monogr>
		<ptr target="http://pascallin.ecs.soton.ac.uk/challenges/VOC/" />
		<title level="m" coord="4,146.47,493.83,115.13,7.77">Visual Object Classes Challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.13,504.79,342.46,7.77;4,146.47,515.75,334.12,7.77;4,146.47,526.71,334.12,7.77;4,146.47,537.67,258.14,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,201.79,504.79,274.79,7.77">Plagiarism detection with state of the art compression programs</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<ptr target="http://www.cs.auckland.ac.nz/CDMTCS/researchreports/247Grozea.pdf" />
		<imprint>
			<date type="published" when="2004-08">Aug 2004</date>
			<pubPlace>Auckland, New Zealand</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Centre for Discrete Mathematics and Theoretical Computer Science, University of Auckland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report CDMTCS-247</note>
</biblStruct>

<biblStruct coords="4,138.13,548.62,342.46,7.77;4,146.47,559.58,334.12,7.77;4,146.47,570.54,219.60,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,271.83,548.62,208.76,7.77;4,146.47,559.58,113.22,7.77">ENCOPLOT: Pairwise Sequence Matching in Linear Time Applied to Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,363.23,559.58,112.21,7.77">UNCOVERING PLAGIARISM</title>
		<editor>
			<persName><surname>Workshop</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.13,581.50,342.46,7.77;4,146.47,592.46,119.55,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,239.07,581.50,237.87,7.77">Who&apos;s the thief? automatic detection of the direction of plagiarism</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,158.67,592.46,34.13,7.77">CICLing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="700" to="710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
