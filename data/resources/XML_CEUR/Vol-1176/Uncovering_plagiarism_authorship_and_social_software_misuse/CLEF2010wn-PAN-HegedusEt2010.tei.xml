<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.20,65.96,330.81,12.91;1,225.00,83.96,159.30,12.91;1,218.28,103.71,178.90,10.76">Novel Balanced Feature Representation for Wikipedia Vandalism Detection Task Lab Report for PAN at CLEF 2010</title>
				<funder>
					<orgName type="full">Bolyai Scholarship</orgName>
				</funder>
				<funder ref="#_HFxYAkX">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_rGm4gND">
					<orgName type="full">NKTH</orgName>
				</funder>
				<funder ref="#_w4ZSBYm">
					<orgName type="full">Hungarian Academy of Sciences</orgName>
				</funder>
				<funder>
					<orgName type="full">Hungarian government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.36,139.76,61.84,8.97"><forename type="first">István</forename><surname>Hegedűs</surname></persName>
							<email>ihegedus@inf.u-szeged.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.40,139.76,65.17,8.97"><forename type="first">Róbert</forename><surname>Ormándi</surname></persName>
							<email>ormandi@inf.u-szeged.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.80,139.76,60.64,8.97"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
							<email>rfarkas@inf.u-szeged.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.56,139.76,53.94,8.97"><forename type="first">Márk</forename><surname>Jelasity</surname></persName>
							<email>jelasity@inf.u-szeged.hu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Szeged</orgName>
								<orgName type="institution" key="instit2">Hungarian Academy of Sciences</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.20,65.96,330.81,12.91;1,225.00,83.96,159.30,12.91;1,218.28,103.71,178.90,10.76">Novel Balanced Feature Representation for Wikipedia Vandalism Detection Task Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A2A3ADDE59FEAA37D813E1B20F07FF1A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In online communities, like Wikipedia, where content edition is available for every visitor users who deliberately make incorrect, vandal comments are sure to turn up. In this paper we propose a strong feature set and a method that can handle this problem and automatically decide whether an edit is a vandal contribution or not. We present a new feature set that is a balanced and extended version of the well known Vector Space Model (VSM) and show that this representation outperforms the original VSM and its attribute selected version as well. Moreover, we describe other features that we used in our vandalism detection system and a parameter estimation method for a weighted voting metaclassifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, Wikipedia is one of the most relevant sources of encyclopedic knowledge. Although it usually provides high quality, relevant information, users more and more often obtain articles which contain false data or even spammed content. Detecting of this type of content manually is a time-consuming and maybe impossible task due to e.g. the size of Wikipedia. For this reasons, it is crucial to support the task of "keeping Wikipedia clear" by automatic or semi-automatic methods. For automatic detection we can use approaches from the field of Natural Language Processing (NLP) since this problem -in an abstract form -can be viewed as a text categorization task where each Wikipedia edit has to be classified as a regular edit, or as a vandalized edit.</p><p>The problem of text categorization <ref type="bibr" coords="1,295.56,472.16,11.60,8.97" target="#b8">[9]</ref> is one of the most important problems of NLP. We can find proposals for solving the classification of query request results into relevant or irrelevant categories from the early 60's, where a Naïve Bayes based training method was used <ref type="bibr" coords="1,220.20,508.04,10.60,8.97" target="#b5">[6]</ref>. Later, more sophisticated methods were proposed, which performs very well in the task of text categorization <ref type="bibr" coords="1,328.44,520.04,11.40,8.97" target="#b4">[5,</ref><ref type="bibr" coords="1,339.85,520.04,11.40,8.97" target="#b9">10]</ref>. As a feature representation, the bag-of-word model or vector space model (VSM) was proposed, which is a common but pretty strong, finite dimensional numeric representation of any textual data <ref type="bibr" coords="1,449.76,543.92,10.88,8.97" target="#b7">[8,</ref><ref type="bibr" coords="1,460.64,543.92,7.25,8.97" target="#b6">7,</ref><ref type="bibr" coords="1,467.89,543.92,7.25,8.97" target="#b1">2]</ref>.</p><p>In this paper we propose a common feature set for the task, which contains basic, word based features and complex bag-of-word based, optimized features as well. After, we introduce a voting based classifier method. We propose a method that helps fine tune the parameters of this meta-classifier avoiding the overfitting during the model building phase. Basically, this study is an overview of our system, which was applied in the PAN 2010 Wikipedia Vandalism Detection shared task <ref type="bibr" coords="2,333.60,128.72,10.60,8.97" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Machine learning based vandalism detection approach</head><p>To solve the Wikipedia vandalism detection problem we decided to propose an inferring method, which is as automatic as possible. This method follows the traditional NLP approach: first, we extract features from the published training set, then, we apply statistical learning algorithms to produce a model which can be used for automatically labeling the evaluation set. The main idea behind our features was to try to capture the vandalism class as much as possible, since the regular one is too general. The description of the features is the following:</p><formula xml:id="formula_0" coords="2,141.72,273.92,101.72,8.97">-BalancedVSM (BVSM)</formula><p>This feature is a specialization of the VSM where the vector of a certain document contains only 0 or 1 values for each dimension. In our case, we used 4 different values as vector elements:</p><p>-when the edit does not contain the word, then the value is n, -when the word is in an added sequence, then the value is a, -when the word is in a removed sequence, then the value is d, -when the word is in a changed sequence, then the vale is c.</p><p>As it is well known using this type of VSM representation is not very successful <ref type="bibr" coords="2,468.96,368.00,11.60,8.97" target="#b2">[3]</ref> due to the fact that the dimension of this representation is 47 324. And so we had to apply a dimension reduction method, which is based on the InfoGain [3] score.</p><p>Our preliminary observations showed that choosing the top 100 attributes results in better representation.</p><p>Since the distribution of the regular and vandalism samples is totally unbalanced (∼93.86% regular samples), the above described attribute extraction step overrepresents the words from the regular edits. Having seen this problem we balanced the VSM representation: initially, we selected all the samples which were classified as vandalism. Then, we iteratively added to this set randomly selected, regular samples of the same quantity. Next, we performed the previously described dimensional reduction step and we stored and summed up the InfoGain scores of the top 100 attributes. Finally, we selected the top 100 attributes based on the aggregated scores and used them as Balanced SVM attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-CharacterStatistic</head><p>This feature family involves two different attributes: the upper case letter and the non-letter character occurrences divided by the number of characters respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-RepeatedCharSequences</head><p>One of the signs of vandalism is when somebody just repeats a sort string e.g. "asdasdasdasdasd". For this reason, we scanned the modifications and the comments to find short and frequent repeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-ValidWordRatio</head><p>In these two attributes, we used dictionaries to provide the feature values. We used a simple English language dictionary and another that contains pejorative English expressions. Finally, the feature values are the number of the word occurrences in the dictionaries normalized by the word occurrences in the given edit. -CommentStatistic (non-edit based feature)</p><p>Commenting on the modification is made available for each user who edits Wikipedia pages. The possible feature values are:</p><p>-deleted if the comment of the edit was deleted, -comment if the user has written into the comment field, -nothing in any other cases. -UserNameOrIP (non-edit based feature)</p><p>The user who edits Wikipedia can either register and choose a nickname or not register and use his IP number. So we added a feature that describes whether a user is registered or not.</p><p>Based on the previously defined features, we built several models applying different learning algorithms. These algorithms are quite common, and their implementations are available from several sources. We used the WEKA <ref type="bibr" coords="3,369.96,274.16,11.60,8.97" target="#b3">[4]</ref> implementations in our experiments. The algorithms used and their short descriptions are the following:</p><p>-ZeroR: the most frequent class classifier.</p><p>-NaïveBayes : Bayes' theorem based classifier.</p><p>-J48: one of the decision tree learning methods.</p><p>-LogReg: the maximum likelihood based Logistic Regression method.</p><p>-SMO: an implementation of the SVM classifier.</p><p>-WeightedVotingMetaclassifier: This classifier combines several underlying classifier algorithms based on a weighted aggregation. This algorithm is the only one which is not an official WEKA algorithm since we had to develop it as a WEKA extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we describe the evaluations we made and present our final result measured on the released evaluation set. Since we knew the correct class labels for the training set only, we could only use this information for evaluation. We used the AUC score as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation of different VSMs</head><p>In our first evaluation, we investigated the representation power of different VSMs.</p><p>The overall results can be seen in Figure <ref type="figure" coords="3,296.64,534.44,3.77,8.97" target="#fig_0">1</ref>. Here, we evaluated several classifiers on the Normal VSM feature set, which is a simple VSM representation of the edits. The second feature set is an Attribute Selected VSM, where we retained the top 100 InfoGain scored VSM features. As one can see this is a much stronger feature set, but by using the Balanced Attribute Selected VSM, we can achieve a higher 10-fold AUC score in the case of each classifier. For this reason, we chose this representation as the base VSM representation in later evaluations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results of the Different Feature Sets</head><p>Our second evaluation focused on the examination of the relation of the defined feature sets to the chosen training algorithms. We defined some feature sets as the subsets of the previously defined feature ranges and performed some learning-evaluation phases applying different training algorithms. The results are summarized in Table <ref type="table" coords="4,436.68,344.36,3.77,8.97" target="#tab_0">1</ref>. In Table <ref type="table" coords="4,185.16,486.56,3.77,8.97" target="#tab_0">1</ref>, the semantics of the feature set labels is the following: "BSVM" means the Balanced VSM representation; "BSVM, stop" is the same as the previous except that we added a stop world list, which ignores the meaningless words; "All features, stop" represents the feature set, where we used all the above defined features and in the case of BVSM the stop world list was also used.</p><p>The most reasonable models are the probability based ones, however in the case of the last feature set (All features, stop) the J48 algorithm, which is a clearly discriminative model based approach, also shows pretty good AUC result.</p><p>In the case of the last feature set, the fact that one of the discriminative model could achieve a similar result than the probability based approaches indicated that this feature set is quite stable. In our further experiments we used this feature set. From the fact that these algorithms work in a completely different way, we assumed that perhaps the algorithms, which were based on different approaches, made different errors. From this naturally comes that idea that we should try to combine the three best algorithms namely the J48, the NaïveBayes and the LogReg, and built the previously introduced Weighted Voting Metaclassifier on the top of these three algorithms. The only questions here are How should we determine the weights of the underlying classifiers? and Are the optimal weights found in the training set optimal on the evaluation set as well?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Voting Based Classification and Parameter Tuning</head><p>We decided that we use the 10-fold AUC scores as the optimality measurement of a weight setting. To check the validity of our optimization process, we splitted the training set into an optimization and an evaluation set by the ratio of 4:1. We performed a 10fold cross validation based optimization of the parameters on the optimization set and we checked whether this selection is optimal on the evaluation set or not. We performed this optimization in the whole parameter space. The summary of our optimization can be seen in Figure <ref type="figure" coords="5,210.00,545.24,3.77,8.97">2</ref>. In this figure, the x-axis and y-axis represent the weight of the J48 and Naïve Bayes classifiers respectively. Since the weights must be normalized, the weight of the LogReg model can be calculated as (1-weight of J48 -weight of NaïveBayes ).</p><p>As one can see, the results of the evaluation set and the results of the optimization sets correlate (the two surfaces are almost the same). So we can say that these optimiza-tion criteria are valid and we found that the optimal weighting of the algorithms is the following (J48 : 0.3; N aiveBayes : 0.09; Logistic : 0.61).</p><p>The achieved AUC 10-fold cross validation based score of the optimally weighted metaclassifier is 0.9129, which is significantly higher than the best score in Table <ref type="table" coords="6,473.16,104.84,3.77,8.97" target="#tab_0">1</ref>. Thus, we used this combined Weighted Voting Metaclassifier model (, which learned on the full train set and used the weights presented above) for making our final predictions on the official evaluation set. Our result makes 0.87669 error score on the official evaluation set of vandalism detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our experiments in the field of detecting vandalism in Wikipedia edits indicated that we should participate in the Wikipedia Vandalism Detection Shared Task. Although our solution made an average performance on the challenge, we feel that our work has a strong contribution. This contribution is twofold. First, we developed a strong feature representation for the task, which can be built in a fully automatic manner and some of these features are pretty complex e.g. the Balanced VSM representation, which is a novel extension of the basic VSM representation and is suitable for learning tasks where the class labels have a highly unbalanced distribution. Second, we successfully combined classification methods in a weighted manner, where the weights had been optimized as hyperparameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,214.20,238.00,186.97,8.07"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. AUC results of different VSM feature sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,138.36,375.52,338.51,74.19"><head>Table 1 .</head><label>1</label><figDesc>Overall results measured on different feature sets (the top two results are highlighted)</figDesc><table coords="4,160.32,406.48,285.04,43.23"><row><cell>Features</cell><cell>ZeroR</cell><cell>J48</cell><cell cols="2">NaïveBayes LogReg</cell><cell>SMO</cell></row><row><cell>BVSM</cell><cell>0.4990</cell><cell>0.5230</cell><cell>0.7220</cell><cell>0.8130</cell><cell>0.5590</cell></row><row><cell>BVSM, stop</cell><cell>0.4990</cell><cell>0.8680</cell><cell>0.7750</cell><cell>0.8430</cell><cell>0.5430</cell></row><row><cell>All features, stop</cell><cell>0.4990</cell><cell>0.8280</cell><cell>0.8830</cell><cell>0.8870</cell><cell>0.5820</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p><rs type="person">M. Jelasity</rs> was supported by the <rs type="funder">Bolyai Scholarship</rs> of the <rs type="funder">Hungarian Academy of Sciences</rs>. This work was partially supported by the <rs type="programName">Future and Emerging Technologies programme FP7-COSI-ICT</rs> of the <rs type="funder">European Commission</rs> through project <rs type="projectName">QLectives</rs> (grant no.: <rs type="grantNumber">231200</rs>). This work was supported in part by the <rs type="funder">NKTH</rs> grant of the <rs type="programName">Jedlik Ányos R&amp;D Programme</rs> (project codename <rs type="projectName">TEXTREND</rs>) of the <rs type="funder">Hungarian government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_w4ZSBYm">
					<orgName type="program" subtype="full">Future and Emerging Technologies programme FP7-COSI-ICT</orgName>
				</org>
				<org type="funded-project" xml:id="_HFxYAkX">
					<idno type="grant-number">231200</idno>
					<orgName type="project" subtype="full">QLectives</orgName>
				</org>
				<org type="funded-project" xml:id="_rGm4gND">
					<orgName type="project" subtype="full">TEXTREND</orgName>
					<orgName type="program" subtype="full">Jedlik Ányos R&amp;D Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.54,361.96,257.89,8.07;6,150.96,373.00,322.17,8.07" xml:id="b0">
	<monogr>
		<ptr target="http://www.uni-weimar.de/medien/webis/research/workshopseries/pan-10/task2-vandalism-detection.html" />
		<title level="m" coord="6,150.96,361.96,186.66,8.07">Pan 2010 lab, task 2: Wikipedia vandalism detection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,383.32,324.84,8.07;6,150.96,394.36,78.59,8.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,192.24,383.32,246.40,8.07">Vandalism detection in wikipedia: a bag-of-words classifier approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belani</surname></persName>
		</author>
		<idno>CoRR abs/1001.0700</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,404.68,309.54,8.07;6,150.96,415.72,254.38,8.07" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m" coord="6,204.00,404.68,248.08,8.07;6,150.96,415.72,34.61,8.07">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,426.04,336.28,8.07;6,150.96,437.08,266.27,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,426.60,426.04,52.22,8.07;6,150.96,437.08,96.21,8.07">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,253.20,437.08,89.48,8.07">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,447.40,308.82,8.07;6,150.96,458.44,310.84,8.07;6,150.96,469.36,305.48,8.07;6,150.96,480.28,193.05,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,200.64,447.40,250.72,8.07;6,150.96,458.44,57.89,8.07">Text categorization with support vector machines: learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://citeseer.ist.psu.edu/joachims97text.html" />
	</analytic>
	<monogr>
		<title level="m" coord="6,348.84,458.44,112.96,8.07;6,150.96,469.36,155.73,8.07">Proceedings of ECML-98, 10th European Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Nédellec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Rouveirol</surname></persName>
		</editor>
		<meeting>ECML-98, 10th European Conference on Machine Learning<address><addrLine>Heidelberg et al.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,490.72,336.70,8.07;6,150.96,501.64,99.11,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,247.68,490.72,220.36,8.07">On relevance, probabilistic indexing and information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Kuhns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,473.52,490.72,5.72,8.07;6,150.96,501.64,20.09,8.07">J. ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="244" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,512.08,290.15,8.07;6,150.96,523.00,328.52,8.07;6,150.96,533.92,327.11,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,249.24,512.08,183.45,8.07;6,150.96,523.00,45.77,8.07">A comparison of event models for naive Bayes text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<ptr target="http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="6,214.44,523.00,260.74,8.07">Learning for Text Categorization: Papers from the 1998 AAAI Workshop</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">752</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,544.36,332.62,8.07;6,150.96,555.28,108.11,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,273.60,544.36,159.94,8.07">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,439.68,544.36,35.48,8.07;6,150.96,555.28,20.09,8.07">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.54,565.72,320.02,8.07;6,150.96,576.64,67.79,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,203.52,565.72,178.84,8.07">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,388.32,565.72,74.24,8.07">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.18,587.08,338.27,8.07;6,150.96,598.00,97.67,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,229.68,587.08,246.70,8.07">Text categorization based on regularized linear classification methods</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Oles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,150.96,598.00,32.12,8.07">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
