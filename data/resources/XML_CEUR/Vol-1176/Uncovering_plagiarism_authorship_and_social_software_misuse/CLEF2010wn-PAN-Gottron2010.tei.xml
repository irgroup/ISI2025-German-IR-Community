<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.60,115.90,324.17,12.90;1,168.44,133.83,278.47,12.90;1,265.43,151.77,84.50,12.90;1,218.28,171.62,178.79,10.75">External Plagiarism Detection Based on Standard IR Technology and Fast Recognition of Common Subsequences Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,274.89,208.03,65.58,8.64"><forename type="first">Thomas</forename><surname>Gottron</surname></persName>
							<email>gottron@uni-koblenz.de</email>
							<affiliation key="aff0">
								<orgName type="department">WeST -Institute for Web Science</orgName>
								<orgName type="institution">Technologies University of Koblenz-Landau</orgName>
								<address>
									<postCode>56070</postCode>
									<settlement>Koblenz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.60,115.90,324.17,12.90;1,168.44,133.83,278.47,12.90;1,265.43,151.77,84.50,12.90;1,218.28,171.62,178.79,10.75">External Plagiarism Detection Based on Standard IR Technology and Fast Recognition of Common Subsequences Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35E6B6CEEDFB527DFA660A820BF5255E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The plagiarism detection system described in this paper is aiming at bringing external plagiarism detection to the desktop. The main ideas are to incorporate standard IR technologies for the candidate selection and efficient data structures for the detailed analysis between a suspicious and a candidate document. Given that the system so far has only reached prototype status, the first results look promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The plagiarism detection system described in this paper was designed following two main aims. First, to implement candidate selection based on standard IR engines. The intention of this aim is to substitute the candidate selection used for the PAN competition with the search API of a web search engine. In this way it is easily possible to lift the competition system to a real world scenario, e.g. to check essays or term papers at university courses for plagiarism from the web. The second aim was to design a system that can run on commodity hardware. Also this intention was motivated by practise, where teachers in educational institutions do not have access to high-end computing machinery, but rather run plagiarism detection software on their laptop or desktop machines.</p><p>Both aims were realised. The candidate selection process was implemented based on using Lucene with an out of the box configuration. By using an efficient data structure to mine similar sub-sequences from a pair of documents, also the detailed analysis is running remarkably fast. Scaling the run-time to a scenario of analysing 20 student papers of 10,000 words each, the whole process can be completed in less than one minute, which is acceptable in practise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>In last years competition, Zechner et.al. <ref type="bibr" coords="1,297.85,644.48,16.60,8.64">[15]</ref> was the only team employing a standard model of textual IR for candidate selection. The source documents were indexed on a sentence level and sentences of suspicous documents were used as queries. Similarity was calculated via the well established cosine measure. The mapping between sentences from the suspicious document to the sentences of source documents also provided the alignment of similar subsequences.</p><p>All other approaches used task specific index structures. Malcolm and Lane [9] used the desktop plagiarism detection system Ferret, which is based on common word tri-grams. Word n-grams are also the method of choice of the WCopyFind tool employed by Vallés Balaguer <ref type="bibr" coords="2,241.33,204.00,15.27,8.64" target="#b12">[13]</ref>. Instead, Basile et.al. <ref type="bibr" coords="2,345.22,204.00,11.62,8.64" target="#b0">[1]</ref> encoded texts as a word length sequence and used a downstream vector-based n-gram distance measure for candidate selection. Kasprzak et.al. <ref type="bibr" coords="2,236.10,227.91,11.62,8.64" target="#b5">[6]</ref> incorporate common text shingles in the pre-selection process and Shcherbinin and Butakov <ref type="bibr" coords="2,273.30,239.87,16.60,8.64" target="#b9">[10]</ref> employed hash-based fingerprints for candidate retrieval. A very different approach was taken by Grozea et.al <ref type="bibr" coords="2,394.25,251.83,10.58,8.64" target="#b2">[3]</ref>. They used string kernels to compute a complete similarity matrix for each pair of source and suspicious document.</p><p>Finding similar subsequences in strings is a problem commonly addressed in bioinformatics. Algorithms finding longest common subsequences, like the Hirshberg Algorithm <ref type="bibr" coords="2,159.44,312.61,11.62,8.64" target="#b3">[4]</ref> are based on the Levenshtein edit distance <ref type="bibr" coords="2,348.62,312.61,11.62,8.64" target="#b6">[7]</ref> and aim at finding a globally optimal alignment. The Smith-Waterman algorithm <ref type="bibr" coords="2,344.92,324.56,16.60,8.64" target="#b10">[11]</ref> is capable of finding also locally similar subsequences.</p><p>In the context of plagiarism detection, Kasprzak et.al. <ref type="bibr" coords="2,374.51,349.48,11.62,8.64" target="#b5">[6]</ref> detect regions that are densely populated with common shingles in both documents. Malcolm and Lane <ref type="bibr" coords="2,468.97,361.43,11.62,8.64" target="#b8">[9]</ref> compute the Jaccard coefficient for similarity on word sequences. A detailed analysis based on a T9-like word encoding and detecting and merging squared shapes in the dotplot <ref type="bibr" coords="2,165.94,397.30,11.62,8.64" target="#b7">[8]</ref> representation was employed by Basile et.al. <ref type="bibr" coords="2,363.37,397.30,10.58,8.64" target="#b0">[1]</ref>. Grozea et.al <ref type="bibr" coords="2,431.92,397.30,11.62,8.64" target="#b2">[3]</ref> have developed an alternative to dotplots: the encoplot representation. The detailed analysis is based on detecting continuous lines in the encoplot.</p><p>Concerning performance optimization, the most common approach is to parallelize the algorithms. Grozea et.al <ref type="bibr" coords="2,248.36,446.12,11.62,8.64" target="#b2">[3]</ref> followed this road and Kasprzak et.al. explained in <ref type="bibr" coords="2,468.97,446.12,11.62,8.64" target="#b4">[5]</ref> in detail how their pre-selection mechanism has been extended to a distributed version running on a cluster-architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">External Plagiarism Detection</head><p>The plagiarism detection system presented in this paper follows the standard designed as described e.g. in <ref type="bibr" coords="2,214.21,547.84,15.27,8.64" target="#b11">[12]</ref>. When provided with a suspicious document the pre-selection component uses the Lucene engine to retrieve candidate documents from the source collection for a detailed comparison. The detailed analysis then provides tuples of sequences from suspicious and candidate documents that already represent detected plagiarised contents. A series of post-processing filters takes care to remove pathological cases.</p><p>Prior to building the Lucene index, all non-English documents were translated into English using Google's translation-service. Essentially, this corresponds to a standard cross-language indexing approach. To be able to easily map the translated parts back onto the original texts, they were translated in small chunks of a few paragraphs. The information which parts of the texts correspond to each other was stored for a later on backward resolution of character positions in plagiarised parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-Selection</head><p>The Lucene index for pre-selection covers only English documents -either English originals or the translations of documents written in other languages. To cope with Lucene's feature of indexing only the first l token in a text, each document d n was split up into smaller parts d m n of a fixed length of l d &lt; l terms. Each part d m n was submitted to the indexing engine as an individual document, but with a reference to the source it was taken from.</p><p>A similar step was taken for finding candidate documents to compare with a given suspicious document. The suspicious document s i was split into even smaller parts s j i of length l q &lt; l d . These parts were used as queries to retrieve relevant documents from the index. For each query the top-k results are collected, that have scored a relevance value ρ Lucene (s j i , d m n ) above a certain threshold value θ. This process is iterated over all parts s j i of a suspicious document. For each source document d n a total candidate score c(s i , d n ) is computed by summing over all obtained scores for all parts of that document and all parts of the suspicious document, so:</p><formula xml:id="formula_0" coords="3,235.54,359.17,245.05,22.66">c(s i , d n ) = j m ρ Lucene (s j i , d m n )<label>(1)</label></formula><p>Ordering the source documents by decreasing candidate scores c(s i , d n ) creates a ranking of candidate documents for a particular suspicious document s i . This ranking is the input to the next step: the detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detailed Analysis</head><p>The ranking computed by the last step has the advantage of assigning a priority to the candidate documents. This can be used to balance time constraints of a fast analysis against completeness, simply be considering more or less documents of the ranked list in a detailed analysis.</p><p>The comparison of a pair consisting of a suspicious and a candidate document comes down to finding similar local token sub-sequences. Talking about tokens in this case is intentionally general, as it can be realised in different ways. The methods described here can be applied to tokens that correspond to characters, terms or larger constructs like n-grams. The setup described in this paper eventually used sorted term 5-grams, where the purpose of sorting the terms in each 5-gram was to partially overcome local changes in word order.</p><p>The problem of finding similar local sub-sequences is commonly known in bioinformatics, where DNA sequences have to be compared for similar fragments. Hence, the first approach was to apply the Smith-Waterman algorithm <ref type="bibr" coords="3,400.85,620.57,15.27,8.64" target="#b10">[11]</ref>. But, given its quadratic complexity, it can only be run on small documents or, again, parts of documents. This turned out not be suitable for a plagiarism detection scenario with booklength documents. We investigated applying Smith-Waterman to local areas in the token sequence around cooccurrances of word n-grams. This approach lead to low values for both, recall and precision, and high values of granularity (see the results in section 3). Also, execution time was still around twice as long as for the solution we eventually applied.</p><p>Another typical approach is to plot the positions at which each token from a candidate document occurs in a suspicious document. This leads to a graph like the one in Figure <ref type="figure" coords="4,163.86,194.28,3.74,8.64" target="#fig_0">1</ref>, sometimes referred to as a dotplot <ref type="bibr" coords="4,315.88,194.28,10.58,8.64" target="#b7">[8]</ref>. While random cooccurrences appear as noise in this kind of plot, regions of plagiarism point out as longer lines. Such a plot can be generated efficiently, by building an inverting index of tokens and their positions in the suspicious document and then looking up each token from the candidate in this index. The advantage of building the index over the suspicious document is, that it can be reused over several candidate documents.</p><p>Essentially, the Smith-Waterman algorithm should detect the lines in such a dotplot, while being flexible enough to tolerate small perturbations in the lines caused by replaced, inserted or deleted words. Other error tolerant line-detection algorithms like RANSAC <ref type="bibr" coords="4,174.38,305.12,11.62,8.64" target="#b1">[2]</ref> are capable to find such lines as well, but still require several iterations over the data during the computation. These algorithms, however, provide more flexibility than actually needed. Effectively, looking for longer common sub-sequences of high similarity corresponds in our case to finding lines in the dotplot which approximately have a 45 degree angle with both axis. Hence, when subdividing the dotplot in stripes as shown in Figure <ref type="figure" coords="4,440.16,632.53,3.74,8.64" target="#fig_1">2</ref>, one can expect each of the lines -which correspond to cases of plagiarism -to lie completely in one stripe.  Each stripes can be represented by a line x i in its centre (cf. the dotted lines in Figure <ref type="figure" coords="5,163.21,574.99,3.60,8.64" target="#fig_2">3</ref>). These lines can be formulated by using a normal vector w, that also includes a definition of the width of the stripes:</p><formula xml:id="formula_1" coords="5,253.64,612.41,226.95,9.65">x i : p, w + 2 • i • | w| = 0<label>(2)</label></formula><p>The advantage of this representation for the stripes is, that for each cooccurrence of a single token in the suspicious and candidate document it is possible to identify immediately, which stripe it falls in. For a cooccurrance at the point (j, k), where j is the position in the source sequence and k the position in the suspicious sequence, the stripe can be determined by essentially calculating the distance of this point to the line</p><formula xml:id="formula_2" coords="6,134.77,142.90,345.83,45.87">x 0 : stripe(j, k) = 1 -2 • | w| j k , w -0.5<label>(3)</label></formula><p>This lead to the idea, not to compute the dotplot at all, but rather a stripe-index. This stripe index stores all cooccurrences of tokens lying in the same stripe as a linked list of entries.</p><p>In this way, it is sufficient to consider only those stripes for line detection where a lot of elements stored in. The stripes with few entries can be skipped as they presumably contain only random cooccurrences. To identify the lines, dots in a stripe are mapped onto the line in its centre, so that in the end, it is enough to consider densities in a one-dimensional data structure.</p><p>Lines caused by plagiarism correspond to dense regions in the stripes and are found by comparing the distances between single points on the central stripe lines. If the distance between n min consecutive points is not larger then δ min each, this is considered the start of a dense region, until the next point in the sequence is further away than δ min .</p><p>One issue to take care of was the special case of a line in the dotplot falling exactly on the border between two stripes. In this case and due to the already mentioned possibility of small perturbations in the line, it might happen that some cooccurrences are captured in one stripe and some in the neighbour stripe. This might cause a dense region to be split into smaller parts across two stripes that individually might become too short or not dense enough to be detected. The solution to this problem is to design the stripes to have an overlapping margin. Though, in extreme cases a dense region might still be split in parts, each of them is long enough to be detected individually. One of the post-processing steps described below takes care to join the parts in this case. The improvements obtained in this way depend on the width of the stripes. Given the width eventually chosen for the stripes here, the improvements were marginal.</p><p>During the entire process of transforming cooccurrences into points in a stripe, the original positions of the tokens are kept in mind. In this way it is possible to reconstruct for a dense region, what are the corresponding first and last token in the original suspicious and candidate document respectively.</p><p>The sub-sequences detected in this way are already a detected region of plagiarism and are resolved into character positions. Before reporting these regions, however, they undergo a few steps of post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-Processing</head><p>Post-processing in the current system consists of three steps:</p><p>1. The first step checks for too short regions. As it might happen that very short similar sub-sequences occur by chance, each region that consists of less than a certain number of characters is dropped.</p><p>2. The second step merges close regions. If regions lie close to each other or even overlap, they are combined into one region. This happens if the distance between the first and last character of two regions is lower than a threshold. 3. Last, when dealing with a translated document, the positions in the translated English version need to be mapped back to the positions of the original text. This is done a simple linear interpolation based on the positions of the corresponding chunks of text created during the initial translation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>As the system contains a lot of parameters, a subset of the first 2,000 suspicious documents of the PAN-09 <ref type="bibr" coords="7,222.38,260.94,16.60,8.64" target="#b13">[14]</ref> corpus was used to optimise performance. Additionally, the pre-selection process, the detailed analysis and the post processing step were technically split up and intermediary results were serialised to a simple file based representation. This allows individual analysis and evaluation of the steps. The pre-selection was analysed on how well the Lucene approach is capable of finding good candidates. The gold-standard candidates from the training corpus were used to compute recall-based measures. As the quality of the ranking based on the candidate scores c(s i , d n ) is of interest as well, recall@10, @20, and @30 as well as the recall over all selected candidates were determined. It turned out that the overall recall differed little from recall@30. So, it seems sufficient to consider only the first 30 candidate documents for a detailed analysis. The recall obtained with the Lucene index was around 0.7. The length l d of the parts for indexing a document and l q for searching the index seemed to have little influence on this values. In the end, l d was set to 5,000 terms and l q to 50 terms. The threshold θ had been set to 0.5 and retrieval was limited to the top-10 documents for each query.</p><p>In the phase of the detailed analysis there are other parameters to tune. The width of the stripes in the stripe-index was set to an equivalent of 50 token, with the stripes having an overlapping margin of 20%. A dense region consisted of at least 5 token with a distance of no more than the equivalent of 10 token.</p><p>The post-processing phase used settings chosen according to results reported in previous years <ref type="bibr" coords="7,195.99,500.63,10.58,8.64" target="#b2">[3]</ref>. Plagiarised sequences of less than 175 characters were considered random cooccurrences and were discarded. Sequences with a distance of less than 500 characters were merged into one plagiarised content covering also the characters in between.</p><p>The performance of this setup can be seen in Table <ref type="table" coords="7,356.00,548.64,3.74,8.64" target="#tab_0">1</ref>. On the training corpus recall and precision obtain quite balanced values and granularity is acceptable. The individual measures as well as the overall score does not reach the best-performing system of last year. But, as the system is a relatively quickly drafted prototype, we did not expect to beat those results.</p><p>For comparison, the table also includes the performance of the shortly mentioned approach based on the localized Smith-Waterman algorithm mentioned above. The performance is much lower under all aspects. However, it remains to be said, that this approach was not followed intensively and that a better performance can be expected when tuning the parameters. The performance on the PAN-10 test corpus was different, though. Here recall was lower while precision was higher. The reason for the lower recall can be explained with the lack of an intrinsic analysis component. Evaluating the results of the detection solely against the external cases of plagiarism, recall lies at 0.3905 and, thus, much closer to the values obtain on the training corpus.</p><p>Surprising was the increase in granularity, which clearly contributes to the overall lower performance of 0.2564 compared to 0.3947 on the training data. New obfuscation strategies and potentially a different distribution and density of the plagiarised contents could be an explanation. However, this unexpected behaviour remains to be investigated in detail.</p><p>Besides the evaluation of effectiveness, also efficiency and run-time performance was of interested. The system was run on commodity hardware: an Lenovo notebook computer with a 2.67 GHz Core i7 processor and 4GB RAM. So far, the system is single threaded and does not make use of parallel execution on several processing cores.</p><p>Building the Lucene index for the PAN-10 corpus is negligible and took less than 7 minutes. The pre-selection of candidates was the longest part in the process and required about 28 hours and 21 minutes. The detailed analysis and post-processing was much faster and took a total of 10 hours and 4 minutes. This leads to a total run-time of 38 hours and 32 minutes. Exploiting the inherit potential of parallelising each step of the process, a speedup factor closely correlated to the number of parallel threads can be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The system presented in this paper makes two main contributions. First, it uses and evaluates a Lucene index for the candidate selection process. Though, the idea of using standard search systems for plagiarism detection is not new, it had not been applied or evaluated by any system in the PAN plagiarism detection competition of last year. Further, and unlike special purpose indexes that are used in other systems <ref type="bibr" coords="8,419.16,541.92,11.52,8.64" target="#b5">[6,</ref><ref type="bibr" coords="8,430.68,541.92,11.52,8.64" target="#b12">13,</ref><ref type="bibr" coords="8,442.19,541.92,7.68,8.64" target="#b0">1,</ref><ref type="bibr" coords="8,449.87,541.92,7.68,8.64" target="#b8">9,</ref><ref type="bibr" coords="8,457.55,541.92,7.68,8.64" target="#b2">3,</ref><ref type="bibr" coords="8,465.23,541.92,11.52,8.64" target="#b9">10]</ref>, this allows to easily adapt the system to search the web for candidate documents via web search engines. Second, with the stripe-index used in the detailed analysis step, we obtained good run-time improvements in detecting similar sub-sequences from two documents, compared to the Smith-Waterman and the RANSAC approach.</p><p>As the system is in an early prototype stage, the results are far from being competitive with other systems. However, there are several components that offer a lot of potential for improvements:</p><p>-Post-processing is working solely on the position and length information of plagiarised parts. No information from the real texts is used. Incorporating the actual contents of the documents into this process will allow a more elaborated analysis and better decisions on when to discard or merge plagiarised parts. -The dotplot was built on sorted term 5-grams. Choosing different tokens as base for the dotplot -especially more fine-grained structures -will lead to a higher recall. It remains to analyse how the trade-off in precision and granularity can be counterbalanced, either by improving the line detection method, or by improving the post-processing steps. -The alignment of positions between the translated and original documents was done in a very simple way. A more fine-grained resolution will improve accuracy for cross-lingual plagiarism detection. -Semantic changes, like replacing words with synonyms, have not been considered at all so far. They might be incorporated into the inverted index structure over the token positions in the suspicious document. -Another interesting question could be to replace the vector space model underlying Lucene with a different standard IR model. Given long texts and queries, a promising approach could be to look into retrieval based on language models. -Finally and as mentioned above, the system can be expected to be sped up also on commodity hardware by implementing thread-based parallelisation.</p><p>Overall, as several components so far were designed to provide merely basic functionality, the system can benefit from several improvements, which might contribute to a far better performance in future competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Acknowledgements</head><p>Some of the initial ideas leading to the system described in this paper, especially considering candidate selection based on standard IR techniques, were developed and discussed with students of a course given at the University of Mainz. Hence, I would like to thank Ricard Anufriev, Christian Auth, Florian Feyand, Willy-Roland Monkam, Christian Sigel and Florian Sturm for their contribution in this course.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,545.11,345.82,8.12;4,134.77,556.41,99.63,7.77;4,206.47,381.38,175.48,134.94"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A dotplot plots the positions of terms from a candidate document against their positions in the suspicious document.</figDesc><graphic coords="4,206.47,381.38,175.48,134.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,159.65,306.07,296.06,8.12;5,206.49,142.56,175.48,134.94"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Lines in the dotplot cause by plagiarism lie in stripes of 45 degree angle.</figDesc><graphic coords="5,206.49,142.56,175.48,134.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,201.92,527.82,211.52,8.12;5,283.25,352.98,175.48,135.18"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The stripe index for storing token cooccurrences.</figDesc><graphic coords="5,283.25,352.98,175.48,135.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,159.48,115.83,296.40,61.97"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the system on Training and Test corpus</figDesc><table coords="8,159.48,136.75,296.40,41.05"><row><cell>Corpus Detailed Analysis</cell><cell cols="3">Recall Precision Granularity Overall Score</cell></row><row><cell cols="2">PAN-09 localized Smith-Waterman 0.2354 0.0789</cell><cell>3.2941</cell><cell>0.0562</cell></row><row><cell>PAN-09 Stripe-Index</cell><cell>0.4689 0.4027</cell><cell>1.1402</cell><cell>0.3947</cell></row><row><cell>PAN-10 Stripe-Index</cell><cell>0.3174 0.5059</cell><cell>1.8701</cell><cell>0.2564</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,535.68,317.54,7.77;9,150.95,546.64,327.55,7.77;9,150.95,557.60,323.93,7.77;9,150.95,568.56,62.01,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,413.58,535.68,46.57,7.77;9,150.95,546.64,239.08,7.77">A plagiarism detection procedure in three steps: Selection, matches and &quot;squares</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cristadoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Degli Esposti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.77,546.64,66.74,7.77;9,150.95,557.60,319.91,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,579.80,334.01,7.77;9,150.95,590.76,327.00,7.77;9,150.95,601.72,23.90,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,255.95,579.80,220.66,7.77;9,150.95,590.76,205.60,7.77">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,362.50,590.76,57.93,7.77">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,612.96,323.98,7.77;9,150.95,623.92,329.64,7.77;9,150.95,634.88,252.20,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,277.96,612.96,188.64,7.77;9,150.95,623.92,109.39,7.77">Encoplot: Pairwise sequence matching in linear time applied to plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,278.20,623.92,202.40,7.77;9,150.95,634.88,183.92,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,646.13,335.41,7.77;9,150.95,657.08,141.61,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,215.79,646.13,258.30,7.77">A linear space algorithm for computing maximal common subsequences</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,150.95,657.08,57.93,7.77">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="341" to="343" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,119.96,298.42,7.77;10,150.95,130.92,328.85,7.77;10,150.95,141.88,178.42,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,309.97,119.96,131.06,7.77;10,150.95,130.92,107.94,7.77">Distributed aspects of the system for discovering similar documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brandejsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,277.45,130.92,202.35,7.77;10,150.95,141.88,152.28,7.77">ITA&apos;09: Proceedings of the 3rd International Conference on Internet Technologies and Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,152.84,307.74,7.77;10,150.95,163.80,302.10,7.77;10,150.95,174.76,209.36,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,294.38,152.84,155.96,7.77;10,150.95,163.80,38.86,7.77">Finding plagiarism by evaluating document similarities</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kripac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,207.50,163.80,245.56,7.77;10,150.95,174.76,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,185.71,326.20,7.77;10,150.95,196.67,201.48,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,216.42,185.71,249.01,7.77">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,150.95,196.67,113.33,7.77">Doklady Akademii Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="845" to="848" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,207.63,316.72,7.77;10,150.95,218.59,303.55,7.77;10,150.95,229.55,127.99,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,240.20,207.63,219.13,7.77;10,150.95,218.59,34.73,7.77">Enhanced graphic matrix analysis of nucleic acid and protein sequences</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Maizel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,191.79,218.59,262.72,7.77;10,150.95,229.55,30.87,7.77">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7665" to="7669" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,240.51,335.97,7.77;10,150.95,251.47,324.25,7.77;10,150.95,262.43,252.20,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,256.56,240.51,222.03,7.77;10,150.95,251.47,103.68,7.77">Tackling the pan&apos;09 external plagiarism detection corpus with a desktop plagiarism detector</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C R</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,272.48,251.47,202.72,7.77;10,150.95,262.43,183.92,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,273.39,332.61,7.77;10,150.95,284.34,315.55,7.77;10,150.95,295.30,151.58,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,256.83,273.39,214.51,7.77">Using microsoft sql server platform for plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shcherbinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Butakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,163.16,284.34,303.34,7.77;10,150.95,295.30,83.30,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="36" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,306.26,336.85,7.77;10,150.95,317.22,160.63,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,258.20,306.26,178.61,7.77">Identification of common molecular subsequences</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Waterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,442.98,306.26,36.11,7.77;10,150.95,317.22,67.99,7.77">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="197" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,328.18,331.22,7.77;10,150.95,339.14,328.20,7.77;10,150.95,350.10,317.99,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,301.24,328.18,156.70,7.77">Meta analysis within authorship verification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meyer Zu Eissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,339.14,328.20,7.77;10,150.95,350.10,73.00,7.77">DEXA &apos;08: Proceedings of the 2008 19th International Conference on Database and Expert Systems Application</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,361.06,336.47,7.77;10,150.95,372.02,329.41,7.77;10,150.95,382.97,209.36,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,224.05,361.06,254.65,7.77;10,150.95,372.02,66.16,7.77">Putting ourselves in sme&apos;s shoes: Automatic detection of plagiarism by the wcopyfind tool</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vallés Balaguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,234.80,372.02,245.56,7.77;10,150.95,382.97,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="34" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,393.93,335.19,7.77;10,150.95,404.89,307.14,7.77" xml:id="b13">
	<monogr>
		<idno>PAN-PC-09</idno>
		<ptr target="http://www.webis.de/research/corpora" />
		<title level="m" coord="10,460.32,393.93,17.11,7.77;10,150.95,404.89,66.50,7.77">PAN Plagiarism Corpus</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Webis at Bauhaus-Universität Weimar ; NLEL at Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.95,415.85,314.67,7.77;10,150.95,426.81,30.38,7.77;10,134.77,437.77,337.74,7.77;10,150.95,448.73,313.65,7.77;10,150.95,459.69,252.20,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,320.09,437.77,152.41,7.77;10,150.95,448.73,92.47,7.77">External and intrinsic plagiarism detection using vector space models</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso ; Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,261.89,448.73,202.72,7.77;10,150.95,459.69,183.92,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
