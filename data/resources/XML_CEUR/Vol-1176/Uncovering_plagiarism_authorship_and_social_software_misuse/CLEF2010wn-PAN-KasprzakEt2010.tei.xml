<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,230.17,115.90,155.02,12.90;1,200.90,133.83,213.56,12.90;1,218.28,155.12,178.79,10.75">Improving the Reliability of the Plagiarism Detection System Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.23,192.96,53.39,8.64"><forename type="first">Jan</forename><surname>Kasprzak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">Masaryk University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,310.99,192.96,66.13,8.64"><forename type="first">Michal</forename><surname>Brandejs</surname></persName>
							<email>brandejs@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">Masaryk University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,230.17,115.90,155.02,12.90;1,200.90,133.83,213.56,12.90;1,218.28,155.12,178.79,10.75">Improving the Reliability of the Plagiarism Detection System Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">56B265E07DFBE57A1CCBA7F1FC8164BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our approach at the PAN 2010 plagiarism detection competition. We refer to the system we have used in PAN'09. We then present the improvements we have tried since the PAN'09 competition, and their impact on the results on the development corpus. We describe our experiments with intrinsic plagiarism detection and evaluate them. We then discuss the computational cost of each step of our implementation, including the performance data from two different computers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discovering plagiarism in text files by means of software is a widely-studied area. Yet it is often not clear how the proposed approaches would perform in real-world scenarios. The PAN competition <ref type="foot" coords="1,220.90,435.85,3.49,6.05" target="#foot_0">1</ref> attempts to provide a testbed for comparing different approaches on the same data.</p><p>We discuss the performance of our plagiarism detection system on two document corpora, used in PAN 2010 competition: The development corpus PAN-PC-09 <ref type="bibr" coords="1,454.31,474.83,16.60,8.64" target="#b10">[11]</ref> is based on the PAN'09 competition data. It contains 14429 source documents, 14428 suspicious documents for the external plagiarism detection, and 6183 suspicious documents for the intrinsic plagiarism detection. The competition corpus contains both the intrinsic and external plagiarism cases in one body of suspicious documents. There are 11147 source documents in the corpus, and 15925 suspicious documents.</p><p>The next section of this paper contains a brief recapitulation of our approach used in PAN'09. We then evaluate incremental modifications to this approach and their impact on the overall score on the development corpus. The two main areas of the research are covered in separate sections: we discuss the cross-language plagiarism detection in Section 4, and the intrinsic plagiarism detection in Section 5. In Section 6 we then evaluate the computational cost of our implementation both on the same hardware which was used for PAN'09, and on the newer hardware. This should give some insight on the scalability of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>We have used our system for tackling the PAN'09 external plagiarism task as a starting point, trying to improve it further. The detailed description of the system can be found in <ref type="bibr" coords="2,145.01,166.18,10.58,8.64" target="#b3">[4]</ref>. In this paper, we provide only a short summary of the main features:</p><p>-Tokenization: text files are split into words (sequences of at least three consecutive alphanumeric characters). We keep the offset of the first and last character of each word for further use. -Chunks: we form partly overlapping word 5-grams, sort the words in them, and compute a MD5 hash <ref type="bibr" coords="2,239.49,234.26,12.14,8.64" target="#b6">[7]</ref>. The most significant 30 bits of the hash is then used as a chunk identification. For each chunk in a given document, we also keep the sequence number of this chunk. -Inverted index: mapping the chunk ID to the list of structures containing the following attributes: (document ID, chunk sequence number, offset of the first character of the chunk, offset of the last character of the chunk). -Computing the similar documents: using the inverted index, we examine the suspicious documents (their chunks are computed the same way as for the source documents). We then find pairs of source and suspicious documents, and their common chunk IDs (including additional data we store for each chunk). We discard document pairs with less than 20 chunks in common. -Evaluating the similar passages: for each document pair with 20 or more common chunks we test whether the chunks form one or more valid intervals (intervals where there gap between two neighbouring common chunks is not bigger than 50 chunks, and which have at least 20 common chunks) both in the source and suspicious document. -Generating the detections: those valid intervals are then reported as plagiarized passages, with the only postprocessing being removal of overlapping detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System performance</head><p>The performance of our system in the PAN'09 was the following:<ref type="foot" coords="2,395.80,482.79,3.49,6.05" target="#foot_1">2</ref> recall = 0.6967, precision = 0.5573, granularity = 1.0228, overall = 0.6093 Unfortunately, we did not save the final version of our software for PAN'09. We had to reimplement the last phase (removal of overlapping detections) from scratch. With this change, the performance on the final version of the PAN-PC-09 data was the following: recall = 0.6442, precision = 0.5725, granularity = 1.0193, overall = 0.5980</p><p>The difference here is probably caused by a different implementation, as well as possible changes between the competition corpus of PAN'09 and the final version of PAN-PC-09.</p><p>In PAN 2010, the competition corpus contained both intrinsic and external plagiarism cases. For evaluating the impact of false-positives in suspicious documents which contain intrinsic plagiarism cases only, we have joined the suspicious documents from both parts of the PAN-PC-09 into one directory (renaming the intrinsic cases and editing their annotations to match the new file names), and tested the performance on this united version of the data: recall = 0.5255, precision = 0.4858, granularity = 1.0480, overall = 0.4882</p><p>In sections 3 and 4 we present the results computed on this united version of PAN-PC-09 only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">External Plagiarism Detection Improvements</head><p>In order to improve the performance for the external plagiarism detection many ad-hoc modifications have been tested. The results from PAN'09 suggested that our method has a good recall 3 , so the focus has been mostly on improving the precision and granularity.</p><p>As for the recall, there is probably not a big room for improvement: the chunking method cannot possibly detect heavily obfuscated passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adjacent Detections</head><p>One modification to the post-processing stage was removing both overlapping detections (instead of e.g. keeping a longer one only), provided they were short enough-the threshold we have used was 600 characters. Interestingly enough, this this has improved the precision score, but did not hurt recall much: recall = 0.5252, precision = 0.4941, granularity = 1.0465, overall = 0.4929 A possible explanation is that those short overlapping detections are passages of relatively common phrases.</p><p>As for the granularity measure, we have attempted to merge the adjacent detections pointing to the same source document. The criteria we have used were the following:</p><p>-When the gap is under 600 characters, merge.</p><p>-When the gap is under 4000 characters and is smaller than half of the average length of both adjacent detections, merge. -Otherwise, keep the both detections separated.</p><p>The resulting score confirmed the improvement in both precision and granularity: recall = 0.5256, precision = 0.5302, granularity = 1.0233, overall = 0.5192</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">False positives</head><p>One of the drawback of the system in PAN'09 has been that some of the document similarities detected were not annotated in the gold standard as plagiarism passages. The examination of those false positives suggested that they are often in structured passages (like table of contents, etc.). The new system attempts to exclude such passages from our detection: it computes the percentage of letter characters in the detection, and the  <ref type="table" coords="4,184.04,339.60,3.74,8.64" target="#tab_0">1</ref>.</p><p>Because of a bug in our software, we had the above values miscomputed during the competition, and the best overall score was with threshold 0.675, which is what we have used for final results computation: recall = 0.5244, precision = 0.5420, granularity = 1.0233, overall = 0.5243 This modification has added the access to the original suspicious documents again to the post-processing stage, which brings a small performance penalty. In principle, the similar effect could be obtained by excluding non-letters (e.g. digits) in the tokenization phase, and then look at the interval length with respect to number of chunks in the said interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-language Plagiarism Detection</head><p>Both the development and competition corpora contained also translated documents. All the suspicious documents were in English, and the source documents were in English, German, and Spanish.</p><p>One of the cheapest ways for improving the performance would be to exclude the non-English documents from processing altogether in order to avoid false positives. Another approach would be to detect cross-language plagiarism using e.g. keyword extraction (for example, by finding words which are much more frequent in the text itself than in the general language).</p><p>With the relatively small document corpus size it was feasible to translate the non-English source documents to English, and to use it as alternative source documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Language Detection</head><p>The automatic language detection is a widely studied area. One possible approach is to compare character n-gram based language models <ref type="bibr" coords="5,336.72,150.82,10.58,8.64" target="#b1">[2]</ref>. We have used the Text::Language::Guess Perl module <ref type="bibr" coords="5,314.79,162.77,11.62,8.64" target="#b7">[8]</ref> from CPAN, primarily because it was ready-to-use and worked well enough. Its classification method is based on detecting stop-words in the document. While not being extremely accurate, it has provided a preselection of documents, which have been then manually checked whether they are indeed in Spanish or German. The most misdetections were on structured data<ref type="foot" coords="5,446.32,208.93,3.49,6.05" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translating the Documents</head><p>Several web sites provide automatic translation services. We have tried to use Yahoo! Babelfish system <ref type="bibr" coords="5,205.59,271.61,11.62,8.64" target="#b0">[1]</ref> and Google Translate <ref type="bibr" coords="5,308.67,271.61,10.58,8.64" target="#b2">[3]</ref>. With Babelfish we had a problem with reliability: for some documents we did not get any reply at all. Google Translate was better, and it even kept the line breaks in the translated text. This allowed us to match the translated plagiarism back to the original position with a good accuracy.</p><p>The only problem with Google Translate is that it silently truncates longer documents. We suspect they impose a limit to the CPU time their hardware spends on a single document. We have therefore split documents to blocks with the size betweeen 15 and 22 KB, splitting preferably at the paragraph boundary or a line boundary. We have got 2562 document parts for PAN-PC-09 for Spanish, 4887 parts for the competition corpus for German, and 2562 parts for the competition corpus for Spanish. Because of time limitations we have omitted German in the development corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Problematic Sentences</head><p>Even with small document parts, there were paragraphs or sentences on which Google Translate stopped processing altogether. One such example was the line 6256 of source document 6696 from the competition corpus: unser Los. Und ich bin ja auch glücklich, wenn ich nur weiß, daß Moina sich vergnügt.&lt; Sie Google Translate has always stopped translating after the word Moina, no mater how many lines of text we have put before or after the above line.</p><p>When splitting the data further was not possible, we have translated the rest of the document, and put blank lines instead of the problematic ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Character Offset Calculation</head><p>We have modified our post-processing stage to recompute the offsets in detections of the translated source documents using line-to-line mapping to the original source documents. Offsets within a given line were linearly interpolated using the offsets of the first and last characters of that line.</p><p>For example, when the detection in the translated source document started at character 1000, which was 20th character out of 40 on line 125, we have mapped it to the offset of the character which was in the middle of line 125 in the original source document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance on the Training Corpus</head><p>After translating all the appropriate source documents from Spanish to English and removing the source documents in German we have re-ran the computation. The final score on the training corpus was the following:<ref type="foot" coords="6,321.74,241.16,3.49,6.05" target="#foot_4">5</ref> recall = 0.5386, precision = 0.5476, granularity = 1.0236, overall = 0.5340</p><p>The performance on the competition corpus is expected to be a bit higher, because of translating also the source documents from German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Intrinsic Plagiarism Detection</head><p>For the intrinsic detection part, the most important aspect was to not lower the overall score by using the generally low-performing intrinsic plagiarism detection. As a proofof-concept, we have reimplemented the approach of the PAN'09 winner of the intrinsic task, described in <ref type="bibr" coords="6,206.70,395.82,10.79,8.64" target="#b8">[9]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Outline</head><p>The intrinsic plagiarism detection system splits the document into a set of partly overlapping windows, and computes the style change function as a distance between the character trigram profile of the whole ocument and of the given window. Those areas of the suspicious document which have higher style change function values are then supposed to be plagiarized <ref type="foot" coords="6,240.74,505.71,3.49,6.05" target="#foot_5">6</ref> .</p><p>The score of the PAN'09 winner was the following: recall = 0.4607, precision = 0.2321, granularity = 1.3839, overall = 0.2462 Our implementation has not been able to match this score. The best overall score our implementation has reached was around 0.172.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Our Modifications</head><p>We have further improved the system by modifying how the plagiarized passages are detected from the style change function. Essentially we should only consider those areas, where the style change function rises from "generally low" values to "generally high" values and stays there for some interval.</p><p>In order to reach a better granularity and to detect steep changes in the style change function, we have computed the Gaussian-smoothed variants of this function:</p><formula xml:id="formula_0" coords="7,246.36,211.11,120.45,30.32">sc σ (x) = n i=1 1 √ 2πσ e -sc(x i ) 2 2σ 2</formula><p>For each document, we have computed two functions: for σ = 1, and for σ = 10. For each window, we have then computed the nearest local minimum and local maximum of the sc 1 (x) function.</p><p>As a beginning of the plagiarised passage we have considered the point where sc 1 (x) becomes higher than sc 10 (x), and the corresponding local minimum is low enough (lower than median of sc(x) + stddev of sc(x)), the corresponding local maximum is high enough (greater than median of sc(x) + 2.2 • stddev of sc(x)), and the sc(x) &gt; median of sc(x) + 1.7 • stddev of sc(x).   After the competition we did further experiments: we have found that the style change function is not as stable with varying document structure as the original paper states.</p><p>We have obtained a significant improvement when considering windows not with the fixed length in terms of characters, but in terms of trigrams instead (skipping trigrams with all non-letter characters). We suspect that using a different dissimilarity function to compute the style change function would be even better. One possibility is to compute the "out-of-place" n-gram profile distance as described in <ref type="bibr" coords="8,430.58,203.03,10.58,8.64" target="#b1">[2]</ref>. Another approach to explore would be to use the cosine similarity.</p><p>With the above modifications, we have been able to obtain a better overall score than the PAN'09 winner: recall = 0.2627, precision = 0.2969, granularity = 1.072, overall = 0.2562 However, we were not able to reach any significant improvement by using the intrinsic detector as a hint to the external plagiarism detection, partly because of time constraints. Thus in our final submission, the intrinsic detection was not used at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance of Our Approach</head><p>The system for external and cross-language plagiarism (without the intrinsic plagiarism detection) has been used to process the competition corpus. The resulting score earned us the first place in the competition: recall = 0.6915, precision = 0.9405, granularity = 1.0004, overall = 0.7968</p><p>We had the highest recall measure from all teams (the second best recall was 0.6907, by the Know-Center Graz team, which was classified at the third place). Also our granularity measure was the best (the second best was 1.0018 by the Universidad de Huelva team, which got the 6th place). Our precision value was the second best of all teams (the best precision of 0.9559 had the Anna University Chennai team on the 9th place).</p><p>The results are somewhat surprising, as they differ significantly from the performance on the development corpus. We have expected to have higher recall score on the competition corpus because of additional translations from German, but the difference is still too big. Also the precision score is higher than expected. Nevertheless, it is higher for many teams in the competition than it was in PAN'09. This could mean that they have improved their systems, or that the competition corpus is significantly different from PAN-PC-09.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Computational Cost</head><p>The performance of the system has been tested on two different computers: the development has been done on a main student's server of Faculty of Informatics, Masaryk University: it is HP DL-585 G6 with four six-core AMD Opteron 8439 SE processors at 2.8 GHz (24 cores total), 128 GB RAM. For purpose of comparison, we have also evaluated the system on the same hardware as in PAN'09 (SGI Altix XE), which has two quad-core Intel Xeon E5472 CPUs at 3.0 GHz (8 cores total), 64 GB RAM.</p><p>Our implementation is written in Perl (tokenization, generating chunks and their hashes, postprocessing), and in plain C (generating the inverted index, searching the inverted index).</p><p>In Table <ref type="table" coords="9,184.89,155.18,3.74,8.64" target="#tab_1">2</ref>, we present the times spent by processing the development and competition corpora. The time is divided into three parts: the first one is tokenizing the source documents and generating the inverted index, the second part is tokenizing the suspicious documents and looking up common chunks, and the last part is postprocessing (removing overlaps, joining adjacent detections, generating the XML files). 7 Future work</p><p>There are several areas where possible improvements of the approach presented in this paper can be present:</p><p>-N-gram profile distance function in the intrinsic plagiarism detector. Different means of computing the distance of the window profile to the profile of the whole document should be further evaluated with the expectations of reaching better stability on variable-sized documents and windows. -Using the intrinsic detector as a hint for the external detector, possibly allowing to detect also intrinsic plagiarism cases if the lower precision will not impact negatively the overall performance. -Density of common chunks. Instead of computing the valid intervals and then joining the detections in the post-processing phase, it could be feasible to allow wider gaps in the detection for either large detections (which we partly use), or for detections where the density of common chunks is higher. -Differences between the corpora. Much better score in the competition when compared to the training corpus is definitely a phenomenon which should be further explored. Because of late release of annotations for the competition corpus we are unable to do it in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have presented several improvements to the system for detecting plagiarism, which is described in <ref type="bibr" coords="9,197.57,644.48,10.58,8.64" target="#b3">[4]</ref>. The result was the best-performing system in the PAN 2010 plagiarism detection competition. The core of this system has also been in production use in the Czech National Archive of Graduate Theses <ref type="bibr" coords="10,344.79,119.31,16.60,8.64" target="#b9">[10]</ref> and several other production systems since 2008, handling milions of text documents. Some modifications we have described would be usable also in large-scale production systems, while others are made purely for the sake of the PAN competition. One such modification with limited impact outside the competition is our approach to detecting cross-language plagiarism. Using public translation services like Google Translate or Yahoo! Babelfish is not feasible for large sets of documents. On the other hand, we have verified that even machine translation can detect at least some cross-language plagiarism cases using the general-purpose external plagiarism detector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,230.43,600.24,154.50,8.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Smoothed style change functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,290.13,644.48,4.15,8.64"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,297.74,644.48,182.84,8.64;7,134.77,656.44,90.48,8.64"><head></head><label></label><figDesc>and 5.2, together with the detailed view of a single plagiarism case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,117.72,345.82,230.51"><head>Table 1 .</head><label>1</label><figDesc>Excluding passages with low percentage of letter characters detections with low percentage of letter characters were excluded. The measurements are in Table</figDesc><table coords="4,226.55,117.72,160.01,150.99"><row><cell cols="4">% of letters recall precision overall</cell></row><row><cell>baseline</cell><cell>0.5256</cell><cell>0.5302</cell><cell>0.5192</cell></row><row><cell>&gt; 0.550</cell><cell>0.5256</cell><cell>0.5323</cell><cell>0.5202</cell></row><row><cell>&gt; 0.575</cell><cell>0.5256</cell><cell>0.5327</cell><cell>0.5204</cell></row><row><cell>&gt; 0.600</cell><cell>0.5255</cell><cell>0.5335</cell><cell>0.5208</cell></row><row><cell>&gt; 0.625</cell><cell>0.5253</cell><cell>0.5348</cell><cell>0.5213</cell></row><row><cell>&gt; 0.650</cell><cell>0.5251</cell><cell>0.5373</cell><cell>0.5224</cell></row><row><cell>&gt; 0.675</cell><cell>0.5244</cell><cell>0.5420</cell><cell>0.5243</cell></row><row><cell>&gt; 0.700</cell><cell>0.5230</cell><cell>0.5501</cell><cell>0.5275</cell></row><row><cell>&gt; 0.725</cell><cell>0.5193</cell><cell>0.5639</cell><cell>0.5321</cell></row><row><cell>&gt; 0.750</cell><cell>0.5160</cell><cell>0.5810</cell><cell>0.5386</cell></row><row><cell>&gt; 0.775</cell><cell>0.3657</cell><cell>0.5946</cell><cell>0.4475</cell></row><row><cell>&gt; 0.800</cell><cell>0.0736</cell><cell>0.4846</cell><cell>0.1264</cell></row><row><cell>&gt; 0.825</cell><cell>0.0023</cell><cell>0.1747</cell><cell>0.0045</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,170.86,234.80,273.64,84.59"><head>Table 2 .</head><label>2</label><figDesc>Computational cost of the system (times as hours:minutes:seconds)</figDesc><table coords="9,185.56,234.80,244.23,63.76"><row><cell></cell><cell cols="2">Development corpus</cell><cell cols="2">Competition corpus</cell></row><row><cell>Task</cell><cell cols="4">8-core SGI 24-core HP 8-core SGI 24-core HP</cell></row><row><cell>Inverted index</cell><cell>1:06:02</cell><cell>0:12:41</cell><cell>0:49:44</cell><cell>0:10:28</cell></row><row><cell>Chunk pairs</cell><cell>2:07:25</cell><cell>0:20:44</cell><cell>1:39:20</cell><cell>0:15:55</cell></row><row><cell cols="2">Postprocessing 0:09:22</cell><cell>0:03:17</cell><cell>0:04:40</cell><cell>0:01:16</cell></row><row><cell>Total</cell><cell>3:22:55</cell><cell>0:36:42</cell><cell>2:33:44</cell><cell>0:27:39</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.08,267.47,7.77"><p>http://pan.webis.de/, see<ref type="bibr" coords="1,270.50,657.08,10.45,7.77" target="#b5">[6]</ref> for the overview of the competition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,645.96,335.86,7.93;2,144.73,657.08,305.05,7.77"><p>We refer to<ref type="bibr" coords="2,188.91,646.13,10.45,7.77" target="#b4">[5]</ref> for explanation of terms recall, precision, and granularity. Also the formula which is used to compute the overall score from these three values is described there.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,657.08,234.93,7.77"><p>In fact, we had the highest recall value from all teams in PAN'09.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,657.08,326.91,7.77"><p>as an example, the source document number 112 from PAN-PC-09 was detected as French.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,144.73,593.34,335.86,7.77;6,144.73,604.30,201.56,7.77;6,154.69,619.35,290.07,7.86"><p>After the competition, we have tried to modify the threshold value as described in Section 3.2. The best score we have got was with threshold of 0.725: recall = 0.5334, precision = 0.5693, granularity = 1.0230, overall = 0.5418</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,144.73,646.13,335.86,7.77;6,144.73,657.08,89.39,7.77"><p>This is an oversimplification. Refer to the original paper<ref type="bibr" coords="6,347.98,646.13,10.45,7.77" target="#b8">[9]</ref> for details on how the plagiarized passages are determined.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Pavel Rychlý</rs> for discussing the properties of n-gram profiling and intrinsic plagiarism detections with us. Also we thank the organizing comitee of the PAN 2010 competition for an interesting event.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,142.61,349.85,199.21,7.77" xml:id="b0">
	<monogr>
		<ptr target="http://babelfish.yahoo.com/" />
		<title level="m" coord="10,150.96,349.85,60.73,7.77">Yahoo! Babelfish</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,360.79,313.18,7.77;10,150.95,371.75,323.71,7.77;10,150.95,382.71,57.53,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,256.11,360.79,118.46,7.77">N-gram-based text categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,402.25,360.79,53.54,7.77;10,150.95,371.75,306.79,7.77">Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,393.65,199.29,7.77" xml:id="b2">
	<monogr>
		<ptr target="http://translate.google.com/" />
		<title level="m" coord="10,150.96,393.65,60.39,7.77">Google Translate</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,404.60,307.74,7.77;10,150.95,415.56,302.10,7.77;10,150.95,426.52,209.36,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,294.39,404.60,155.96,7.77;10,150.95,415.56,38.86,7.77">Finding plagiarism by evaluating document similarities</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Křipač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,207.50,415.56,245.56,7.77;10,150.95,426.52,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,437.46,310.16,7.77;10,150.95,448.42,280.40,7.77;10,150.95,459.38,314.05,7.77;10,150.95,470.34,140.48,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,344.98,437.46,107.78,7.77;10,150.95,448.42,73.99,7.77">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.10,448.42,188.25,7.77;10,150.95,459.38,197.58,7.77">Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010) (to appear)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (COLING 2010) (to appear)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">Aug 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,481.28,337.98,7.77;10,150.95,492.24,304.36,7.77;10,150.95,503.20,287.53,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,362.71,481.28,117.88,7.77;10,150.95,492.24,127.99,7.77">Overview of the 1st international competition on plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,296.90,492.24,158.42,7.77;10,150.95,503.20,228.23,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,514.14,251.07,7.77;10,150.95,525.10,144.44,7.77" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rivest</surname></persName>
		</author>
		<ptr target="http://www.rfc-editor.org/rfc/rfc1321.txt" />
		<title level="m" coord="10,190.95,514.14,174.34,7.77">RFC1321: The MD5 Message-Digest Algorithm</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,536.05,196.55,7.77;10,150.95,547.00,318.07,7.77;10,150.95,557.96,23.90,7.77" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schilli</surname></persName>
		</author>
		<ptr target="http://search.cpan.org/˜mschilli/Text-Language-Guess-0.02/lib/Text/Language/Guess.pm" />
		<title level="m" coord="10,226.08,536.05,108.77,7.77">Language::Guess Module</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,568.91,298.21,7.77;10,150.95,579.87,328.00,7.77;10,150.95,590.83,126.92,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,207.24,568.91,218.14,7.77">Intrinsic plagiarism detection using character n-gram profiles</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,579.87,328.00,7.77;10,150.95,590.83,58.64,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,601.77,276.05,7.77" xml:id="b9">
	<analytic>
		<title/>
		<ptr target="http://theses.cz/" />
	</analytic>
	<monogr>
		<title level="j" coord="10,150.96,601.77,156.60,7.77">Czech National Archive of Graduate Theses</title>
		<imprint>
			<date type="published" when="2008">2008-2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,612.71,337.18,7.77;10,150.95,623.67,307.14,7.77" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bauhaus-Universität</forename><surname>Webis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weimar</surname></persName>
		</author>
		<idno>PAN-PC-09</idno>
		<ptr target="http://www.webis.de/research/corpora" />
		<title level="m" coord="10,292.34,612.71,187.07,7.77;10,150.95,623.67,66.50,7.77">NLEL at Universidad Polytécnica de Valencia: PAN Plagiarism Corpus</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
