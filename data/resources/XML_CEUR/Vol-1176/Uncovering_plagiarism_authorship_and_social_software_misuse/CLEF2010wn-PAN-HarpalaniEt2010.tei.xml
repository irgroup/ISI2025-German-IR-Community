<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.39,115.90,298.59,12.90;1,218.28,135.75,178.79,10.75">Wiki Vandalysis -Wikipedia Vandalism Analysis Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.86,172.15,66.10,8.64"><forename type="first">Manoj</forename><surname>Harpalani</surname></persName>
							<email>mharpalani@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.62,172.15,76.53,8.64"><forename type="first">Thanadit</forename><surname>Phumprao</surname></persName>
							<email>tphumprao@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.47,172.15,50.07,8.64"><forename type="first">Megha</forename><surname>Bassi</surname></persName>
							<email>mbassi@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.05,172.15,51.29,8.64"><forename type="first">Michael</forename><surname>Hart</surname></persName>
							<email>mhart@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.75,172.15,51.76,8.64"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.39,115.90,298.59,12.90;1,218.28,135.75,178.79,10.75">Wiki Vandalysis -Wikipedia Vandalism Analysis Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C06C0E9665FD341E81F004322CF63CFA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wikipedia describes itself as the "free encyclopedia that anyone can edit". Along with the helpful volunteers who contribute by improving the articles, a great number of malicious users abuse the open nature of Wikipedia by vandalizing articles. Deterring and reverting vandalism has become one of the major challenges of Wikipedia as its size grows. Wikipedia editors fight vandalism both manually and with automated bots that use regular expressions and other simple rules to recognize malicious edits <ref type="bibr" coords="1,336.27,310.77,9.88,7.77" target="#b4">[5]</ref>. Researchers have also proposed Machine Learning algorithms for vandalism detection <ref type="bibr" coords="1,386.70,321.73,14.41,7.77" target="#b18">[19,</ref><ref type="bibr" coords="1,401.11,321.73,10.81,7.77" target="#b14">15]</ref>, but these algorithms are still in their infancy and have much room for improvement. This paper presents an approach to fighting vandalism by extracting various features from the edits for machine learning classification. Our classifier uses information about the editor, the sentiment of the edit, the "quality" of the edit (i.e. spelling errors), and targeted regular expressions to capture patterns common in blatant vandalism, such as insertion of obscene words or multiple exclamations. We have successfully been able to achieve an area under the ROC curve (AUC) of 0.91 on a training set of 15000 human annotated edits and 0.887 on a random sample of 17472 edits from 317443.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wikipedia defines vandalism as "any addition, removal, or change of content made in a deliberate attempt to compromise the integrity of Wikipedia" <ref type="bibr" coords="1,378.10,489.06,17.23,8.64" target="#b20">[21]</ref>. Vandalism can take many forms, including deleting all the content from a page, modifying a page to be so long that is becomes difficult to load, and inserting profanity, nonsense, unrelated information, inaccurate information, opinionated text, or spam.</p><p>Vandalism detectors attempt to automatically distinguish integrity-violating edits from integrity-preserving edits. Wikipedia currently uses a combination of manual and automated vandalism detection. The automated "bots" employed by Wikipedia use regular expressions and other simple rules to detect vandalism <ref type="bibr" coords="1,366.31,572.75,12.65,8.64" target="#b4">[5]</ref>. Researchers have previously suggested more advance vandalism detection algorithms based on content in the edit <ref type="bibr" coords="1,149.89,596.66,15.13,8.64" target="#b15">[16]</ref>, author information <ref type="bibr" coords="1,245.45,596.66,16.53,8.64" target="#b19">[20]</ref>, compression ratio of the article with and without the revision <ref type="bibr" coords="1,166.11,608.62,15.67,8.64" target="#b9">[10]</ref>, and Bayesian-classifiers built on the differences between the new and old revisions <ref type="bibr" coords="1,170.00,620.57,15.66,8.64" target="#b18">[19]</ref>. These early approaches leave much room for improvement.</p><p>This paper presents a machine-learning-based vandalism detector that uses several features to classify vandalism and that achieves an AUC score of over 0.887. Measuring the performance of the vandalism detection algorithm in terms of AUC instead of other measures makes sense as AUC score denotes the probability by which a classifier is able to distinguish a randomly sampled vandalism edit from a randomly sampled regular edit. Our results significantly outperform a baseline classifier based on a previous approach <ref type="bibr" coords="2,168.98,155.18,17.11,8.64" target="#b18">[19]</ref>. Our classifier uses several simple features to catch obvious "silly" forms of vandalism, such as inserting obscenities or long, repeating patterns of text. We also use subtler content-based features, such as misspelled words, grammatical errors, and changes in sentiment, which tend to indicate an edit violates the Wikipedia editing policy. Finally, the classifier uses information about the source of an edit, e.g. whether it is anonymous, to make its decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training corpus &amp; Feature Extraction</head><p>The training corpus for the classification was provided by the organizers of the PAN workshop <ref type="bibr" coords="2,169.83,291.57,13.15,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,182.98,291.57,13.15,8.64" target="#b13">14]</ref>. The corpus consisted of 15000 edits coupled with the previous revisions. Along with the training corpus in WikiMarkup format, we were also provided with meta-data including the edit id, the old revision id, the new revision id, the user name or IP of the author who performed the edit, the comment of the author, and whether the edit vandalized the article.</p><p>Wikipedia articles in the training set were formatted in WikiMarkup <ref type="bibr" coords="2,430.34,352.23,18.71,8.64">[22]</ref>. Wiki-Markup includes not only the content, but link and display instructions, much like HTML. For our purposes, we converted the WikiMarkup directly to plain text using the Bliki engine <ref type="bibr" coords="2,197.52,388.10,12.20,8.64" target="#b3">[4]</ref>, which eliminated the formatting and link information. Therefore, we are unable to detect template or link vandalism.</p><p>We focus on content based vandalism as it is the most prominent type of vandalism that exists. After analyzing samples of vandalized and regular edits, we observed that certain attributes of the edits distinguished vandalism from regular edits. We used the following features of the edits for classification: Edit Distance: Our classifier calculates the Damerau-Levenstein Distance using the LingPipe API <ref type="bibr" coords="2,188.00,473.56,13.83,8.64" target="#b1">[2]</ref> to determine the number of changes required to convert the old revision of an article to its new revision. Edit Type: Our classifier determines whether the edit inserted, deleted and/or modified text or a combination of these actions. Text Changes: Our classifier determines the edit length, word count, words inserted, words deleted, and words changed using java-diff <ref type="bibr" coords="2,336.51,535.11,11.01,8.64" target="#b8">[9]</ref>. It also uses LingPipe's English Sentence chunker to tokenize an article into sentences and calculate exact changes sentence-by-sentence using java-diff. Spelling Errors: Our classifier counts the number of apparent spelling mistakes in the edit and the ratio of spelling errors to correctly spelled words. Our spell-checking <ref type="bibr" coords="2,463.99,583.82,16.60,8.64" target="#b11">[12]</ref> software contained 200K English words, including named entities such as proper names and geographic places. Obscene Words: Our classifier enumerates the total number of obscene words in the edit and the ratio of obscene words to benign words. We started with a dictionary of obscene words <ref type="bibr" coords="2,191.31,644.48,13.45,8.64" target="#b2">[3]</ref> and manually added other obscene words that we observed frequently in vandalized edits of our training set. Repeated Patterns: "Silly" vandalism often employs repeated patterns like upper case words, exclamation marks, and repetition of words/letters (e.g. "heyyyyyy", "oh!!!!!", "wow wow wow", "hahahaha", "WIKIPEDIAAA"). Our classifier counts these patterns using the regular expressions. Grammatical errors: Wikipedia editors strive for good grammar, but vandals do not generally follow these rules. They may insert words or phrases into the middle of existing sentences or write ungrammatical sentences deliberately or unintentionally. Our classifier parses the edits into sentences that had been inserted or changed by using java-diff and LingPipe's sentence tokenizer and counts the grammatical errors in them using CMU's Link Grammar Parser <ref type="bibr" coords="3,275.96,227.91,11.66,8.64" target="#b6">[7]</ref>. Sentiment Analysis: Statements expressing opinions are common features of vandalism. Our classifier performs a sentiment analysis of the edit to uncover subjective or opinionated (positive or negative) text. We use LingPipe's Sentiment Analysis Tool trained on movie review data. The classifier counts objective or opinionated sentences and measures the change in total number of positive or subjective sentences. Sum of metrics: This simple but effective meta-feature is the sum of the number of Repeated Letters, Repeated Words, Capitalized Words and Multiple Exclamation Marks. Article History: Vandals do not necessarily perform vandalism across all articles at the same rate. Rather, some articles receive a disproportionate amount of vandalism than others (e.g. Michael Jackson). The feature is the number of times an article was vandalized in the previous 5000 edits on the article. We denote vandalism as a comment left by an editor that contains "reverted", "user" and "vandalism" in this order. We count also how many times an article was reverted, regardless if it was explicitly vandalism or not. Editor information: Knowing who contributed an edit can give us some expectation of the quality of the change. For example, we expect an active and registered Wikipedia editor with several thousand edits to be more trustworthy compared to an unregistered editor that is only identifiable from an IP address and who has not contributed before. Our classifier uses several editor-based features: whether the editor is registered or unregistered, how long the editor has been registered, the total number of contributions made to Wikipedia during the period that training data was collected, the total number of edits made to Wikipedia by the editor up to the date the training data was finalized, the number of reverts on previous revisions by the author deemed to be vandalism (using the same heuristic as for article history) and the total number of previous modifications made by the editor on the article they are revising. We were careful to make sure for edit totals to not include future modifications. If the user is not registered, the classifier uses their IP address as a proxy for their identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classifiers</head><p>Modeling Classifiers. We compare in this section two different models for building classifiers. We first investigate the Naive Bayes with Bag Of Words approach described in Smets et al <ref type="bibr" coords="3,190.84,644.48,15.02,8.64" target="#b18">[19]</ref>. We introduce a modification to this approach by combining Naive Bayes with Rank Boost <ref type="bibr" coords="3,225.32,656.44,12.45,8.64" target="#b5">[6]</ref>. We compare this to a second approach that uses the NBTree &lt;= 0.  classifier (a hybrid between decision trees and Bayesian classifiers) and the features described above.</p><p>Baseline: Naive Bayes with the Bag Of Words model. In order to set a baseline for comparison, we first started with classification of edits using the Bag of Words approach. We used Mallet <ref type="bibr" coords="4,228.37,385.68,17.04,8.64" target="#b12">[13]</ref> to create a vector space model based on unigrams from the edit difference containing inserted and changed text in the new revision of the article to create the Bag of Words and then used word count of the words after removing the stop words as a feature to the Naive Bayes classifier. We used Rank Boost on this weak classifier to maximize its area under the ROC curve. In this experiment, boosting the Naive Bayes classifier 15 times performed the best. Building classifiers. We built classifiers using the features presented in the previous section with three different algorithms: Naive Bayes, C4.5 Decision Trees <ref type="bibr" coords="4,441.72,469.37,16.35,8.64" target="#b17">[18]</ref>, and NBTrees <ref type="bibr" coords="4,167.99,481.32,18.99,8.64" target="#b10">[11]</ref> using Weka <ref type="bibr" coords="4,232.96,481.32,13.70,8.64" target="#b7">[8]</ref>, a machine learning and data mining library provided by University of Waikato. The NBTree performed the best among all the other classifiers. An NBTree is a decision tree with Bayesian classifiers as its leaves. The classifier builds a decision tree to optimally partition the training data to build several Bayesian classifiers that will perform better than either a decision tree or Bayesian classifier alone. We combined all the features described in the previous section and performed a 10-fold cross validation on all three algorithms. NBTree yielded the best result with the default settings, which uses 5-fold cross validation of Naive Bayes at a node to compute its utility and a split threshold of 5% of the relative reduction on error. Figure <ref type="figure" coords="4,448.24,576.96,4.98,8.64" target="#fig_0">1</ref> shows the NBTree we constructed from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation &amp; Results</head><p>Our test corpus contained 15000 edits. Registered users contributed 67% of these edits, while anonymous users contributed 33%. Among these 93.9% of edits were not in-  <ref type="table" coords="5,236.10,434.56,3.36,8.06">1</ref>. Top ten features ranked by information gain stances of vandalism. Not surprisingly, unregistered users more frequently vandalized articles than registered users, 16% to 1% respectively. See Figure <ref type="figure" coords="5,398.93,495.89,4.98,8.64" target="#fig_1">2</ref> for Venn diagrams which highlight how often an article had text modified, deleted, inserted, or any combination of these actions. Note that vandals are significantly more likely to insert text and are much less likely to make multiple changes in one revision.</p><note type="other">Regular Edits Vandalism</note><p>In Table <ref type="table" coords="5,186.76,546.28,3.74,8.64">1</ref>, we present our top 10 ranked features according to information gain. Table <ref type="table" coords="5,159.64,558.23,4.98,8.64" target="#tab_1">2</ref> compares the performance of our classifiers using stratified 10-fold cross validation on the training data. Table <ref type="table" coords="5,276.28,570.19,4.98,8.64">3</ref> presents the results on the PAN 2010 Wikipedia Vandalism test corpus <ref type="bibr" coords="5,220.05,582.14,12.20,8.64" target="#b0">[1]</ref>.</p><p>The authors of <ref type="bibr" coords="5,215.28,596.66,16.60,8.64" target="#b16">[17]</ref> observed that there are a small number of registered users (0.1%) that generate over half the content on Wikipedia. Therefore, we report not only the overall performance, but also statistics relating to important subsets of editors. This will let us gauge, for example, if the false positive rate is much too high to be acceptable for frequent contributors. We encourage future papers relating to the efficacy of vandalism detectors to analyze the performance for important subsets of editors. We report  <ref type="table" coords="6,207.02,286.70,3.36,8.06">3</ref>. Overall performance of classifiers on PAN 2010 testing data statistics for both registered and unregistered users as well as edits made by those users or IP addresses that frequently contribute to the article in Table <ref type="table" coords="6,388.04,343.85,3.74,8.64">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Our NBTree-based classifier outperforms the baseline classifier for two reasons. First, the features we selected -editor information, sentiment change, etc. -convey more information about each edit than the simple bag of words model. Second, by using an NBTree, we enable the classifier to partition the diverse space of edits into regions that are more amenable to Bayesian classification.</p><p>Five of our top ten features involved editor information. Current Wikipedia bots make limited use of editor information, but our results show that they could do more. In fact, our NBTree approach benefited most from how many contributions a particular Wikipedia User or IP address has made. Other important editor attributes were edit frequency, i.e. whether the editor is currently active, and whether the editor had contributed to the article being revised.</p><p>Another observation yielded by our highest ranked features is that simple features outperformed our more sophisticated features. The only sophisticated feature that appeared in the top ten ranked attributes was change in sentiment. Figure <ref type="figure" coords="6,431.62,560.80,4.98,8.64">3</ref> shows the values of the change in sentiment score. We note that for vandalizing edits, the mean change in sentiment was -0.14 with a standard deviation of 2.0 and for regular edits the mean change was 0.03 with a standard deviation of 1.1. Although most edits, vandalism and otherwise, had zero change in sentiment score, we note that vandalism skews towards a negative change in sentiment and regular edits positively, which appears to make sense intuitively. Because of time and resource constraints, we were not able to confirm if either A) vandalism introduces more polarizing than subjective statements, B) the polarity detector is more effective than the subjectivity detector software or C)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type of user FP rate Recall Precision</head><p>Registered users &lt; 0.1% 22.0% 68.4% Registered users that edited this article 10 times or more &lt;0.01% 0.0% 0.0% Unregistered users 3.9% 40.8% 67.2% IP addresses that edited this article 10 times or more 1.7% 33.3% 50.0% Table <ref type="table" coords="7,158.68,174.81,3.36,8.06">4</ref>. Performance of our classifier for registered and unregistered users as well as frequent contributors Figure <ref type="figure" coords="7,161.50,395.56,3.36,8.06">3</ref>. This box plot captures the change in sentiment between regular and vandalized edits. The outliers are shown as circles. The majority of edits had zero change in sentiment, but we do see that vandalized edits skew negatively and regular edits skew in a positive difference subjective or polarizing statements do not constitute a significant percentage of vandalism. We approached the more sophisticated features like the sentiment analysis with an off-the-shelf approach. Therefore, we used pre-existing datasets and software APIs to gather feature values. We hypothesize that employing Wikipedia specific examples may perform more precisely in detecting subjectivity and polarity in article revisions.</p><p>As mentioned before, the initial partition performed by the NBTree evaluated how many contributions a registered user or an IP address has made to Wikipedia. Interestingly the benefit of this feature is not exclusive to registered users. IP addresses that have contributed significantly in the past have a lower vandalism to regular edit ratio in this dataset than those IP addresses that have only contributed a handful of edits. In our training data, the ratio of regular edits to vandalized edits for IP addresses with more than 10000 previous edits and IP addresses with less than 100 previous edits is 76.0 and 3.6 respectively. We gathered information about 5 IP addresses in the training set with the most contributions and found that they were located near universities and one was owned by Microsoft (according to the reverse DNS). Therefore, what does this suggest for Wikipedia? Certain IP addresses and geographic areas are conducive to recruiting future Wikipedians -that these individuals have demonstrated a commitment to the in-tegrity of the site. This would grant more accountability and perhaps better expose the malicious edits from these "good" IP addresses. Two of our features unexpectedly behaved similarly to the edit distance feature. First, we note that the larger the edit distance, the more likely the edit is not vandalizing the article. We found that our grammar and misspelled words feature ultimately behaved in a similar fashion. For misspelled words, the larger the edit, the more likely our dictionary found misspelled words. Analyzing the top 10 edits with the most misspelled words (all non-vandalizing edits), we observed that these articles contained many non-English or unique pronouns. For example, one article on the movie "Blue Hills Avenue" had many misspellings because the dictionary could not recognize several of the character names. Therefore, we suggest for future dictionaries to build an article specific dictionary that incorporates words that survive several revisions or appear to be integral to the article (a summarization technique could be a good first approximation for this approach). Our grammar checker also had similar issues, arising from difficulties with part-of-speech tagging certain words. Another problem unique to the grammar checker is that several different types of tabulations and enumerations are used throughout Wikipedia. An obvious improvement for implementing a grammar checker is to ignore any modification that occurs in either a table, list or infobox. In future testing, we plan to implement these changes.</p><p>Our classifier had tremendous difficulty classifying vandalism by registered users. A sizable chunk of these vandalizing edits come from users that registered just before submitting the edit, and therefore are very suspicious, but the rest of these vandalizing edits are much more difficult to detect. Registered users infrequently vandalize articles, so we have little training data to learn from. We noted in two of ten cases that we manually inspected that there was a strong argument to not classify the edit as vandalism. For example, one edit introduced a link to "Sex Magic", which may appear on face value to be nonsense. On further investigation, it turns out that this is indeed an article in Wikipedia. As we are indebted to Potthast et al. for their dataset <ref type="bibr" coords="8,397.70,465.44,15.58,8.64" target="#b13">[14]</ref>, we recommend that the edits labeled vandalism by frequent and active contributors to be examined more closely by an active Wikipedian. Fortunately, this constitutes a small minority of edits, but detecting vandalism among registered users is a challenging obstacle. In the future, we will examine techniques for training with unbalanced datasets to see if we can further improve our detection for this type of vandalism.</p><p>Lastly, we hypothesize the success of the NBTree arises from the fact that vandalism is too diverse to be captured by a "one-size-fits-all" approach and requires partitioning the training space to find edits with common features. In our experiments, the Weka toolkit allowed us to test several different types of classifiers including meta-classifiers, neural networks, support vector machines and decision trees. Ultimately, decision trees fared better than other classification schemes and the C4.5 and NBTree the best. Some partitions in the NBTree, as seen in Figure <ref type="figure" coords="8,306.21,620.57,3.74,8.64" target="#fig_0">1</ref>, take advantage of the fact that it is highly unlikely that edits which meet a certain criterion will contain vandalism (e.g. a frequent contributor to Wikipedia). We hope to explore how we can further identify particular types of vandalism and build precise and effective classifiers for them automatically. This paper presents new features and algorithms to improve classification of vandalism. We found that features regarding information about the editor, misspellings, obscene words, detection of repetitive patterns and change in sentiment effectively classified vandalism. These features combined with the NBTree classifier gave an AUC score of 91% for a 10-fold stratified cross validation of our training corpus and 88.7% on the PAN 2010 testing corpus. We hope to expand upon these features and look for new ways to compare the content inserted, modified or deleted with previous revisions to determine if such actions constitute vandalism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,281.77,345.83,8.12;4,134.77,293.08,345.83,7.77;4,134.77,304.04,301.62,7.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The structure of the NBTree after building the classifier on all training examples. The branches of the tree denote the decision path taken along with the threshold chosen for taking a particular branch. The leaves are Naive Bayesian classifiers used to classify the edit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,273.67,345.83,8.12;5,134.77,284.97,190.11,7.77;5,136.16,115.84,180.00,132.63"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The above Venn diagrams show how often the text of an article was inserted into, changed, deleted or any combination of these actions</figDesc><graphic coords="5,136.16,115.84,180.00,132.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.82,117.72,345.73,177.04"><head>Table 2 .</head><label>2</label><figDesc>Overall performance of classifiers with 10 fold stratified cross validation on training set</figDesc><table coords="6,158.13,117.72,299.09,177.04"><row><cell>Metric</cell><cell cols="5">NB+BoW NB+BoW+RankBoost NB C4.5 Decision Tree NBTree</cell></row><row><cell>Precision</cell><cell>27.8%</cell><cell>34.1%</cell><cell>15.8%</cell><cell>53.2%</cell><cell>64.3%</cell></row><row><cell>Recall</cell><cell>32.6%</cell><cell>26.6%</cell><cell>93.2%</cell><cell>36.9%</cell><cell>36.4%</cell></row><row><cell cols="2">Accuracy 87.5%</cell><cell>89.7%</cell><cell>69.2%</cell><cell>94.1%</cell><cell>94.8%</cell></row><row><cell cols="2">F-measure 30.1%</cell><cell>29.9%</cell><cell>27.1%</cell><cell>43.6%</cell><cell>46.5%</cell></row><row><cell>AUC</cell><cell>69%</cell><cell>62%</cell><cell>88.5%</cell><cell>80.5%</cell><cell>91%</cell></row><row><cell></cell><cell>Metric</cell><cell cols="3">Naive Bayes C4.5 Decision Tree NBTree</cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>19.0%</cell><cell>51.0%</cell><cell>61.5%</cell><cell></cell></row><row><cell></cell><cell>Recall</cell><cell>92.0%</cell><cell>26.7%</cell><cell>25.2%</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>72.0%</cell><cell>91.6%</cell><cell>92.3%</cell><cell></cell></row><row><cell></cell><cell>F-measure</cell><cell>35.5%</cell><cell>35.1%</cell><cell>35.8%</cell><cell></cell></row><row><cell></cell><cell>AUC</cell><cell>86.6%</cell><cell>76.9%</cell><cell>88.7%</cell><cell></cell></row><row><cell cols="2">Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We would like to acknowledge <rs type="person">Tamara Berg</rs> and <rs type="person">Luis Ortiz</rs> for their guidance and suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,350.19,312.77,7.77;9,150.95,361.15,292.69,7.77;9,150.95,372.11,302.01,7.77" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename></persName>
		</author>
		<ptr target="http://www.uni-weimar.de/medien/webis/research/workshopseries/pan-10/task2-vandalism-detection.htm" />
		<title level="m" coord="9,194.22,350.19,261.16,7.77;9,150.95,361.15,33.39,7.77">Evaluation Campaign on Plagiarism Detection and Wikipedia Vandalism Detection</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,383.07,305.16,7.77" xml:id="b1">
	<monogr>
		<ptr target="http://alias-i.com/lingpipe/" />
		<title level="m" coord="9,150.96,383.07,82.80,7.77">Lingpipe 4.0.0</title>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
	<note>Alias-i</note>
</biblStruct>

<biblStruct coords="9,142.61,394.03,326.37,7.77;9,150.95,405.83,258.23,6.31" xml:id="b2">
	<monogr>
		<ptr target="http://expressionengine.com/?ACT=51&amp;fid=8&amp;aid=1830_mvLZ2WkoRucNvRegThbL&amp;board_id=" />
		<title level="m" coord="9,150.96,394.03,134.74,7.77">anonymous: English obscene wordlist</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,415.95,201.69,7.77;9,150.95,427.75,177.53,6.31" xml:id="b3">
	<monogr>
		<ptr target="http://code.google.com/p/gwtwiki/" />
		<title level="m" coord="9,150.96,415.95,186.06,7.77">axelclk: gwtwiki -Java Wikipedia API (Bliki engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,437.87,329.68,7.77" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carter</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/Wikipedia:CLUEBOT" />
		<title level="m" coord="9,190.44,437.87,28.12,7.77">ClueBot</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,448.84,302.94,7.77;9,150.95,459.80,108.06,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,320.25,448.84,125.30,7.77;9,150.95,459.80,81.92,7.77">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,470.76,332.31,7.77;9,150.95,481.72,295.98,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,287.08,470.76,161.70,7.77">A robust parsing algorithm for link grammars</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sleator</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.95,481.72,269.83,7.77">Proceedings of the Fourth International Workshop on Parsing Technologies</title>
		<meeting>the Fourth International Workshop on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,492.68,336.26,7.77;9,150.95,503.64,266.40,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,426.59,492.68,52.28,7.77;9,150.95,503.64,96.28,7.77">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,253.22,503.64,89.43,7.77">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,514.60,158.53,7.77;9,150.95,526.40,247.96,6.31" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><surname>Incava</surname></persName>
		</author>
		<ptr target="http://www.incava.org/projects/java/java-diff/" />
		<title level="m" coord="9,193.22,514.60,104.37,7.77">Difference algorithm for Java</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,536.52,329.52,7.77;9,150.95,547.48,290.03,7.77;9,150.95,558.44,329.55,7.77;9,150.95,569.40,82.28,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,259.27,536.52,212.49,7.77;9,150.95,547.48,47.04,7.77">Using dynamic markov compression to detect vandalism in the wikipedia</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Y</forename><surname>Itakura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.20,547.48,224.78,7.77;9,150.95,558.44,233.57,7.77">SIGIR &apos;09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="822" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,580.36,329.16,7.77;9,150.95,591.32,317.48,7.77;9,150.95,602.28,23.90,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,194.29,580.36,261.25,7.77">Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.95,591.32,266.32,7.77">Second International Conference on Knoledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,613.24,307.92,7.77;9,150.95,625.04,172.15,6.31" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Llc</surname></persName>
		</author>
		<ptr target="http://softcorporation.com/products/spellcheck/dictionaries/english/" />
		<title level="m" coord="9,186.57,613.24,64.20,7.77">English dictionary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,635.16,262.98,7.77" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,207.24,635.16,171.83,7.77">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,646.13,335.63,7.77;9,150.95,657.08,194.01,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,199.28,646.13,166.99,7.77">Crowdsourcing a Wikipedia Vandalism Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,384.73,646.13,93.13,7.77;9,150.95,657.08,129.21,7.77">33rd Annual International ACM SIGIR Conference (to appear)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-07">Jul 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,119.96,310.36,7.77;10,150.95,130.92,306.95,7.77;10,150.95,141.88,298.71,7.77;10,150.95,152.84,230.53,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,279.48,119.96,157.15,7.77">Automatic vandalism detection in wikipedia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,413.56,130.92,44.35,7.77;10,150.95,141.88,76.27,7.77">Advances in Information Retrieval</title>
		<title level="s" coord="10,232.93,141.88,126.47,7.77">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4956</biblScope>
			<biblScope unit="page" from="663" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,163.80,310.36,7.77;10,150.95,174.76,303.56,7.77;10,150.95,185.71,282.10,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,279.48,163.80,157.15,7.77">Automatic vandalism detection in wikipedia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,174.76,303.56,7.77;10,150.95,185.71,73.08,7.77">ECIR&apos;08: Proceedings of the IR research, 30th European conference on Advances in information retrieval</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="663" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,196.67,311.76,7.77;10,150.95,207.63,310.13,7.77;10,150.95,218.59,323.22,7.77;10,150.95,229.55,60.35,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,420.87,196.67,33.12,7.77;10,150.95,207.63,156.06,7.77">Creating, destroying, and restoring value in wikipedia</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Priedhorsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Panciera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,325.22,207.63,135.87,7.77;10,150.95,218.59,204.37,7.77">GROUP &apos;07: Proceedings of the 2007 international ACM conference on Supporting group work</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,240.51,322.54,7.77;10,150.95,251.47,65.99,7.77" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m" coord="10,197.28,240.51,135.61,7.77">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,262.43,338.35,7.77;10,150.95,273.39,318.76,7.77;10,150.95,284.34,306.96,7.77;10,150.95,295.30,223.43,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,287.11,262.43,193.48,7.77;10,150.95,273.39,101.67,7.77">Automatic vandalism detection in wikipedia: Towards a machine learning approach</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Smets</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.97,273.39,198.74,7.77;10,150.95,284.34,306.96,7.77;10,150.95,295.30,107.10,7.77">Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI) Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI08)</title>
		<meeting>the Association for the Advancement of Artificial Intelligence (AAAI) Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI08)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,306.26,338.35,7.77;10,150.95,317.22,324.68,7.77;10,150.95,328.18,228.55,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,266.68,306.26,213.91,7.77;10,150.95,317.22,71.38,7.77">Detecting wikipedia vandalism via spatio-temporal analysis of revision metadata</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.63,317.22,235.01,7.77;10,150.95,328.18,56.69,7.77">EUROSEC &apos;10: Proceedings of the Third European Workshop on System Security</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,339.14,137.64,7.77;10,150.95,350.94,258.23,6.31;10,134.77,361.06,106.12,7.77;10,150.95,372.86,242.09,6.31" xml:id="b20">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Help:Wiki_markup" />
		<title level="m" coord="10,150.96,339.14,125.03,7.77">Wikipedia: Definition of vandalism</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
