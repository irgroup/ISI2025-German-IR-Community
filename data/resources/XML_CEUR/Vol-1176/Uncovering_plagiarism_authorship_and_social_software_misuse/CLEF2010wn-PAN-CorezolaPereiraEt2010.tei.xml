<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.24,152.53,276.41,15.90;1,258.72,174.13,78.02,15.90;1,215.16,194.41,176.51,11.95">UFRGS@PAN2010: Detecting External Plagiarism Lab Report for Pan at CLEF 2010</title>
				<funder>
					<orgName type="full">INCT-Web</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq-Brazil</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.56,232.36,94.14,8.96"><forename type="first">Rafael</forename><forename type="middle">Corezola</forename><surname>Pereira</surname></persName>
							<email>rcpereira@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Informática</orgName>
								<orgName type="institution">Universidade Federal do Rio Grande do Sul (UFRGS</orgName>
								<address>
									<addrLine>Caixa Postal 15.064 -91.501</addrLine>
									<postCode>-970</postCode>
									<settlement>Porto Alegre -RS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.12,232.36,75.06,8.96"><forename type="first">Viviane</forename><forename type="middle">P</forename><surname>Moreira</surname></persName>
							<email>viviane@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Informática</orgName>
								<orgName type="institution">Universidade Federal do Rio Grande do Sul (UFRGS</orgName>
								<address>
									<addrLine>Caixa Postal 15.064 -91.501</addrLine>
									<postCode>-970</postCode>
									<settlement>Porto Alegre -RS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.08,232.36,61.06,8.96"><forename type="first">Renata</forename><surname>Galante</surname></persName>
							<email>galante@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Informática</orgName>
								<orgName type="institution">Universidade Federal do Rio Grande do Sul (UFRGS</orgName>
								<address>
									<addrLine>Caixa Postal 15.064 -91.501</addrLine>
									<postCode>-970</postCode>
									<settlement>Porto Alegre -RS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.24,152.53,276.41,15.90;1,258.72,174.13,78.02,15.90;1,215.16,194.41,176.51,11.95">UFRGS@PAN2010: Detecting External Plagiarism Lab Report for Pan at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FA1F3DD76D2725DE40CA49E8CB45299</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our approach to detect plagiarism in the PAN'10 competition. To accomplish this task we applied a method which aims at detecting external plagiarism cases. The method is specially designed to detect crosslanguage plagiarism and is composed by five phases: language normalization, retrieval of candidate documents, classifier training, plagiarism analysis, and post-processing. Our group got the seventh place in the competition with an overall score of 0.5175. It is important to notice that the final score was affected by our low recall (0.4036) which arose as a result of not detecting intrinsic plagiarism cases, which were also present in the competition corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our participation on the plagiarism detection task during the PAN competition at CLEF 2010. In order to detect the plagiarism cases present in the competition corpus we used the method described in <ref type="bibr" coords="1,325.08,450.16,10.69,8.96" target="#b5">[7]</ref>, which focuses on detecting plagiarism based on a reference collection. In particular, the method is specially designed to detected cross-language plagiarism, which is also present in the competition corpus. Thus, our task is to detect the plagiarized passages in the suspicious documents and their corresponding text passages in the source documents.</p><p>The method is composed by five phases: language normalization, retrieval of candidate documents, classifier training, plagiarism analysis, and post-processing. Since the method is also designed to detect cross-language plagiarism, an automatic translation tool is used to translate the documents into a common language. A classification algorithm is used to build a model that is able to differentiate a plagiarized text passage from a non-plagiarized one. Note that the use of classification algorithms is common in the area of intrinsic plagiarism analysis <ref type="bibr" coords="1,333.36,579.52,10.89,8.96" target="#b1">[2,</ref><ref type="bibr" coords="1,347.04,579.52,7.26,8.96" target="#b2">4]</ref>, but not in the area of external plagiarism analysis.</p><p>Based on the text passages extracted from the suspicious documents, an information retrieval (IR) system is used to retrieve the passages that are more likely to be the source of plagiarism cases. This is an important phase since the time necessary to perform a complete analysis of each suspicious document against all the documents in the reference collection would not be feasible. Only after the candidate passages of the source documents are retrieved, the plagiarism analysis is performed. Finally, a postprocessing technique is applied in the results in order to join the contiguous plagiarized passages.</p><p>The remainder of this paper is organized as follows: Section 2 presents the employed method. Section 3 describes how training was done and shows the results achieved in the competition. Finally, Section 4 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Method</head><p>We present here a brief description of how the method we used in the experiments works. A detailed description can be seen in <ref type="bibr" coords="2,309.96,235.12,10.69,8.96" target="#b5">[7]</ref>. The applied method is divided into five main phases, which are all briefly described below:</p><p>• Language Normalization: at this phase, the documents in the collection are translated into a default language so they can be analyzed in a uniform way. The English language was chosen as the default language. A language guesser is used to identify the documents that must be translated and an automatic translation tool is used to translate the documents.</p><p>• Retrieval of Candidate Documents: at this phase, an information retrieval system is used to retrieve, based on each suspicious document, the documents in the source collection that are candidates of being used as source of plagiarism. Before indexing the source documents, they are divided into several subdocuments, each one containing a single paragraph of the original document. Thus, when submitting a query to the system it will only return the relevant subdocuments, not the entire source document. For each passage in the suspicious document, the index is queried and the most relevant subdocuments are returned. These candidate subdocuments are the ones selected to be analyzed in the next phases of the method. It is important to notice that both the terms passage and subdocument represent a paragraph of the suspicious or source document.</p><p>• Feature Selection and Classifier Training: at this phase, a classification model is built to enable the method differentiates between a plagiarized and a nonplagiarized text passage. Thus, for each pair [suspicious passage, candidate subdocument] the following features are considered during the classifier training: (i) the cosine similarity between the suspicious passage and the candidate subdocument; (ii) the score assigned by the IR system to the candidate subdocument; (iii) the position of the candidate subdocument in the rank returned by the IR system; (iv) the length (in characters) of both the suspicious passage and the candidate subdocument. Note that a training collection (with the plagiarism cases annotated) must be supplied in order to create the training instances to train the classifier.</p><p>• Plagiarism Analysis: at this phase, for each pair [suspicious passage, candidate subdocument] we extract the necessary information to create the test instance and pass it to the classifier. Thus, the classifier is able to decide whether the suspicious passage is plagiarized from the candidate subdocument.</p><p>• Post-Processing: at this phase, the detection results of each suspicious document are post-processed to join the contiguous plagiarized passages. The goal is to report a plagiarism case as a whole instead of several small plagiarism cases. The following heuristic is applied: (i) separate the detections in groups, each group containing the detections referring to a single source document; (ii) for each group, sort the detections in order of appearance in the suspicious document; (iii) join adjacent detections that are close to each other (less than a pre-defined num-ber of characters); (iv) for each plagiarized passage, keep only the detection with the largest length in the source document, i.e., do not report more than one possible source of plagiarism for the same passage in the suspicious document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setting up the detector</head><p>In order to tune our plagiarism detector to the PAN'10 competition, we used the PAN-PC-09 <ref type="bibr" coords="3,154.08,255.76,11.71,8.96" target="#b0">[1]</ref> training corpus, which is a large-scale corpus containing artificial plagiarism offenses. It is important to mention that all the steps presented here are the same ones described in <ref type="bibr" coords="3,196.68,279.28,10.69,8.96" target="#b5">[7]</ref>, the only difference is that we analyzed a different corpus.</p><p>As in <ref type="bibr" coords="3,160.32,291.04,11.71,8.96" target="#b5">[7]</ref> , we used the Terrier Information Retrieval Platform <ref type="bibr" coords="3,387.00,291.04,11.71,8.96" target="#b4">[6]</ref> as our IR system. We also employed the same IR techniques: the TF-IDF weighting scheme, stop-word removal (a list of 733 words included in the Terrier Platform), and stemming <ref type="bibr" coords="3,442.68,314.56,27.91,8.96;3,124.80,326.32,49.09,8.96">(Porter Stemmer [8]</ref>). To train our classifier, we used the Weka Data Mining Software <ref type="bibr" coords="3,445.32,326.32,10.69,8.96">[9]</ref>. In particular, we applied the J48 classification algorithm to build the classifier.</p><p>We divided the source documents into several subdocuments before translation in order to keep the original offset and length of each passage in the original document. As mentioned before, during the language normalization phase, we translate all the non-English documents in the corpus to English. We used LEC Power Translator 12 <ref type="bibr" coords="3,124.80,396.88,11.71,8.96" target="#b3">[5]</ref> as our translation tool and the Google Translator [3] as our language guesser. After all documents in the reference collection are divided into subdocuments and translated into English, the collection is indexed. To reduce index size and speed up retrieval, only the subdocuments longer then 250 characters were indexed.</p><p>Before analyzing each document, we first have to train the classifier. To accomplish this, we randomly selected 50 suspicious documents. For each suspicious passage the top ten candidate subdocuments were retrieved. Based on each pair [suspicious passage, candidate subdocument], we can extract the information necessary to create the 500 training instances. The annotations provided with the corpus allowed us to check if the suspicious passage was actually plagiarized from the candidate subdocument. After the training instances were created, we generated the ARRF (Attribute-Relation File Format) file containing the training instances according to the Weka file format. Once we have the ARRF file with examples of plagiarized and nonplagiarized passages, we applied the J48 classification algorithm to build the classification model. After the classifier is trained, we can proceed to the analysis of the suspicious documents of the training corpus.</p><p>To analyze the suspicious documents, we divided them into passages. For each passage, we queried the index to get the top ten candidate subdocuments. Thus, for each pair [suspicious passage, candidate subdocument] we extracted the information needed by the classifier, and let it decide whether the suspicious passage was, in fact, plagiarized from the candidate subdocument. After we analyzed all the suspicious documents, we post-processed the results to join the contiguous plagiarized passages according to the heuristic described previously.</p><p>The parameters shown on Table <ref type="table" coords="3,267.00,667.36,4.98,8.96" target="#tab_0">1</ref> were defined based on tests with the training corpus. These same parameters were used for analyzing the competition corpus. As shown in Table <ref type="table" coords="4,216.84,265.24,3.76,8.96" target="#tab_0">1</ref>, to reduce the index size and speed up retrieval, we only indexed the subdocuments with length greater than 250 characters. The IR system returned at most 10 candidate subdocuments for each suspicious passage. Also, to speed retrieval, instead of using all the terms of the suspicious passage to query the index, we discarded the terms which had an IDF (inverse document frequency) value lower than 8. We also discarded the subdocuments that received (by the IR system) a score lower than 11. Finally, in the post-processing phase, we joined the contiguous plagiarized passages that were at most 3000 characters distant from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>In order to analyze the competition corpus, we proceeded the same way described in the previous section. Note that we used the same classifier built during the analysis of the training corpus. Table <ref type="table" coords="4,231.96,403.48,4.98,8.96" target="#tab_1">2</ref> shows our overall result in the competition as well as the result of the analysis when considering only the external plagiarism cases. Note that since the competition corpus had both external and intrinsic plagiarism cases mixed up, the recall value ended up getting affected since the applied method was designed to detect only external plagiarism cases.</p><p>With the final score of 0.5175 our group got the seventh place in the competition. Table <ref type="table" coords="4,150.60,474.04,4.98,8.96" target="#tab_2">3</ref> shows an in-depth analysis of the results. We provide an overall analysis considering the results of the competition and we also analyze our results in detecting only the external plagiarism cases (which is the focus of the applied method). To analyze in which situations the method performs better, we investigated how well it handles text obfuscation and in what level the length of the plagiarized passage affects its overall performance. We divided the plagiarized passages according to their textual lengths: short (less than 1500 characters), medium (from 1501 to 5000 characters), and large (greater than 5000 characters).</p><p>According to Table <ref type="table" coords="4,220.08,568.12,3.76,8.96" target="#tab_2">3</ref>, during the competition the method detected 29,486 out of 68,558 plagiarized passages (i.e., 43%). When ignoring the intrinsic plagiarism cases, the method detected 29,486 out of 55,723 plagiarized passages (i.e., 53%). It is possible to see that the method performed poorly while detecting short plagiarized passages. This is partially explained by our decision of indexing only the subdocuments with length greater than 250 characters (to speed up retrieval). Table <ref type="table" coords="4,417.00,626.92,4.98,8.96" target="#tab_2">3</ref> also shows that, other than translation, the intrinsic plagiarism cases did not suffered any kind of obfuscation. While detecting medium plagiarized passages, the performance of the method decreased as the level of obfuscation increased (none to high). It is worth noticing that the translated and the simulated plagiarized passages did not seem to have a negative impact in the performance of the method, since the percentage of the passages detected is not lower than for the other types of obfuscation. Finally, when detecting large plagiarized passages the method detected almost all of them, regardless of the type of obfuscation (note that that were no large simulated plagiarized passage). This paper described our approach to the plagiarism detection task during the PAN competition at CLEF 2010. We applied the method presented in <ref type="bibr" coords="6,384.72,181.84,10.69,8.96" target="#b5">[7]</ref>, which focuses on detecting external plagiarism. In particular, the method is specially designed to detect cross-language plagiarism, which is also present in the competition corpus. We used the training corpus PAN-PC-09 to set up the detector. The training corpus was also used to build the classification model used during the analysis of the competition corpus. With an overall score of 0.5175 we ended up in the seventh place in the competition. Our overall score was affected by our low recall (0.4036) since the applied method was designed to detect only the external plagiarism cases, leading the detector to ignore the intrinsic plagiarism cases present in the competition corpus.</p><p>An in-depth analysis was conducted to check in what situations the method performs better. Regarding the textual length of the plagiarized passage, the larger is the passage the easier is the detection. In fact, when analyzing large plagiarized passages the method detected almost all of them, regardless of the type of obfuscation. However, the method performed poorly while detecting short passages. We attribute this low performance to the fact that we only indexed subdocuments with length greater than 250 characters. Finally, the translated and the simulated plagiarized passages did not seem to have a negative impact in the performance of the method, since the percentage of the passages detected are not lower than the other types of obfuscation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,181.20,150.04,227.46,100.16"><head>Table 1 . Method Parameters.</head><label>1</label><figDesc></figDesc><table coords="4,181.20,168.16,227.46,82.04"><row><cell>Retrieval Parameters</cell><cell></cell></row><row><cell>Subdocument length (in characters)</cell><cell>250</cell></row><row><cell>Subdocuments retrieved per suspicious passage</cell><cell>10</cell></row><row><cell>IDF threshold</cell><cell>8</cell></row><row><cell>IR score threshold</cell><cell>11</cell></row><row><cell>Post-Processing Parameters</cell><cell></cell></row><row><cell>Merge threshold (in characters)</cell><cell>3000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,189.48,203.08,235.23,88.16"><head>Table 2 . Overall results.</head><label>2</label><figDesc></figDesc><table coords="5,189.48,221.20,235.23,70.04"><row><cell>---</cell><cell cols="2">Competition Only External Cases</cell></row><row><cell>Recall</cell><cell>0.4036</cell><cell>0.4966</cell></row><row><cell>Precision</cell><cell>0.7242</cell><cell>0.7242</cell></row><row><cell>F-Measure</cell><cell>0.5183</cell><cell>0.5892</cell></row><row><cell>Granularity</cell><cell>1.0024</cell><cell>1.0017</cell></row><row><cell>Final Score</cell><cell>0.5175</cell><cell>0.5881</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,137.88,312.28,316.26,371.04"><head>Table 3 . In-depth analysis of the results.</head><label>3</label><figDesc></figDesc><table coords="5,137.88,330.21,316.26,353.10"><row><cell></cell><cell></cell><cell cols="3">Short Plagiarized Passages</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell></cell><cell>Competition</cell><cell></cell><cell></cell><cell>Only External</cell><cell></cell></row><row><cell>Obfuscation</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell></row><row><cell>None</cell><cell>78</cell><cell>9395</cell><cell>0.83</cell><cell>78</cell><cell>4088</cell><cell>1.90</cell></row><row><cell>Low</cell><cell>63</cell><cell>3798</cell><cell>1.65</cell><cell>63</cell><cell>3798</cell><cell>1.65</cell></row><row><cell>High</cell><cell>37</cell><cell>3729</cell><cell>0.99</cell><cell>37</cell><cell>3729</cell><cell>0.99</cell></row><row><cell>Translated</cell><cell>194</cell><cell>2417</cell><cell>8.02</cell><cell>194</cell><cell>1754</cell><cell>11.06</cell></row><row><cell>Simulated</cell><cell>211</cell><cell>2362</cell><cell>8.93</cell><cell>211</cell><cell>2362</cell><cell>8.93</cell></row><row><cell></cell><cell></cell><cell cols="3">Medium Plagiarized Passages</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell></cell><cell>Competition</cell><cell></cell><cell></cell><cell>Only External</cell><cell></cell></row><row><cell>Obfuscation</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell></row><row><cell>None</cell><cell>2509</cell><cell>9907</cell><cell>25.32</cell><cell>2509</cell><cell>5911</cell><cell>42.44</cell></row><row><cell>Low</cell><cell>1832</cell><cell>4722</cell><cell>38.79</cell><cell>1832</cell><cell>4722</cell><cell>38.79</cell></row><row><cell>High</cell><cell>1415</cell><cell>4752</cell><cell>29.77</cell><cell>1415</cell><cell>4752</cell><cell>29.77</cell></row><row><cell>Translated</cell><cell>980</cell><cell>2358</cell><cell>41.56</cell><cell>980</cell><cell>1851</cell><cell>52.94</cell></row><row><cell>Simulated</cell><cell>268</cell><cell>624</cell><cell>42.94</cell><cell>268</cell><cell>624</cell><cell>42.94</cell></row><row><cell></cell><cell></cell><cell cols="3">Large Plagiarized Passages</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell></cell><cell>Competition</cell><cell></cell><cell></cell><cell>Only External</cell><cell></cell></row><row><cell>Obfuscation</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell><cell>Detected</cell><cell>Total</cell><cell>%</cell></row><row><cell>None</cell><cell>6755</cell><cell>8733</cell><cell>77.35</cell><cell>6755</cell><cell>6785</cell><cell>99.55</cell></row><row><cell>Low</cell><cell>6343</cell><cell>6363</cell><cell>99.68</cell><cell>6343</cell><cell>6363</cell><cell>99.68</cell></row><row><cell>High</cell><cell>6171</cell><cell>6275</cell><cell>98.34</cell><cell>6171</cell><cell>6275</cell><cell>98.34</cell></row><row><cell>Translated</cell><cell>2630</cell><cell>3123</cell><cell>84.21</cell><cell>2630</cell><cell>2709</cell><cell>97.08</cell></row><row><cell>Simulated</cell><cell>0</cell><cell>0</cell><cell>100.00</cell><cell>0</cell><cell>0</cell><cell>100.00</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This project was partially supported by <rs type="funder">CNPq-Brazil</rs> and <rs type="funder">INCT-Web</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,128.20,463.29,342.45,8.10;6,135.84,473.85,334.77,8.10;6,135.84,484.41,223.17,8.10" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,318.72,463.29,151.93,8.10;6,135.84,473.85,68.06,8.10">Universidad Politécnica de Valencia PAN Plagiarism Corpus</title>
		<author>
			<persName coords=""><forename type="first">Bauhaus-Universität</forename><surname>Webis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp;</forename><surname>Weimar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nlel</surname></persName>
		</author>
		<idno>PAN-PC-09</idno>
		<ptr target="http://www.webis.de/research/corpora" />
		<editor>M. Potthast, A. Eiselt, B. Stein, A. Barrón-Cedeño, and P. Rosso</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.20,494.97,342.37,8.10;6,135.84,505.53,334.82,8.10;6,135.84,516.09,184.41,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,244.56,494.97,226.02,8.10;6,135.84,505.53,38.58,8.10">Measuring the Usefulness of Function Words for Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Levitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,192.24,505.53,278.42,8.10;6,135.84,516.09,39.90,8.10">Association for Literary and Linguistic Computing/ Association Computer Humanities</title>
		<meeting><address><addrLine>, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University Of Victoria</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.20,537.21,342.37,8.10;6,135.84,547.77,334.79,8.10;6,135.84,558.33,39.93,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,229.44,537.21,227.64,8.10">Authorship Verification as a One-Class Classification Problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,135.84,547.77,262.07,8.10">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.21,568.89,272.92,8.10" xml:id="b3">
	<monogr>
		<ptr target="http://www.lec.com/power-translator-software.asp" />
		<title level="m" coord="6,133.92,568.89,42.24,8.10">LEC Power</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.20,579.45,342.43,8.10;6,135.84,590.01,334.74,8.10;6,135.84,600.57,177.33,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,410.04,579.45,60.60,8.10;6,135.84,590.01,84.47,8.10">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,237.96,590.01,232.62,8.10;6,135.84,600.57,71.26,8.10">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.20,611.13,342.43,8.10;6,135.84,621.69,334.80,8.10;6,135.84,632.25,328.41,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,300.84,611.13,169.79,8.10;6,135.84,621.69,47.92,8.10">A New Approach for Cross-Language Plagiarism Analysis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Galante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,200.04,621.69,270.60,8.10;6,135.84,632.25,124.42,8.10">Proceedings of the CLEF 2010 Conference on Multilingual and Multimodal Information Access Evaluation</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Agosti</surname></persName>
		</editor>
		<meeting>the CLEF 2010 Conference on Multilingual and Multimodal Information Access Evaluation<address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.20,642.81,342.40,8.10;6,135.84,653.37,114.93,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,185.16,642.81,119.47,8.10">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,321.24,642.81,122.53,8.10">Readings in information retrieval</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
