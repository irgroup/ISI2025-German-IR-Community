<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,214.89,115.90,185.58,12.90;1,218.28,135.79,178.79,10.75">ZOT! to Wikipedia Vandalism Lab Report for PAN at CLEF 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,236.85,172.23,51.19,8.64"><forename type="first">James</forename><surname>White</surname></persName>
							<email>&lt;jpwhite@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.41,172.23,71.09,8.64"><forename type="first">Rebecca</forename><surname>Maessen</surname></persName>
							<email>rmaessen&gt;@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,214.89,115.90,185.58,12.90;1,218.28,135.79,178.79,10.75">ZOT! to Wikipedia Vandalism Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A4E260D3D602C1DB5D647AE229C4B95</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This vandalism detector uses features primarily derived from a wordpreserving differencing of the text for each Wikipedia article from before and after the edit, along with a few metadata features and statistics on the before and after text. Features computed from the text difference are then a combination of statistics such as length, markup count, and blanking along with a selected number of TFIDF values for words and bigrams. Our training set was expanded from that supplied for the shared task to include the 5K vandalism edit corpus from West et al. Vandalism edits in the training set that were classified as "regular" by a classifier trained on all the data were removed from the training set used for the final classifier. Classification was performed using bagging of the Weka J48graft (C4.5) decision tree [3] which resulted in an evaluation score of 0.84340 AUC. It is unclear whether the expanded vandalism data improved or degraded performance because that changed the ratio of regular to vandalism edits in the training set and we did not make any adjustment for that when training the classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although it has been shown by West et al <ref type="bibr" coords="1,304.01,456.76,11.62,8.64" target="#b3">[4]</ref> that metadata features alone can identify some vandalism, there are many edits that can only classified accurately by examining the text. The design of this Wikipedia vandalism detector largely follows that of Potthast et al <ref type="bibr" coords="1,189.76,492.63,11.62,8.64" target="#b1">[2]</ref> and Amit Belani <ref type="bibr" coords="1,273.58,492.63,11.62,8.64" target="#b0">[1]</ref> in that it combines a modest number of features derived from metadata and textual statistics with a bag-of-words approach to classification. Rather than use term counts in the article before and after edit as in Belani, we perform textual differencing and use characteristic stemmed terms and bigrams from that inserted text. We also compute statistics regarding edit size and changes to Medi-aWiki markup which prove to be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Sources and Training Edits</head><p>The training set provided for the PAN @ CLEF vandalism detection task is a corpus of 15,000 training cases with 944 classified as vandalism, which comprise 6% of the total training set. This is comparable to the real world ratio of 'good' to 'bad' edits on Wikipedia. The small number of vandalism edits in this unbalanced data provide little information to automatically generalize over the entire Wikipedia. In order to find more patterns specific for vandalism we decided to include additional vandalism edits into our data set. The West et al data set (http://www.cis.upenn.edu/ westand/) contains 5.7million automatically determined offending edits along with 5,286 manually confirmed vandalism edits. We used these 5,286 manually classified vandalism edits to expand our training set. After expansion, our training dataset holds 14,076 regular edits and 6,207 vandalism edits. Compared to 6% ill-intended edits in the provided training set, this expanded set contains 31% vandalism edits (close to the ratio Potthast et al used in their work <ref type="bibr" coords="2,209.37,203.00,10.45,8.64" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Preparation</head><p>The data set as provided has few nominal or numerical features directly suitable for the regular vs. vandalism classification task. We have focused on deriving features using text analysis methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Differencing</head><p>The primary text for each edit consists of the entire article text (in WikiMedia "wikimarkup" format) from before the edit and after the edit. While we do use some features based on those whole text versions of the article (such as length and markup counts), much of our classification approach is based on trying to characterize just the new text added by the editor in this edit. The reason for trying to just deal with the edit's new text is that most (but certainly not all) vandalism consists of inserting vulgar or nonsense words and phrases into articles.</p><p>We searched for text differencing algorithms (with implementations) and tried several different methods in order to find one well suited to our task. A common problem with line-oriented text differencing algorithms is that they do not handle lines that get split into two (or more) lines very well. Usually in that case at least one of the "new" lines will be flagged as the insertion of text. But we want to recognize that all of the text in both lines is from the original, otherwise the text actually inserted by the vandal will be dilute by the (presumably) "regular" text from the old version of the article. The implementation we used is "google-diff-match-patch" by Neil Fraser (http://code.google.com/p/google-diff-match-patch/). It does a character-oriented diff with some line-oriented optimizations. There are also a couple of "clean up" methods and we are using the "semantic clean up" which employs useful heuristics. The results are not quite ideal for this application though, because (unlike sentences) we would like to get complete words when any part of a word is changed so that our word-oriented features will match properly. We implemented a character-for-word substitution codec in order to achieve that behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Vectors</head><p>One set of features is derived from the text inserted by the edit (as determined by the text differencing routine described above) by using the Word Vector Tool for Rapid-Miner (http://wvtool.sf.net/). In order to put the text in the form required by the tool, the inserted text for each edit is put into a separate file (using the editid as the file name) and with the "vandalism" edits in one directory and the "regular" edits in another. The current settings are TFIDF (term frequency/inverse document frequency), prune below 3 (terms must appear in at least three documents), string tokenization, Snowball stemming, and N-grams up to length 2.</p><p>The Word Vecor tool resulted in a word vector of 29,106 attributes. We were able to run logistic regression on this data in a reasonable time. However, when we added the metadata features to the word vector our classifications took too long to run, even though this was an addition of only 20 attributes. To make our data more manageable, we used CorpusBasedWeighting and AttributeWeightSelection tools in RapidMiner to select the words from the word vector with the most significance to our classification by choosing 300 features from each of the two classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Historical Information</head><p>Some useful metadata is time sensitive, such as the length of time an editor has been using Wikipedia, the number of edits they've made, and whether they've vandalized Wikipedia before. In order to ensure that we don't use "future" information in our classification, calculation of those features is performed in order based on the 'edittime' feature. Due to the limited time span covered by the training set, we did not actually gain much information using these computed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Edit Features</head><p>The raw metadata features and article text files were processed to generate the following features:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anonymous editor</head><p>A registered Wikipedia user is less likely to vandalize. If the editor displays an IP address the edit is made by an unregistered user. The value of this feature is 1 if the editor feature matches the pattern for an IP address, 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Editor edit count</head><p>The number of edits the editor has made prior to this one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Editor revert count</head><p>The number of edits the editor has made that have been reverted prior to this one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revert comment</head><p>1 if editcomment feature contains a pattern matching "revert" (ignoring case), 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vandal comment</head><p>1 if editcomment feature contains a pattern matching "vandal" (ignoring case), 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is a reversion</head><p>1 if this edit is a reversion to a prior version of the article, 0 otherwise. An edit is considered to be a reversion if the 'Revert comment' or 'Vandal comment' features are true, or if the MD5 hash of the new version of the article text matches that seen for a prior version of the article.</p><p>We used a decision tree stucture to distinguish vandalism edits from regular edits. The datamining tool RapidMiner v4.6 functioned as a framework for our algorithm. We used the Weka J48-graft classifier to generate a C4. 5 decision tree <ref type="bibr" coords="5,378.34,167.01,10.58,8.64" target="#b2">[3]</ref>. Parameters that affect the results of the classification are the level of pruning and the minimum number of instances per leaf in the tree. The defaults were used for all settings (C=0.25, M=2, all option flags set to false). To improve our results we used bagging of that learner. The parameters are number of iterations (11) and the fraction of examples (0.9) selected for each iteration.</p><p>The classifier was trained two times. The first training pass used all the training data and was then used to classify the training data. Vandalism edits that were classified as "regular" by that classifier were removed from the data set used to train the final classifier. The rationale being that because our classifier would be unable to classify certain types of vandalism as vandalism (e.g. those that either have Neutral Point Of View issues or are carefully disguised pranks) that precision would improve by not using those cases. Cross-validation testing showed a substantial improvement in performance using this method.</p><p>Decision trees are interesting classifiers because they have the potential for being implemented in such a way that expensive-to-calculate features are only evaluated when they would be useful, which would be a consideration for a production implementation of the classifier for Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluated our classifier by running a ten-fold cross validation on our training set. The 10-fold cross validation on our training data using the W-J48graft learner and bagging resulted in an f-measure of 82.03 and an AUC of 0.960. Using the trained learner on the test data gave an AUC of 0.84340.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Source Code</head><p>The source code for our implementation is a combination of Groovy and RapidMiner scripts and is available via Subversion under the GPL v2 license. Go to http://code.google.com/p/zot2wpv/ for access details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The research that we performed shows that decision trees are a good approach to find ill-intended Wikipedia edits. Additionally, decision trees train a lot faster than other classifiers that we looked at, such as logistic regression. Efficient performance is an important consideration for a production implementation of the classifier for Wikipedia.</p><p>Our findings confirm that top down feature analysis on text difference as well as statistical information from the word vector is relevant to classifying vandalism edits.</p><p>We were able to demonstrate this by classifying using only the metadata features or the word vector features first, and subsequently run the same classifiers on the features combined. The fact that neither attribute set is as predictive as the attribute sets together indicates that both type of features are significant for predicting vandalism.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>Most of this work was performed in the course of CS175 at <rs type="institution">UC Irvine</rs> taught by <rs type="person">Arthur Asuncion</rs> during Spring 2010. We also thank <rs type="person">Professor Bill Tomlinson</rs> for his support.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comment length</head><p>The length of the 'Edit comment' after removal of the automatically generated section label ("/* SECTION */"), if any. This is a valuable feature because many vandals leave no comment about their edit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Old article size</head><p>Number of characters in the article text prior to the edit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New article size</head><p>Number of characters in the article text after the edit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size difference</head><p>Difference in the number of characters in the article text (new -old). Vandalism tends to be short, but even very lengthy vandalism is rarely more than 1000 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size difference ratio</head><p>Difference in the number of characters as a ratio (if old != 0, (new -old)/old).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Old markup count</head><p>A measure of the amount of wikimarkup in the article text prior to the edit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New markup count</head><p>A measure of the amount of wikimarkup in the article text after the edit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markup count difference</head><p>Difference in the amount of wikimarkup in the article text (new -old). Vandals usually do not use wikimarkup in their additions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markup count difference ratio</head><p>Difference in the amount of wikimarkup as a ratio (if old != 0, (new -old)/old).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is article blanked</head><p>1 if the new article text is blank. Blanking an entire article is very often vandalism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is article replacement</head><p>1 if the new text is effectively a complete replacement for the old text, 0 otherwise. Note that the determination of what constitutes a "complete replacement" is based on the text differencing logic which tries to distinguish between coincidental similarity and actual changes. Is section replacement 1 if the edit completely replaced an existing section with new text (including removing the previous section header), 0 otherwise. This is characteristic of many vandalizing edits because of the tendency for vandals to click the "edit" button for a section and then replace the entire section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is section content replacement</head><p>Section header is preserved in the new text. Another common pattern for vandals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All CAP word count</head><p>The number of "words" (currently runs of at least 4 alphabetic characters) that are all capitals. Regular edits have few words longer than 3 letters that are all capitals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bad word count</head><p>The number of matches to a regular expression for various commonly used vulgar words.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,278.37,179.82,7.77;6,370.64,278.37,69.22,7.77;6,146.47,289.33,228.63,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,187.81,278.37,130.14,7.77;6,370.64,278.37,65.32,7.77">Vandalism detection in Wikipedia: a classifier approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belani</surname></persName>
		</author>
		<idno>CoRR) abs/1001.0700</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct coords="6,138.13,300.28,311.62,7.77;6,146.47,311.24,303.56,7.77;6,146.47,322.20,282.10,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,275.00,300.28,158.62,7.77">Automatic vandalism detection in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,146.47,311.24,303.56,7.77;6,146.47,322.20,73.08,7.77">ECIR&apos;08: Proceedings of the IR research, 30th European conference on Advances in information retrieval</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="663" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,333.16,341.67,7.77;6,146.47,344.12,77.94,7.77" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Webb</surname></persName>
		</author>
		<title level="m" coord="6,185.09,333.16,203.00,7.77">Decision tree grafting from the all-tests-but-one partition</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,355.08,340.24,7.77;6,146.47,366.04,324.68,7.77;6,146.47,377.00,228.55,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,262.54,355.08,215.83,7.77;6,146.47,366.04,71.38,7.77">Detecting Wikipedia vandalism via spatio-temporal analysis of revision metadata</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,236.14,366.04,235.01,7.77;6,146.47,377.00,56.69,7.77">EUROSEC &apos;10: Proceedings of the Third European Workshop on System Security</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
