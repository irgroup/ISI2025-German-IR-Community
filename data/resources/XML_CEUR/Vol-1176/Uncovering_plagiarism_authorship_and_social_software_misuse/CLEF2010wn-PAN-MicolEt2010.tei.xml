<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.65,115.90,334.05,12.90;1,191.22,133.83,232.92,12.90;1,218.28,153.84,178.79,10.75">A Textual-Based Similarity Approach for Efficient and Scalable External Plagiarism Analysis Lab Report for PAN at CLEF 2010</title>
				<funder ref="#_UCRRFfq">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundación Caja-Murcia</orgName>
				</funder>
				<funder ref="#_4dWEHDa">
					<orgName type="full">University of Alicante</orgName>
				</funder>
				<funder ref="#_XqZbfk8">
					<orgName type="full">Conselleria d&apos;Educació of the Spanish Generalitat Valenciana</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.47,190.40,50.96,8.64"><forename type="first">Daniel</forename><surname>Micol</surname></persName>
							<email>dmicol@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="laboratory">Research Group on Natural Language Processing and Information Systems</orgName>
								<orgName type="institution">University of Alicante San Vicente</orgName>
								<address>
									<addrLine>del Raspeig</addrLine>
									<postCode>E-03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.30,190.40,63.81,8.64"><forename type="first">Óscar</forename><surname>Ferrández</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="laboratory">Research Group on Natural Language Processing and Information Systems</orgName>
								<orgName type="institution">University of Alicante San Vicente</orgName>
								<address>
									<addrLine>del Raspeig</addrLine>
									<postCode>E-03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.83,190.40,64.08,8.64"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="laboratory">Research Group on Natural Language Processing and Information Systems</orgName>
								<orgName type="institution">University of Alicante San Vicente</orgName>
								<address>
									<addrLine>del Raspeig</addrLine>
									<postCode>E-03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.27,190.40,56.62,8.64"><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
							<email>rafael@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="laboratory">Research Group on Natural Language Processing and Information Systems</orgName>
								<orgName type="institution">University of Alicante San Vicente</orgName>
								<address>
									<addrLine>del Raspeig</addrLine>
									<postCode>E-03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.65,115.90,334.05,12.90;1,191.22,133.83,232.92,12.90;1,218.28,153.84,178.79,10.75">A Textual-Based Similarity Approach for Efficient and Scalable External Plagiarism Analysis Lab Report for PAN at CLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF8A7D8921CDF51E43471BA9269332B6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an approach to detect external plagiarism based on textual similarity. This is an efficient and precise method that can be applied over large sets of documents. The system that we have developed contains a first phase of document selection that uses a variant of tf -idf applied over the terms that appear in the two documents of the pair being compared. After this is done, we apply a more complex and accurate function based on character n-grams over the subset of documents resulting from the first step in order to extract the plagiarized passages, or matches. Once all matches for a given document are extracted, we perform a greedy match merging operation to allow in-between text in order to be compatible with certain levels of plagiarism obfuscation. In our participation in the 2nd International Competition on Plagiarism Detection, we achieved an overall score of 0.2222, ranking 11 th out of 18 participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>External plagiarism analysis is a complex task that attempts to determine if a suspicious document contains one or more appropriations of another text which belongs to a set of source candidates. It can be a very expensive task if the number of documents to compare against is large. This is the case for the corpora provided by the organizers of the 1st and 2nd International Competitions on Plagiarism Detection <ref type="bibr" coords="1,408.35,524.77,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="1,424.12,524.77,11.83,8.64" target="#b11">12]</ref>, which we have used as training and evaluation sets for our participation in the latter competition. These corpora contain two sets of documents, namely suspicious and source. The first of them contains those documents that may include one or more plagiarisms extracted from documents from the second of these sets. In total, the corpora provided for this competition contain several thousands of documents of both kinds.</p><p>Given the large amount of data to process, and the number of document comparisons to perform, one of the main goals for the systems that participate in these competitions is to be highly efficient. For this purpose, we decided to apply a lightweight document similarity function that would be used as heuristic to determine if a given suspicious and source documents are similar enough to hold a plagiarism relation. After a set of candidate source documents is extracted, we apply a more expensive and accurate function to detect the corresponding plagiarized fragments. This two-step architecture is in line with the current state of the art systems.</p><p>The remainder of this paper is structured as follows. The next section will describe the state of the art in external plagiarism analysis. The third will describe the methods implemented in our system, while the fourth one contains the experimental results of our approach. Finally, the fifth and last section presents our conclusions and proposes future work based on our current research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State of the art</head><p>Most of the research approaches in the field of external plagiarism analysis contain a simple and efficient heuristic retrieval to reduce the number of source documents to compare every suspicious text against, and a more complex and costly detailed analysis that attempts to extract the exact position of the plagiarized fragment, if any <ref type="bibr" coords="2,461.50,283.24,15.27,8.64" target="#b10">[11]</ref>. As mentioned before, the system that we have developed is in line with this two-step architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heuristic retrieval</head><p>Given the large amount of data to process, and the number of document comparisons to perform, one of the main goals of the systems that participate in the previously mentioned competitions is to be highly efficient. Theoretically, every source document could be the origin of a given plagiarism from a suspicious document, so we need to compare all of them with each other. This has a squared complexity, assuming the number of documents in both sets is of the same magnitude, and therefore presents performance and scalability limitations. Because of this, performing a lightweight heuristic retrieval that reduces the number of comparisons to perform is highly recommendable. Some authors, like <ref type="bibr" coords="2,227.43,453.15,10.58,8.64" target="#b5">[6]</ref>, create an inverted index of the corpus documents' contents in order to be able to retrieve efficiently a set of texts that contain a given n-gram. Other authors, such as <ref type="bibr" coords="2,202.54,477.06,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="2,213.33,477.06,7.19,8.64" target="#b4">5]</ref>, decided to apply a document similarity function that would be used as heuristic to determine if a given suspicious and source documents are similar enough to hold a plagiarism relation.</p><p>On the other hand, some authors, like <ref type="bibr" coords="2,300.13,512.97,10.58,8.64" target="#b4">[5]</ref>, implement a character-level n-gram comparison and apply a cosine similarity based on term frequency weights. With this approach they extract the 51 most similar source documents to the suspicious one being analyzed. Other authors, such as <ref type="bibr" coords="2,265.23,548.84,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="2,276.01,548.84,7.19,8.64" target="#b5">6]</ref>, decided to implement a word-level n-gram comparison. The first of them, <ref type="bibr" coords="2,240.95,560.80,10.58,8.64" target="#b1">[2]</ref>, keeps the 10 most similar using a custom distance function based on frequency weights, while the second, <ref type="bibr" coords="2,342.64,572.75,10.58,8.64" target="#b5">[6]</ref>, keeps those source documents that share at last 20 word-grams of length 5. Low granularity word n-grams, with a size of 1, have been explored by <ref type="bibr" coords="2,254.09,596.66,10.58,8.64" target="#b7">[8]</ref>, applying cosine similarity using frequency weights to extract the two most similar partitions for every sentence in a document, using the source documents' sentences as centroid. Finally, <ref type="bibr" coords="2,332.70,620.57,16.60,8.64" target="#b12">[13]</ref> has applied the well-known document fingerprinting approach described in <ref type="bibr" coords="2,310.87,632.53,16.60,8.64" target="#b13">[14]</ref> with 50 character chunks and overlap of 30. The candidate source documents will be those that share at least one value with the suspicious document's fingerprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detailed analysis</head><p>Regarding the detailed analysis, <ref type="bibr" coords="3,266.06,140.15,11.62,8.64" target="#b4">[5]</ref> extracts character-level n-grams, and later on performs a computation of the distances of adjacent matches, joining them based on a Monte Carlo optimization. Afterwards, they propose a refinement of the obtained section pairs. <ref type="bibr" coords="3,176.98,176.02,11.62,8.64" target="#b5">[6]</ref> extracts matches of word n-grams of length 5, and applies a match merging heuristic to obtain larger matches. Then they extract the maximal size which share at least 20 matches, including the first and the last n-gram of the matching sections, and for which 2 adjacent matches are at most 49 not-matching n-grams apart. On the other hand, <ref type="bibr" coords="3,160.05,223.84,11.62,8.64" target="#b1">[2]</ref> performs a greedy match merging if the distance of the matches is not too high.</p><p>A more strict approach has been presented by <ref type="bibr" coords="3,331.12,247.93,10.58,8.64" target="#b7">[8]</ref>, requiring exact sentence matches, and afterwards applying a match merging approach by greedily joining consecutive sentences. In this method, gaps are allowed if the respective sentences are similar to the corresponding sentences in the other document. Finally, <ref type="bibr" coords="3,357.82,283.80,16.60,8.64" target="#b12">[13]</ref> requires an exact match of document fingerprints, doing an extraction of the pairs of sections which are obtained by enlarging matches and joining adjacent matches. Gaps are required to be below a certain Levenshtein edit distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>As mentioned before, the corpora that we want to use our system against contain several thousands suspicious and source documents, where every suspicious may include plagiarisms of one or more source documents. If we were to compare all of them with each other, we would have to do millions of document comparisons. Given that every document can contain thousands or millions of characters, this set of comparisons becomes intractable.</p><p>To make this problem computationally feasible, we must reduce by a large factor the number of document comparisons to perform, by generating a subset of candidate source documents for every suspicious text. After this subset has been computed, we will be able to apply a more complex function that will detect the plagiarized fragments, if any. This measure should be as independent as possible of the document sizes, given that otherwise our system will not scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document selection</head><p>As mentioned before, the first step would be to select a subset of candidate source documents that will later on be compared against a given suspicious document. This should reduce by a large factor the number of document comparisons to perform. To generate this set we will have to loop through all source documents, and given that this set is large, this operation needs to be efficient.</p><p>Our approach to solve this problem is to weight the words in every document and then compare the weights of those terms that appear in both the suspicious and the source documents being compared. The similarity score between the aforementioned two documents will be the sum of the common term weights.</p><p>Similarity measure To measure the similarity between a suspicious and a source document, we will use a variant of the term frequency inverse document frequency function <ref type="bibr" coords="4,134.77,143.22,15.27,8.64" target="#b14">[15]</ref>, or tf -idf , commonly used in information retrieval. It is applied over a term, t i , in a given document, d j , and is defined as follows:</p><formula xml:id="formula_0" coords="4,263.89,178.77,216.70,9.65">tf -idf i,j = tf i,j • idf i (1)</formula><p>where the term frequency of t i in document d j , or tf i,j , is:</p><formula xml:id="formula_1" coords="4,274.41,213.25,206.18,36.25">tf i,j = n i,j k n k,j<label>(2)</label></formula><p>being n i,j the number of occurrences of the considered term, t i , in document d j , and the denominator is the sum of the number of occurrences of all terms in document d j . Furthermore, the inverse document frequency of term t i , or idf i , is defined as:</p><formula xml:id="formula_2" coords="4,251.04,300.16,225.68,23.23">idf i = log |D| |{d : t i ∈ d}| (<label>3</label></formula><formula xml:id="formula_3" coords="4,476.72,307.22,3.87,8.64">)</formula><p>where |D| is the number of documents in our corpus. For our purpose this definition of tf -idf is not optimal. The tf value is normalized by the length of a document to prevent longer documents from having a higher weight. This makes sense in information retrieval applications such as search engines. However, in our case it would be better to skip this normalization given that the more times a word appears in a document, the higher likelihood it will have to contain a plagiarized fragment, regardless of its size. This is because there will be more passages that are similar and that therefore could be plagiarized. Therefore, tf in our case will be defined as:</p><formula xml:id="formula_4" coords="4,284.43,449.74,196.17,9.65">tf i,j = n i,j<label>(4)</label></formula><p>With regards to idf , given that in our case the number of documents in the corpus is constant and we will only need the relative tf -idf value, not the absolute, we can simplify this function by removing the |D| and the logarithm:</p><formula xml:id="formula_5" coords="4,265.63,510.32,214.96,23.22">idf i = 1 |{d : t i ∈ d}|<label>(5)</label></formula><p>Finally, the similarity score of two documents, being d j the suspicious and d k the source one, will be defined as shown in the following equation:</p><formula xml:id="formula_6" coords="4,159.20,581.18,321.39,27.55">sim dj ,d k = tw∈dj ∩d k (tf w,j • idf w,j ) = tw∈dj ∩d k n w,j • 1 |{d : t w ∈ d}|<label>(6)</label></formula><p>where |d| represents the number of texts in the source documents corpus that contain a term that appears in both the suspicious and the source documents. Therefore, the similarity of two documents will be higher the more words they have in common, and also the fewer documents those terms appear in.</p><p>Choosing the candidate source documents Now that the document similarity measure is defined, we describe the approach to compare suspicious and source documents. As mentioned before, the similarity measure will be applied over those terms that appear in both the corresponding suspicious and source documents being compared. For this purpose, we normalize and tokenize the contents of every document. This means that we only keep alphanumerical symbols, and the rest are replaced by spaces. After this process is done, we do a space-based tokenization to keep all tokens, filtering stop words and calculating the tf -idf of all remaining terms.</p><p>When we compare two documents we will have two arrays with the tf -idf of the terms in both of them, and will be able to calculate a similarity score as defined above. Each of these scores will be stored in a dictionary data structure where the key will be the corresponding term, and the value its tf -idf . Now comparing two documents is a linear complexity task.</p><p>For every suspicious document we will perform a comparison with all source documents in our corpus, and keep those that have the highest similarity scores. If we maintain a larger number of source documents we will have a higher recall for the next phase, but also higher response time, and vice versa for lower number of source documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage selection</head><p>Once we have a small set of source documents to compare against for every suspicious one, we can perform a more accurate and costly comparison between pairs of documents in order to detect the plagiarized fragments, or passages, if any.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match extraction</head><p>The approach that we have implemented first normalizes the contents of every document by keeping only alphanumerical characters, and removing spaces and punctuation symbols. This last point is a difference with the normalization mentioned before, and the reason for this is that we want our system to behave correctly against certain cases of low obfuscation like word-breaking or concatenation. Therefore, we use character-based n-grams over the normalized document contents. If we used word-based n-grams instead, our passage selection algorithm wouldn't behave well with the aforementioned obfuscation cases.</p><p>After the text has been normalized, we try to find the largest common substring between suspicious and source documents, requiring a minimum length which will be the n-gram size. Assuming that the suspicious document has n characters, and the source one has m, the complexity of a brute force solution for this problem would be O(n • m), which is of the order of O(n 2 ). This is too expensive and presents scalability limitations, especially for large documents.</p><p>An optimization to this approach is to hash all overlapping n-grams of both the suspicious and source documents, storing, for every n-gram, the positions where they appear in the text. Once the n-grams of the source document being compared against have been hashed, we will iterate through the contents of the suspicious document, extract n-grams starting at every given offset, look them up in the hash of n-grams of the aforementioned source document, and go directly to the positions where the given ngram appears, limiting unnecessary comparisons. From these points we will try to find the largest common substring to both documents. A similar n-gram hashing technique is described in <ref type="bibr" coords="6,194.81,131.27,15.27,8.64" target="#b13">[14]</ref>.</p><p>In our system we wanted to choose an n-gram size that would maintain a passage's meaning, and that is also not too strict. Smaller n-grams have the problem that they might match parts of a word and therefore the meaning would be lost. On the other hand, longer n-grams don't behave well when there is plagiarism obfuscation, so if the plagiarized sentence has been somehow altered, they won't work. In our experiments we used an n-gram size of 30 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match merging</head><p>The result of the match extraction step will be a set of identical substrings that appear in both the suspicious and source documents. It would be beneficial to make this comparison less restrictive by allowing additional words or characters to appear in-between matches, so that our system works well on certain kinds of low level obfuscations. This is common among plagiarism when the corresponding person introduces additional words or rearranges part of the appropriated text. To overcome this issue, we perform a match merging operation, which attempts to group matches, and the text in-between, if they are close enough in both source and suspicious documents. This is a greedy process that recursively attempts to merge matches if they satisfy the following two heuristics:</p><p>-The length of the text in-between two matches, m i and m j , is smaller than the length of m i plus the length of m j . This applies to the source and the suspicious part of the matches. -The length of the merged match cannot be longer or equal than twice the length of m i plus the length of m j . This applies to the source and the suspicious part of the matches.</p><p>These two heuristics are similar to those presented in <ref type="bibr" coords="6,375.19,456.24,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="6,385.98,456.24,7.19,8.64" target="#b5">6]</ref>, and they have been obtained empirically.</p><p>In addition, at the end of the merging operation we will discard those matches that contain less than 100 characters in either source or suspicious document, as the minimum plagiarism size for the corpus that we use is 50 words <ref type="bibr" coords="6,388.96,505.13,15.27,8.64" target="#b10">[11]</ref>, and therefore we believe that 100 characters is a safe lower limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation and results</head><p>We have performed two sets of experiments with our system. The first of them used an annotated training corpora to tune it, and the second one used an unannotated corpora in order to evaluate our approach.</p><p>The machine we used to carry out the experiments described in this section had 8 CPU cores, each of them running at a frequency of 2.53 GHz, and 16 GB of RAM. To maximize the usage of this machine's capabilities, we designed our system to be multi-threaded as most of the methods presented in this paper are fully parallelizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>To tune our system to behave optimally, we used the external plagiarism corpora from the 1st International Competition on Plagiarism Detection, given that it contained annotations. This corpora contains a source documents corpus composed of 14, 429 texts, and a suspicious documents corpus composed of 14, 428 elements. Half of the suspicious texts contain a plagiarism, which ranges between 0% and 100% of the corresponding source document. In addition, the plagiarism length is evenly distributed between 50 and 5, 000 words.</p><p>The first aspect that we experimented with was trying to determine the optimal number of documents to be selected, given that a larger number of elements would lead to higher accuracy, but would affect performance negatively given that we would have more documents to process. The opposite applies to smaller selected document sets. Table <ref type="table" coords="7,181.37,270.90,4.98,8.64" target="#tab_0">1</ref> shows the results from this experiment using different set sizes, where column Captured represents the number of plagiarisms that are contained within the set of source documents, and Missed those that are not included in this set. Given the values shown in the previous table, we decided to use a number of documents of 10, since we believe it is the best trade-off between amount of texts and recall. After this step, we executed the passage selection, obtaining the following results: overall = 0.3902, f -score = 0.5665, precision = 0.6873, recall = 0.4819, and granularity = 1.7354. If we had participated in the 1st International Competition on Plagiarism Detection, our system would have ranked 4 th out of 11 participants, as shown in Table <ref type="table" coords="7,200.89,596.66,3.74,8.64" target="#tab_1">2</ref>. As we can see in this table, the strongest aspect of our system is its precision, where it ranks the third among all participants. On the other hand, recall and granularity were not as good, but still within the top half. The reason why recall is lower is in part due to the fact that we chose 10 source documents per suspicious text to evaluate, giving a maximum coverage value of 77.81%, as shown in Table <ref type="table" coords="7,447.57,644.48,3.74,8.64" target="#tab_0">1</ref>. Apart from this, and since our method is purely textual, we miss plagiarisms that are not writ-ten in similar ways. Finally, documents that are translated will also lower our recall. On the other hand, granularity would have been lower if we had been more aggressive at merging matches, although then precision might have suffered. This experiment took around 8 hours to process in the computer previously described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>Our system was evaluated against the corpora provided for the 2nd International Competition on Plagiarism Detection, using the parameters obtained with the training set. These corpora are of the same nature as the ones used to train our system, but with the inclusion of additional novel cases of plagiarism <ref type="bibr" coords="8,329.67,465.15,15.27,8.64" target="#b11">[12]</ref>. In addition, they contain a larger number of documents. Concretely, they are composed of 11, 148 source and 15, 925 suspicious documents.</p><p>Our system obtained the following results after being applied over the aforementioned corpora: overall = 0.2222, f -score = 0.3762, precision = 0.9308, recall = 0.2357, and granularity = 2.2332. As we can see in these values, the recall value was considerably lower to that shown in the training section. We believe this is due to the addition of novel cases of plagiarism in the corpus provided for the 2nd International Competition on Plagiarism Detection, as our system was tuned to work for the cases present in the corpora of the 1st International Competition on Plagiarism Detection. This same reason made precision higher as well as granularity. The resulting overall score is considerably lower in the evaluation phase of the experimentation compared to the training because our system is not able to detect the new plagiarism cases introduced in the corpora used for the former. In the end, our system ranked 11 th out of 18 participants.</p><p>This experiment took around 9 hours to process in the computer previously described.</p><p>In this paper we have presented an efficient and scalable approach to detect external plagiarisms based on textual similarity comparisons. This has proven to work fairly well as our system's performance has been among the best of the approaches that participated in the 1st International Competition on Plagiarism Detection. In the second edition, however, results were considerably worse because our system has the disadvantage that it will only behave well for levels of plagiarism obfuscation that are not high, although given the size of the corpora we worked with, applying more complex techniques, such as semantic or syntactic analysis, doesn't seem feasible due to performance constraints. Because of this reason, our system works well with low or medium levels of plagiarism obfuscation, but not with high levels or when the plagiarized document has been translated into a different language.</p><p>As future work we would like to focus on smaller corpora in order to be able to apply more complex techniques such as the usage of semantic and syntactic knowledge. For instance, we could use textual entailment recognition techniques, such as the ones presented in <ref type="bibr" coords="9,188.00,309.23,10.58,8.64" target="#b3">[4]</ref>, to detect plagiarisms that have a high level of obfuscation because they have been rewritten using a different word order or equivalent terms. Furthermore we would also like to apply document language recognition techniques and automatic translators to overcome the problem with translated plagiarized documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,199.13,325.46,217.10,160.80"><head>Table 1 .</head><label>1</label><figDesc>Metrics using different selected document set sizes.</figDesc><table coords="7,249.94,346.43,113.24,139.83"><row><cell>Size Recall Captured Missed</cell></row><row><cell>1 0.3260 23, 970 49, 552</cell></row><row><cell>5 0.6875 50, 547 22, 975</cell></row><row><cell>10 0.7781 57, 206 16, 316</cell></row><row><cell>20 0.8282 60, 893 12, 629</cell></row><row><cell>30 0.8479 62, 340 11, 182</cell></row><row><cell>40 0.8595 63, 189 10, 333</cell></row><row><cell>50 0.8698 63, 947 9, 575</cell></row><row><cell>60 0.8760 64, 403 9, 119</cell></row><row><cell>70 0.8820 64, 843 8, 679</cell></row><row><cell>80 0.8869 65, 205 8, 317</cell></row><row><cell>90 0.8905 65, 473 8, 049</cell></row><row><cell>100 0.8941 65, 734 7, 788</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,173.60,345.82,160.89"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our system against the ones that participated in the 1st International Competition on Plagiarism Detection.</figDesc><table coords="8,184.03,205.42,245.05,129.07"><row><cell cols="3">Rank Overall F-score Precision Recall Granularity Participant</cell></row><row><cell cols="2">1 0.6957 0.6976 0.7418 0.6585 1.0038</cell><cell>[5]</cell></row><row><cell cols="2">2 0.6093 0.6192 0.5573 0.6967 1.0228</cell><cell>[6]</cell></row><row><cell cols="2">3 0.6041 0.6491 0.6727 0.6272 1.1060</cell><cell>[2]</cell></row><row><cell>4 0.3902 0.5665 0.6873 0.4819</cell><cell>1.7354</cell><cell>This work</cell></row><row><cell cols="2">5 0.3045 0.5286 0.6689 0.4370 2.3317</cell><cell>[9]</cell></row><row><cell cols="2">6 0.1885 0.4603 0.6051 0.3714 4.4354</cell><cell>[8]</cell></row><row><cell cols="2">7 0.1422 0.6190 0.7473 0.5284 19.4327</cell><cell>[13]</cell></row><row><cell cols="2">8 0.0649 0.1736 0.6552 0.1001 5.3966</cell><cell>[10]</cell></row><row><cell cols="2">9 0.0264 0.0265 0.0136 0.4586 1.0068</cell><cell>[16]</cell></row><row><cell cols="2">10 0.0187 0.0553 0.0290 0.6048 6.7780</cell><cell>[7]</cell></row><row><cell cols="2">11 0.0117 0.0226 0.3684 0.0116 2.8256</cell><cell>[1]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors of this paper would like to thank the organizers of the 1st and <rs type="institution">2nd International</rs> Competitions on Plagiarism Detection for providing the corpora and evaluation metrics that we have used extensively to develop our system, and for having organized the aforementioned competitions. In addition, we would like to thank <rs type="person">Dario Bigongiari</rs> and <rs type="person">Michael Schueppert</rs> for their help in setting up and running the experiments described in this paper.</p><p>This research has been partially funded by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> (grant <rs type="grantNumber">TIN2009-13391-C04-01</rs>), the <rs type="funder">Conselleria d'Educació of the Spanish Generalitat Valenciana</rs> (grants <rs type="grantNumber">PROMETEO/2009/119</rs> and <rs type="grantNumber">ACOMP/2010/286</rs>), and the <rs type="funder">University of Alicante</rs> post-doctoral fellowship program funded by <rs type="funder">Fundación Caja-Murcia</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UCRRFfq">
					<idno type="grant-number">TIN2009-13391-C04-01</idno>
				</org>
				<org type="funding" xml:id="_XqZbfk8">
					<idno type="grant-number">PROMETEO/2009/119</idno>
				</org>
				<org type="funding" xml:id="_4dWEHDa">
					<idno type="grant-number">ACOMP/2010/286</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,570.09,318.68,7.77;9,150.95,581.05,328.00,7.77;9,150.95,592.01,206.61,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,185.82,570.09,259.56,7.77">Submission to the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.95,581.05,328.00,7.77;9,150.95,592.01,58.64,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,602.63,304.73,7.77;9,150.95,613.59,324.47,7.77;9,150.95,624.55,306.74,7.77;9,150.95,635.51,212.44,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,400.27,602.63,47.07,7.77;9,150.95,613.59,248.77,7.77">A Plagiarism Detection Procedure in Three Steps: Selection, Matches and &quot;Squares</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cristadoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Esposti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,421.88,613.59,53.54,7.77;9,150.95,624.55,306.74,7.77;9,150.95,635.51,24.12,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,646.13,305.83,7.77;9,150.95,657.08,299.14,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,344.73,646.13,103.70,7.77;9,150.95,657.08,76.24,7.77">An example of mathematical authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Esposti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,232.72,657.08,117.25,7.77">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="125211" to="125230" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,119.96,317.27,7.77;10,150.95,130.92,323.73,7.77;10,150.95,141.88,319.61,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,334.99,119.96,124.88,7.77;10,150.95,130.92,143.48,7.77">A Perspective-Based Approach for Solving Textual Entailment Recognition</title>
		<author>
			<persName coords=""><forename type="first">Ó</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,312.72,130.92,161.97,7.77;10,150.95,141.88,142.02,7.77">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,152.84,325.86,7.77;10,150.95,163.80,312.08,7.77;10,150.95,174.76,327.14,7.77;10,150.95,185.71,87.16,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,277.96,152.84,190.51,7.77;10,150.95,163.80,135.51,7.77">ENCOPLOT: Pairwise Sequence Matching in Linear Time Applied to Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,304.62,163.80,158.42,7.77;10,150.95,174.76,228.23,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,196.67,311.95,7.77;10,150.95,207.63,303.60,7.77;10,150.95,218.59,329.40,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,294.39,196.67,160.18,7.77;10,150.95,207.63,40.24,7.77">Finding Plagiarism by Evaluating Document Similarities</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Křipač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.99,207.63,245.56,7.77;10,150.95,218.59,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,229.55,329.61,7.77;10,150.95,240.51,302.62,7.77;10,150.95,251.47,327.14,7.77;10,150.95,262.43,87.16,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,256.56,229.55,215.67,7.77;10,150.95,240.51,126.13,7.77">Tackling the PAN&apos;09 External Plagiarism Detection Corpus with a Desktop Plagiarism Detector</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C R</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,295.15,240.51,158.42,7.77;10,150.95,251.47,228.23,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,273.39,337.98,7.77;10,150.95,284.34,320.26,7.77;10,150.95,295.30,305.50,7.77;10,150.95,306.26,64.49,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,325.36,273.39,155.23,7.77;10,150.95,284.34,98.93,7.77">External and Intrinsic Plagiarism Detection Using Vector Space Models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,268.49,284.34,202.72,7.77;10,150.95,295.30,183.92,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,317.22,324.70,7.77;10,150.95,328.18,299.10,7.77;10,150.95,339.14,329.40,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,209.35,317.22,257.96,7.77;10,150.95,328.18,31.99,7.77">Counter plagiarism detection software&quot; and &quot;Counter counter plagarism detection</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,204.49,328.18,245.56,7.77;10,150.95,339.14,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,350.10,324.82,7.77;10,150.95,361.06,317.79,7.77;10,150.95,372.02,329.40,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,298.72,350.10,168.34,7.77;10,150.95,361.06,54.49,7.77">Intrinsic Plagiarism Detection Using Character n-gram Profiles</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Galante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,223.18,361.06,245.56,7.77;10,150.95,372.02,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="36" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,382.97,291.80,7.77;10,150.95,393.93,306.83,7.77;10,150.95,404.89,329.41,7.77;10,150.95,415.85,126.01,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,363.43,382.97,70.61,7.77;10,150.95,393.93,180.34,7.77">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,349.45,393.93,108.33,7.77;10,150.95,404.89,278.31,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,426.81,294.78,7.77;10,150.95,437.77,301.35,7.77;10,150.95,448.73,327.27,7.77;10,150.95,459.69,64.49,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,363.43,426.81,73.60,7.77;10,150.95,437.77,180.34,7.77">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,349.45,437.77,102.85,7.77;10,150.95,448.73,278.31,7.77">Proceedings of the CLEF&apos;10 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the CLEF&apos;10 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,470.65,300.75,7.77;10,150.95,481.60,297.11,7.77;10,150.95,492.56,329.40,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,252.35,470.65,190.63,7.77;10,150.95,481.60,33.39,7.77">Using Microsoft SQL Server Platform for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Scherbinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Butakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,202.50,481.60,245.56,7.77;10,150.95,492.56,141.08,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,503.52,325.13,7.77;10,150.95,514.48,284.78,7.77;10,150.95,525.44,225.24,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,304.71,503.52,162.65,7.77;10,150.95,514.48,50.45,7.77">Winnowing: Local Algorithms for Document Fingerprinting</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schleimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,219.45,514.48,216.29,7.77;10,150.95,525.44,74.10,7.77">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,536.40,311.55,7.77;10,150.95,547.36,202.25,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,215.95,536.40,237.84,7.77;10,150.95,547.36,28.50,7.77">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,184.86,547.36,93.64,7.77">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,558.32,335.59,7.77;10,150.95,569.28,304.26,7.77;10,150.95,580.23,305.50,7.77;10,150.95,591.19,64.49,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,224.41,558.32,253.42,7.77;10,150.95,569.28,83.85,7.77">Putting Ourselves in SME&apos;s Shoes: Automatic Detection of Plagiarism by the WCopyFind tool</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vallés Balaguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.49,569.28,202.72,7.77;10,150.95,580.23,183.92,7.77">Proceedings of the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<meeting>the SEPLN&apos;09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="34" to="35" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
