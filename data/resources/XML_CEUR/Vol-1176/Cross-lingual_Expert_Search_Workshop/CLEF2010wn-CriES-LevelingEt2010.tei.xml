<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.98,115.96,337.41,12.62;1,244.89,133.89,125.57,12.62">HITS and Misses: Combining BM25 with HITS for Expert Search</title>
				<funder ref="#_6G68yKR">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.77,171.56,79.47,8.74"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<email>jleveling@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Centre for Next Generation Localisation (CNGL</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.94,171.56,81.65,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Centre for Next Generation Localisation (CNGL</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.98,115.96,337.41,12.62;1,244.89,133.89,125.57,12.62">HITS and Misses: Combining BM25 with HITS for Expert Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B1A519F202AD258B264B34DEB02D9F5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Expert Search</term>
					<term>Information Retrieval</term>
					<term>BM25</term>
					<term>HITS Algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Dublin City University in the CriES (Cross-Lingual Expert Search) pilot challenge. To realize expert search, we combine traditional information retrieval (IR) using the BM25 model with reranking of results using the HITS algorithm. The experiments were performed on two indexes, one containing all questions and one containing all answers. Two runs were submitted. The first one contains the combination of results from IR on the questions with authority values from HITS; the second contains the reranked results from IR on answers with authority values. To investigate the impact of multilinguality, additional experiments were conducted on the English topic subset and on all topics translated into English with Google Translate. The overall performance is moderate and leaves much room for improvement. However, reranking results with authority values from HITS typically improved results and more than doubled the number of relevant and retrieved results and precision at 10 documents in many experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CriES pilot challenge <ref type="bibr" coords="1,254.12,507.35,10.52,8.74" target="#b1">[1]</ref> aims at multilingual expert search and is based on a subset of the data provided by Yahoo! Research Webscope<ref type="foot" coords="1,411.00,517.73,3.97,6.12" target="#foot_0">1</ref> . The complete Yahoo QA dataset comprises 4.5M natural language questions and 35.9M answers. Questions are associated with one or more answers and the best answer is marked by users of the web portal. Questions are also annotated with categories from a hierarchical classification system. The Yahoo QA dataset has been previously used in <ref type="bibr" coords="1,192.70,579.08,10.52,8.74" target="#b2">[2]</ref> to train a learning to rank approach. The CriES data subset was extracted with the preprocessing tool provided by the organizers. This subset contains 780,193 questions, posted by more than 150,000 users.</p><p>For the CriES expert search experiments described in this paper, different approaches to find experts likely to answer a question were investigated: 1. Finding experts by matching the current question with previously given answers.</p><p>This corresponds to a standard information retrieval approach on answer documents. 2. Finding experts by matching the current question with questions which have previously been answered. This approach is typically employed in FAQ (frequently asked questions) search and corresponds to IR on questions. 3. + 4. Reranking the results of the two former approaches by interpreting HITS authority values of question and answer documents as the level of expertise.</p><p>The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes the theoretical background and the system setup for the CriES experiments. Section 4 presents the experimental setup and results, followed by an analysis and discussion of results in Section 5 and the paper concludes with an outlook on future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Expert search on question answer (Q/A) pairs is a relatively new research area which is related to search in FAQs, social network analysis, and question answering (see, for example <ref type="bibr" coords="2,247.57,324.01,10.30,8.74" target="#b3">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FAQ Search</head><p>Burke, et al. <ref type="bibr" coords="2,195.10,379.05,10.52,8.74" target="#b4">[4,</ref><ref type="bibr" coords="2,207.28,379.05,7.75,8.74" target="#b5">5]</ref> introduce FAQ finder, a system for finding answers to frequently asked questions. Their experiments are based on a small set of FAQ files from Usenet newsgroups. A weighted sum of vector similarity between question and Q/A pairs, term overlap, and WordNet-based lexical similarity between questions is computed to find the best results.</p><p>Wu, et al. <ref type="bibr" coords="2,195.87,439.35,10.52,8.74" target="#b6">[6]</ref> use a probabilistic mixture model for FAQ finding in the medical domain. Questions and answers are first categorized and the Q/A pairs are interpreted as a set of independent aspects. WordNet <ref type="bibr" coords="2,365.43,463.26,10.52,8.74" target="#b7">[7]</ref> and HowNet <ref type="bibr" coords="2,435.28,463.26,10.52,8.74" target="#b8">[8]</ref> are employed as lexical resources for question classification. Answers are paragraphed, and clustered by LSA and k-means. A probabilistic mixture model is used to interpret questions and answers based on independent aspects. Optimal weights in the probabilistic mixture model are estimated by expectation maximization. This approach outperforms FAQ finder <ref type="bibr" coords="2,308.36,523.04,10.52,8.74" target="#b4">[4]</ref> in the medical domain.</p><p>Jijkoun and de Rijke <ref type="bibr" coords="2,240.25,535.52,10.52,8.74" target="#b9">[9]</ref> describe FAQ finding based on a collection of crawled web pages. Q/A pairs are extracted and questions are answered by retrieving matching pairs. The approach is based on the vector space model and Lucene, using a linear combination of retrieval in different fields.</p><p>Chiu, et al. <ref type="bibr" coords="2,204.27,583.86,15.50,8.74" target="#b10">[10]</ref> use a combination of hierarchical agglomerative clustering (HAC) and rough set theory for FAQ finding. HAC is applied to create a concept hierarchy. Lower/upper approximation from rough set theory helps to classify and match user queries. They conclude that rough set theory can significantly improve classification of user queries.</p><p>Several retrieval experiments described in this paper are also based on finding experts who answered similar questions by indexing all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Expert Search</head><p>Balog, Azzopardi, and de Rijke <ref type="bibr" coords="3,277.10,147.74,15.50,8.74" target="#b11">[11]</ref> propose two models to find experts based on documents for the TREC enterprise track<ref type="foot" coords="3,334.39,158.12,3.97,6.12" target="#foot_1">2</ref> . The first approach is to locate knowledge from experts' documents; the second approach aims at finding documents on topics and extract associated experts. To this end, they analyze the communication link structure. They find that the second approach consistently outperforms the first one.</p><p>MacDonald and Ounis <ref type="bibr" coords="3,254.62,221.23,15.50,8.74" target="#b12">[12]</ref> perform experiments on expert finding on the TREC enterprise data. They find that increasing the precision in the document retrieval step does not always result in better precision for the expert search.</p><p>Yang, et al. <ref type="bibr" coords="3,204.55,258.86,15.50,8.74" target="#b13">[13]</ref> present the expert finding system EFS, which employs experts' profiles created from their lists of publications. Category links are extracted from Wikipedia. Nine different areas of expertise are differentiated.</p><p>Similar to extracting experts from retrieved documents, some retrieval experiments described in this paper rely on retrieving answers to a given question and extracting their experts (authors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Link Analysis</head><p>The HITS (Hyperlink-Induced Topic Search) algorithm is a link analysis algorithm for rating web pages <ref type="bibr" coords="3,273.80,399.81,14.61,8.74" target="#b14">[14]</ref>. PageRank <ref type="bibr" coords="3,346.45,399.81,15.50,8.74" target="#b15">[15]</ref> produces a static, queryindependent score for web pages, taking the incoming and outgoing links of a web page into account. In contrast, HITS produces two values for a web page: its authority and its hub value. HITS values are computed at query time and on results retrieved with an initial retrieval, i.e. the computations are performed only on initially retrieved results, not across all linked web pages. Recent variants of HITS have been concerned with stability of the algorithm <ref type="bibr" coords="3,423.03,471.54,15.50,8.74" target="#b16">[16]</ref> and with modifications of the algorithm to improve precision <ref type="bibr" coords="3,361.61,483.50,14.61,8.74" target="#b17">[17]</ref>.</p><p>For our CriES experiments, we selected the HITS algorithm, as its values are computed at query time and on a smaller document base. Thus, the HITS algorithm does not require re-indexing the document collection to recompute scores after modifications or extensions to the algorithm. HITS scores also highly correlate with in/outdegree of linked nodes, which intuitively correspond to the level of expertise: the more information a person produces on a given topic, the higher her/his level of expertise should be.</p><p>The experiments for the CriES pilot challenge can be based on two different types of data which were provided by the organizers: a collection of Q/A pairs and a linked graph model extracted from this collection. Furthermore, the CriES challenge is unique in that it aims at expert finding in a multilingual setting, i.e. topics are provided in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic and Document Processing</head><p>Interpreting individual questions and answers as documents, standard IR techniques can be applied for expert search. In our work, the Lucene toolkit<ref type="foot" coords="4,456.34,183.51,3.97,6.12" target="#foot_2">3</ref> was utilized to preprocess the topics and documents, and to index and search the document collection. Standard Lucene modules were employed to tokenize the questions and answers and to fold upper case characters to lower case. Stopword lists from Jacques Savoy's web page on multilingual IR resources <ref type="foot" coords="4,418.08,231.33,3.97,6.12" target="#foot_3">4</ref> were used to identify stopwords. Stemming of topics and documents was performed using the Snowball stemmer for the corresponding language provided in Lucene. For all retrieval experiments, only the topic fields for 'title' and 'description' were used to create IR queries for Lucene (TD). The fields 'narrative' and 'questioner' were omitted for query formulation. The 'answerer' field was used to form documents IDs. Figure <ref type="figure" coords="4,187.43,304.64,4.98,8.74" target="#fig_0">1</ref> shows a sample topic.</p><p>The CriES question-answer set was preprocessed by us to generate two types of documents from the original CriES documents: answer documents (A) and question documents (Q). The first type of document contains the 'answerer' ID as a document ID and the text of his answer concatenated with the category of the question. This retrieval approach realizes standard IR by finding answers based on the replies the users have already generated. The second type of document contains the 'answerer' ID as a document ID and the question text concatenated with all category labels from the original document. Thus, retrieval on these documents aims at finding experts by matching the input question with previous questions the answerer has replied to. In detail, documents for indexing were created as follows: answer documents were extracted from answers given (i.e. 'bestanswer' ); question documents consist of the question text (i.e. 'subject', 'content' ). Both types of documents were concatenated with the category fields (i.e. 'cat', 'maincat', 'subcat' ). In addition, the link graph consisting of nodes representing experts and links between questioner and answerer (provided as part of the CriES challenge) was employed as input for the HITS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Information Retrieval System</head><p>Support for the BM25 retrieval model <ref type="bibr" coords="4,307.56,557.06,15.50,8.74" target="#b18">[18,</ref><ref type="bibr" coords="4,324.72,557.06,12.73,8.74" target="#b19">19]</ref> and for the corresponding BRF approach (see Equation 1 and 2) was implemented for Lucene by one of the authors. The BM25 score for a document and a query Q is defined as: &lt;topic lang="en"&gt; &lt;identifier&gt;3938625&lt;/identifier&gt; &lt;title&gt;What is the origin of "foobar"?&lt;/title&gt; &lt;description&gt;I want to know the meaning of the word and how to explain to my friends.&lt;/description&gt; &lt;narrative/&gt; &lt;category&gt;Programming &amp;amp; Design&lt;/category&gt; &lt;questioner&gt;u1061966&lt;/questioner&gt; &lt;answerer&gt;u25724&lt;/answerer&gt; &lt;/topic&gt; where Q is the query, containing terms t and w (1) is the RSJ (Robertson / Sparck-Jones) weight of t in Q <ref type="bibr" coords="5,271.27,301.15,14.61,8.74" target="#b20">[20]</ref>:</p><formula xml:id="formula_0" coords="4,244.86,603.40,235.73,26.80">t∈Q w (1) (k 1 + 1)tf K + tf (k 3 + 1)qtf k 3 + qtf<label>(1)</label></formula><formula xml:id="formula_1" coords="5,211.32,321.86,269.27,22.31">w (1) = (d + 0.5)/(D -d + 0.5) (n -d + 0.5)/(N -n -D + d + 0.5)<label>(2)</label></formula><p>where k 1 , k 3 , and b are model parameters. The default parameters for the BM25 model used are b = 0.75, k 1 = 1.2, and k 3 = 7. N is the number of documents in the collection and D is the number of documents known or presumed to be relevant for the current topic. For the experiments described in this paper, D was set to 0, i.e. no blind relevance feedback was employed, because the number of experts and precision of our initial retrieval were presumed to be very low. n is the document frequency for the term and d is the number of relevant documents containing the term. tf is the frequency of the term within a document; qtf is the frequency of the term in the topic. K = k 1 ((1 -b) + b • doclen/avg doclen) doclen and avg doclen are the document length and average document length, respectively. The BM25 retrieval model has been employed for many years in evaluation campaigns such as TREC <ref type="bibr" coords="5,297.09,487.09,14.61,8.74" target="#b19">[19]</ref>, but can still be considered as a stateof-the-art IR approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reranking with HITS</head><p>The HITS algorithm is a link analysis algorithm for rating web pages <ref type="bibr" coords="5,442.52,548.52,14.61,8.74" target="#b14">[14]</ref>. Unlike PageRank <ref type="bibr" coords="5,203.53,560.48,14.61,8.74" target="#b15">[15]</ref>, which produces a static, query-independent score, HITS produces two values for a web page: its authority and its hub value. In contrast to PageRank, HITS values are computed at query time and on results retrieved with an initial retrieval <ref type="bibr" coords="5,240.55,596.34,14.61,8.74" target="#b14">[14]</ref>. The computations are performed only on initially retrieved results, not across all linked web pages. The authority estimates the value of the content of a web page (also referred to as item in the rest of the paper, because the CriES data does not comprise web pages). In terms of expert search, the authority value indicates the quality of answers given, and indirectly the experts' level of expertise. The hub value estimates the value of its links to other pages. Authority and hub values are defined recursively and in terms of one another. The authority value is calculated as the sum of the scaled hub values that of items linking to that item. The hub value of an item is computed by the sum of the scaled authority values of the items it links to.</p><p>To apply HITS for expert search, the expert graph is viewed as a linked graph of experts (corresponding to web pages) with directed connections (links) from questioners to answerers if the answerer provided an answer to a question.  For the experiments described in this paper, the hub and authority values for an item are calculated with the following algorithmic steps, iterating steps (2)-( <ref type="formula" coords="6,155.32,533.77,4.11,8.74">4</ref>) for k times (see also Figure <ref type="figure" coords="6,288.96,533.77,3.87,8.74" target="#fig_1">2</ref>):</p><p>(1) Initialize: Set the hub and authority value for each item (node) to 1.</p><p>(2) Update authority values: Update the authority value of each item to be equal to the sum of the hub values of each item that points to it. That is, items with a high authority value are linked to by items that are recognized as informational hubs. (3) Update hub values: Update the hub value of each item to be equal to the sum of the authority values of each item that it points to. That is, items with a high hub value link to items that can be considered to be authorities on the subject. Applied to expert search, hubs can be interpreted as persons interested in a topic, and authorities can be seen as experts on a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The dataset consists of Q/A pairs which are maintained and verified by experts. In contrast to IR, results represent experts which may be associated with different levels of expertise; in comparison with FAQ finding, expert search focuses on looking for people most capable of providing an answer. In the simplest case, people have already provided that answer to the same or to similar questions. A graph model was provided as part of the CriES data, which consists of a directed graph representation where nodes denote topics, incoming links are questions and outgoing links represent answers.</p><p>The answer documents (A) and question documents (Q) generated from the CriES data were indexed separately. The following experimental settings were varied:</p><p>index: retrieval on answer documents (A) or question documents (Q) language: no topic translation; topic translation (using Google Translate) <ref type="foot" coords="7,473.36,380.69,3.97,6.12" target="#foot_4">5</ref> ; English topics only. retrieval method: using standard IR (BM25); combining BM25 with HITS authority values from top 50/100 results (HITS 50/100).</p><p>The BM25 retrieval model was used with default parameters (b = 0.75, k 1 = 1.2, and k 3 = 7), retrieving the top 100 results for each topic. The HITS algorithm was applied on the top 50 or top 100 results retrieved by standard retrieval with the BM25 model. The HITS algorithm was run for 50 iterations (k = 50). The experimental setting were chosen empirically after initial retrieval experiments on CriES test data. Table <ref type="table" coords="7,304.10,497.88,4.98,8.74" target="#tab_1">1</ref> shows results for official and additional expert search experiments on the CriES data. The submitted runs were obtained by retrieving 100 results via IR and reranking these results with the HITS authority value. The top ten results for each topic were extracted for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Analysis</head><p>As described in <ref type="bibr" coords="7,204.86,601.62,9.96,8.74" target="#b1">[1]</ref>, a baseline run resulting from BM25+Z-Score was generated by the organizers of the pilot challenge. This baseline experiment was based on different language-specific indexes, using Google Translate for topic translation. Z-Score normalization was employed to aggregate final results. Two different sets of relevant judgments were provided by the organizers, corresponding to strict evaluation (experts likely able to answer are considered relevant) and lenient evaluation (experts likely able to answer and experts which may be able to answer are relevant). A comparison of our experimental results to the provided baseline reveals that our best experimental results are slightly higher in terms of P@10 (0.24 vs. 0.19 for strict, 0.42 vs. 0.39 for lenient relevance judgments). The results do not consistently outperform the BM25 baseline, and show much lower performance than the best results reported by the organizers <ref type="bibr" coords="8,404.33,516.18,9.96,8.74" target="#b1">[1]</ref>. To test if this behavior was caused by the missing topic translation, the data was analyzed in more detail with respect to the languages.</p><p>The topic set contains 60 topics. The lenient relevance assessment contains 3602 relevant entries (60.03 relevant items on average per topic), the strict assessment 1736 entries (28.93 relevant items average). Table <ref type="table" coords="8,399.01,596.34,4.98,8.74" target="#tab_2">2</ref> shows the distribution of languages in topics, questions, and answers. As the topics are equally distributed among four languages (15 topics per language), a more detailed analysis of the language of questions and answers was performed. This analysis shows that languages of Q/A pairs are not equally distributed among these languages, i.e. there is a bias towards English (91.3%). The question index and answer index both contain 780,133 documents with the language distribution shown in Table <ref type="table" coords="9,320.25,267.74,3.87,8.74" target="#tab_2">2</ref>. <ref type="foot" coords="9,328.00,266.17,3.97,6.12" target="#foot_5">6</ref> While the topics are equally distributed between the four languages German (DE), English (EN), Spanish (ES), and French (FR), the majority of question and answer documents are in English.</p><p>As an additional experiment, the experimental results were calculated for the English topics only (the first 15 topics). However, there seems to be little bias towards English in relevant items compared to all items, because MAP slightly decreases from 0.1867 to 0.1800 and the number of relevant items is about a quarter of relevant items for all topics (27 vs. 112).</p><p>A comparison of retrieval on question documents and answer documents shows that results (i.e. rel ret, MAP, and P@10) are slightly higher for IR on answer documents. A possible explanation is that answers are typically longer than questions and provide more terms to match. Thus, a lexical mismatch may be less likely. The reranking with HITS authority typically shows considerable improvement in the number of relevant and retrieved results (rel ret), mean average precision (MAP), and P@10, more than doubling P@10 for the lenient evaluation.</p><p>There are several possible explanations of the moderate performance in comparison to the best CriES runs submitted by other participants (see, for example, the overview in <ref type="bibr" coords="9,204.53,487.65,10.30,8.74" target="#b1">[1]</ref>):</p><p>Multilinguality: All 60 topics (corresponding to queries) are equally distributed between German, English, Spanish, and French. Assuming that the questions and answers in the CriES data was similarly distributed, no topic or document translation was performed for our official experiments and all documents in their original language were organized in a single index. However, most answers in the CriES data are written in English. The baseline experiment in <ref type="bibr" coords="9,470.07,560.96,10.52,8.74" target="#b1">[1]</ref> was in contrast conducted on different language-specific indexes, combining results by Z-Score. Additional experiments on the subset of English topics and on translated topics show that there is in fact no bias towards English documents in the relevance assessments (see Table <ref type="table" coords="9,308.09,608.78,3.87,8.74" target="#tab_1">1</ref>).</p><p>Expert model: Experts are represented by a set of individual questions or answers. An aggregated model, i.e. combining all questions and answers into a single representation (e.g. document or weighted term vector) has not been investigated. The main reason for this is that a single category (unrelated to the current topic) may dominate in all contributions of a single user.</p><p>External resources: No additional external resources have been used for the experiments described in this paper. Standard approaches for FAQ search typically utilize resources such as WordNet <ref type="bibr" coords="10,324.01,206.60,10.52,8.74" target="#b7">[7]</ref> to bridge the lexical gap between questions and answers. However, in a multilingual problem setting, WordNet may be of limited use, because WordNet synsets contain only English terms. Multilingual resources such as EuroWordNet <ref type="bibr" coords="10,332.53,242.46,15.50,8.74" target="#b21">[21]</ref> seem to be more suitable, but suffer from a limited lexical coverage.</p><p>Link analysis: Traditional approaches for link analysis of the link graph provided in the CriES data have not been used. Instead, the HITS algorithm, which is typically applied for reranking web pages has been employed for reranking results. Interestingly, reranking initial retrieval results with HITS authority values improved performance considerably in most cases, increasing MAP and P@10 and yielding more than double the number of relevant results compared to the corresponding BM25 experiment, even for poor initial results. This result was unexpected, because the initial precision is very low for the result set retrieved with the BM25 model. Standard query expansion techniques such as blind relevance feedback also aim at improving at performance by reranking documents in a second retrieval phase, but build on the assumption that top-ranked documents in an initial retrieval phase are relevant (i.e. the initial precision is already high). If initial results have low precision, standard query expansion techniques will typically add noise instead of useful terms. Reranking results with HITS authority values improves performance despite low initial precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>The experiments on the CriES data show that traditional IR methods alone (i.e. the BM25 retrieval model) may not be suitable for this kind of task and social network or link analysis may be more successful. Reranking results with HITS authority values seems to improve performance even when the initial precision is low. The multilingual aspect introduced in the CriES challenge seems artificial because most contributions in the data are in English. However, experiments on the English topic subset did not show a bias. Future work will include adding knowledge from external resources such as Wikipedia and expanding the use of categories and other metadata provided in the CriES data. Also, reranking with HITS authority scores for ad-hoc IR will be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,213.62,125.87,188.11,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample topic from the CriES topic set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,172.23,235.58,270.91,7.89;6,144.18,257.17,112.98,7.47;6,144.18,268.13,84.73,7.47;6,252.45,268.13,70.61,7.47;6,158.30,279.09,51.78,7.47;6,252.45,279.09,169.46,7.47;6,158.30,290.05,51.78,7.47;6,252.45,290.05,141.22,7.47;6,144.18,301.00,56.49,7.47;6,252.45,301.00,150.64,7.47;6,158.30,311.96,47.07,7.47;6,252.45,311.96,122.39,7.47;6,158.30,322.92,47.07,7.47;6,252.45,322.92,94.15,7.47;6,158.30,333.88,225.95,7.47;6,172.42,344.84,174.17,7.47;6,252.45,355.80,145.93,7.47;6,186.55,366.76,70.61,7.47;6,172.42,377.72,103.56,7.47;6,158.30,388.68,197.71,7.47;6,172.42,399.63,174.17,7.47;6,252.45,410.59,150.63,7.47;6,186.55,421.55,75.32,7.47;6,172.42,432.51,98.85,7.47;6,158.30,443.47,193.00,7.47;6,172.42,454.43,70.61,7.47;6,172.42,465.39,70.61,7.47"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Variant of the HITS algorithm used for CriES experiments.I := set of linked items FOR EACH i IN I DO // (initialize) i.auth := 1 // initial authority value of item i i.hub := 1 // initial hub value of item i FOR t=1 TO k // run the algorithm for k steps a_sum := 0 // sum of authority values h_sum := 0 // sum of hub values FOR EACH i in I DO // (update authority values) FOR EACH j IN i.incomingNeighbours do // process items that link to i i.auth += j.hub a_sum += i.auth*i.auth FOR EACH i in I DO // (update hub values) FOR EACH j IN i.outgoingNeighbours do // process items that i links to i.hub += j.auth h_sum += i.hub *i.hub FOR EACH i in I DO // (normalize values) i.auth /= a_sum i.hub /= h_sum</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,133.99,118.99,346.60,8.74;7,151.70,130.95,328.89,8.74;7,151.70,142.90,257.62,8.74"><head>( 4 )</head><label>4</label><figDesc>Normalize values: Normalize the authority and hub values by dividing each authority value by the sum of the squares of all authority values, and dividing each hub value by the sum of the squares of all hub values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,136.31,115.91,339.79,62.50"><head>Table 1 .</head><label>1</label><figDesc>Results for CriES experiments. Official experiments are set in italics.</figDesc><table coords="8,343.26,138.82,108.41,7.86"><row><cell>strict</cell><cell>lenient</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,116.41,345.83,106.33"><head>Table 2 .</head><label>2</label><figDesc>Language distribution of topics, Q/A documents and questions and answers from relevant experts.</figDesc><table coords="9,311.79,150.28,35.33,7.86"><row><cell>language</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,122.39,7.47"><p>http://research.yahoo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.44,188.79,7.47"><p>http://www.ins.cwi.nl/projects/trec-ent/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,646.48,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,657.44,249.49,7.47"><p>http://members.unine.ch/jacques.savoy/clef/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,144.73,657.44,131.81,7.47"><p>http://translate.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,144.73,634.88,215.32,7.86"><p>The original number of documents reported by<ref type="bibr" coords="9,340.56,634.88,9.73,7.86" target="#b1">[1]</ref> is</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6" coords="9,363.61,634.88,116.98,7.86;9,144.73,645.84,335.87,7.86;9,144.73,656.80,55.31,7.86"><p>780,193, but a small number of documents do not include a language code or contain invalid XML and have not been indexed.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This material is based upon works supported by the <rs type="funder">Science Foundation Ireland</rs> under Grant No. Grant <rs type="grantNumber">07/CE/I1142</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6G68yKR">
					<idno type="grant-number">07/CE/I1142</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,136.31,186.51,121.79,7.86;8,328.62,186.51,150.43,7.86;8,136.31,197.47,173.49,7.86;8,328.62,197.47,150.42,7.86;8,136.31,208.43,168.89,7.86;8,324.02,208.43,155.03,7.86;8,136.31,224.39,122.05,7.86;8,328.62,224.39,150.43,7.86;8,136.31,235.35,173.75,7.86;8,328.62,235.35,150.42,7.86;8,136.31,246.30,169.14,7.86;8,324.01,246.30,155.03,7.86;8,136.31,264.76,98.91,7.86;8,328.62,264.76,65.99,7.86;8,413.06,264.76,65.99,7.86;8,136.31,275.71,146.01,7.86;8,328.62,275.71,65.99,7.86;8,413.06,275.71,65.99,7.86;8,136.31,291.68,99.17,7.86;8,328.62,291.68,65.99,7.86;8,413.06,291.68,65.99,7.86;8,136.31,302.63,146.27,7.86;8,333.23,302.63,61.38,7.86;8,413.06,302.63,65.99,7.86;8,136.31,321.09,111.10,7.86;8,328.62,321.09,65.99,7.86;8,413.06,321.09,65.99,7.86;8,136.31,332.05,158.20,7.86;8,324.01,332.05,155.03,7.86;8,136.31,343.00,111.36,7.86;8,328.62,343.00,65.99,7.86;8,413.06,343.00,65.99,7.86;8,136.31,353.96,158.46,7.86;8,328.62,353.96,150.43,7.86;11,134.77,192.15,62.94,10.52" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bm25</surname></persName>
		</author>
		<idno>.1250 119 0.0213 0.1983</idno>
		<imprint/>
	</monogr>
	<note>no topic translation, HITS (100) 56 0.0123 0. A, no topic translation, HITS (50) 146 0. topic translation 38 0. no topic translation, HITS (100) 46 0.0113 0. no topic translation, HITS (50) 128 0. A, English topics 12 0.0107. A, English topics, HITS (50) 18 0.0139 0.. English topics 10 0.0070 0. English topics, HITS (50) 9 0.0046 0.. Google Translate. 50) 101 0. Google Translate. 50) 75 0.0290 0</note>
</biblStruct>

<biblStruct coords="11,142.96,220.55,337.64,7.86;11,151.52,231.51,299.73,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,277.46,220.55,203.13,7.86;11,151.52,231.51,56.76,7.86">Overview of the cross-lingual expert search (CriES) pilot challenge</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,230.36,231.51,188.33,7.86">Working Notes of the CLEF 2010 Lab Sessions</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,243.15,337.64,7.86;11,151.52,254.11,329.07,7.86;11,151.52,265.06,329.07,7.86;11,151.52,276.02,257.64,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,338.91,243.15,141.68,7.86;11,151.52,254.11,85.07,7.86">Learning to rank answers on large online QA collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.15,254.11,221.44,7.86;11,151.52,265.06,183.02,7.86">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2008">June 15-20, 2008. 2008</date>
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,287.66,337.63,7.86;11,151.52,298.62,329.07,7.86;11,151.52,309.58,218.90,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,295.07,287.66,185.52,7.86;11,151.52,298.62,167.12,7.86">Finding answers in large collections of texts: Paragraph indexing + abductive inference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,339.42,298.62,141.17,7.86;11,151.52,309.58,159.72,7.86">Proceedings of the AAAI Fall Symposium on Question Answering Systems</title>
		<meeting>the AAAI Fall Symposium on Question Answering Systems</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,321.22,337.63,7.86;11,151.52,332.18,329.07,7.86;11,151.52,343.14,329.07,7.86;11,151.52,354.10,157.22,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,151.52,332.18,310.14,7.86">Natural language processing in the FAQ finder system: Results and prospects</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kulyukin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lytinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,343.14,329.07,7.86;11,151.52,354.10,97.32,7.86">Proceedings of the 1997 AAAI Spring Symposium on Natural Language Processing for the World Wide Web</title>
		<meeting>the 1997 AAAI Spring Symposium on Natural Language Processing for the World Wide Web</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,365.74,337.63,7.86;11,151.52,376.70,329.07,7.86;11,151.52,387.66,329.07,7.86;11,151.52,398.61,71.42,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,151.52,376.70,329.07,7.86;11,151.52,387.66,51.63,7.86">Question answering from frequently-asked-question files: Experiences with the FAQ finder system</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kulyukin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lytinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schoenberg</surname></persName>
		</author>
		<idno>TR-97-05</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,142.96,410.25,337.63,7.86;11,151.52,421.19,307.89,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,285.11,410.25,195.48,7.86;11,151.52,421.21,27.64,7.86">Domain-specific FAQ retrieval using independent aspects</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,187.20,421.21,201.76,7.86">ACM Transactions on Asian Language Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,432.85,337.64,7.86;11,151.52,443.81,116.71,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,230.26,432.85,168.58,7.86">Wordnet. An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,455.45,337.63,7.86;11,151.52,466.41,163.65,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<title level="m" coord="11,236.29,455.45,171.44,7.86">HowNet And the Computation of Meaning</title>
		<meeting><address><addrLine>River Edge, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific Publishing</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,478.05,337.64,7.86;11,151.52,489.01,329.07,7.86;11,151.52,499.97,23.04,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,253.60,478.05,226.99,7.86;11,151.52,489.01,41.22,7.86">Retrieving answers from frequently asked questions pages on the web</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,214.03,489.01,34.94,7.86">CIKM&apos;05</title>
		<meeting><address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11-05">October 31-November 5 2005. 2005</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,511.61,337.98,7.86;11,151.52,522.54,329.07,7.89;11,151.52,533.53,60.92,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,295.14,511.61,181.50,7.86">Dynamic FAQ retrieval with rough set theory</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,522.57,307.54,7.86">IJCSNS International Journal of Computer Science and Network Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="204" to="211" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,545.17,337.97,7.86;11,151.52,556.12,329.07,7.86;11,151.52,567.08,329.07,7.86;11,151.52,578.04,120.56,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,311.28,545.17,169.30,7.86;11,151.52,556.12,51.39,7.86">Formal models for expert finding in enterprise corpora</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,226.66,556.12,253.93,7.86;11,151.52,567.08,281.84,7.86">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,589.68,337.98,7.86;11,151.52,600.64,329.07,7.86;11,151.52,611.60,262.15,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,256.53,589.68,220.14,7.86">The influence of the document ranking in expert search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.30,600.64,315.29,7.86;11,151.52,611.60,69.64,7.86">CIKM &apos;09: Proceeding of the 18th ACM conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1983" to="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,623.24,337.98,7.86;11,151.52,634.20,281.08,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,343.40,623.24,137.19,7.86;11,151.52,634.20,138.41,7.86">EFS: Expert finding system based on Wikipedia link pattern analysis</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,311.89,634.20,56.72,7.86">IEEE explore</title>
		<imprint>
			<biblScope unit="page" from="631" to="635" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,645.84,337.97,7.86;11,151.52,656.77,110.51,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,209.89,645.84,205.72,7.86">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,423.97,645.84,56.62,7.86;11,151.52,656.80,21.75,7.86">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,73.47,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,224.48,119.67,238.31,7.86">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,130.63,329.07,7.86;12,151.52,141.59,3.58,7.86">WWW7: Proceedings of the seventh international conference on World Wide Web 7</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science Publishers B. V</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,163.51,337.97,7.86;12,151.52,174.47,329.07,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,60.92,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,315.81,163.51,141.85,7.86">Stable algorithms for link analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,174.47,329.07,7.86;12,151.52,185.43,213.24,7.86">SIGIR &apos;01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="258" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.98,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,250.90,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,283.41,207.34,197.19,7.86;12,151.52,218.30,41.06,7.86">Improvement of HITS-based algorithms on web documents</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,218.23,218.30,262.36,7.86;12,151.52,229.26,67.35,7.86">WWW &apos;02: Proceedings of the 11th international conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="527" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.97,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,329.07,7.86;12,151.52,273.10,159.48,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,151.52,251.18,70.55,7.86">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,327.03,251.18,153.56,7.86;12,151.52,262.14,86.83,7.86">Overview of the Third Text Retrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology (NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,284.06,337.97,7.86;12,151.52,295.02,329.07,7.86;12,151.52,305.98,329.07,7.86;12,151.52,316.93,203.06,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,326.86,284.06,153.73,7.86;12,151.52,295.02,146.26,7.86">Okapi at TREC-7: Automatic ad hoc, filtering, VLC and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,405.62,295.02,74.97,7.86;12,151.52,305.98,129.39,7.86">The Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology (NIST</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,327.89,337.97,7.86;12,151.52,338.83,271.79,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,296.53,327.89,144.73,7.86">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sparck-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,450.23,327.89,30.35,7.86;12,151.52,338.85,194.11,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,349.81,337.97,7.86;12,151.52,360.77,329.07,7.86;12,151.52,371.73,90.10,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,206.37,349.81,122.32,7.86;12,359.02,349.81,121.57,7.86;12,151.52,360.77,162.45,7.86">EuroWordNet: a multilingual database with lexical semantic networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="1" to="17" />
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to EuroWordNet</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
