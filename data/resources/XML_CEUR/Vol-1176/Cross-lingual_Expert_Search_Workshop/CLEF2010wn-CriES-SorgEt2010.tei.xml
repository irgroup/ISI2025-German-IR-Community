<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.87,116.95,315.62,12.62;1,223.11,134.89,169.14,12.62">Overview of the Cross-lingual Expert Search (CriES) Pilot Challenge</title>
				<funder ref="#_rYRuzWV">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_wFDQEW9">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder>
					<orgName type="full">Monnet</orgName>
				</funder>
				<funder>
					<orgName type="full">Multipla</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.31,172.66,54.41,8.74"><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
							<email>philipp.sorg@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute AIFB</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.27,172.66,71.54,8.74"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
							<email>cimiano@cit-ec.uni-bielefeld.de</email>
							<affiliation key="aff1">
								<orgName type="department">Center of Excellence (CITEC)</orgName>
								<orgName type="laboratory">Cognitive Interaction Technology</orgName>
								<orgName type="institution">Bielefeld University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.37,172.66,58.95,8.74"><forename type="first">Antje</forename><surname>Schultz</surname></persName>
							<email>antjeschultz|sizov@uni-koblenz.de</email>
							<affiliation key="aff2">
								<orgName type="department">Information Systems &amp; Semantic Web</orgName>
								<orgName type="institution">University of Koblenz</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.25,172.66,52.33,8.74"><forename type="first">Sergej</forename><surname>Sizov</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Information Systems &amp; Semantic Web</orgName>
								<orgName type="institution">University of Koblenz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.87,116.95,315.62,12.62;1,223.11,134.89,169.14,12.62">Overview of the Cross-lingual Expert Search (CriES) Pilot Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4232EF48ACBC73284EDDD05FA905F9C8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an overview of the cross-lingual expert search pilot challenge as part of the cross-lingual expert search (CriES) workshop collocated with the CLEF 2010 conference. We present a detailed description of the dataset used in the challenge. This dataset is a subset of an official crawl of Yahoo! Answers published in the context of the Yahoo! Webscope program. Further we describe the selection process of the 60 multilingual topics used in the challenge. The Gold Standard for these topics was created by human assessors who evaluated pooled results of submitted runs. We present data showing that the experts relevant for our chosen topics indeed speak different languages. This corroborates the fact that we need to design retrieval systems that build on a cross-lingual notion of relevance for the expert retrieval task. Finally we summarize the results of the four groups that participated in this challenge using standard evaluation measures. Additionally we also analyze the overlap of retrieved experts in the submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CriES workshop -Cross-lingual Expert Search: Bridging CLIR and Social Media -addresses the problem of multilingual expert search in social media environments. The main topics are multilingual expert retrieval methods, social media analysis with respect to expert search, selection of datasets and evaluation of expert search results.</p><p>In this paper we describe the pilot challenge as part of the CriES workshop. This includes a detailed description of the dataset, the selection process for the topics used in the challenge and the evaluation methodology including relevance assessment. We also present an overview of the results submitted by the participating groups.</p><p>Motivation. Online communities generate major economic value and form pivotal parts of corporate expertise management, marketing, product support, CRM, product innovation and advertising. In many cases, large-scale online communities are multilingual by nature (e.g. developer networks, corporate knowledge bases, blogospheres, Web 2.0 portals). Nowadays, novel solutions are required to deal with both the complexity of large-scale social networks and the complexity of multilingual user behavior.</p><p>At the same time, it becomes more and more important to efficiently identify and connect the right experts for a given task across locations, organizational units and languages. The key objective of the lab is to consider the problem of multilingual retrieval in the novel setting of modern social media leveraging the expertise of individual users.</p><p>Pilot Challenge Topic and Goals. We instantiate the problem setting by an expert finding task, i.e. our goal is to identify the expertise of online community members and to provide expert suggestions for solving new problems, questions, or help requests in multilingual social media. In many cases, expert users in online communities are multilingual, i.e. they participate in discussions in several languages. Frequently, the actual expertise of the user is language-independent, so he/she could provide meaningful assistance and support to questions and requests stated in any of the known languages. The combined analysis of multilingual user contributions (e.g. answers or postings from the past) together with mining of his social environment (e.g. interaction with other community members in the past, contact/favorite lists, etc.) may provide better indications that the user has the necessary expertise for addressing the request irrespective of the language. The key research challenges addressed by the expert finding task can be summarized as follows:</p><p>-User characterization: the use of multilingual evidence of social media for building expert profiles; -Community analysis: mining of social relationships in collaborative environments for multilingual retrieval scenarios; -User-centric recommender algorithms: development of retrieval and recommendation algorithms that allow for similarity search and ranked retrieval of expert users in online communities (in contrast to more common document retrieval tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>We used the dataset from the Yahoo! Answers portal introduced by Surdeanu et al. <ref type="bibr" coords="2,161.03,571.65,9.96,8.74" target="#b2">[3]</ref>. <ref type="foot" coords="2,174.31,570.08,3.97,6.12" target="#foot_0">4</ref> Yahoo! Answers is currently the biggest community QA portal. According to Google ad planner statistics<ref type="foot" coords="2,303.46,582.04,3.97,6.12" target="#foot_1">5</ref> the portal attracts 97M unique visitors and 1.1B page views per month.<ref type="foot" coords="2,275.57,593.99,3.97,6.12" target="#foot_2">6</ref> </p><p>The dataset published by Yahoo! contains 4.5M questions with 35.9M answers. For each question one answer is marked as best answer. In the portal, the best answer is determined by either the user who submitted the question or via other users ratings. The dataset contains IDs of authors of questions and best answers, whereas authors of non-best answers are anonymous. Questions are organized into categories which form a category taxonomy.</p><p>The dataset used in the CriES pilot challenge is a subset of the Yahoo! Answers Webscope dataset, considering only questions and answers in the topic fields defined by the following three top level categories including their sub categories: "Computer &amp; Internet", "Health" and "Science &amp; Mathematics". As our approach is targeted at the expert retrieval problem, by choosing very "technical categories" our goal was to yield a dataset with a high number of technical questions requiring domain expertise to be answered. As many questions in the dataset serve the only purpose of diversion, it was important to find categories were the share of such questions is low.</p><p>In the challenge, 4 languages are considered: English, German, French and Spanish. As category names are language-specific, questions and answers from categories corresponding to the selected categories in other languages are also included, e.g. "Gesundheit" (German), "Sant√©" (French) and "Salud" (Spanish) that correspond to the "Health" category.</p><p>The selected dataset consists of 780,193 questions, each question having exactly one best answer. The answers were posted by 169,819 different users, i.e. potential experts in our task. The answer count per expert follows a power log distribution, i.e. 54% of the experts posted one answer, 93% 10 or less, 96% 20 or less. 410 experts published answers in more than one language. These multilingual experts posted 8,976 answers, which shows that they are active users in the portal. The language of questions and answers are distributed over languages as shown in the following 185,994 91% 1% 2% 6%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic Selection</head><p>The topics we use in the challenge consist of 15 questions in each language (60 topics in total). As these topics are questions posted by users of the portal, they express a real information need. Considering the multilingual dimension of our expert retrieval scenario, we defined the following criteria for topic selection to ensure that the selected topics are indeed applicable for our task:</p><p>-International domain. People from other countries should be able to answer the question. In particular, answering the question should not require knowledge that is specific to a geographic region, country or culture. Examples:</p><p>Pro: Why doesn't an optical mouse work on a glass table? Contra: Why is it so foggy in San Francisco? -Expertise questions. As the goal of our system is to find experts in the domain of the question, all questions should require domain expertise to answer them. This excludes for example questions that ask for opinions or do not expect an answer at all. Examples:</p><p>Pro: What is a blog? Contra: What is the best podcast to subscribe to?</p><p>We performed the following steps to select the topics:</p><p>1. Selection of 100 random questions per language from the dataset (total of 400 candidate topics). 2. Manual assessment of each candidate topic by three human assessors. They were instructed to check the fulfillment of the criteria defined above. 3. For each question the language coverage was computed. The language coverage tries to quantify how much potentially relevant experts are contained in the dataset for each topic and for each of the different languages. The language coverage was calculated by translating a topic into the different languages (using Google Translate) and then using a standard IR system to retrieve all the expert profiles that contain at least one of the terms in the translated query. Topics were assigned high language coverage if they matched an average number of experts in all of the languages. In this way we ensure that the topics are well covered in the different languages under consideration but do not match too many experts profiles in each language. This is important for our multilingual task as we intend to find experts in different languages. 4. Candidate questions are sorted first by the manual assessment and then by language coverage. The top 15 questions in each language were selected as topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relvance Assessment</head><p>We used result pooling for the evaluation of the retrieval results of the participating groups. For each pooled run, the top 10 experts were pooled and evaluated. The assessment of experts was based on expert profiles. Assessors received topics and the complete profile of experts, consisting of all answers posted by the expert in question. Based on this information they assigned topic-expert tuples to the following relevance classes:</p><p>2 Expert is likely able to answer. 1 Expert may be able to answer. 0 Expert is probably not able to answer. The assessors were instructed to only use evidence in the dataset for their judgments. It is assumed that experts expressed all their knowledge in the answer history and will not have expertise about other topics, unless it can be inferred from existing answers. Overall, 6 assessors evaluated 7,515 pairs of topics and expert profiles. The distribution of relevant users for the topics in the four different languages is presented in Figure <ref type="figure" coords="6,223.68,191.87,3.87,8.74" target="#fig_0">1</ref>. In order to visualize the multilingual nature of the task we also classified relevant users to languages using their answers in the dataset. The distribution of relevant users for the topics in the four languages is shown separately for each user group. The analysis of the relevant user distribution shows that for all topics the main share of relevant users publish answers either in the topic language or in English. This motivates the cross-language expert retrieval task we consider, as mono-lingual retrieval in the topic language or cross-lingual retrieval from the topic language to English do clearly not suffice. The number of relevant experts posting in a different language than the topic language or English constitute a small share. However the percentage is large enough -for example Spanish experts for German topics -in order not to consider these experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Baseline. In addition to the submitted runs, we defined a standard IR baseline: BM25+Z-Score. This baseline uses language specific indexes of expert text profiles. These profiles consist of all former answers of each expert in a specific language. Topics are translated to each language using Google Translate and the BM25 model <ref type="bibr" coords="6,212.33,428.48,10.52,8.74" target="#b0">[1]</ref> is used to get language specific results. Using the Z-Score normalization <ref type="bibr" coords="6,197.26,440.43,9.96,8.74" target="#b1">[2]</ref>, the final scores for each expert for a specific topic are obtained by aggregation.</p><p>Evaluation of Submitted Runs. Four different groups participated in the pilot challenge. Results based on the relevance assessment of the top 10 retrieved experts are presented in Figure <ref type="figure" coords="6,279.33,500.95,3.87,8.74" target="#fig_0">1</ref>. In addition to the submitted runs we also present results for the baseline define above. We use two different evaluation measures: Precision at cutoff level 10 (P@10) and Mean Reciprocal Rank (MRR). The best runs achieved promising retrieval results with P@10 of .62 (strict assessment, iftene run2) and .87 (lenient assessment, herzig 3-boe-07-02-01-q01m). Both runs significantly improve the baseline that achieves P@10 of .19 (strict assessment) and .39 (lenient assessments). Precision / Recall curves for each run are presented in Figure <ref type="figure" coords="6,240.59,584.64,4.98,8.74">2</ref> using strict assessment and in Figure <ref type="figure" coords="6,416.41,584.64,4.98,8.74">3</ref> using lenient assessment.</p><p>Overlap of Retrieved Experts. The overlap of retrieved experts between runs is presented in Table <ref type="table" coords="6,221.26,633.20,3.87,8.74">2</ref>. Comparing any combination of two runs, the presented numbers correspond to the count of retrieved experts for each topic that are not retrieved by both runs.   <ref type="table" coords="8,164.48,586.41,4.13,7.89">2</ref>. Dissimilarity matrix for retrieved experts of the submitted runs. The presented numbers correspond to the count of retrieved experts of each run for all topics that are not retrieved by the compared run.</p><p>The presented statistics show that the overlap of retrieved experts across the four groups is very low. Even the best performing runs of two different groups (iftene run2, herzig 3-boe-07-02-01-q01m) have a small overlap of 14% while having similar values for P@10 and MRR. This shows that the combination of different approaches is an important topic for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,555.57,345.82,7.89;5,134.77,566.56,345.82,7.86;5,134.77,577.52,221.37,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of relevant users for topics in different languages. Users are classified to languages based on their answers submitted to the Yahoo! Answers portal and for each user class a separate distribution is visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,144.68,622.87,325.99,7.89"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Precision/Recall Curves based on interpolated Recall (strict assessment).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,194.13,443.61,223.39,77.34"><head>table :</head><label>:</label><figDesc></figDesc><table coords="3,194.13,463.99,223.39,56.96"><row><cell></cell><cell>Language share</cell></row><row><cell>Category</cell><cell>Questions EN DE FR ES</cell></row><row><cell>Comp. &amp; Internet</cell><cell>317,074 89% 1% 3% 6%</cell></row><row><cell>Health</cell><cell>294,944 95% 1% 2% 2%</cell></row><row><cell>Science &amp; Math.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,151.03,345.82,161.64"><head>Table 1 .</head><label>1</label><figDesc>Results of the runs submitted to the CriES pilot challenge. Precision at cutoff level 10 (P@10) and Mean Reciprocal Rank (MRR) are used as evaluation measures. Results are presented for both strict and lenient assessments.</figDesc><table coords="7,195.90,151.03,222.17,129.21"><row><cell></cell><cell></cell><cell>Strict</cell><cell>Lenient</cell></row><row><cell></cell><cell>Run Id</cell><cell cols="2">P@10 MRR P@10 MRR</cell></row><row><cell></cell><cell>iftene run2</cell><cell>.62 .84</cell><cell>.83 .94</cell></row><row><cell></cell><cell>iftene run0</cell><cell>.52 .80</cell><cell>.82 .94</cell></row><row><cell></cell><cell>herzig 3-boe-07-02-01-q01m</cell><cell>.49 .76</cell><cell>.87 .93</cell></row><row><cell>t</cell><cell>herzig 1-boe-06-03-01-q01m iftene run1</cell><cell>.48 .77 .47 .77</cell><cell>.86 .94 .77 .93</cell></row><row><cell></cell><cell>herzig 2-boe-06-03-01-q01</cell><cell>.35 .65</cell><cell>.61 .74</cell></row><row><cell></cell><cell>leveling DCUa</cell><cell>.09 .14</cell><cell>.40 .51</cell></row><row><cell></cell><cell>leveling DCUq</cell><cell>.08 .16</cell><cell>.42 .54</cell></row><row><cell></cell><cell>bastings</cell><cell>.07 .15</cell><cell>.25 .43</cell></row><row><cell></cell><cell>BM25 + Z-Score</cell><cell>.19 .40</cell><cell>.39 .63</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,613.96,335.86,8.12;2,144.73,624.92,335.86,8.12;2,144.73,635.88,149.40,7.86"><p>This dataset is provided by the Yahoo! Research Webscope program (see http:// research.yahoo.com/) under the following ID: L6. Yahoo! Answers Comprehensive Questions and Answers (version 1.0)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,647.48,150.64,7.47"><p>http://www.google.com/adplanner/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,144.73,657.79,107.89,7.86"><p>Statistics from 2010/05/17</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="9,144.73,647.48,151.13,7.47"><p>http://www.multipla-project.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="9,144.73,658.44,137.01,7.47"><p>http://www.monnet-project.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by the <rs type="funder">Multipla</rs> project 7 sponsored by the <rs type="funder">German Research Foundation (DFG)</rs> under grant number <rs type="grantNumber">38457858</rs> as well as by the <rs type="funder">Monnet</rs> project 8 funded by the <rs type="funder">European Commission</rs> under <rs type="grantNumber">FP7</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wFDQEW9">
					<idno type="grant-number">38457858</idno>
				</org>
				<org type="funding" xml:id="_rYRuzWV">
					<idno type="grant-number">FP7</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,302.99,342.24,7.86;9,146.91,313.95,333.67,7.86;9,146.91,324.90,333.68,7.86;9,146.91,335.86,138.14,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,262.58,302.99,218.01,7.86;9,146.91,313.95,163.74,7.86">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,330.86,313.95,149.73,7.86;9,146.91,324.90,312.35,7.86">Proceedings of the 17th International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 17th International Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,346.82,342.24,7.86;9,146.91,357.78,333.67,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,188.30,346.82,274.51,7.86">Data fusion for effective european monolingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,146.91,357.78,245.10,7.86">Multilingual Information Access for Text, Speech and Images</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,368.74,342.25,7.86;9,146.91,379.70,333.67,7.86;9,146.91,390.66,289.41,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,321.30,368.74,159.30,7.86;9,146.91,379.70,57.75,7.86">Learning to rank answers on large online QA collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,225.87,379.70,254.71,7.86;9,146.91,390.66,133.79,7.86">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
