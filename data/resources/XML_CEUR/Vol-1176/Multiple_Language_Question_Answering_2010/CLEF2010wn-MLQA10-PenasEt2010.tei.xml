<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.80,74.05,411.23,12.72;1,233.28,90.13,128.76,12.72">Overview of ResPubliQA 2010: Question Answering Evaluation over European Legislation</title>
				<funder ref="#_GD5g7UA">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder>
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,107.52,129.01,61.83,11.04"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.60,129.01,58.75,11.04"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.20,129.01,62.82,11.04"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff2">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.36,129.01,68.50,11.04"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.16,129.01,61.38,11.04"><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">UAIC and RACAI</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,464.76,129.01,55.18,11.04"><forename type="first">Cristina</forename><surname>Mota</surname></persName>
							<email>cmota@ist.utl.pt</email>
							<affiliation key="aff5">
								<orgName type="institution">SINTEF ICT</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.80,74.05,411.23,12.72;1,233.28,90.13,128.76,12.72">Overview of ResPubliQA 2010: Question Answering Evaluation over European Legislation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">37651BC91CC1DD5A8138D59519B3EC93</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the second round of ResPubliQA, a Question Answering (QA) evaluation task over European legislation, a LAB of CLEF 2010. Two tasks have been proposed this year: Paragraph Selection (PS) and Answer Selection (AS). The PS task consisted of extracting a relevant paragraph of text that satisfies completely the information need expressed by a natural language question. In the AS task, the exercise was to demarcate the shorter string of text corresponding to the exact answer supported by the entire paragraph. The general aims of this exercise are (i) to move towards a domain of potential users; (ii) to propose a setting which allows the direct comparison of performance across languages; (iii) to allow QA technologies to be evaluated against IR approaches; (iv) to promote validation technologies to reduce the amount of incorrect answers by leaving some questions unanswered. These goals are achieved through the use of parallel aligned document collections (JRC-Acquis and EUROPARL) and the possibility to return two different types of answers, either passages or exact strings. The paper describes the task in more detail, presenting the different types of questions, the methodology for the creation of the test sets and the evaluation measure, and analyzing the results obtained by systems and the more successful approaches. Thirteen groups participated in both PS and AS tasks submitting 49 runs in total.</p><p>Beside the question types used last year (Factoid, Definition, Procedure) two additional question categories were added in the 2010 campaign: Opinion and a miscellanea called Other. Moreover, Reason and Purpose categories were merged into a single one as the distinction between them was a little blurred in the past edition. The following are examples of these types of questions:</p><p>Factoid. Factoid questions are fact-based questions, asking for the name of a person, a location, the extent of something, the day on which something happened, etc. For example:</p><p>Q: What percentage of people in Italy relies on television for information? P: In Italy, 80% of the people get their daily information from television. If that television is not broadcasting all voices, then people do not get the chance to make their own decisions. That is fundamental to democracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The ResPubliQA 2010 exercise is aimed at retrieving answers to a set of 200 questions over EUROPARL and ACQUIS collections. Questions were offered in 8 different languages: Basque (EU), English (EN), French (FR), German (DE), Italian (IT), Portuguese (PT), Romanian (RO) and Spanish (ES). All Monolingual and Crosslanguage subtasks combinations of questions between the last 7 languages above were activated, including monolingual English <ref type="bibr" coords="1,158.16,511.81,18.02,11.04">(EN)</ref>. Basque (EU), instead, was included exclusively as a source language, as there is no Basque translation of the document collection, which means that no monolingual EU-EU sub-task could be enacted.</p><p>The design of the ResPubliQA 2010 evaluation campaign was to a large extent the repetition of the previous year's exercise <ref type="bibr" coords="1,134.76,569.29,11.71,11.04" target="#b0">[1]</ref> with the addition of a number of refinements. Thus, the main goals of the lab this year are basically the same: Moving towards a domain of potential users; Moving to an evaluation setting able to compare systems working in different languages; Comparing current QA technologies with pure Information Retrieval (IR) approaches; Allowing more types of questions; Introducing in QA systems the Answer Validation technologies developed in the past campaigns <ref type="bibr" coords="1,256.32,615.25,11.01,11.04" target="#b1">[2,</ref><ref type="bibr" coords="1,267.33,615.25,7.34,11.04" target="#b2">3,</ref><ref type="bibr" coords="1,274.68,615.25,7.34,11.04" target="#b4">5]</ref>.</p><p>As a difference with the previous campaign, this year participants had the opportunity to return both paragraph and exact answers as system output. Another novelty this year is the addition of a portion of the EUROPARL collection <ref type="foot" coords="1,110.40,660.90,3.00,6.65" target="#foot_0">1</ref> in the languages involved in the task. The subject of EUROPARL's parliamentary domains is different in style and content from ACQUIS while being fully compatible with it. This has given participants the opportunity to adapt their systems in a way which widens their coverage in compatible domains; and for the organizers it has represented the opportunity to widen the scope of the questions (through the introduction of new types of question, as for example opinion).</p><p>The paper is organized as follows: Section 2 illustrates the document collection; Section 3 gives an overview of the different types of question developed; Section 4 addresses the various steps to create the ResPubliQA data set; Section 5 provides an explanation of the evaluation measure and of how systems have been evaluated; Section 6 gives some details about participation in this year evaluation campaign; Section 7 presents and discusses the results achieved by participating systems and across the different languages; Section 8 shows the approaches used by participating systems; and Sections 9 draws some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DOCUMENT COLLECTION</head><p>Two sets of documents have been included in ResPubliQA 2010 collection: a subset of the JRC-ACQUIS Multilingual Parallel Corpus 2 and a small portion of the EUROPARL collection. Both are multilingual parallel collections. JRC-ACQUIS 3 is a freely available parallel corpus containing the total body of European Union (EU) documents, mostly of legal nature. It comprises contents, principles and political objectives of the EU treaties; the EU legislation; declarations and resolutions; international agreements; and acts and common objectives. Texts cover various subject domains, including economy, health, information technology, law, agriculture, food, politics and more. This collection of legislative documents currently includes selected texts written between 1950 and 2006 with parallel translations in 22 languages. The corpus is encoded in XML, according to the TEI guidelines.</p><p>The subset used in ResPubliQA consists of 10,700 parallel and aligned documents per language (Bulgarian, English, French, German, Italian, Portuguese, Romanian and Spanish). The documents are grouped by language, and inside each language directory, documents are grouped by year. All documents have a numerical identifier called the CELEX code, which helps to find the same text in the various languages. Each document contains a header (giving for instance the download URL and the EUROVOC codes) and a text (which consists of a title and a series of paragraphs).</p><p>EUROPARL 4 is a collection of the Proceedings of the European Parliament dating back to 1996. European legislation is a topic of great relevance to a large number of potential users from citizens to lawyers, government agencies politicians and many others. EUROPARL comprises texts in each of the 11 official languages of the European Union (Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish). With the enlargement of the European Union to 25 member countries in May 2004, the European Union has begun to translate texts into even more languages. However, translations into Bulgarian and Romanian start from January 2009 and for this reason we only compiled documents from the European Parliament site (http://www.europarl.europa.eu/) starting from that date. In this way, we ensured a parallel collection for 9 languages (Bulgarian, Dutch, English, French, German, Italian, Portuguese, Romanian and Spanish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TYPES OF QUESTIONS</head><p>P: (a) Dishwashers with 10 or more place settings shall have an energy efficiency index lower than 0,58 as defined in Annex IV to Commission Directive 97/17/EC of 16 April 1997 implementing Council Directive 92/75/EEC with regard to energy labelling of household dishwashers(1), using the same test method EN 50242 and programme cycle as chosen for Directive 97/17/EC. A: 0,58 Definition. Definition questions are questions such as "What/Who is X?", i.e. questions asking for the role/job/important information about someone, or questions asking for the mission/full name/important information about an organization. For example: Q: What is avian influenza? P: (1) Avian influenza is an infectious viral disease in poultry and birds, causing mortality and disturbances which can quickly take epizootic proportions liable to present a serious threat to animal health and to reduce sharply the profitability of poultry farming. Under certain circumstances the disease may also pose a risk to human health. There is a risk that the disease agent might be spread to other holdings, to wild birds and from one Member State to other Member States and third countries through the international trade in live birds or their products.</p><p>A: an infectious viral disease in poultry and birds, causing mortality and disturbances which can quickly take epizootic proportions liable to present a serious threat to animal health and to reduce sharply the profitability of poultry farming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>What does MFF signify in a financial context? P: 1. Recalls that its political priorities and its assessment of the budgetary framework for the year 2010 were set out in its resolution of 10 March 2009, where Parliament was highly critical of the tight margins available in most of the headings of the Multiannual Financial Framework (MFF); A: Multiannual Financial Framework Reason_Purpose. Reason_Purpose questions ask for the reasons/goals for something happening. For example: Q: Why was Perwiz Kambakhsh sentenced to death? P: I. whereas the 23 year-old Afghan journalist Perwiz Kambakhsh was sentenced to death for circulating an article about women's rights under Islam, and whereas, after strong international protests, that sentence was commuted to 20 years" imprisonment, A: for circulating an article about women's rights under Islam Q: What were the objectives of the 2001 Doha Round? P: A. whereas the Doha Round was launched in 2001 with the objectives of creating new trading opportunities, strengthening multilateral trade rules, addressing current imbalances in the trading system and putting trade at the service of sustainable development, with an emphasis on the economic integration of developing countries, especially the least developed countries (LDCs), arising from the conviction that a multilateral system, based on more just and equitable rules, can contribute to fair and free trade at the service of the development of all continents, A: creating new trading opportunities, strengthening multilateral trade rules, addressing current imbalances in the trading system and putting trade at the service of sustainable development, with an emphasis on the economic integration of developing countries, especially the least developed countries (LDCs), arising from the conviction that a multilateral system, based on more just and equitable rules, can contribute to fair and free trade at the service of the development of all continents, Procedure. Procedure questions ask for a set of actions which is the official or accepted way of doing something. For example: Q: How do you calculate the monthly gross occupancy rate of bed places? P: The gross occupancy rate of bed places in one month is obtained by dividing total overnight stays by the product of the bed places and the number of days in the corresponding month (sometimes termed bed-nights) for the same group of establishments, multiplying the quotient by 100 to express the result as a percentage. A: by dividing total overnight stays by the product of the bed places and the number of days in the corresponding month (sometimes termed bed-nights) for the same group of establishments, multiplying the quotient by 100 to express the result as a percentage Q: How do you make a blank test? P: 7.1. A blank test shall be made regularly using an ashless filter paper (5.8) moistened with a mixture of 90 ml (4.1) sodium citrate solution, 1 ml saturated solution of calcium chloride (4.2), 0,5 ml of liquid rennet (4.5), and washed with 3 x 15 ml of distilled water before mineralisation by the Kjeldahl method as described at IDF standard 20A 1986. A: using an ashless filter paper (5.8) moistened with a mixture of 90 ml (4.1) sodium citrate solution, 1 ml saturated solution of calcium chloride (4.2), 0,5 ml of liquid rennet (4.5), and washed with 3 x 15 ml of distilled water before mineralisation by the Kjeldahl method as described at IDF standard 20A 1986 Opinion. Opinion questions ask for the opinions/feelings/ideas about people, topics, events. For example: Q: What did the Council think about the terrorist attacks on London? P: <ref type="bibr" coords="4,82.92,208.57,16.63,11.04" target="#b9">(10)</ref> On 13 July 2005, the Council reaffirmed in its declaration condemning the terrorist attacks on London the need to adopt common measures on the retention of telecommunications data as soon as possible. A: condemning the terrorist attacks on London Q: What is the Socialist Group position with respect to the case of Manuel Rosales? P: -Madam President, concerning the next vote, on 'Venezuela: the case of Manuel Rosales', the Socialist Group, of course, has withdrawn its signature from the compromise resolution. We have not taken part in the debate and we will not take part in the vote. A: has withdrawn its signature from the compromise resolution. We have not taken part in the debate and we will not take part in the vote.</p><p>Other. It is used for any reasonable questions which do not fall into the other categories. For example: Q: What is the e-Content program about? P: A multiannual programme "European digital content for the global networks" (hereinafter referred to as "eContent") is hereby adopted. A: European digital content for the global networks Q: By whom was the Treaty of Lisbon rejected? P: The Treaty of Lisbon, which is 96 per cent identical to the draft Constitutional Treaty, was rejected in the referendum in Ireland. Prior to that, the draft Constitutional Treaty was rejected in referendums in France and the Netherlands. A: was rejected in the referendum in Ireland Q: Which ideals are central to the EU? P: (1) Security incidents resulting from terrorism are among the greatest threats to the ideals of democracy, freedom and peace, which are the very essence of the European Union. A: democracy, freedom and peace</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TEST SET PREPARATION</head><p>Three hundred questions were initially formulated, manually verified against the document collection, translated into English and collected in a common XML format using a web interface specifically designed for this purpose. To avoid a bias towards a language, the 300 questions were developed by 4 different annotators originally in 4 different languages (75 each). All questions had at least one answer in the target corpus of that language. Then, a second translation from English back into all the nine languages of the track was performed. Translators checked whether a question initially created for a particular language had an answer or not in all other languages.</p><p>Beside the paragraph containing the answer, annotators were also required to demarcate the shorter string of text that responses to a question in all different languages. Pinpointing the precise extent of an answer is a more difficult problem than finding a paragraph that contains an answer. The purposes of demarcating exact responses are (i) to show to the evaluators what the question creators considered to be the exact answer, and (ii) to create a GoldStandard which has been used to automatically compare the responses retrieved by the systems against those manually collected by the annotator. Nevertheless, the exact answer returned by a system was judged by human assessors besides the automatic evaluation.</p><p>The final pool of 200 questions was selected out of the 300 produced, attempting to balance the question set according to the different question types (factoid, definition, reason/purpose, procedure, opinion and others). The distribution of the different questions types in the collection is shown in Table <ref type="table" coords="5,388.44,105.49,3.76,11.04" target="#tab_0">1</ref>. 130 questions had an answer in JRC-ACQUIS and 70 in EUROPARL. All the questions were formulated in such a way that they have an answer in all the collections, that is, there were no NIL questions. All language dependent tasks (question creation, translation and assessments of runs) have been performed by native speakers in a distributed setting. For this reason, a complete set of guidelines for each of these tasks have been shared among annotators and central coordination has been maintained in order to ensure consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION METHODOLOGY</head><p>Systems were allowed to participate in one or both tasks (PS and/or AS) which operated simultaneously on the same input questions. A maximum of two runs in total per participant could be submitted, i.e. two PS runs, two AS runs or one PS plus one AS run. Participants were allowed to submit just one response per question.</p><p>As in the previous campaign, systems had two options as output for each question:</p><p>1.</p><p>To give an answer (which could be one full paragraph for the PS task; or the shortest possible string of text which contains an exact answer to the question, for the AS task) 2. To choose not to answer the particular question (if the system considers that it is not able to find a correct answer). This option is called NoA answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measure</head><p>NoA answers should be used when a system is not confident about the correctness of its answer to a particular question. The goal is to reduce the amount of incorrect responses, keeping the number of correct ones, by leaving some questions unanswered. Systems should ensure that only the portion of wrong answers is reduced, maintaining as high as possible the number of correct answers. Otherwise, the reduction in the number of correct answers is punished by the evaluation measure for both the answered and unanswered questions. We used c@1 as a measure to make account of this behaviour.</p><p>c@1, which was introduced in ResPubliQA 2009, was used also this year as the main evaluation measure for both PS and AS tasks. The formulation of c@1 is given in:</p><formula xml:id="formula_0" coords="5,240.24,625.11,114.46,32.29">) ( 1 1 @ n n n n n c R U R + =</formula><p>where n R : number of questions correctly answered. n U : number of questions unanswered. n: total number of questions (200 in this edition)</p><p>Regarding the evaluation of exact answers, we also provide a measure of Answer Extraction performance, that is, the proportion of exact answers correctly extracted from correctly selected paragraphs.</p><p>Optionally, a system can also give the discarded candidate answer when responding NoA. These candidate answers were also assessed by evaluators in order to give a feedback to the participants about the validation performance of their systems, even though they are not considered in the main evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Assessment for Paragraph Selection</head><p>Each returned paragraph had a binary assessment: Right (R) or Wrong (W). Questions which were left unanswered were automatically filtered and marked as U (Unanswered). However, the discarded candidate answers given to these questions were also evaluated. Human assessors didn't know that these answers belong to unanswered questions.</p><p>The evaluators were guided by the initial "gold" paragraph, which contained the answers. This "gold" paragraph was only a hint, since there could be other responsive paragraphs in the same or different documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Assessment for Answer Selection</head><p>In order to judge the exact answer strings (AS task), assessors had to take into account also the paragraph as it provided the context and a justification to the exactness of the answer. Each paragraph/answer couple was manually judged and assessed considering one of the following judgments:</p><p>-R (Right): the answer-string consists of an exact and correct answer, supported by the returned paragraph;</p><p>-X (ineXact): the answer-string contains either part of a correct answer present in the returned paragraph or it contains all the correct answer plus unnecessary additional text; this option allowed the judge to indicate the fact that the answer was only partially correct (for example, because of missing information, or because the answer was more general/specific than required by the question, etc.)</p><p>-M (Missed): the answer-string does not contain a correct answer even in part but the returned paragraph in fact does contain a correct answer. In other words, the answer was there but the system missed it completely (i.e. the system did not extract it correctly);</p><p>-W (Wrong): the answer-string does not contain a correct answer and moreover the returned paragraph does not contain it either; or it contains an unsupported answer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Automatic Assessments</head><p>As human assessment is a time consuming and resource expensive task, this year it was decided to make some experiment with automatic evaluation in order to reduce the amount of work for human evaluators. The evaluation was performed in two steps:</p><p>1. Each run for both the PS and AS tasks were firstly automatically compared against the Gold Standard manually produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Non-matching paragraphs/ answers were judged by human assessors</head><p>The automatic script filtered out the answers that exactly match with the GoldStandard, assigning them correct values (R), leaving to human assessors only the evaluation of non-matching paragraphs/answers. The parameters which allow determining the correctness of a response are based on the exact match of Document identifier, Paragraph identifier, and the text retrieved by the system with respect to those in the GoldStandard.</p><p>Almost 31% of the answers (91% of them for Paragraph Selection and 9% for Answer selection) did match the GoldStandard and so it was possible to automatically mark them as correct.</p><p>The rest of the paragraphs and answers returned by systems were manually evaluated by native speaker assessors who considered if the system output was responsive or not. Answers were evaluated anonymously and simultaneously for the same question to ensure that the same criteria are being applied to all systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Tools and Infrastructure</head><p>This year, CELCT has developed a series of infrastructures to help the management of the ResPubliQA exercise. We had to deal with many processes and requirements:</p><formula xml:id="formula_1" coords="7,70.92,110.07,5.97,4.45">o</formula><p>First of all, the need to develop a proper and coherent tool for the management of the data produced during the campaign, to store it and to make it re-usable, as well as to facilitate the analysis and comparison of results.</p><p>o Secondly, the necessity of assisting the different organizing groups in the various tasks of the data set creation and to facilitate the process of collection and translation of questions and their assessment.</p><p>o Finally, the possibility for the participants to directly access the data, submit their own runs (this also implied some syntax checks of the format), and later, get the detailed viewing of the results and statistics.</p><p>A series of automatic web interfaces were specifically designed for each of these purposes, with the aim of facilitating the data processing and, at the same time, showing the users only what is important for the task they had to accomplish. So, the main characteristics of these interfaces are the flexibility of the system specifically centred on the user's requirements.</p><p>While designing the interfaces for question collection and translation one of the first issues which was to be dealt with, was the fact of having many assessors, a big amount of data, and a long process. So tools must ensure an efficient and consistent management of the data, allowing:</p><p>1.</p><p>Edition of the data already entered at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Revision of the data by the users themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Consistency propagation ensuring that modifications automatically re-model the output in which they are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Statistics and evaluation measures are calculated and updated in real time.</p><p>In particular, ensuring the consistency of data is a key feature in data management. For example, if a typo is corrected in the Translation Interface, the modification is automatically updated also in the GoldStandard files, in the Test Set files, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PARTICIPANTS</head><p>Out of the 24 groups who had previously registered showing interest in the task, a total of 13 groups participated in the ResPubliQA 2010 tasks in 8 different languages (German, English, Spanish, Basque, French, Italian, Portuguese and Romanian), as shown in Table <ref type="table" coords="7,266.16,465.25,3.76,11.04" target="#tab_1">2</ref>. The list of participating systems, teams and the reference to their reports are shown in Table <ref type="table" coords="7,200.88,476.77,3.76,11.04" target="#tab_1">2</ref>.  <ref type="table" coords="8,377.64,71.05,4.98,11.04" target="#tab_2">3</ref> shows the runs submitted in each language as well as the distribution among PS and AS runs. As usual, the most popular language was English (with 21 submitted runs), with Spanish and French as second (with 7 submissions each). Almost all runs were monolingual; only two participating teams attempted a crosslanguage task (EU-EN and EN-RO) that produced 4 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Overall Results for Paragraph Selection</head><p>The use of the same set of questions in all the languages allows, as in last year, a general comparison among different languages. Table <ref type="table" coords="8,180.36,406.45,4.98,11.04" target="#tab_3">4</ref> shows the c@1 value for all systems. Systems were able to find answers for more than 70% of questions in all languages (combination row in Table <ref type="table" coords="8,347.40,417.85,4.18,11.04" target="#tab_3">4</ref>) except Portuguese where there was only one participant. Considering all languages, 99% of questions received at least one correct answer by at least one system. This is an indication that the task is feasible for current systems. It also suggests that multi-stream systems might obtain good results. One way of obtaining this improvement could be the inclusion of the validation step to choose among the different systems (streams). Some IR based baselines were developed last year in order to compare the performance of pure IR approaches against more sophisticated QA technologies. These baselines were produced using the Okapi-BM25 ranking function <ref type="bibr" coords="8,107.28,532.93,11.71,11.04" target="#b4">[5]</ref> and are described in more detail in <ref type="bibr" coords="8,267.12,532.93,10.69,11.04" target="#b3">[4]</ref>. In this edition, the UNED group sent two similar baselines for English and Spanish and these are described in <ref type="bibr" coords="8,283.68,544.45,15.43,11.04" target="#b15">[16]</ref>. Therefore, these runs can be used for comparing QA technologies with pure IR systems in this edition. Although we cannot compare these results directly with those of last year, there seems to be a certain improvement in performance. Whereas the best result this year is a little higher than last year's one (from c@1 of 0.73 in English compared to 0.68) there has been a considerable improvement in the average results, with an increase from 0.39 to 0.54 in c@1 in the monolingual PS task.</p><p>EUROPARL turned out to be an easier collection than ACQUIS: 84% of all the answers by all systems over EUROPARL were correct whereas only 20% were over ACQUIS.</p><p>Table <ref type="table" coords="9,99.36,420.37,4.98,11.04" target="#tab_3">4</ref> shows the proportion of correct answers given by all systems to each different question type. Surprisingly, Definition questions turned out to be more difficult and Reason/Purpose slightly easier than the rest of the question types. These results contradict the performance obtained in past campaigns of QA@CLEF, where a very good performance was usually obtained in Definition questions. However, in ResPubliQA, Definition questions tend to be considerably more complex than those which appeared in earlier campaigns based on newspaper articles. Finally, considering the UNED baselines runs we can see that once again they performed extremely well. For English only three of the seventeen runs where better than the baselines. For Spanish, only one of the five runs was better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results per Language in the Paragraph Selection task</head><p>Tables 5-12 show the individual results by target language of each participant system at the PS task. Moreover, a combination of systems in each language is also given in these Tables. This combination represents the number of questions correctly answered by at least one system. All the results are ranked by c@1 values. These tables contain the following columns:</p><p>• c@1: official measure at ResPubliQA 2010.</p><p>• #R: number of questions correctly answered.</p><p>• #W: number of questions wrongly answered.</p><p>• #NoA: number of questions left unanswered.</p><p>• #NoA R: number of questions unanswered where the candidate answer was Right. In this case, the system took the bad decision of leaving the question unanswered.</p><p>• #NoA W: number of questions unanswered where the candidate answer was Wrong. In this case, the system took a good decision leaving the question unanswered.</p><p>• #NoA empty: number of questions unanswered where no candidate answer was given. Since all the questions had an answer, these cases were considered as if the candidate answer were wrong for accuracy calculation purposes.</p><p>Overall general statistics, together with test set questions and adjudicated runs are available at the RespubliQA website http://celct.isti.cnr.it/ResPubliQA/ under Past Campaigns.</p><p>The best results for German were obtained by the systems that include a validation step. These systems showed a very good performance validating answers (more than 75% of the rejected answers were actually incorrect). This means that these systems are able to improve their performance in the future by trying to answer the questions they left unanswered. The combination of English systems shows that 94% of questions were correctly answered by at least one system, which means that the task is feasible for current technologies. There are still some systems that perform worse than the IR baseline. As already discussed in the last edition, participant should care more about the correct tuning of the IR engine.</p><p>Most of the systems that left some questions unanswered didn't provide the candidate answer, so the organizers couldn't provide feedback about the actual state of validation technologies in English. There is some evidence that more efforts should be applied to the validation step in English for improving overall results as has been shown in German. With respect to Spanish, 80% of questions were correctly answered by at least one system. However, this combination is almost a 50% higher than the best system. Only one system (nlel101PSeses) performed better than the IR baseline. This system was able to reduce the number of incorrect answers while maintaining the same number of correct answers as the baseline. This is what allowed it to obtain a better performance according to c@1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results in the Answer Selection Task</head><p>Tables 12-14 show the results by language of participant systems at the AS task. The results for all the languages are given in Table <ref type="table" coords="12,150.60,181.57,8.38,11.04" target="#tab_12">12</ref>. These tables contain similar information to the PS tables plus the following additional information:</p><p>-#M: number of questions where the paragraph contained a correct answer, but the answer string given was wrong -#X: number of questions where the answer string given was judged as inexact.</p><p>All runs were monolingual. Three groups (iles, ju_c and nlel ) submitted seven runs for the Answer Selection (AS) task. Each of these groups submitted one EN run. In addition, iles submitted one FR run and nlel submitted one ES, one FR and one IT run. Thus there were three attempts at EN and two at FR, allowing some comparison. For ES and IT there was only one run each. Considering EN first, the best system by c@1 was ju_c with a score of 0.26. Interestingly, while iles scored only 0.09, it had a high number of X answers (44). Thus, iles was identifying the vicinity of answers better than ju_c but was not demarcating them exactly right. Of course, the demarcation in cases of question types like reason is not beyond debate. Finally, the third system nlel scored 0.07.  Notice that these figures are all very low compared to traditional factoid QA where figures of 0.8 can be obtained. We can attribute this to the inclusion of difficult question types which go beyond the factoid concept with its dependence on the Named Entity concept. Recall that the breakdown of questions was 40 factoids and 32 definitions, with 32 each of opinion, procedure, reason-purpose and "other" questions. Thus 64% of questions fell into the latter four "difficult" types.</p><p>Another consideration is the effect of allowing systems to mark questions as unanswered even though they had in fact answered them. Only in the case of EN and the ju_c run was there any loss of score incurred by not answering. For ju_c, 24 unanswered questions had a missed answer, i.e. ju_c identified the correct paragraph containing the exact answer, but was not able to demarcate it. For all the other EN runs (and indeed all the other runs), unanswered questions had an empty answer, so nothing can be said about how close these other systems were to getting the right answer in the case of unanswered questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">SYSTEM DESCRIPTIONS</head><p>A summary of the techniques reported by participants is shown in Table <ref type="table" coords="13,363.36,257.29,8.38,11.04" target="#tab_15">15</ref>. Most of the systems that analyze the questions use manually built patterns. Regarding the IR model, BM25 has been applied by almost half of the participants that reported the retrieval model used. The other reported models were the standard ones supplied by Lucene.  <ref type="table" coords="14,387.72,71.05,8.38,11.04" target="#tab_16">16</ref>. The most common processing was the use of named entities, numeric and temporal expressions, while some systems relied on syntactic processing by means of chunking, dependency parsing or syntactic transformations. The validation of answers was applied by 9 of the 13 participants. According to Table <ref type="table" coords="14,438.60,423.61,8.38,11.04" target="#tab_17">17</ref>, which shows the different validation techniques applied by participants, the most common processing was to measure the lexical overlapping between questions and candidate answers (it was performed by 5 participants). On the other hand, more complex techniques such as syntactic similarity or theorem proving were applied by very few participants.</p><p>These observations are different from the ones obtained last year, where participants applied more techniques and performed more complex analysis like semantic similarity or the combination of different classifiers. That is, participants at ResPubliQA 2010 relied on naive techniques for performing validation. However, the experience during the last years shows that the validation step can improve results if it is performed carefully. Otherwise, the effect will be the opposite, the harming results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS</head><p>An important result demonstrated in 2009 was that a good IR system can be better than a QA system if the IR parameters are carefully tuned to the requirements of the domain. A relevant portion of participants already moved towards better IR models, although there are still many systems that don't outperform IR baselines.</p><p>While the Paragraph Selection task is just paragraph retrieval, the main difference from pure IR systems is to add the decision of leaving the question unanswered, that is, the validation step. Best performing systems in German, Spanish and French have accurate validation steps.</p><p>The PS task allows the inclusion of more complex questions, as well as their evaluation in a simple and natural way. However, when the aim is to extract an exact answer (as in the AS task), it turns out to be very difficult for systems to perform well, except were answers are named entities. This is because NE is a well-studied and largely solved problem. On the other hand, "exact" answer demarcation for more complex queries against documents such as those used in ResPubliQA needs further study by both system designers and evaluation task organizers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,215.64,153.71,164.19,118.10"><head>Table 1 : Distribution of question types</head><label>1</label><figDesc></figDesc><table coords="5,216.24,165.71,162.22,106.10"><row><cell>Question type</cell><cell>Total number</cell></row><row><cell></cell><cell>of questions</cell></row><row><cell>DEFINITION</cell><cell>32</cell></row><row><cell>FACTOID</cell><cell>35</cell></row><row><cell>REASON/PURPOSE</cell><cell>33</cell></row><row><cell>PROCEDURE</cell><cell>33</cell></row><row><cell>OPINION</cell><cell>33</cell></row><row><cell>OTHER</cell><cell>34</cell></row><row><cell>Total</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,70.92,501.95,453.56,233.54"><head>Table 2 : Systems and teams with the reference to their reports System</head><label>2</label><figDesc></figDesc><table coords="7,72.84,515.39,440.11,185.06"><row><cell></cell><cell>Team</cell><cell>Reference</cell></row><row><cell>bpac</cell><cell>SZTAKI, HUNGARY</cell><cell>Nemeskey</cell></row><row><cell></cell><cell>Dhirubhai Ambani Institute of Information and Communication</cell><cell></cell></row><row><cell>dict</cell><cell>Technology, INDIA</cell><cell>Sabnani et al</cell></row><row><cell>elix</cell><cell>University of Basque Country, SPAIN</cell><cell>Agirre et al</cell></row><row><cell>icia</cell><cell>RACAI, ROMANIA</cell><cell>Ion et al</cell></row><row><cell>iles</cell><cell>LIMSI-CNRS, FRANCE</cell><cell>Tannier et al</cell></row><row><cell>ju_c</cell><cell>Jadavpur University, INDIA</cell><cell>Pakray et al</cell></row><row><cell>loga</cell><cell>University Koblenz, GERMANY</cell><cell>Glöckner and Pelzer</cell></row><row><cell>nlel</cell><cell>U. politecnica Valencia, SPAIN</cell><cell>Correa et al</cell></row><row><cell>prib</cell><cell>Priberam, PORTUGAL</cell><cell>-</cell></row><row><cell>uaic</cell><cell>Al.I.Cuza\" University of Iasi, Faculty of Computer Science, ROMANIA</cell><cell>Iftene et al</cell></row><row><cell>uc3m</cell><cell>Universidad Carlos III de Madrid, SPAIN</cell><cell>Vicente-Díez et al</cell></row><row><cell>uiir</cell><cell>University of Indonesia, INDONESIA</cell><cell>Toba et al</cell></row><row><cell>uned</cell><cell>UNED, SPAIN</cell><cell>Rodrigo et al</cell></row></table><note coords="7,70.92,712.93,453.51,11.04;7,70.92,724.45,453.55,11.04;8,70.92,71.05,303.10,11.04"><p>A total of 49 runs were officially submitted considering both the PS and AS tasks. Specifically, 42 submissions in the PS task and only 7 in the AS task. It is quite encouraging that compared to last year, both the number of participating teams and the number of submissions have increased. Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,70.92,107.63,453.03,166.94"><head>Table 3 : Tasks and corresponding numbers of submitted runs. In brackets, the number of PS and AS runs Target languages (corpus and answer)</head><label>3</label><figDesc></figDesc><table coords="8,113.39,139.93,371.01,134.64"><row><cell>Source languages (questions)</cell><cell>DE 4 (4,0) Total 4 (4,0) 21 (18,3) 7 (6,1) 7 (5,2) 3 (2,1) 1 (1,0) 6 (6,0) 49 (42,7) EN ES FR IT PT RO Total DE 4 (4,0) EN 19 (16,3) 2 (2,0) 21 (18,3) ES 7 (6,1) 7 (6,1) EU 2 (2,0) 2 (2,0) FR 7 (5,2) 7 (5,2) IT 3 (2,1) 3 (2,1) PT 1 (1,0) 1 (1,0) RO 4 (4,0) 4 (4,0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,130.68,581.03,334.18,167.90"><head>Table 4 : c@1 in participating systems in the PS task according to the language</head><label>4</label><figDesc></figDesc><table coords="8,168.36,593.03,259.74,155.90"><row><cell>System</cell><cell>DE EN ES</cell><cell>FR</cell><cell>IT PT RO</cell></row><row><cell>Combination</cell><cell cols="3">0.75 0.94 0.82 0.74 0.73 0.56 0.70</cell></row><row><cell>uiir101</cell><cell>0.73</cell><cell></cell><cell></cell></row><row><cell>dict102</cell><cell>0.68</cell><cell></cell><cell></cell></row><row><cell>bpac102</cell><cell>0.68</cell><cell></cell><cell></cell></row><row><cell>loga102</cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell>loga101</cell><cell>0.59</cell><cell></cell><cell></cell></row><row><cell>prib101</cell><cell></cell><cell></cell><cell>0.56</cell></row><row><cell>nlel101</cell><cell cols="3">0.49 0.65 0.56 0.55 0.63</cell></row><row><cell>bpac101</cell><cell>0.65</cell><cell></cell><cell></cell></row><row><cell>elix101</cell><cell>0.65</cell><cell></cell><cell></cell></row><row><cell>IR baseline (uned)</cell><cell>0.65 0.54</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,166.32,505.31,262.78,104.42"><head>Table 4 : Correct answers according to different question type Question type % of correct answers</head><label>4</label><figDesc></figDesc><table coords="9,193.20,538.69,180.25,71.04"><row><cell>DEFINITION</cell><cell>28.64%</cell></row><row><cell>FACTOID</cell><cell>46.53%</cell></row><row><cell>REASON_PURPOSE</cell><cell>53.18%</cell></row><row><cell>PROCEDURE</cell><cell>41.62%</cell></row><row><cell>OPINION</cell><cell>42.80%</cell></row><row><cell>OTHER</cell><cell>44.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,159.48,320.63,268.02,92.42"><head>Table 5 : Results for German in the PS task</head><label>5</label><figDesc></figDesc><table coords="10,159.48,332.63,268.02,80.42"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell>combination</cell><cell>0.75 150 50</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">loga102PSdede 0.62 105 59</cell><cell>36</cell><cell>2</cell><cell>29</cell><cell>5</cell></row><row><cell cols="2">loga101PSdede 0.59 101 65</cell><cell>34</cell><cell>2</cell><cell>27</cell><cell>5</cell></row><row><cell cols="2">nlel101PSdede 0.49 90 93</cell><cell>17</cell><cell>2</cell><cell>15</cell><cell>0</cell></row><row><cell cols="2">nlel102PSdede 0.44 88 112</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,152.64,547.31,283.74,205.94"><head>Table 6 : Results for English in the PS task System c@1 #R #W #NoA #NoA R</head><label>6</label><figDesc></figDesc><table coords="10,369.24,559.31,67.14,20.42"><row><cell>#NoA</cell><cell>#NoA</cell></row><row><cell>W</cell><cell>empty</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,70.92,221.03,453.54,173.18"><head>Table 7 : Results for Spanish in the PS task</head><label>7</label><figDesc>Similar results were obtained for French and Romanian as target, where the difference between the combination row and the best system is relevant. Again, the system nlel shows that accurate validation technologies have been developed.</figDesc><table coords="11,151.56,233.03,282.90,112.46"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell>combination</cell><cell>0.82 165 35</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>nlel101PSeses</cell><cell>0.56 108 86</cell><cell>6</cell><cell>1</cell><cell>5</cell><cell>0</cell></row><row><cell cols="2">IR baseline (uned) 0.54 108 92</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uned101PSeses</cell><cell>0.54 92 73</cell><cell>35</cell><cell>22</cell><cell>13</cell><cell>0</cell></row><row><cell cols="2">uc3m102PSeses 0.52 104 96</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">uc3m101PSeses 0.51 101 99</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>nlel102PSeses</cell><cell>0.20 39 161</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,163.08,408.35,261.42,104.66"><head>Table 8 : Results for French in the PS task</head><label>8</label><figDesc></figDesc><table coords="11,163.08,420.35,261.42,92.66"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell cols="2">combination 0.74 148 52</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">nlel101PSfrfr 0.55 105 86</cell><cell>9</cell><cell>2</cell><cell>7</cell><cell>0</cell></row><row><cell cols="2">nlel102PSfrfr 0.55 109 88</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>0</cell></row><row><cell cols="3">iles102PSfrfr 0.36 62 105 33</cell><cell>0</cell><cell>0</cell><cell>33</cell></row><row><cell cols="3">uaic101PSfrfr 0.30 54 124 22</cell><cell>0</cell><cell>0</cell><cell>22</cell></row><row><cell cols="2">uaic102PSfrfr 0.24 47 153</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,156.96,529.91,273.06,119.30"><head>Table 9 : Results for Romanian in the PS task System c@1 #R #W #NoA #NoA R</head><label>9</label><figDesc></figDesc><table coords="11,363.84,541.91,66.18,20.54"><row><cell>#NoA</cell><cell>#NoA</cell></row><row><cell>W</cell><cell>empty</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="11,164.04,666.11,258.54,68.42"><head>Table 10 : Results for Italian in the PS task</head><label>10</label><figDesc></figDesc><table coords="11,164.04,678.11,258.54,56.42"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell cols="2">combination 0.73 146 54</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">nlel101PSitit 0.63 124 72</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>0</cell></row><row><cell cols="2">nlel102PSitit 0.53 105 94</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="12,161.16,86.99,264.18,44.42"><head>Table 11 : Results for Portuguese in the PS task</head><label>11</label><figDesc></figDesc><table coords="12,161.16,98.99,264.18,32.42"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell cols="2">prib101PSptpt 0.56 111 88</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="12,85.92,333.23,418.62,135.02"><head>Table 12 : General Results in the AS task</head><label>12</label><figDesc></figDesc><table coords="12,85.92,345.23,418.62,123.02"><row><cell>System</cell><cell cols="6">c@1 #R #W #M #X #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>M</cell><cell>X</cell><cell>empty</cell></row><row><cell>combination</cell><cell cols="4">0.30 60 140 0 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>ju_c101ASenen</cell><cell cols="2">0.26 31</cell><cell>12</cell><cell>10 8</cell><cell>115</cell><cell>0</cell><cell>40</cell><cell>24</cell><cell>0</cell><cell>75</cell></row><row><cell>iles101ASenen</cell><cell cols="4">0.09 17 124 6 44</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>9</cell></row><row><cell>iles101ASfrfr</cell><cell cols="4">0.08 14 128 7 36</cell><cell>15</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>15</cell></row><row><cell>nlel101ASenen</cell><cell cols="2">0.07 10</cell><cell>97</cell><cell>20 6</cell><cell>67</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>67</cell></row><row><cell>nlel101ASeses</cell><cell cols="4">0.06 12 138 21 1</cell><cell>28</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>28</cell></row><row><cell>nlel101ASitit</cell><cell>0.03</cell><cell>6</cell><cell cols="2">139 18 7</cell><cell>30</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>30</cell></row><row><cell>nlel101ASfrfr</cell><cell>0.02</cell><cell>4</cell><cell cols="2">132 13 11</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="12,70.92,540.11,453.03,117.38"><head>Table 13 : Results for English in the AS task System c@1 #R #W #M #X #NoA #NoA R</head><label>13</label><figDesc></figDesc><table coords="12,367.56,552.11,125.70,20.54"><row><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell>W</cell><cell>M</cell><cell>X</cell><cell>empty</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="12,103.68,671.63,384.66,69.26"><head>Table 14 : Results for French in the AS task</head><label>14</label><figDesc></figDesc><table coords="12,103.68,683.63,384.66,57.26"><row><cell>System</cell><cell cols="3">c@1 #R #W #M #X #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>M</cell><cell>X</cell><cell>empty</cell></row><row><cell>combination</cell><cell>0.8 17 183 0 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>iles101ASfrfr</cell><cell>0.08 14 128 7 36</cell><cell>15</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>15</cell></row><row><cell>nlel101ASfrfr</cell><cell>0.02 4 132 13 11</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="13,96.12,316.91,407.41,402.86"><head>Table 15 : Methods used by participating systems</head><label>15</label><figDesc>Answer Extraction techniques in the AS task is given in Table</figDesc><table coords="13,96.12,329.63,407.41,390.14"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Question Analyses</cell><cell>Retrieval Model</cell><cell cols="6">Linguistic Unit which is indexed</cell></row><row><cell>Syste m name</cell><cell>No Question</cell><cell>Analyses</cell><cell>Manually</cell><cell>done Patterns</cell><cell>automatically</cell><cell>acquired</cell><cell>patterns</cell><cell>Other</cell><cell></cell><cell>Words</cell><cell>Lemmas</cell><cell>Stems</cell><cell>N-grams</cell><cell>Chunks/</cell><cell>phrases</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lemmatization,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>POS tagging and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*very* minimal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bpac</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pattern usage</cell><cell>Okapi BM25</cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>dict</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lemmatization,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>part of speech</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>elix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tagging</cell><cell>BM25</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lucene Boolean Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>icia</cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Engine</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>iles</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>standard lucene model;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>loga</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>word senses are indexed</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Distance Density N-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nlel</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>gram Model , BM25</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>ju_c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell>Apache Lucene</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>prib</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell><cell cols="2">x</cell></row><row><cell>uaic</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LUCENE</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uc3m</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Passage IR</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uiir</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uned</cell><cell></cell><cell></cell><cell cols="2">X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BM25</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="14,165.36,119.15,264.13,291.98"><head>Table 16 : Methods used by systems for extracting answers Answer Extraction -Further processing</head><label>16</label><figDesc></figDesc><table coords="14,165.36,150.09,264.13,261.04"><row><cell>System name</cell><cell>Chunking</cell><cell>n-grams Named Entity</cell><cell>Recognition</cell><cell>Temporal expressions</cell><cell>Numerical expressions</cell><cell>Dependency analysis</cell><cell>Syntactic</cell><cell>transformations</cell><cell>Logic representation</cell><cell>Theorem prover</cell><cell>Other</cell><cell>None</cell></row><row><cell>bpac</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>dict</cell><cell cols="2">x x</cell><cell cols="3">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>elix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>icia</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>iles</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell cols="2">x x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>loga</cell><cell></cell><cell></cell><cell></cell><cell cols="2">x x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>nlel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>ju_c</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>prib</cell><cell></cell><cell></cell><cell cols="3">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uaic</cell><cell></cell><cell></cell><cell cols="3">x x x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uc3m</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uiir</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uned</cell><cell></cell><cell></cell><cell cols="3">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="14,159.00,554.51,277.16,200.78"><head>Table 17 : Techniques used for the Answer Validation component System name</head><label>17</label><figDesc></figDesc><table coords="14,175.80,578.15,243.13,177.14"><row><cell></cell><cell>No answer validation</cell><cell>Machine Learning</cell><cell>Redundancies in the</cell><cell>collection</cell><cell>Lexical similarity (term</cell><cell>overlapping)</cell><cell>Syntactic similarity</cell><cell>Theorem prooving or</cell><cell>similar</cell><cell>Other</cell></row><row><cell>bpac</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dict</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>elix</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>icia</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>iles</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>loga</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="2">x</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.20,742.71,113.26,9.97"><p>http://www.europarl.europa.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Our thanks are due to all the annotators who took care of the translations of the questions in the different languages and of the evaluation of the respective submitted runs:</p><p>-<rs type="person">Anna Kampchen</rs>, <rs type="person">Julia Kramme</rs> (<rs type="affiliation">University of Hagen, Hagen, Germany</rs>), and <rs type="person">Stefan Rigo</rs> (<rs type="affiliation">FBK, Trento, Italy</rs>) for the German language -<rs type="person">Iñaki Alegria</rs> (<rs type="affiliation">University of Basque Country, Spain</rs>) for the Basque language -<rs type="person">Patricia Hernandez Llodra</rs> (<rs type="affiliation">UNED, Madrid, Spain</rs>) for the French language -<rs type="person">Rosário Silva</rs> (<rs type="affiliation">Linguateca/FCCN, Portugal</rs>) for the Portuguese language Special thanks are also due <rs type="person">Giovanni Moretti</rs> (<rs type="affiliation">CELCT, Trento, Italy</rs>) for the technical support in the management of all data of the campaign. This work has been partially supported by the <rs type="funder">Regional Government of Madrid</rs> under the <rs type="programName">Research Network MA2VICMR</rs> (<rs type="grantNumber">S-2009/TIC-1542</rs>), the <rs type="funder">Education Council of the Regional Government of Madrid</rs> and the <rs type="funder">European Social Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GD5g7UA">
					<idno type="grant-number">S-2009/TIC-1542</idno>
					<orgName type="program" subtype="full">Research Network MA2VICMR</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,74.68,579.85,449.65,11.04;15,70.92,591.25,453.54,11.04;15,70.92,602.77,453.46,11.04;15,70.92,614.29,453.55,11.04;15,70.92,625.93,453.69,11.04;15,70.92,637.33,67.75,11.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,263.04,591.25,261.42,11.04;15,70.92,602.77,105.94,11.04">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,100.56,614.29,423.91,11.04;15,70.92,625.93,113.55,11.04">Multilingual Information Access Evaluation Vol. I Text Retrieval Experiments, Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mostefa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roda</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09-30">2009. 30 September -2 October</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. (to be published)</note>
</biblStruct>

<biblStruct coords="15,74.68,654.85,449.68,11.04;15,70.92,666.37,453.54,11.04;15,70.92,677.89,332.01,11.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,287.64,654.85,185.86,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,474.12,666.37,50.34,11.04;15,70.92,677.89,204.62,11.04">Advances in Multilingual and Multimodal Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09">2007. September 2008</date>
			<biblScope unit="volume">5152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,74.69,695.29,449.69,11.04;15,70.92,706.81,453.43,11.04;15,70.92,718.33,453.46,11.04;15,70.92,729.85,395.01,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,344.16,695.29,180.22,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentín</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,100.68,718.33,423.70,11.04;15,70.92,729.85,122.42,11.04">Evaluation of Multilingual and Multi-modal Information Retrieval, 7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-20">2006. September 20-22, 2006</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="16,74.68,71.05,449.89,11.04;16,70.92,82.57,288.93,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,476.76,71.05,47.82,11.04;16,70.92,82.57,179.42,11.04">Information Retrieval Baselines for the ResPubliQA Task</title>
		<author>
			<persName coords=""><forename type="first">Joaquín</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,257.28,82.57,45.00,11.04">CLEF 2009</title>
		<imprint>
			<biblScope unit="volume">6241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.68,99.97,449.80,11.04;16,70.92,111.49,453.43,11.04;16,70.92,123.01,453.45,11.04;16,70.92,134.53,326.97,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,287.64,99.97,185.86,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,430.92,111.49,93.43,11.04;16,70.92,123.01,453.45,11.04;16,70.92,134.53,45.00,11.04">Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Th</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-17">2008. September 17-19, 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="16,74.68,152.05,449.79,11.04;16,70.92,163.57,453.52,11.04;16,70.92,174.97,347.49,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,253.44,152.05,271.03,11.04;16,70.92,163.57,129.36,11.04">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,222.84,163.57,301.60,11.04;16,70.92,174.97,262.33,11.04">SIGIR &apos;94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.68,192.49,449.65,11.04;16,70.92,204.01,194.46,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,255.72,192.49,90.35,11.04">FIDJI @ ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,375.84,192.49,148.50,11.04;16,70.92,204.01,65.61,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.68,221.53,449.77,11.04;16,70.92,233.05,298.02,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,245.52,221.53,232.68,11.04">Question Answering System: Retrieving relevant passages</title>
		<author>
			<persName coords=""><forename type="first">Hitesh</forename><surname>Sabnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasenjit</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,484.92,221.53,39.54,11.04;16,70.92,233.05,169.18,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.68,250.57,449.80,11.04;16,70.92,261.97,453.67,11.04;16,70.92,273.49,108.06,11.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,70.92,261.97,209.04,11.04">JU_CSE_TE: System Description QA@CLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santanu</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,287.64,261.97,215.49,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,291.01,445.07,11.04;16,70.92,302.53,453.51,11.04;16,70.92,314.05,168.06,11.04" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,70.92,302.53,250.22,11.04">Document Expansion for Cross-Lingual Passage Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olatz</forename><surname>Ansa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xabier</forename><surname>Arregi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maddalen</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xabier</forename><surname>Saralegi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,331.44,302.53,192.99,11.04;16,70.92,314.05,39.23,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,331.57,445.07,11.04;16,70.92,342.97,340.14,11.04" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,221.40,331.57,298.58,11.04">Contextual Approach for Paragraph Selection in Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">Hapnes</forename><surname>Toba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirna</forename><surname>Adriani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,70.92,342.97,211.17,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,360.49,445.16,11.04;16,70.92,372.01,453.66,11.04;16,70.92,383.53,95.58,11.04" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,423.60,360.49,100.86,11.04;16,70.92,372.01,178.88,11.04">Question Answering on Romanian, English and French Languages</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trandabat</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Husarciuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,259.80,372.01,231.09,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,401.05,445.31,11.04;16,70.92,412.45,121.38,11.04" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,175.68,401.05,101.27,11.04">SZTAKI @ ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nemeskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,305.52,401.05,214.17,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,429.97,408.56,11.04;16,516.60,429.97,7.86,11.04;16,70.92,441.49,453.70,11.04;16,70.92,453.01,243.90,11.04" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,398.64,429.97,89.22,11.04;16,516.60,429.97,7.86,11.04;16,70.92,441.49,329.29,11.04">Temporal information in ResPubliQA: an attempt to improve accuracy. The UC3M Participation at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Teresa</forename><surname>María</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julián</forename><surname>Vicente-Díez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paloma</forename><surname>Moreno-Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,429.12,441.49,95.50,11.04;16,70.92,453.01,115.05,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,470.53,445.08,11.04;16,70.92,482.05,243.90,11.04" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,228.96,470.53,188.04,11.04">The LogAnswer Project at ResPubliQA 2010</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,425.76,470.53,98.62,11.04;16,70.92,482.05,115.05,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,499.45,445.16,11.04;16,70.92,510.97,453.51,11.04;16,70.92,522.49,168.06,11.04" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,478.68,499.45,45.78,11.04;16,70.92,510.97,270.22,11.04">A Question Answering System based on Information Retrieval and Validation</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joaquin</forename><surname>Perez-Iglesias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,348.72,510.97,175.71,11.04;16,70.92,522.49,39.23,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,540.01,445.19,11.04;16,70.92,551.53,453.49,11.04;16,70.92,563.05,217.02,11.04" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,70.92,551.53,317.63,11.04">Monolingual and Multilingual Question Answering on European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Ceausu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Ştefănescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elena</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Verginica</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mititelu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,396.84,551.53,127.57,11.04;16,70.92,563.05,88.07,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date>22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,580.45,445.20,11.04;16,70.92,591.97,243.90,11.04" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,306.72,580.45,90.23,11.04">NLEL at RespubliQA</title>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,427.20,580.45,97.30,11.04;16,70.92,591.97,115.05,11.04">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 22-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
