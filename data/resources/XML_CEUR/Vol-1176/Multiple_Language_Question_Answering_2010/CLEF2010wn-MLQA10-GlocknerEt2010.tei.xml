<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.83,115.96,319.70,12.62">The LogAnswer Project at ResPubliQA 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,234.09,153.89,61.26,8.74"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>ingo.gloeckner@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems Group (IICS)</orgName>
								<orgName type="institution">University of Hagen</orgName>
								<address>
									<postCode>59084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.50,153.89,54.30,8.74"><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
							<email>bpelzer@uni-koblenz.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Artificial Intelligence Research Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<addrLine>Universitätsstr. 1</addrLine>
									<postCode>56070</postCode>
									<settlement>Koblenz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.83,115.96,319.70,12.62">The LogAnswer Project at ResPubliQA 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">93A759CB246B177B232F6C68412632E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LogAnswer project investigates the potential of deep linguistic processing and logical reasoning for question answering. The paragraph selection task of ResPubliQA 2010 offered the opportunity to validate improvements of the LogAnswer QA system that reflect our experience from ResPubliQA 2009. Another objective was to demonstrate the benefit of QA technologies over a pure IR approach. Two runs were produced for ResPubliQA 2010: The first run corresponds to LogAnswer with standard configuration. The accuracy of 0.52 and c@1 score of 0.59 witness that LogAnswer has matured (in 2009, accuracy was 0.40 and c@1 was 0.44). In the second run, a special index that only indexes terms from the definiendum of definitions was used for answering definition questions. The resulting accuracy was 0.54 with c@1 score 0.62. For definition questions, accuracy increased by 21%. The deep linguistic analysis of LogAnswer and its validation techniques made a substantial difference compared to a pure IR approach. Using the retrieval stage of LogAnswer as the IR baseline, we found a 27% gain in accuracy and 37% gain in c@1 due to the powerful validation techniques of LogAnswer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LogAnswer project investigates the potential of deep linguistic processing and logical reasoning for question answering. <ref type="foot" coords="1,332.94,518.35,3.97,6.12" target="#foot_0">3</ref> The LogAnswer QA system developed in this project was first presented in CLEF 2008 <ref type="bibr" coords="1,384.56,531.87,9.96,8.74" target="#b5">[6]</ref>, where it took part in the QA@CLEF track for German. After consolidation it participated in Res-PubliQA 2009 where it was the third-best system under the C@1 / Best IR baseline metric. Still, the paragraph selection of LogAnswer were only slightly better than the very strong retrieval baseline. The paragraph selection (PS) task of ResPubliQA 2010 has offered the opportunity to validate improvements of the LogAnswer QA system that reflect our experience from ResPubliQA 2009:</p><p>-LogAnswer showed a very low accuracy for DEFINITION questions (16.8%).</p><p>One reason for the low accuracy for definition questions was the restrictive way in which queries to the passage retrieval system were constructed; another problem was that the domain-specific way in which definitions are expressed in regulations was not recognized by LogAnswer. In ResPubliQA 2010, we have addressed these problems by improving the use of the retrieval system for definition questions and by building a dedicated definition index that also covers definitions in the form typically found in regulations. -LogAnswer also showed a low accuracy for PROCEDURE questions (29.1%).</p><p>We now tackle the difficulty to recognize sentences that describe a procedure by additional procedure triggers whose presence in the text marks a sentence as expressing a procedure. Moreover the question classification for PROCEDURE questions was incomplete (only 20.3% of these questions were recognized as such), so recognition rules had to be added to close this gap. -LogAnswer depends on the parsing quality of the linguistic analysis stage since only sentences with a full parse allow a logic-based validation. The document collections of ResPubliQA with their administrative language are very difficult to parse, though -in the last year, the parser used by LogAnswer only managed to find a full parse for 26.2% of all sentences. Therefore one of our goals for ResPubliQA 2010 was to improve the parsing rate for JRC Acquis, and to ensure acceptable results for the new Europarl collection. -In order to achieve a general performance improvement, we decided to try a more thorough use of compound decomposition, knowing that compound nouns abound in administrative texts written in German.</p><p>Apart from evaluating the current state of the LogAnswer prototype, our main objective was that of demonstrating a clear advantage of using QA technologies over a pure IR approach. We also wanted to show that LogAnswer can cope with the novel challenges of ResPubliQA 2010, i.e. with the Europarl corpus and with OPINION questions.</p><p>In the paper, we first explain how the LogAnswer QA system works and how we prepared it for ResPubliQA 2010. We then present the results of LogAnswer on the ResPubliQA question set for German. The discussion of these results and some additional experiments will highlight the strengths and some remaining weak points of the system. We conclude with a summary of the progress made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>LogAnswer rests on a deep linguistic analysis of the document collections by the WOCADI parser <ref type="bibr" coords="2,233.22,596.34,9.96,8.74" target="#b7">[8]</ref>. The pre-analysed sentences are stored in a Lucene index,using (synonym normalized) word senses from each sentence and occurrences of answer types (like PERSON) as index terms. Questions are also parsed by WOCADI. The (synonym normalized) word senses in a question and its expected answer type are used for retrieving linguistic analyses of 200 sentences. Various shallow features judging the question/snippet match are computed (e.g. lexical overlap). For sentences with a full parse, a relaxation proof of the question from sentence and background knowledge provides additional logic-based features <ref type="bibr" coords="3,172.85,340.69,9.96,8.74" target="#b2">[3]</ref>. The best answer sentence is selected by a validation model based on rank-optimizing decision trees <ref type="bibr" coords="3,281.45,352.64,9.96,8.74" target="#b4">[5]</ref>. If the validation score of the best sentence exceeds a quality threshold, then the sentence is expanded into the final answer paragraph; otherwise 'no answer' is shown. The basic setup of the system is the same as described in <ref type="bibr" coords="3,226.15,388.51,9.96,8.74" target="#b6">[7]</ref>. In the following we detail some of the processing stages in the Q/A pipeline of LogAnswer, using a fixed question as a running example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linguistic Analysis of the Question</head><p>The questions are first subjected to a linguistic analysis, using the WOCADI parser for German <ref type="bibr" coords="3,215.76,458.86,9.96,8.74" target="#b7">[8]</ref>. WOCADI generates a meaning representation of the question in the semantic network formalism MultiNet <ref type="bibr" coords="3,356.22,470.82,9.96,8.74" target="#b8">[9]</ref>. Let us consider question #119 from the ResPubliQA 2010 test set for German:</p><p>Was hat Herr Barroso bei der Unterzeichnung des Nabucco-Abkommens gesagt? (What did Mr Barroso say at the time of signing the Nabucco agreement?)</p><p>The relational structure 4 of the MultiNet representation generated for the example question is shown in Fig. <ref type="figure" coords="3,275.64,554.70,3.87,8.74" target="#fig_0">1</ref>. The indexed symbols like sagen.1.1 are word sense identifiers. The MultiNet representation provides the basis for question classification and for the subsequent generation of a logical query.</p><p>Note that a complete MultiNet representation is only available if WOCADI finds a full question parse. This requirement was not problematic since WOCADI achieved a full parse rate of 97.5% on the ResPubliQA 2010 question set.</p><p>Apart from generating a semantic representation, the parser also provides the results of its morphological and lexical analysis stage. Note that the inclusion of name lexica allows the parser to tag named entities in the text by types such as last-name (the family name of person), etc. WOCADI further provides information on the decomposition of compound words that occur in a sentence. In our running example, it finds out that nabucco-abkommen.1.1 is a regular compound built from abkommen.1.1 (agreement) and nabucco (an unknown word that could represent a name nabucco.0, or a regular concept nabucco. <ref type="bibr" coords="4,436.33,178.77,17.71,8.74">1.1)</ref>.</p><p>This kind of morpho-lexical and named-entity information is also available for sentences with a partial or failed parse and can thus be used for implementing fallback methods that replace a logical validation in the case of a parsing failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Classification</head><p>A rule-based question classification is used to determine the expected answer type of the question and to identify the descriptive core of the question. The expected answer types known to LogAnswer are a refinement of the question categories of ResPubliQA. Apart from OPINION, PROCEDURE, PURPOSE, REASON, and DEFINITION questions, the system distinguishes several types of factoid questions such as city-name, mountain-name, first-name, last-name, island-name, etc. These types correspond to the supported named entity types that WOCADI recognizes in the text.</p><p>For ResPubliQA 2010, the existing rule base for question classification was extended. In particular, the coverage of rules for PROCEDURE and PURPOSE questions was improved. In order to recognize compound triggers like Arbeitsverfahren (working procedure) or Hauptaufgabe (main task), LogAnswer now treats all nominal compounds that modify a known trigger word as additional trigger words for the corresponding question type. Finally, 25 rules for recognizing OPINION questions were added. The resulting rule base now comprises 240 classification rules, compared to 165 rules used for ResPubliQA 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval of pre-analyzed sentences</head><p>Experiments with a paragraph-level and document-level segmentation of the texts have shown no benefit over sentence segmentation in the ResPubliQA 2009 task <ref type="bibr" coords="4,156.13,519.52,9.96,8.74" target="#b6">[7]</ref>. Therefore we decided to work with a simple sentence-level index.</p><p>Prior to indexing, all documents in the considered JRC Acquis and Europarl collections<ref type="foot" coords="4,179.65,541.95,3.97,6.12" target="#foot_1">5</ref> must be analyzed by WOCADI. In order to achieve an acceptable parsing rate, some automatic regularizations of the documents were performed (such as removal of paragraph numbers at the beginning of sentences). The preprocessing also included the application of an n-gram recognizer for complex references to (sections of) regulations, such as "(EWG) Nr. 1408/71 <ref type="bibr" coords="4,443.60,591.35,12.25,8.74" target="#b2">[3]</ref>" (see <ref type="bibr" coords="4,134.77,603.30,10.30,8.74" target="#b6">[7]</ref>). In order to simplify the parsing of more sentences involving such constructions, the training data of the section recognizer was considerably extended. The achieved parsing rates in Table <ref type="table" coords="5,268.98,244.10,4.98,8.74" target="#tab_0">1</ref> show a positive effect of these changes, but both JRC Acquis and Europarl are still very hard to parse. The pre-analysed sentences are stored in a Lucene-based retrieval system.<ref type="foot" coords="5,476.12,266.90,3.97,6.12" target="#foot_2">6</ref> Note that instead of word forms or stems, the system indexes all possible word senses for each sentence. Moreover nominalization relationships are utilized for enriching the index. A system of 49,000 synonym classes involving 112,000 lexemes is used for normalizing synonyms, i.e. a canonical representative is chosen for all terms in a given synonym class. A special treatment of compounds was added so that all parts of the compound are indexed in addition to the compound itself. Moreover, occurrences of expected answer types (like PERSON, DATE) in each sentence are indexed. The recognition of these answer types rests on the named entity information provided by WOCADI. The presence of certain word senses and regular expressions defined on the morpho-lexical analysis of WOCADI can also trigger the recognition of these answer types. Currently there are 897 such triggers (including 704 newly added triggers for OPINION questions, and some additional triggers for the PROCEDURE type).</p><p>For definition questions, a special definition index was generated. In this index, only the definiendum of a definition recognized in a sentence is used for indexing. For example, consider this definition:</p><p>Hopfenpulver: Das durch Mahlen des Hopfens gewonnene Erzeugnis, das alle natürlichen Bestandteile des Hopfens enthält. (Hop powder: the product obtained by milling the hops, containing all the natural elements thereof)</p><p>Here, only the word sense hopfenpulver.1.1 of the defined term Hopfenpulver (Hop powder) is added to the definition index. This ensures a high retrieval precision for definition questions. The recognition of definitions in the texts was adjusted such as to cover the typical forms of definitions in administrative texts.</p><p>For retrieving a set of candidate sentences, the JRC Acquis index and the Europarl index are searched in parallel (using Lucene's MultiSearcher), and the 200 best sentences for the given query are fetched.</p><p>The retrieval query is constructed from disambiguated word senses in the question analysis if a full parse exists. Otherwise a frequency criterion is used to select a unique word sense from the set of alternatives for each word in the question. Synonyms are again normalized by choosing a canonical representative. In the example, the retrieval query becomes: The expansion of the compound Nabucco-Abkommen is intended to improve recall. The retrieval query will be extended by an additional term that expresses the expected answer type, in this case atype:OPINION. Note that the atype:x term is always an optional part of the retrieval query (it can be dropped at the expense of the retrieval score). This is different from the approach in Res-PubliQA 2009 where for definition questions, atype:DEFINITION was treated as a required subexpression so that sentences not recognized as containing a definition were completely dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Extraction of Shallow Validation Features</head><p>As the basis for selecting the best retrieved sentence, several features that describe the quality of the question/snippet match are computed. We start by describing some shallow features that can be computed for arbitrary sentences regardless of the success of parsing. Apart from the obvious expected answer type check, another important method is a lexical overlap test. To this end, LogAnswer determines the list of all (synonym normalized) word senses for each word in the question (except stopwords). Each list of alternative word senses is treated as a disjunction one of whose elements must find a match in the sentence to be validated. In our runnung example, we get: A recent change to LogAnswer is the treatment of nominal compounds: Each compound is split in two conjuncts so that a full match is possible if the text either contains the compound directly (Nabucco-Abkommen), or alternatively, if it contains the components of the compound (i.e. both Nabucco and Abkommen).</p><p>In the example, this candidate sentence is found that answers the question: Am 13. Juli bei der Unterzeichnung des Nabucco-Abkommens in Ankara sagte Herr Barroso, die Gas-Pipelines seien aus Stahl. (On 13 July in Ankara, at the time of signing the Nabucco agreement, Mr Barroso said that the gas pipelines were made from steel.)</p><p>LogAnswer then extracts the following (synonym normalized) word senses and numerals from the morpho-lexical analysis of this sentence: Here, every list of alternative word senses from the question representation finds a match in the shallow sentence representation. Thus, there are 0 cases of matching failure (100% matching rate). For details on the shallow features, see e.g. <ref type="bibr" coords="7,456.81,142.90,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Extraction of Logic-Based Validation Features</head><p>For sentences with a full parse, a relaxation proof of the (logical representation of the) question from the logical representation of the sentence and the available background knowledge is also tried, resulting in additional logic-based features. The prover works on synonym-normalized representations. The background knowledge comprises more than 10,500 facts (e.g. describing nominalizations), and 114 rules for basic inferences, see e.g. <ref type="bibr" coords="7,350.43,251.03,9.96,8.74" target="#b3">[4]</ref>.</p><p>Recalling our running example, the following logical query is constructed, based on the question parse and the result of question classification: attr(X1, X2), sub(X1, herr.1.1), val(X2, barroso.0), sub(X2, familiename.1.1), obj(X3, X4), subs(X3, unterzeichnung.1.1), sub(X4, nabucco-abkommen.1.1), agt(X5, X1), circ(X5, X3), subs(X5, sagen.1.1), mcont(X5, F ) All variables are assumed to be existentially quantified. Comma means conjunction. The variable F is the question focus (it expresses the queried information).</p><p>The logical representation of the correct answer sentence shown above is: Due to a parsing error, the logical query shown above cannot be proved from the representation of the sentence -the parser has used an unspecific assoc relation instead of the circ relation in the query. Thus, the critical literal will be skipped from the query, resulting in a proof of the remaining query fragment. Among the logic-based features that summarize the relaxation proof, there is one feature reporting that a single literal had to be skipped, and another feature reporting that 10/11 ≈ 91% of the query literals have been proved. For details on these logic-based features and the use of relaxation in LogAnswer, see <ref type="bibr" coords="7,456.61,595.81,9.96,8.74" target="#b2">[3]</ref>.</p><formula xml:id="formula_0" coords="7,144.74,381.53,29.46,7.86">val(c10,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Selection of the best answer candidate</head><p>The selection of the best answer candidate is based on the shallow features obtained by matching the question and the sentence terms, and (for sentences with a full parse) also on features obtained from a relaxation proof of the question from the candidate sentence. Rank-optimizing decision trees <ref type="bibr" coords="8,400.12,130.95,10.52,8.74" target="#b4">[5]</ref> are used for assigning a validation score to each retrieved sentence that allows the selection of the best candidate. For the moment, LogAnswer still uses a validation model based on annotated training data from the QA@CLEF 2007 and 2008 evaluations. Using the ResPubliQA 2009 test set for preparing training data seemed too complicated for a non-expert of EU legislation.</p><p>The c@1 evaluation metric of ResPubliQA rewards QA systems that validate their answers and prefer not answering over wrong answers. Thus results with a low validation score should be dropped since their probability of being correct is so low that showing these results would reduce the c@1 score of LogAnswer. The threshold θ = 0.09 for accepting the best answer, or generating a 'NO ANSWER' response if the validation score falls below the threshold, was chosen such as to optimize the c@1 score of LogAnswer on the ResPubliQA 2009 test set.</p><p>Finally, if the best sentence is not rejected, then it is expanded to the corresponding full paragraph, as required by the ResPubliQA PS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Reasoning Support by the E-KRHyper Theorem Prover</head><p>E-KRHyper <ref type="bibr" coords="8,190.06,351.12,15.50,8.74" target="#b10">[11]</ref> was used as the reasoning component. E-KRHyper is an automated theorem prover (ATP) for first-order logic with equality. It is based on an extended form of the hyper tableaux calculus <ref type="bibr" coords="8,329.64,375.03,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="8,340.16,375.03,7.01,8.74" target="#b1">2]</ref>. The system is implemented in OCaml<ref type="foot" coords="8,165.76,385.41,3.97,6.12" target="#foot_3">7</ref> , and it is available under the Gnu GPL from the E-KRHyper website <ref type="foot" coords="8,473.36,385.41,3.97,6.12" target="#foot_4">8</ref> .</p><p>While we developed this prover for embedding in knowledge representation applications, it can operate as a stand-alone theorem prover which accepts input problems in the syntax used by the TPTP logic problem library <ref type="bibr" coords="8,413.22,423.10,14.61,8.74" target="#b15">[16]</ref>, a standard in automated theorem proving. The TPTP website<ref type="foot" coords="8,356.99,433.48,3.97,6.12" target="#foot_5">9</ref> provides a periodically updated performance listing of a number of ATP systems with respect to the problem library. At the time of this writing E-KRHyper solves 26% of the problems. In comparison the Otter [10] system solves 19%; Otter serves as a benchmark in ATP testing due to its long history and stability. While leading ATP systems like E <ref type="bibr" coords="8,164.37,494.83,15.50,8.74" target="#b14">[15]</ref> and Vampire <ref type="bibr" coords="8,243.02,494.83,15.50,8.74" target="#b13">[14]</ref> exceed 50% in the TPTP rankings, E-KRHyper is very suitable to the type of logic problems arising in knowledge representation and question answering, characterized by a large number of clauses, of which only a select few are actually necessary for the eventual proof. Regarding this problem class our system generally outperforms other theorem provers <ref type="bibr" coords="8,446.71,542.65,9.96,8.74" target="#b2">[3]</ref>. The QA-oriented logic problems used in our tests were derived from computations in previous CLEF competitions. We have since submitted a selection of about 200 of these problems to the TPTP, and they have been included in the problem library in the CSR domain (common sense reasoning) as of TPTP v4.0.1.</p><p>E-KRHyper has several features which support its role as a reasoning server within an application like LogAnswer. Logic extensions like equational reasoning, negation as failure, arithmetic evaluation and list processing enable the prover to handle aspects of knowledge representation systems which cannot be expressed within the limits of first-order logic. Most ATP systems are designed to work on a single problem only. They terminate once they have found a result, and they must be started anew for each problem. This mode of operation would be impractical for a reasoning server within LogAnswer, as it would entail reloading the extensive logical knowledge base for each query, every time rebuilding the indexing structures which the prover requires for fast clause access. Instead E-KRHyper can remain in operation indefinitely, loading the knowledge base only once during the initialization of LogAnswer. Any additional clauses required by the queries can be loaded and retracted during the operation.</p><p>For query relaxation E-KRHyper supplies LogAnswer with information about partially successful proof attempts. LogAnswer selects the most promising way to relax the query, and then the prover continues with the shortened query, keeping any previous derivation results to avoid repeating inferences.</p><p>When E-KRHyper finds a proof, it extracts the answer substitution from the proof and transfers it to the main LogAnswer system for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results on the ResPubliQA 2010 Test Set for German</head><p>Two runs were produced for ResPubliQA 2010: The first run, loga101PSdede, represents the LogAnswer system in its standard configuration without the experimental definition index. In loga102PSdede, by contrast, the definition index was activated. While the system was configured to retrieve 200 candidate sentences for each question, the actual number of available sentences was smaller in some cases. In the loga101PSdede run, a total of 39,200 candidate sentences was retrieved (196 per question). About 37.0% of the retrieved candidate sentences had a full parse, thus allowing logical validation. The remaining candidate sentences with a chunk parse (41.9%) or failed parse (21.1%) were only subjected to a shallow validation (numbers for the loga102PSdede run are similar). The results obtained for the two submitted runs are shown in Table <ref type="table" coords="9,408.56,596.34,3.87,8.74" target="#tab_1">2</ref>. The use of the definition index in loga102PSdede was a clear improvement.</p><p>Table <ref type="table" coords="9,176.55,620.25,4.98,8.74" target="#tab_2">3</ref> shows the results of corresponding shallow-only runs (generated with the prover switched off), and the results of two IR baseline runs in which the top-ranked sentence of the retrieval stage was directly used for choosing the corresponding answer paragraph. Note that a different 'no answer' threshold of θ = 0.76 was used for the IR baseline runs. In this case the treshold was used for cutting off results with a poor Lucene retrieval score. The value of θ = 0.76 was again chosen such as to optimize the corresponding c@1 score on the Res-PubliQA 2009 questions. The table clearly shows that the use of logical validation techniques provided no extra benefit in the ResPubliQA task -the results of the standard system (with logical validation) and of the configuration that only uses shallow features for validation was about the same. Comparing the detailed results of the runs of the full system (loga101PSdede) and the shallow only configuration (SH-101), we found that the validation models resulted in a different choice of shown answer only for 15% of the questions. For loga102PSdede vs. SH-102, the chosen answer paragraphs were different for 14% of the questions.</p><p>Given the high accuracy achieved by the shallow validation technique, logical validation was obviously not called for by the ResPubliQA task. An interesting finding was that (with one exception), all questions for which deep validation outperformed the shallow-only technique were DEFINITION questions.</p><p>On the other hand, the validation techniques of LogAnswer achieved a strong benefit compared to using the retrieval score only. Comparing loga101PSdede and the IR-101 run, for example, accuracy increases by 27% and the c@1 score increases by 37% due the use of validation instead of the plain retrieval result.</p><p>A breakdown of results by question category is shown in Table <ref type="table" coords="10,421.62,584.39,3.87,8.74" target="#tab_3">4</ref>. LogAnswer was best for REASON/PURPOSE and FACTOID questions. DEFINITION and OTHER questions performed worst. OPINION questions also proved difficult. The use of a special definition index in the second LogAnswer run increased the accuracy for definition questions from 34% to 41% compared to the first run with a single index for all questions. This result is encouraging, though more work has to be done here. Compared to the results of LogAnswer in ResPubliQA 2009 <ref type="bibr" coords="10,467.30,656.12,9.96,8.74" target="#b6">[7]</ref>, The threshold θ = 0.09 on validation quality that serves for cutting of poor answers was chosen such as to maximize the c@1 score of LogAnswer on last year's ResPubliQA question set. In retrospect, the optimal threshold for loga101PSdede would have been θ = 0.11, resulting in a c@1 score of 0.60 (overall accuracy 0.52). For loga102PSdede, the optimal threshold would have been θ = 0.13, yielding a c@1 score of 0.63 (overall accuracy 0.54). The closeness of these optimal values to the results obtained using θ = 0.09 demonstrates that the method for determining the NOA threshold was effective.</p><p>We have determined the reason of failure for a sample of questions with wrong answers in the LogAnswer runs. This analysis has shown that the Lucene scoring metric in the retrieval stage of LogAnswer has a strong bias to short sentences. A short sentence that contains only one term from the IR query is often prefered to a longer sentence that contains all query terms. We thus agree with Pérez at al <ref type="bibr" coords="11,207.99,492.40,15.50,8.74" target="#b11">[12]</ref> that switching to a BM25 ranking function makes sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In ResPubliQA 2010, LogAnswer scored much better than in the last year. The improved accuracy for all question types shows that the general improvements of LogAnswer and the thorough utilization of compound decompositions were effective. Specifically, PROCEDURE questions are no longer problematic for LogAnswer. Compared to a system configuration with a single index, the accuracy for definition questions was increased by 21% by adding a specialized definition index. The large difference between the c@1 and accuracy scores of LogAnswer indicates a good validation performance.</p><p>Due to the low parsing rate for JRC Acquis and Europarl, we were unable to demonstrate a benefit of logical processing on the quality of results. However, as witnessed by the results of the shallow-only matching technique, the ResPubliQA task did not seem to call for sophisticated logic-based validation either.</p><p>The deep linguistic analysis of LogAnswer and its validation techniques made a substantial difference compared to a pure IR approach. Using the retrieval stage of LogAnswer as the IR baseline, we found a 27% gain in accuracy and 37% gain in c@1 due to the powerful validation techniques of LogAnswer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,263.81,345.83,7.89;3,134.77,274.79,198.24,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. MultiNet representation of the example question Was hat Herr Barroso bei der Unterzeichnung des Nabucco-Abkommens gesagt?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,151.70,159.98,338.37,8.74;6,151.70,171.94,105.22,8.74"><head></head><label></label><figDesc>herr.1.1 barroso.0 bei.1.1 unterzeichnung.1.1 nabucco-abkommen.1.1 nabucco.0 abkommen.1.1 sagen.1.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,151.70,431.93,319.46,7.86;6,151.70,443.88,301.61,7.86;6,151.70,455.84,91.50,7.86;6,134.77,472.81,348.72,8.74"><head>(</head><label></label><figDesc>herr.1.1) (barroso.0) (signieren.1.1 unterzeichnung.1.1) (nabucco-abkommen.1.1 nabucco.0 nabucco.1.1) (abkommen.1.1 nabucco-abkommen.1.1) (sagen.1.1 besagen.1.1 sagen.2.1), where the canonical synonym signieren.1.1 replaces the original unterzeichnen.1.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,151.70,632.89,311.95,7.86;6,151.70,644.84,336.55,7.86;6,151.70,656.80,292.83,7.86"><head>abkommen. 1 . 1 ,</head><label>11</label><figDesc>ankara.0, barroso.0, familiename.1.1, gas.1.1, gaspipeline.1.1, herr.1.1, monat.1.1, nabucco-abkommen.1.1, name.1.1, past.0, pipeline.1.1, present.0, sagen.1.1, sein.3.8, stadt.1.1, stahl.1.1, tag.1.1, unterzeichnung.1.1, 7, 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,175.73,381.53,266.94,7.86;7,144.74,395.48,256.61,7.86;7,144.74,409.42,320.71,7.86;7,144.74,423.35,314.27,7.89;7,144.74,437.32,281.50,7.86;7,144.74,451.27,276.87,7.86;7,144.74,465.21,309.07,7.86;7,144.74,479.16,325.88,7.86;7,144.74,493.11,310.43,7.86"><head></head><label></label><figDesc>c7), sub(c10, monat.1.1), obj(c14, c17), subs(c14, unterzeichnung.1.1), loc(c17, c262), sub(c17, nabucco-abkommen.1.1), origm(c180, c257), pred(c180, gaspipeline.1.1), arg1(c182, c180), arg2(c182, c257), temp(c182, present.0), subs(c182, sein.3.8), assoc(c22, c14), mcont(c22, c182), agt(c22, c31), temp(c22, c8), temp(c22, past.0), subs(c22, sagen.1.1), attr(c24, c25), sub(c24, stadt.1.1), val(c25, ankara.0), sub(c25, name.1.1), sub(c257, stahl.1.1), in(c262, c24), attr(c31, c32), sub(c31, herr.1.1), val(c32, barroso.0), sub(c32, familienname.1.1), attr(c8, c10), attr(c8, c9), val(c9, c6), sub(c9, tag.1.1), assoc(gaspipeline.1.1, gas.1.1), sub(gaspipeline.1.1, pipeline.1.1), sub(nabucco-abkommen.1.1, abkommen.1.1)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,175.17,115.91,261.94,89.52"><head>Table 1 .</head><label>1</label><figDesc>Parsing Rate of WOCADI on the ResPubliQA corpora</figDesc><table coords="5,232.56,142.38,150.24,63.06"><row><cell>Corpus</cell><cell cols="2">full parse partial parse</cell></row><row><cell>JRC Acquis</cell><cell>26.2%</cell><cell>54.0%</cell></row><row><cell>(2009 parse)</cell><cell></cell><cell></cell></row><row><cell>JRC Acquis</cell><cell>35.1%</cell><cell>61.5%</cell></row><row><cell>(new parse)</cell><cell></cell><cell></cell></row><row><cell>Europarl</cell><cell>34.2%</cell><cell>82.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,115.91,345.83,76.82"><head>Table 2 .</head><label>2</label><figDesc>Results of LogAnswer in ResPubliQA 2010. #right cand. is the number of correct paragraphs at top rank before applying θ, and accuracy = #right cand./#questions</figDesc><table coords="9,171.26,162.55,272.83,30.18"><row><cell>run</cell><cell>description</cell><cell cols="3">#right cand. accuracy c@1 score</cell></row><row><cell cols="2">loga101PSdede standard system</cell><cell>103</cell><cell>0.52</cell><cell>0.59</cell></row><row><cell cols="2">loga102PSdede special def index</cell><cell>107</cell><cell>0.54</cell><cell>0.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,115.91,345.83,89.52"><head>Table 3 .</head><label>3</label><figDesc>Ablation Results for LogAnswer: The shallow-only runs were performed with the prover switched off. The IR baseline runs simply return the best retrieved candidate.</figDesc><table coords="10,170.95,153.34,273.45,52.10"><row><cell>run</cell><cell>description</cell><cell cols="3">#right cand. accuracy c@1 score</cell></row><row><cell>SH-101</cell><cell>shallow-only validation</cell><cell>105</cell><cell>0.53</cell><cell>0.60</cell></row><row><cell>SH-102</cell><cell>shallow-only, def index</cell><cell>108</cell><cell>0.54</cell><cell>0.62</cell></row><row><cell>IR-101</cell><cell>IR baseline</cell><cell>82</cell><cell>0.41</cell><cell>0.43</cell></row><row><cell>IR-102</cell><cell>IR baseline, def index</cell><cell>88</cell><cell>0.44</cell><cell>0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,140.87,236.88,333.62,67.61"><head>Table 4 .</head><label>4</label><figDesc>Accuracy by question category. The runs only differ for definitions</figDesc><table coords="10,140.87,263.35,333.62,41.14"><row><cell>Run</cell><cell cols="6">REAS/PURP FACTOID PROC OPINION OTHER DEFINITION</cell></row><row><cell></cell><cell>(33)</cell><cell>(35)</cell><cell>(33)</cell><cell>(33)</cell><cell>(34)</cell><cell>(32)</cell></row><row><cell>loga101PSdede</cell><cell>0.70</cell><cell>0.66</cell><cell>0.52</cell><cell>0.45</cell><cell>0.41</cell><cell>0.34</cell></row><row><cell>loga102PSdede</cell><cell>0.70</cell><cell>0.66</cell><cell>0.52</cell><cell>0.45</cell><cell>0.41</cell><cell>0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,115.91,345.82,217.79"><head>Table 5 .</head><label>5</label><figDesc>Success rate of question classification was a strong improvement for all question types. PROCEDURE questions (shown as PROC in the table) are no longer problematic for LogAnswer.Results on the success rate of question classification are shown in Table5. Despite the high overall recognition rate (81.5%), the novel class of OPINION questions was obviously not yet well covered by the classification rules. Moreover, some more ways of expressing PROCEDURE questions should be supported.</figDesc><table coords="11,184.09,142.38,247.17,84.97"><row><cell>Category</cell><cell cols="4">#questions #recognized recog-rate (last year)</cell></row><row><cell>REAS/PURP</cell><cell>33</cell><cell>30</cell><cell>90.9%</cell><cell>70.1%</cell></row><row><cell>FACTOID</cell><cell>35</cell><cell>31</cell><cell>88.6%</cell><cell>70.5%</cell></row><row><cell>DEFINITION</cell><cell>32</cell><cell>28</cell><cell>87.5%</cell><cell>85.3%</cell></row><row><cell>OTHER</cell><cell>34</cell><cell>29</cell><cell>85.3%</cell><cell>-</cell></row><row><cell>PROCEDURE</cell><cell>33</cell><cell>26</cell><cell>78.8%</cell><cell>20.3%</cell></row><row><cell>OPINION</cell><cell>33</cell><cell>19</cell><cell>57.8%</cell><cell>-</cell></row><row><cell>(total)</cell><cell>200</cell><cell>163</cell><cell>81.5%</cell><cell>65.6%</cell></row></table><note coords="11,134.77,265.16,22.17,8.74"><p>there</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,623.92,335.87,7.86;1,144.73,634.88,335.87,7.86;1,144.73,645.84,335.87,7.86;1,144.73,656.80,86.48,7.86"><p>Funding of this work by the DFG (Deutsche Forschungsgemeinschaft) under contracts FU 263/12-2 and GL 682/1-2 (LogAnswer) is gratefully acknowledged. Thanks to Tim vor der Brück for his n-gram recognizer, and to Sven Hartrumpf for adapting the WOCADI parser.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="4,144.73,634.88,335.86,8.12;4,144.73,645.84,335.87,7.86;4,144.73,656.80,236.17,8.12"><p>see http://wt.jrc.it/lt/Acquis/ and http://www.europarl.europa.eu/; the specific fragment of JRC Acquis and Europarl used by ResPubliQA is available from http://celct.isti.cnr.it/ResPubliQA/Downloads.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,144.73,656.80,132.59,8.12"><p>see http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="8,144.73,635.53,61.20,7.47"><p>caml.inria.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="8,144.73,646.48,202.42,7.47"><p>http://www.uni-koblenz.de/~bpelzer/ekrhyper</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="8,144.73,657.44,56.49,7.47"><p>www.tptp.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,228.95,337.64,7.86;12,151.52,239.91,101.94,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,330.84,228.95,64.29,7.86">Hyper Tableaux</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niemelä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,418.00,228.95,36.74,7.86">JELIA&apos;96</title>
		<title level="s" coord="12,463.04,228.95,17.56,7.86;12,151.52,239.91,31.91,7.86">Proceedings</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,250.28,337.64,7.86;12,151.52,261.24,208.45,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,322.52,250.28,121.73,7.86">Hyper Tableaux with Equality</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,465.49,250.28,15.10,7.86;12,151.52,261.24,179.78,7.86">Automated Deduction -CADE-21, Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,271.61,337.64,7.86;12,151.52,282.57,329.07,7.86;12,151.52,293.53,80.48,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,308.14,271.61,172.45,7.86;12,151.52,282.57,143.47,7.86">An application of automated reasoning in natural language question answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,301.65,282.57,80.03,7.86">AI Communications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="241" to="265" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>PAAR Special Issue</note>
</biblStruct>

<biblStruct coords="12,142.96,303.90,337.64,7.86;12,151.52,314.86,261.22,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,204.50,303.90,276.10,7.86;12,151.52,314.86,34.36,7.86">Filtering and fusion of question-answering streams by robust textual inference</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.84,314.86,98.49,7.86">Proceedings of KRAQ&apos;07</title>
		<meeting>KRAQ&apos;07<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,325.23,337.63,7.86;12,151.52,336.19,329.07,7.86;12,151.52,347.15,185.06,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,203.04,325.23,236.96,7.86">Finding answer passages with rank optimizing decision trees</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,459.20,325.23,21.39,7.86;12,151.52,336.19,329.07,7.86;12,151.52,347.15,48.28,7.86">Proc. of the Eighth International Conference on Machine Learning and Applications (ICMLA-09)</title>
		<meeting>of the Eighth International Conference on Machine Learning and Applications (ICMLA-09)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,357.52,337.63,7.86;12,151.52,368.48,158.97,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,247.87,357.52,232.72,7.86;12,151.52,368.48,18.39,7.86">Combining logic and machine learning for answering questions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
		<editor>Peters et al.</editor>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,378.85,337.63,7.86;12,151.52,389.81,329.07,7.86;12,151.52,400.77,196.91,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,249.40,378.85,134.21,7.86">The LogAnswer project at CLEF</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,424.81,378.85,55.78,7.86;12,151.52,389.81,329.07,7.86;12,151.52,400.77,87.33,7.86">Results of the CLEF 2009 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">2009. Sep 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,411.14,337.63,7.86;12,151.52,422.10,145.58,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,212.82,411.14,214.57,7.86">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,432.47,337.64,7.86;12,151.52,443.43,62.50,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,200.20,432.47,275.83,7.86">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,453.80,337.97,7.86;12,151.52,464.76,84.78,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,209.38,453.80,122.97,7.86">OTTER 3.3 Reference Manual</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mccune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Argonne, Illinois</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,475.13,337.97,7.86;12,151.52,486.09,200.39,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,254.71,475.13,128.31,7.86">System Description: E-KRHyper</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,404.18,475.13,76.40,7.86;12,151.52,486.09,116.30,7.86">Automated Deduction -CADE-21, Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,496.46,337.98,7.86;12,151.52,507.42,329.07,7.86;12,151.52,518.38,329.07,7.86;12,151.52,529.34,73.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,396.10,496.46,84.49,7.86;12,151.52,507.42,131.14,7.86">Information retrieval baselines for the respubliqa task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,305.11,507.42,175.49,7.86;12,151.52,518.38,296.84,7.86">Results of the CLEF 2009 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,539.71,337.97,7.86;12,151.52,550.67,329.07,7.86;12,151.52,561.63,329.07,7.86;12,151.52,572.58,329.07,7.86;12,151.52,583.54,141.25,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,285.83,550.67,194.76,7.86;12,151.52,561.63,329.07,7.86;12,151.52,572.58,14.22,7.86">Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kurimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,173.99,572.58,45.01,7.86">CLEF 2008</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename></persName>
		</editor>
		<meeting><address><addrLine>Aarhus, Denmark; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">September 17-19. 2009</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="12,142.62,593.91,337.98,7.86;12,151.52,604.87,142.12,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,266.63,593.91,171.64,7.86">The design and implementation of Vampire</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Riazanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Voronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,445.58,593.91,35.01,7.86;12,151.52,604.87,48.43,7.86">AI Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,615.24,337.97,7.86;12,151.52,626.20,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,197.66,615.24,122.34,7.86">E -a brainiac theorem prover</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,327.87,615.24,81.48,7.86">AI Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,636.57,337.98,7.86;12,151.52,647.53,190.63,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,251.71,636.57,192.71,7.86">The TPTP Problem Library: CNF Release v1.2.1</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Suttner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,450.24,636.57,30.35,7.86;12,151.52,647.53,100.02,7.86">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="203" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
