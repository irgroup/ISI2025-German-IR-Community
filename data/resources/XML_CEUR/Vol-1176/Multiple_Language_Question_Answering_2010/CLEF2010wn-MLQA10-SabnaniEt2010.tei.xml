<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.54,151.67,291.81,12.54;1,272.09,169.07,51.39,12.54">Question Answering System: Retrieving relevant passages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.81,207.32,60.69,9.05"><forename type="first">Hitesh</forename><surname>Sabnani</surname></persName>
							<email>hiteshsabnani3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dhirubhai Ambani Institute of Information and Communication Technology</orgName>
								<address>
									<settlement>Gandhinagar</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.99,207.32,79.08,9.05"><forename type="first">Prasenjit</forename><surname>Majumder</surname></persName>
							<email>prasenjit.majumdar@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dhirubhai Ambani Institute of Information and Communication Technology</orgName>
								<address>
									<settlement>Gandhinagar</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.54,151.67,291.81,12.54;1,272.09,169.07,51.39,12.54">Question Answering System: Retrieving relevant passages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57E7585CDC816B82E8D8C99F71F817BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Information Retrieval</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses the QA system submitted by Dhirubhai Ambani Institute of Information and Communication Technology, India in the ResPubliQA 2010. We have participated in the monolingual en-en task. Our system retrieves a candidate paragraph that contains the answer to a natural language question. Depending on the n-gram similarity score of the candidate paragraph, a decision is made whether to answer the question or not. The objective of our participation is to test our implementation of various strategies like Query Expansion, n-gram similarity matching, and non-answering criteria.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A question-answering system returns textual strings (answer) from a given document collection (corpus), in response to a natural language question <ref type="bibr" coords="1,400.16,487.55,10.66,8.96" target="#b0">[1]</ref>. The task of developing a question-answering system can be decomposed into sub-problems like question processing, passage retrieval, and answer extraction <ref type="bibr" coords="1,373.87,510.47,10.69,8.96" target="#b1">[2]</ref>. A generic questionanswering architecture is shown in Fig. <ref type="figure" coords="1,284.57,521.99,3.76,8.96" target="#fig_0">1</ref>. Here, the first step is to index the corpus for facilitating fast and accurate retrieval. In step 2, the natural language questions are converted to structured queries that are to be used on the passage retrieval engine. In step 3, the structured queries developed are used against the index to retrieve a ranked list of passages. Finally, the semantics or the structuring of the question is used along For every question, our system either returns an answer passage or chooses not to answer it. We are using Lemur Toolkit <ref type="bibr" coords="2,289.28,172.62,11.69,8.96" target="#b2">[3]</ref> for passage retrieval. Further sections of this paper discuss the Lemur Toolkit, Methodology of our system, Results, and Conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lemur Toolkit and the Indri Query Language</head><p>Lemur Toolkit is a joint initiative of the CIIR from the University of Massachusetts and LTI from the Carnegie Mellon University for the facilitation of research in the field of Information Retrieval. Indri Search Engine is a part of the Lemur Toolkit. The toolkit is used for indexing the corpus and retrieving the passages for the queries against an index.</p><p>The structured query file generated by processing the natural language questions is fed as an input to the Indri Search Engine. This file contains various parameters like the query operators, number of paragraphs to be retrieved, index of the corpus, etc. An example of a query operator is 'combine' which computes the ranked list for a query based on the score calculated by a scoring function <ref type="bibr" coords="2,365.47,357.45,10.78,8.96" target="#b3">[4]</ref>. For passage retrieval, the field to be retrieved can be specified in the structured query file. Some of the query operators of the Indri Query Language <ref type="bibr" coords="2,375.86,380.49,12.01,8.96" target="#b4">[5]</ref> that have been used are described below:</p><p>1. #combine[](x1 x2): It ranks the documents/passages based on the occurrences of the query terms x1 and x2. If we wish to extract a field rather than a document, we tag the field while indexing and retrieve it by using the query #combine[p] where 'p' is the field. For instance, in our case, we have to retrieve paragraphs instead of documents, which are described by &lt;p&gt; tags in the corpus. So, we index at paragraph level and retrieve using the query #combine </p><formula xml:id="formula_0" coords="2,236.21,530.70,65.65,24.25">#combine = Π b i (1/n) i = 1</formula><p>Here, b i is the i th term and n is the number of terms, in the #combine operator. For e.g. in the query #combine[](x1 x2), score would be (score for x1) (1/2) * (score for x2) (1/2) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">#n(x1 x2): It is used for finding the occurrences of x1 and x2 within proximity of</head><p>'n' words. We have used #1(x1 x2) for phrases where x1 and x2 are phrase terms and come one after the other.</p><p>Experimentally (ResPubliQA 2009 data), it was found that the results of #combine operator were best suited to our requirements and it gave better retrieval results. So, we have preferred #combine operator over other operators of the toolkit.</p><p>We begin by indexing the corpus with the IndriBuildIndex application of the Lemur Toolkit. Krovetz's algorithm is used for stemming of the terms in the index. For enabling the retrieval of paragraph tags rather than complete documents, documents are indexed at paragraph level. This is done by indicating the paragraph tag &lt;p&gt; in the index parameter file. So, the paragraphs (&lt;p&gt; tags in the corpus) are treated as different documents and retrieved in the retrieval phase.</p><p>In the next step, natural language queries are converted to Indri Query Language queries. Initially, the natural language queries are annotated using a POS tagger. We are using Stanford Log-Linear Part-of-Speech Tagger <ref type="bibr" coords="3,343.44,266.46,11.69,8.96" target="#b5">[6]</ref> developed by Stanford NLP Lab. The first set of structured queries (used for the baseline run -run1) is generated by having the noun, pronoun, adjective, adverb, preposition, and non-auxiliary verb terms as the parameters of the #combine operator.</p><p>This set of structured queries is inputted to the Indri Search Engine, and against the index of step 1 (in Fig. <ref type="figure" coords="3,235.42,323.97,3.62,8.96" target="#fig_2">2</ref>), we retrieve the top 100 passages for each question and save the results in a file.</p><p>In the next step, the file (containing the top 100 passages for each query) is reindexed. Further retrieval (in the forthcoming steps) is done on this new index in an effort to reduce the retrieval time by performing the retrieval on a smaller index.</p><p>Here, the assumption is that in most cases the passage containing the answer exists in the ranked list of top 100 passages. In our experiments on the ResPubliQA 2009 data, we found this was the case for 449 out of 500 questions. The maximum score (c@1) that any participant achieved in ResPubliQA 2009 was 0.61 <ref type="bibr" coords="3,382.75,415.89,10.60,8.96" target="#b6">[7]</ref>. So, it was a fair decision to re-index the data in order to reduce the retrieval time. The query analysis on the ResPubliQA 2009 data helped us to know how the common questions are structured and answered. For e.g. a reason question can be asked by 'Why', 'What is the reason', 'What is the purpose' and its answer usually contains terms like 'reason', 'in order', 'due to', and 'because'. Similarly, answer to a question asking a definition is likely to contain terms like 'means' and 'is defined as'. This analysis is used to expand the queries in the query expansion phase. The type of questions has been found by lexical analysis of the terms. We have also extracted phrases from a natural language question. The chunks in which a noun term follows an adjective or a noun, and vice-versa have been identified as phrases. 'United Nations', 'migrating workers', and 'Jason Gibbs' are some of the examples. These phrases have been added to the query using '#1' operator which ensures that the terms in the #1 operator will not be separated by a distance of more than 1 word. For e.g. #1(United Nations) will rank the passages containing 'United Nations' but not containing 'United States of America and Canada are two big nations in North America.' Reason and definition questions have been expanded the most in the sense that they include both phrase terms along with the added terms which are not actually part of the question.</p><p>The expanded query in the previous step is now inputted afresh to the Indri Search Engine. The index developed in step 4 (in Fig. <ref type="figure" coords="4,316.81,356.61,4.17,8.96" target="#fig_2">2</ref>) is used here. The result of this step is a new set of 100 candidate passages for each query. Further processing is done on this new ranked list.</p><p>We use 2 approaches for predicting the final answer. For questions whose expected answer type is found to be a location, person, or an organization, we have used the Stanford Named Entity Recognizer <ref type="bibr" coords="4,313.06,414.09,10.84,8.96" target="#b7">[8]</ref>, developed by Stanford NLP Lab. These questions are basically the 'Who', 'Whose', 'Whom', 'Which country' questions. The Named Entity Recognizer tags the candidate passage with the 'location', 'person', or 'organization' tags. The top ranked passage having these tags is selected to be the final selected passage. We then use n-gram similarity approach. Previous ResPubliQA results have shown that there is a strong correlation between terms in question and answer passages and n-gram similarity can be used to exploit this correlation for achieving better results <ref type="bibr" coords="4,297.23,494.63,10.75,8.96" target="#b8">[9]</ref>. We compute the score for a passage by summing all possible x-gram matches where x is less than or equal to the number of terms in the question. We do this summation till the end of question is reached. Then, we divide the above generated sum by the maximum possible n-gram score (i.e. n*(n+1)/2). The final score is a fraction between 0 and 1.</p><p>Finally, we set a threshold value for the score match to be 0.15. If n-gram match score exceeds 0.15, we answer the question. Otherwise, we choose not to answer the question. For the question, 'Who is the father of Tom Dickens?', and a candidate answer 'Ronald Dickens is the father of Tom Dickens', n-gram sum is equal to the number of x-gram matches where x is a number from 1 to n and n is the number of terms in the question which is 7 in this case. So, n-gram sum here is 21 (6 1-gram, 5 2-gram, 4 3gram, 3 4-gram, 2 5-gram, and 1 6-gram matches). Maximum possible n-gram sum is 28 (7 1-grams, 6 2-grams, 5 3-grams, 4 4-grams, 3 5-grams, 2 6-grams, and 1 7gram). So, n-gram score is 21/28 = 0.75. Hence, there is a high chance of this candidate answer to be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We participated in the monolingual en-en task of ResPubliQA 2010. Of the two runs submitted, the queries in the 1 st run are created after treating the questions with the POS Tagger and removing the stop-words, and having important terms (nouns, pronouns, adjectives, adverbs, prepositions, non-auxiliary verbs) in the query. These queries are passed into the Indri Search Engine and we get a ranked list of top 100 passages (step 3 in Fig. <ref type="figure" coords="5,225.01,352.79,3.62,9.05" target="#fig_2">2</ref>). The top ranked passage is selected as the answer in this run. The 2 nd run includes our implementation of techniques like Query Expansion and n-gram similarity matching. We have also included Named Entity Recognition. In Fig. <ref type="figure" coords="5,143.04,387.35,3.76,9.05" target="#fig_2">2</ref>, step 8 results in the final output of the 2 nd run wherein a decision is made about whether to answer the question or not. In the case of unanswered questions, we submitted the top ranked passage in step 7 as a candidate answer. Of the 31 unanswered questions in the 2 nd run, the candidates of 14 were incorrect. So, proportion of the answers correctly discarded is 14/31 = 0.45. The table below summarizes the results of our 2 runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have discussed our system that has participated in ResPubliQA 2010. The results show that our implementations of Query Expansion and n-gram similarity help the system to achieve a better score. The re-indexing done in step 4 (Fig. <ref type="figure" coords="5,147.10,639.13,4.17,9.05" target="#fig_2">2</ref>) reduces the retrieval time which can be an important factor when the corpus size is large. Also, the fact that our c@1 score is higher than the accuracy (run2) shows that the use of our non-answering criteria is justified. However, there is scope for further experiments on large sets of data to decide on the threshold value (in step 8 of Fig. <ref type="figure" coords="5,153.81,685.12,4.17,9.05" target="#fig_2">2</ref>) for the n-gram similarity score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,124.82,683.06,205.01,8.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A generic question-answering architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,359.36,472.43,111.32,8.96;2,138.38,483.95,332.50,8.96;2,138.38,495.37,332.44,9.06;2,138.38,506.99,324.82,8.96;2,272.93,518.39,4.98,8.96;2,231.17,533.63,4.98,8.96"><head></head><label></label><figDesc>[p]. If search is also to be performed at the paragraph level, we use #combine[p](x1.(p) x2.(p)). This ranks the passages according to the occurrences of x1 or x2 in the paragraph tag 'p' rather than the complete document. The scoring function of #combine operator is n b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,124.82,680.42,141.10,8.96"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,202.85,586.67,24.96,8.96;4,138.38,598.98,326.77,12.76;4,198.53,614.27,36.06,8.96;4,259.73,614.27,132.79,8.96;4,138.38,637.19,331.96,8.96;4,124.82,648.71,311.75,8.96;4,138.38,671.78,247.75,8.96;4,195.77,683.30,110.64,8.96"><head></head><label></label><figDesc>sum = Σ Σ (m*1), where y is the x-gram under consideration, m = 1 if x=1 y=x there is a match, otherwise m = 0 For maximum possible n-gram sum, in the above formula, m = 1 for all values of x and y as all x-grams match. So, maximum possible n-gram sum = (n*(n+1)/2) n-gram score = n-gram sum / maximum possible n-gram sum = n-gram sum / (n*(n+1)/2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,136.22,467.89,320.52,67.98"><head>Table 1 :</head><label>1</label><figDesc>Runs of the system</figDesc><table coords="5,141.74,480.47,315.00,55.40"><row><cell>Run</cell><cell>c@1</cell><cell>Correct</cell><cell>Incorrect</cell><cell>Unanswered</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>0.64</cell><cell>127</cell><cell>73</cell><cell>0</cell><cell>0.64</cell></row><row><cell>2</cell><cell>0.68</cell><cell>117</cell><cell>52</cell><cell>31</cell><cell>0.67</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,128.58,177.32,341.87,9.05;6,142.82,188.84,327.99,9.05;6,142.82,200.36,150.14,9.05" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,267.77,177.32,202.69,9.05;6,142.82,188.84,21.56,9.05">Overview of the TREC 2007 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,185.21,188.84,281.06,9.05">Proceedings of The Sixteenth Text REtrieval Conference, TREC 2007</title>
		<meeting>The Sixteenth Text REtrieval Conference, TREC 2007<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,211.76,342.12,9.05;6,142.82,223.28,130.85,9.05" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,192.89,211.76,273.81,9.05">Open-Domain Question Answering from Large Text Collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Chicago University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,234.80,188.28,9.05" xml:id="b2">
	<monogr>
		<ptr target="http://lemurproject.org/" />
		<title level="m" coord="6,142.82,234.80,74.14,9.05">The Lemur Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,246.32,145.63,9.05;6,142.82,257.84,208.49,9.05" xml:id="b3">
	<monogr>
		<ptr target="http://ciir.cs.umass.edu/~metzler/indriretmodel.html" />
		<title level="m" coord="6,142.82,246.32,126.74,9.05">Indri Retrieval Model Overview</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,269.36,175.34,9.05;6,142.82,280.79,212.82,9.05" xml:id="b4">
	<monogr>
		<ptr target="http://ciir.cs.umass.edu/~metzler/indriquerylang.html" />
		<title level="m" coord="6,142.82,269.36,156.81,9.05">Indri Query Language Quick Reference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,292.31,189.14,9.05;6,142.82,303.83,178.27,9.05" xml:id="b5">
	<monogr>
		<ptr target="http://nlp.stanford.edu/software/tagger.shtml" />
		<title level="m" coord="6,142.82,292.31,170.51,9.05">Stanford Log-linear Part-of-Speech Tagger</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,315.35,341.93,9.05;6,142.82,326.87,327.60,9.05;6,142.82,338.39,327.55,9.05;6,142.82,349.79,210.59,9.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,333.99,326.87,136.43,9.05;6,142.82,338.39,242.15,9.05">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.65,338.39,62.73,9.05;6,142.82,349.79,117.42,9.05">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,361.31,342.11,9.05;6,142.82,372.83,44.61,9.05" xml:id="b7">
	<monogr>
		<ptr target="http://nlp.stanford.edu/software/CRF-NER.shtml" />
		<title level="m" coord="6,142.82,361.31,162.07,9.05">Stanford Named Entity Recognizer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.58,384.35,342.01,9.05;6,142.82,395.87,274.38,9.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,298.99,384.35,150.71,9.05">NLEL-MAAT at CLEF-ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,142.82,395.87,181.31,9.05">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
