<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.38,142.82,301.85,11.84">Question Answering for Machine Reading Evaluation</title>
				<funder>
					<orgName type="full">European Social Fund</orgName>
				</funder>
				<funder ref="#_EwJPp37">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_HSENsKz">
					<orgName type="full">Programa Nacional de Movilidad de Recursos Humanos del Plan Nacional de I+D+i</orgName>
				</funder>
				<funder ref="#_JHXHBRH">
					<orgName type="full">US Advanced Defense Research Programs Agency DARPA</orgName>
				</funder>
				<funder ref="#_cKqXJH4">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_qypfHBE">
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
				<funder>
					<orgName type="full">Spanish Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,245.94,178.90,59.31,8.48"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.99,178.90,58.37,8.48"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.16,189.64,50.40,8.48"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu3celctpianta@fbk.eu</email>
							<affiliation key="aff1">
								<orgName type="institution">USC-ISI</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.96,189.64,63.52,8.48"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
						</author>
						<title level="a" type="main" coord="1,160.38,142.82,301.85,11.84">Question Answering for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">82BD238C1A3E7569C83312C4D9AB2394</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Machine Reading</term>
					<term>Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question Answering (QA) evaluation potentially provides a way to evaluate systems that attempt to understand texts automatically. Although current QA technologies are still unable to answer complex questions that require deep inference, we believe QA evaluation techniques must be adapted to drive QA research in the direction of deeper understanding of texts. In this paper we propose such evolution by suggesting an evaluation methodology focused on the understanding of individual documents at a deeper level.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) evaluations measure the performance of systems that seek to "understand" texts. However, this understanding has so far been evaluated using simple questions that require almost no inferences to find the correct answers. These surface-level evaluations have promoted QA architectures based on Information Retrieval (IR) techniques, in which the final answer(s) is/are obtained after focusing on selected portions of retrieved documents and matching sentence fragments or sentence parse trees. No real understanding of documents is performed, since none is required by the evaluation.</p><p>Other evaluation tasks have proposed a deeper analysis of texts. These include the Recognizing Textual Entailment (RTE) Challenges<ref type="foot" coords="1,338.70,514.90,2.82,5.08" target="#foot_0">1</ref> , the Answer Validation Exercise (AVE) <ref type="bibr" coords="1,168.90,526.18,2.82,5.08" target="#b1">2</ref> , and the pilot tasks proposed at the last RTE Challenges <ref type="bibr" coords="1,384.24,526.18,2.82,5.08" target="#b2">3</ref> .</p><p>Recently, Machine Reading (MR) has been defined as a new version of an old challenge for NLP. This task requires the automatic understanding of texts at a deeper level [4]. The objective of an MR system is to extract the knowledge contained in texts for improving the performance of systems in tasks that involves some kind of reasoning. However, there is not yet a clear evaluation strategy for MR systems.</p><p>Given that MR systems use the knowledge of texts in reasoning tasks, in this paper we propose to evolve QA evaluations in order to evaluate MR systems. That is, we propose to design an evaluation methodology for MR systems which takes advantage of the experience obtained in QA, RTE, and AVE evaluations, with the objective of evaluating a task where a deeper level of inference is required. The paper is structured as follows: Section 2 proposes an evaluation methodology of MR systems. Section 3 describes previous evaluations related to the one proposed in this paper. Finally, some conclusions are given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Proposal</head><p>Objective. The objective of an MR system is to understand the knowledge contained in texts in order to improve the performance of systems carrying out reasoning tasks. MR is a task strongly connected with Natural Language Processing (NLP). Given this connection between NLP and MR, it is expected that MR can benefit from the experience obtained in more than 50 years of NLP research. We think the current state of NLP technologies offers a good opportunity for proposing an evaluation of MR systems.</p><p>We propose an evaluation approach that represents an evolution of previous NLP evaluations, which are described in Section 3. Form of test. In the evaluation proposed here, systems would receive a set of documents and a questionnaire for each document. Each questionnaire would be used for checking the understanding of a single document. Thus, the typical IR step of finding relevant documents is not required, and the system can focus on understanding the document.</p><p>The evaluation would measure the quality of a system's answers to the questions for each document. The objective of a system is to pass the test associated to each document, indicating that the system understood the document. The final evaluation measure would count the number of documents that have been understood by a system.</p><p>This evaluation requires systems to understand the test questions, analyze the relation among entities contained in questions and entities expressed by candidate answers, and understand the information contained in documents. An answer must be selected for each question, and the reasoning behind the answer must be given. Therefore, it is a task in which different NLP tasks converge, including QA, RTE, and Answer Validation (AV). Question content. A series of increasingly sophisticated questions, and increasingly challenging answer forms, can be developed over the years. Question evolution can for example proceed as follows:</p><p> simple factoids: facts that (as in traditional QA evaluation) are explicitly present in the text  facts that are explicitly present but are not explicitly related (for example, they do not appear in the same sentence, although any human would understand they are connected)</p><p> facts that are not explicitly mentioned in the text, but that are one inferential step away (as in the RTE challenge)  facts that are explicitly mentioned in the text but that require some inference to be connected to form the answer  facts that are not explicitly mentioned in the text and that require some inference to be connected to form the answer Answer forms. Different answer types can be suggested. We propose to begin with multiple-choice tests, where a list of possible answers for each question is given and the system has to select only one of the given answers. As systems increase capabilities, the answer form can evolve from multiple-choice tests to cloze tests to open-ended answer formulation tests to task performance tests. An example multiple-choice test is shown in Fig. <ref type="figure" coords="3,346.28,272.68,4.71,8.48" target="#fig_1">1</ref> (the question is about a text of junk food). This evaluation is similar to tests for people learning a new language, whose reading comprehension is checked using different tests that measure their understanding of what they are reading. Hence, we see our evaluation as a test where systems obtain marks that represent their understanding of texts. These tests can be of different difficulty depending on the understanding level required.</p><p>According to the article, some parents:  Another popular test for language learners is the cloze test, in which the learner (in our case: the system) has to fill one or more words into a given sentence or phrase. The phrase is carefully constructed so that a more-than-superficial reading is required in order to fill the gap correctly. Continuing the example in Fig. <ref type="figure" coords="3,388.01,493.96,3.58,8.48">2</ref>.</p><p>According to the article, some parents teach their children bad eating habits by __________________________ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Example cloze question</head><p>Test procedure. Given the fact that systems might contain built-in background knowledge, it is important to determine as baseline how much a system knows before it reads the given text. We therefore propose to apply the test twice for each text:</p><p> First, the system tries to answer the questions without having seen any text;  Second, the system reads the text;  Third, the system answers the same questions.</p><p>The system's score on the first trial (before it has seen the text) is subtracted from the system's score on the second. Domain. Regarding document collections, we suggest using documents from different topics. For this reason we propose to perform the evaluation over world news. This domain covers several categories that contain different phenomena. Therefore, these documents offer the possibility of evaluating general-purpose systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The evaluation proposed in this paper is an extension of previous evaluations of automatic NLP systems. The first of these evaluations is QA, where a system receives questions formulated in natural language and returns one or more exact answers to these questions, possibly with the locations from which the answers were drawn as justification [2]. The evaluation of QA systems began at the eighth edition of the Text Retrieval Conference (TREC) <ref type="bibr" coords="4,258.24,320.98,2.82,5.08" target="#b3">4</ref>  [6], and has continued in other editions of TREC, at the Cross Language Evaluation Forum (CLEF) <ref type="bibr" coords="4,326.10,331.78,2.82,5.08" target="#b4">5</ref> in the EU, and at the NII-NACSIS Test Collection for IR Systems (NTCIR) <ref type="bibr" coords="4,296.70,342.58,2.82,5.08" target="#b5">6</ref> in Japan.</p><p>Most of the questions used in these evaluations ask about facts (as for example Who is the president of XYZ?) or definitions (for instance What does XYZ mean?). These questions do not require the application of inferences or even a deeper-thansurface-parse analysis for finding correct answers. Besides, since systems could search for answers among several documents (using IR engines), it was generally possible to find in some document a "system-friendly" statement that contained exactly the answer information stated in an easily matched form. This made QA both shallow and relatively easy. In contrast, by giving only a single document per test, our evaluation requires systems to try to understand every statement, no matter how obscure it might be, and to try to form connections across statement in case the answer is spread over more than one sentence.</p><p>On the other hand, our evaluation benefits from past research in QA systems and QA-based evaluations, specifically in the analysis and classification of questions, different ways of evaluating different QA behaviors, etc.</p><p>The following related evaluation is the Recognizing of Textual Entailment (RTE), where a system must decide whether the meaning of a text (the Text T) entails the meaning of another text (the Hypothesis H): whether the meaning of the hypothesis can be inferred from the meaning of the text [3].</p><p>RTE systems have been evaluated at the RTE Challenges, whose first edition was proposed in 2005. The RTE Challenges encourage the development of systems that have to treat different semantic phenomena. Each participant system at the RTE Challenges received a set of text-hypothesis (T-H) pairs and had to decide for each T-H pair whether T entails H. These evaluations are more focused on the understanding of texts than QA because they evaluate whether the knowledge contained in a text imply the knowledge contained in another. Then, our evaluation would benefit from the RTE background in the management of knowledge.</p><p>Our evaluation differs from RTE because RTE is a simple classification task (either T entails H or it does not), whereas we require extracting the knowledge that answers a question, not for checking whether the text is contained in another text.</p><p>A combination of QA and RTE evaluations was done in the Answer Validation Exercise (AVE) [8,9,10]. Answer Validation (AV) is the task of deciding, given a question and an answer from a QA system, whether the answer is correct or not. AVE was a task focused on the evaluation of AV systems and it was defined as a problem of RTE in order to promote a deeper analysis in QA.</p><p>Our evaluation has some similarities with AVE. The multiple choice test we propose can be approached with an AV system that selects the answer with more chances of being correct. However, we would give as support a whole document while in AVE only a short snippet was used. Besides, AVE used questions defined in the QA task at CLEF, which were simpler (they required less inference and analysis) than the ones we propose.</p><p>The proposal of ResPubliQA 2009 at CLEF [5] had the objective of transferring the lessons learned at AVE to QA systems. With this purpose, ResPubliQA allowed to leave a question unanswered in case of a system was not sure about finding a correct answer to that question. The objective was to reduce the amount of incorrect answers while keeping the number of correct ones, by leaving some questions unanswered. Thus, it was promoted the use of AV modules for deciding whether to ask or not a question.</p><p>Another application of RTE, similar to AVE, in the context of Information Extraction is going to be made in a pilot task defined at the RTE-6 <ref type="bibr" coords="5,403.20,421.72,2.82,5.08" target="#b6">7</ref> with the aim of studying the impact of RTE systems in Knowledge Base Population (KBP) <ref type="bibr" coords="5,444.72,432.52,2.82,5.08" target="#b7">8</ref> . The objective of this pilot task is to validate the output of participant systems at the KBP slot filling task that was celebrated at the Text Analysis Conference (TAC) <ref type="bibr" coords="5,424.74,454.12,2.82,5.08" target="#b8">9</ref> .</p><p>Systems participating at the KBP slot filling task must extract from documents some values for a set of attributes of a certain entity. Given the output of participant systems at KBP, the RTE KBP validation pilot consists of deciding whether each of the values detected for an entity is correct according to the supporting document. For taking this decision, participant systems at the RTE KBP validation pilot receive a set of T-H pairs, where the hypothesis is built combining an entity, an attribute and a value.</p><p>This task is similar to the one proposed here because it checks the correctness of a set of facts extracted from a document. However, the KBP facts are very simple because they ask about properties of an entity, whereas the QA evaluation we propose can in principle ask about anything. Therefore, our task would, as it evolves, require a deeper level of inference.</p><p>Finally, we want to remark that there have been other efforts closer to our proposal for evaluating understanding systems, as the "ANLP/NAACL 2000 Workshop on Reading comprehension tests as evaluation for computer-based language understanding systems<ref type="foot" coords="6,228.78,173.08,5.70,5.08" target="#foot_8">10</ref> ".</p><p>This workshop proposed to evaluate understanding systems by means of Reading Comprehension (RC) tests. These tests are similar to the ones suggested in this paper. That is, the evaluation consisted of a set of texts and a series of questions about each text. Although the approach may have not changed, the field has now made many steps forward and we think that the current state of systems is more appropriate for suggesting this evaluation. In fact, most of the approaches presented at that workshop showed how to adapt QA systems to such kind of evaluation.</p><p>A more complete evaluation methodology of MR systems has been reported in [7], where the authors proposed to use also RC tests. However, the objective of these tests was to extract correct answers from documents, what is similar to QA without an IR engine. In our evaluation, we would ask for selecting a correct answer from a set of candidate ones, where the correct answer contains a knowledge that is present in the document, but this knowledge is written in a different way. Thus, we ask for a better understanding of documents than in RC tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Current research on NLP technologies has gradually led to systems that may now attempt a deeper-than-surface understanding of texts. A series of evaluations of previous systems has allowed these advances. However, a new evaluation is required to drive the increasing deepening of understanding and use of inference to augment surface-level and deeper structure matching. We propose here an evaluation using question answering on single documents, where the answers require increasingly deep levels of inference. This evaluation moves effort away from retrieval and toward reasoning, which is a prerequisite for true text understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,176.70,369.73,136.00,8.72;3,176.70,380.53,260.05,8.72;3,176.70,391.39,202.33,8.72;3,176.70,402.19,260.51,8.72"><head></head><label></label><figDesc>A. tend to overfeed their children B. believe their children don't need as many vitamins as adults C. claim their children should choose what to eat D. regard their children's bad eating habits as a passing phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,236.04,429.40,139.87,7.62"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example multiple choice question</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.26,625.84,164.70,7.62"><p>http://pascallin.ecs.soton.ac.uk/Challenges/RTE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="1,148.26,645.64,156.38,7.62"><p>http://www.nist.gov/tac/2010/RTE/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,148.26,625.84,64.48,7.62"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,148.26,635.74,104.77,7.62"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,148.26,645.64,98.07,7.62"><p>http://research.nii.ac.jp/ntcir/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="5,148.26,625.84,156.38,7.62"><p>http://www.nist.gov/tac/2010/RTE/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="5,148.26,635.74,120.03,7.62"><p>http://nlp.cs.qc.cuny.edu/kbp/2010/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="5,148.26,645.64,101.62,7.62"><p>http://www.nist.gov/tac/2010/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="6,151.08,645.64,167.00,7.62"><p>http://www.aclweb.org/anthology/W/W00/#0600</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p>This work has been partially supported by The <rs type="funder">Spanish Government</rs> through the "<rs type="funder">Programa Nacional de Movilidad de Recursos Humanos del Plan Nacional de I+D+i</rs>" <rs type="grantNumber">2008-2011</rs> (Grant <rs type="grantNumber">PR2009-0020</rs>), the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the project <rs type="projectName">QEAVis-Catiex</rs> (<rs type="grantNumber">TIN2007-67581-C02-01</rs>), the <rs type="funder">Regional Government of Madrid</rs> under the <rs type="projectName">Research Network MA2VICMR</rs> (<rs type="grantNumber">S-2009/TIC-1542</rs>), the <rs type="funder">Education Council of the Regional Government of Madrid</rs>, the <rs type="funder">European Social Fund</rs> and the <rs type="funder">US Advanced Defense Research Programs Agency DARPA</rs>, under contract number <rs type="grantNumber">FA8750-09-C-0172</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HSENsKz">
					<idno type="grant-number">2008-2011</idno>
				</org>
				<org type="funded-project" xml:id="_cKqXJH4">
					<idno type="grant-number">PR2009-0020</idno>
					<orgName type="project" subtype="full">QEAVis-Catiex</orgName>
				</org>
				<org type="funded-project" xml:id="_EwJPp37">
					<idno type="grant-number">TIN2007-67581-C02-01</idno>
					<orgName type="project" subtype="full">Research Network MA2VICMR</orgName>
				</org>
				<org type="funding" xml:id="_qypfHBE">
					<idno type="grant-number">S-2009/TIC-1542</idno>
				</org>
				<org type="funding" xml:id="_JHXHBRH">
					<idno type="grant-number">FA8750-09-C-0172</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,146.52,166.72,322.23,7.62;7,154.02,176.44,314.75,7.62;7,154.02,186.22,314.77,7.62;7,154.02,195.94,172.90,7.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,223.42,176.44,245.36,7.62;7,154.02,186.22,70.03,7.62">How to Evaluate your Question Answering System Every Day and Still Get Real Work Done</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.61,186.22,229.18,7.62;7,154.02,195.94,83.92,7.62">Proceedings of the Second Conference on Language Resources and Evaluation (LREC-2000)</title>
		<meeting>the Second Conference on Language Resources and Evaluation (LREC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1495" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.51,205.60,322.23,7.62;7,154.02,215.38,314.76,7.62;7,154.02,225.10,50.56,7.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,449.94,205.60,18.80,7.62;7,154.02,215.38,105.29,7.62">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,278.52,215.38,190.26,7.62;7,154.02,225.10,25.21,7.62">Proceedings of the Tenth Text REtrieval Conference (TREC)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.52,234.82,322.28,7.62;7,154.02,244.60,314.75,7.62;7,154.02,254.26,52.44,7.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,343.28,234.82,125.53,7.62;7,154.02,244.60,72.22,7.62">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,241.28,244.60,119.83,7.62">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.52,263.98,322.26,7.62;7,154.02,273.76,207.52,7.62" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,346.96,263.98,56.03,7.62">Machine reading</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,418.06,263.98,50.72,7.62;7,154.02,273.76,183.20,7.62">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.53,283.48,322.17,7.62;7,154.02,293.19,314.61,7.62;7,154.02,302.91,314.72,7.62;7,154.02,312.63,314.75,7.62;7,154.02,322.35,314.74,7.62;7,154.02,332.13,314.70,7.62;7,154.02,341.85,144.45,7.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,378.71,293.19,89.92,7.62;7,154.02,302.91,235.80,7.62">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,426.31,312.63,42.47,7.62;7,154.02,322.35,314.74,7.62;7,154.02,332.13,99.92,7.62">Multilingual Information Access Evaluation Vol. I Text Retrieval Experiments, Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mostefa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roda</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09-30">2009. 30 September -2 October</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. (to be published)</note>
</biblStruct>

<biblStruct coords="7,146.52,351.57,322.13,7.62;7,154.02,361.29,204.18,7.62" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,289.29,351.57,175.85,7.62">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,163.16,361.29,118.52,7.62">Text Retrieval Conference TREC-8</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="83" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.52,371.01,322.22,7.62;7,154.02,380.79,272.24,7.62" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,350.57,371.01,118.17,7.62;7,154.02,380.79,140.32,7.62">Reading comprehension tests for computer-based understanding evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,299.84,380.79,53.87,7.62">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.52,390.51,322.12,7.62;7,154.02,400.23,314.63,7.62;7,154.02,409.95,314.62,7.62;7,154.02,419.67,137.62,7.62" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,337.32,390.51,131.33,7.62;7,154.02,400.23,29.09,7.62">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.31,409.95,199.34,7.62;7,154.02,419.67,29.84,7.62">Advances in Multilingual and Multimodal Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09">2007. September 2008</date>
			<biblScope unit="volume">5152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.52,429.39,322.24,7.62;7,154.02,439.17,314.85,7.62;7,154.02,448.89,314.78,7.62;7,154.02,458.55,314.85,7.62;7,154.02,468.33,245.97,7.62" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,382.48,429.39,86.28,7.62;7,154.02,439.17,67.12,7.62">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentín</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,312.99,448.89,155.81,7.62;7,154.02,458.55,314.85,7.62;7,154.02,468.33,15.22,7.62">Evaluation of Multilingual and Multi-modal Information Retrieval, 7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-20">2006. September 20-22, 2006</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="7,150.92,478.05,317.71,7.62;7,154.02,487.77,314.69,7.62;7,154.02,497.55,314.75,7.62;7,154.02,507.21,314.70,7.62;7,154.02,516.93,168.41,7.62" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,337.21,478.05,131.41,7.62;7,154.02,487.77,29.06,7.62">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,213.55,497.55,255.22,7.62;7,154.02,507.21,243.88,7.62">Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Th</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-17">2008. September 17-19, 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
