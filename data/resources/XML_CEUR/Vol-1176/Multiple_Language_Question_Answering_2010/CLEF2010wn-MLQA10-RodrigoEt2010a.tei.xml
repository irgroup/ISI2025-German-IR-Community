<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,195.24,116.06,224.82,12.91;1,184.32,134.06,246.66,12.91">A Question Answering System based on Information Retrieval and Validation</title>
				<funder ref="#_8aKFH9U">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_jhMdyCd">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,229.92,169.46,58.91,11.13"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.56,171.62,88.11,8.97"><forename type="first">Joaquín</forename><surname>Pérez-Iglesias</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.28,183.62,59.88,8.97"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.88,183.62,72.92,8.97"><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
							<email>ggarrido@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.32,183.62,62.94,8.97"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,195.24,116.06,224.82,12.91;1,184.32,134.06,246.66,12.91">A Question Answering System based on Information Retrieval and Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3918ABEBF08D67A2211C2AD8AF696986</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our participation at ResPubliQA 2010 was based on applying an Information Retrieval (IR) engine of high performance and a validation step for removing incorrect answers. The IR engine received additional information from the analysis of questions, what produces a slight improvement in results. However, the validation module discarded sometimes too much correct answers, contributing to reduce the overall performance. These errors were due to the application of too strict constraints. Therefore, future work must be focused on reducing the amount of false negatives returned by the validation module. On the other hand, we observed that IR ranking offers important information for selecting the final answer, but better results could be obtained if additional sources of information were also considered.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The NLP &amp; IR group at UNED participated at ResPubliQA 2010 after the successful results of its previous participation. The system used in 2009 was based on an Information Retrieval step of high performance and Answer Validation.</p><p>ResPubliQA 2010 proposed two tasks related to Question Answering (QA): one for retrieving a paragraph with a correct answer given a question, and a second one where both the paragraph and the exact answer string must be returned. Both tasks were developed using the same set of questions and over the same collections (JRC Acquis<ref type="foot" coords="1,476.76,482.18,3.49,6.28" target="#foot_0">1</ref> and EuroParl<ref type="foot" coords="1,188.04,494.06,3.49,6.28" target="#foot_1">2</ref> ). We have participated in monolingual English and Spanish Paragraph Selection (PS) tasks.</p><p>This year we proposed to improve the Information Retrieval (IR) step by adding information about the question. Thus, we wanted to increase the recall of the IR engine as well as increase the ranking given to promising candidate paragraphs. Furthermore, we applied an Answer Validation (AV) similar to the one performed last year, including some minor changes for solving some errors. This validation was focused on removing paragraphs that show evidences of not having a correct answer.</p><p>The structure of this paper is as follows: the main components of our system are described in Section 2. The description of the runs sent to ResPubliQA is given in Section 3, while the results of these runs are shown in Section 4 and their analysis in Section 5. Finally, some conclusions and future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>This section describes the main components of our QA system. Figure <ref type="figure" coords="2,431.28,139.82,4.98,8.97" target="#fig_0">1</ref> shows the architecture of the system. The different phases of the system work for guiding the search to the most promising answers, removing the ones that are considered incorrect. The following subsections describe in detail each one of the different components of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>The objective of this step is to obtain features from the question that could be helpful in the following steps. All the information obtained by this module is given to the following steps of the system.</p><p>The information extracted is:</p><p>-The expected answer type, which is an information that offers an important constraint to be accomplish by correct answers. We performed a classification based on handmade patterns where the categories were: count, time, location, organization, person, definition and other. -The question focus, which is a word close to the interrogative term that supplies additional information about the type of the expected answer. The detection of the focus is important for extracting the answer from candidate paragraphs. However, as we participated this year only at the PS task, we used the question focus for other purposes.</p><p>The question focus defines sometimes the context of the question and it is likely that the focus does not appear close to the correct answer. For example, if we have the question What country was Nadal born in?, the question focus is country and it is likely that a correct answer to this question does not contain the word country. Therefore, we used this intuition for creating the query which will be submitted to the IR engine (more details are given in Section 2.2).</p><p>-The Named Entities (NE) contained in the question. These NEs are important for supporting the correctness of an answer contained in a candidate paragraph. Hence, NEs represent an important information for detecting correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Retrieval</head><p>The mission of the IR module is to perform a first selection of paragraphs that are considered relevant to the input question. We decided to use BM25 <ref type="bibr" coords="3,419.76,206.18,11.72,8.97" target="#b2">[3]</ref> last year, a model that can be adapted to fit the specific characteristics of the data in use. More information about the implementation and successful results of last year IR engine are given in <ref type="bibr" coords="3,169.20,242.06,10.69,8.97" target="#b1">[2]</ref>.</p><p>We decided to keep this model adding some minor modifications with the purpose of improving the recall and ranking of correct paragraphs. The modifications added this year were related to the creation of the query submitted to the IR engine from the input question. These changes were:</p><p>-As it was mentioned in Section 2.1, the question focus usually does not appear in correct answers. Thus, we consider that the presence of the focus has to receive a lower importance in the query. -The NEs of a question represent important information and it is likely they appear in correct answers. Therefore, our intuition was to give a higher importance to NEs in the query.</p><p>The procedure for including these two new features was to assign different boost factors to the terms of a query. Then, given a question we built the corresponding query to be used in the IR phase following these steps:</p><p>1. Removal of stopwords 2. Stemming pre-process based on Snowball implementation of Porter algorithm 3. Use of different weights, considering three possibilities:</p><p>-high discriminative power: this value is given to NEs -medium discriminative power: this value is given to the rest of the terms of the query -low discriminative power: this value is given to the question focus (if it exists)</p><p>In order to select the values for the different boost factors, we performed several experiments at the development period. We selected the following values after performing several experiments:</p><p>-High discriminative terms received boost factor 2.</p><p>-Low and normal discriminative terms received boost factor 1. We decided to give the same boost factor to these terms because a lower boost factor of the focus produced worse results in the development period.</p><p>The IR engine returned a maximum of 100 candidate paragraphs to the following steps of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Validation</head><p>The mission of this step is to eliminate possible incorrect paragraphs contained in the list returned by the IR engine. Thus, there are more possibilities of giving at the end of the process a correct answer. We say that this module validates a paragraph when it is considered that the paragraph is correct. If a paragraph is considered as incorrect, we say that the paragraph is rejected.</p><p>This phase works in a pipeline processing, where a set of constraints are checked for each candidate paragraph in each step. Only candidate paragraphs that accomplish all the constraints are returned at the end of this pipeline.</p><p>It is important to remark that this phase is not focused on checking the correctness of a candidate paragraph. It is focused on detecting paragraphs which show some feature that leads to think that they are incorrect. The module was implemented in this way because that it is usually easier to detect incorrect answers than to detect correct ones.</p><p>We applied the same three modules used in the last edition. Next sections describe each of these modules in short. More details can be seen in <ref type="bibr" coords="4,371.88,293.66,10.69,8.97" target="#b4">[5]</ref>.</p><p>Expected Answer Type Only paragraphs that contain a NE of the same type that the expected answer type are validated. This validation is performed only for questions where the expected answer was count, time, location, organization or person. All the paragraphs given to other types of questions are validated by this module.</p><p>The Named Entity Recognizer (NER) gave us the distinction among location, organization and person entities only in Spanish. This is why we performed two kinds of matching:</p><p>-Fine grained matching: location, organization and person questions must be answered by paragraphs with at least a NE of the corresponding class. For example, if we have a question asking about a person, only paragraphs with a person entity will be validated. -Coarse grained matching: since location, organization and person entities in English were grouped by the NER in a single category (this category is called enamex), we decided to group also questions asking about this kind of entities in a single category (enamex questions). Then, each of these questions can be answered with an entity of this category. For example, if a question asks about a location, a paragraph with a organization entity will be validated.</p><p>On the other hand, based on our experience, we grouped in both languages count and time questions into a category that can be answered by a numeric or time expression. We took this decision because the NER sometimes assigns the class time to numeric expressions and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entities Presence</head><p>The validation process performed by this module follows the intuition that the NEs of a question must appear in a correct answer <ref type="bibr" coords="4,425.64,620.30,10.69,8.97" target="#b3">[4]</ref>. We could have applied this restriction in the retrieval phase (retrieving only paragraphs that contain these NEs), but we obtained better results when the restriction is applied at this step.</p><p>Only paragraphs that contain all the NEs of the question are validated by this module and returned as output. If a question does not have any NE, all the paragraphs are validated by this module because there are no evidences for rejecting them.</p><p>The restriction of containing the exact NE could seem very strict. In fact, it produced some errors in the last edition. We thought about using a relaxed version for allowing a little difference between NEs using the edit distance of Levenshtein <ref type="bibr" coords="5,411.00,180.38,10.69,8.97" target="#b0">[1]</ref>. However, we saw that this option produced false positives in NEs with a similar wording but that refer to different entities.</p><p>Since we were not sure about what matching was better, and taking into account the importance of NEs for supporting correctness, we decided to apply the strict version.</p><p>Acronym Checking This module is applied only in definition questions that ask for the meaning of an acronym (as for example What is UNESCO? or What does UNESCO stand for? Only paragraphs that are considered to contain a definition of the acronym are validated.</p><p>In order to apply this module, definition questions are analyzed to check whether they are asking about the meaning of an acronym. In that case, the acronym is extracted. Then, only paragraphs that contain the acronym inside a pair of brackets are validated in the current implementation of this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Selection of Final Answer</head><p>This module received the answers that accomplish the constraints checked in the previous step and decided the final answer for each question. In case of not having any candidate answer after the AV phase, the option NoA (what in ResPubliQA means that a system is not sure about finding a correct answer to a question and prefers not to answer it) is selected. NoA answers can receive the hypothetical answer that would be given in case of answering the question. These hypothetical answers are used for evaluating the validation performance. We gave in these cases the first answer according to the IR ranking.</p><p>If there is more than one paragraph at the end of the AV phase, we had two options for ranking answers and selecting the final one last year:</p><p>-The ranking given by the IR engine -A ranking based on lemmas overlapping, with the possibility of including textual entailment (more details are given in <ref type="bibr" coords="5,300.12,580.58,10.57,8.97" target="#b4">[5]</ref>).</p><p>The ranking based on lemmas offered better results. However, we wanted to compare in this edition the pure IR system with the combination of IR and AV. Since we could only send two runs per language, we decided to use the IR ranking in both runs for a better comparison. Thus, we can study how the removal of paragraphs considered as incorrect affects the IR ranking.</p><p>The runs submitted were selected taking into account the objectives of our participation. These objectives were to study the improvement of the IR phase using more information about the question and its combination with a validation step.</p><p>We decided to submit two runs per language (we participated in English and Spanish) for the PS task: one run based only on the IR phase described in Section 2.2 and a second one that added the validation step (described in Section 2.3) to the output of the IR engine. More in detail, the submitted runs were as follows:</p><p>-Spanish</p><p>• Run 1: the validation modules described in Section 2.3 (using the fine grained matching for the expected answer type) were applied to the output of the IR engine. If there was no paragraph after the validation process, the question was not answered (NoA option) and the first paragraph in the IR ranking was given as the hypothetical answer. In case of having more than a paragraph after the validation phase, the paragraph with the highest ranking according to the IR ranking among the validated paragraphs was given as answer.</p><p>• Run 2: all the questions were answered using the first paragraph returned by the IR engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-English</head><p>• Run 1: this run was similar to the Spanish first run except that it uses the coarse grained matching for the expected answer type. • Run 2: similar to the Spanish second run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The answers of each run were evaluated by human assessors and tagged as correct (R) or incorrect (W). The hypothetical answers given in case of choosing not to answer a question were evaluated as unanswered with a correct candidate answer (UR), or unanswered with an incorrect candidate answer (UI). The main evaluation measure was c@1 (Formula (1)), while accuracy (Formula (2)) was used as a secondary evaluation measure.</p><formula xml:id="formula_0" coords="6,233.52,544.00,247.16,22.63">c@1 = #R n + #R n * #U R + #U I n<label>(1)</label></formula><formula xml:id="formula_1" coords="6,254.28,579.28,226.40,22.63">accuracy = #R + #U R n<label>(2)</label></formula><p>The results obtained by our system are shown in Table <ref type="table" coords="6,381.24,608.42,4.98,8.97" target="#tab_0">1</ref> for Spanish and Table <ref type="table" coords="6,134.76,620.30,4.98,8.97" target="#tab_1">2</ref> for English. These tables show also the validation performance of the system. This validation performance is calculated as the ratio of wrong hypothetical answers with respect to the whole amount of NoA answers. That is, if all the hypothetical answers were incorrect, the validation performance would be perfect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of results</head><p>According to Tables <ref type="table" coords="7,218.40,309.50,4.98,8.97" target="#tab_0">1</ref> and<ref type="table" coords="7,243.24,309.50,3.77,8.97" target="#tab_1">2</ref>, our runs performed over 0.5 for both accuracy and c@1. However, we can see how the second run in each language, which did not include validation, performed better than the first one (second runs gave more correct answers). Therefore, the addition of the validation step reduced the performance of the system. These results were different to the ones obtained last year, where the validation phase improved the results. The results of the two runs in each language were very similar according to accuracy, what is another indication of the low performance of validation. However, we have seen that the validation step allowed to remove some incorrect answers, contributing to return in the second runs some correct answers that were not given by the first runs. Therefore, the validation step can help in improving results, but it must reduce the amount of false negatives that it produces.</p><p>Most of the errors produced by the validation step were due to the NEs presence module. As it was already mentioned above, the criteria for deciding whether a NE appears in a paragraph can be too strict. This leads to errors discarding paragraphs, increasing the amount of false negatives given by the module. An example of these errors happened in question 13 (What procedure does Mr. Sarkozy advocate concerning the internet?), where the NE of the question was Mr. Sarkozy, and in some of the candidate paragraphs appeared Mr Sarkozy (without the dot after Mr). This simple change in the wording led to not answering to that question. Therefore, it is evident that we have to relax this constrain with the objective of reducing the amount of false negatives, while keeping the number of false positives.</p><p>One of the modifications included in our system this year was the use of different boost factors for different terms in the IR step. These different boost factors were given taking into account the NEs and focus of a question. This modification allowed a slight improvement in the performance of the IR engine and the overall results by increasing the ranking of correct answers.</p><p>Nevertheless, the IR engine has already a really good performance, and the question analysis output must be taken into account by more modules of the system if we want to obtain a higher improvement of the overall results.</p><p>We consider that better results could be obtained by improving the selection of the final answer. The improvement could be achieved by adding information from the question analysis and validation steps. In fact, the ranking based on lemmas that was used last year showed that additional information based on lemmas overlapping improved ranking.</p><p>In conclusion, the results showed that the IR performance is quite good. A better validation performance combined with more information for selecting the final answer is what our system needs for improving overall results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have described in this paper our participation at ResPubliQA 2010. Our system has taken advantage of a powerful IR engine that has been slightly improved adding information from the question analysis.</p><p>Besides, a validation step was applied in order to remove possible incorrect answers from the pool of paragraphs returned by the IR engine. The validation has contributed to find more correct answers to some questions, but some of its components were too strict, removing also correct answers. Therefore, a relaxation of some of the constraints implemented in the validation step must be applied with the purpose of reducing the amount of false negatives without increasing the number of false positives.</p><p>On the other hand, the selection of the final answer was based only in the IR ranking after validation. A way of improving the overall performance would be to select the final answer taking into account also information of validation as well as the analysis of the question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,246.60,339.94,119.89,8.07;2,145.87,216.36,323.62,108.76"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the system</figDesc><graphic coords="2,145.87,216.36,323.62,108.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,206.76,116.02,201.86,55.59"><head>Table 1 .</head><label>1</label><figDesc>Results for Spanish runs.</figDesc><table coords="7,206.76,129.94,201.86,41.67"><row><cell cols="3">Run #R #W #UR #UI c@1 accuracy validation</cell></row><row><cell></cell><cell></cell><cell>performance</cell></row><row><cell cols="2">run 1 92 73 22 13 0.54 0.57</cell><cell>0.37</cell></row><row><cell>run 2 108 92 0</cell><cell>0 0.54 0.54</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,206.76,194.74,201.86,55.71"><head>Table 2 .</head><label>2</label><figDesc>Results for English runs.</figDesc><table coords="7,206.76,208.66,201.86,41.79"><row><cell cols="3">Run #R #W #UR #UI c@1 accuracy validation</cell></row><row><cell></cell><cell></cell><cell>performance</cell></row><row><cell cols="2">run 1 117 66 13 4 0.63 0.65</cell><cell>0.24</cell></row><row><cell>run 2 129 71 0</cell><cell>0 0.65 0.65</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,645.70,87.45,8.07"><p>http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.72,656.86,112.05,8.07"><p>http://www.europarl.europa.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the project <rs type="projectName">QEAVis-Catiex</rs> (<rs type="grantNumber">TIN2007-67581-C02-01</rs>), the <rs type="funder">Regional Government of Madrid</rs> under the <rs type="programName">Research Network MA2VICMR</rs> (<rs type="grantNumber">S-2009/TIC-1542</rs>), the <rs type="funder">Education Council of the Regional Government of Madrid</rs> and the <rs type="funder">European Social Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jhMdyCd">
					<idno type="grant-number">TIN2007-67581-C02-01</idno>
					<orgName type="project" subtype="full">QEAVis-Catiex</orgName>
				</org>
				<org type="funding" xml:id="_8aKFH9U">
					<idno type="grant-number">S-2009/TIC-1542</idno>
					<orgName type="program" subtype="full">Research Network MA2VICMR</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.10,548.74,342.48,8.07;8,146.52,559.78,243.32,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,230.64,548.74,249.95,8.07;8,146.52,559.78,12.55,8.07">Binary Codes Capable of Correcting Deletions, Insertions and Reversals</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Levensthein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,175.08,559.78,87.34,8.07">Soviet Physics -Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.10,568.30,342.42,9.99;8,146.52,581.14,334.12,8.07;8,146.52,592.18,49.88,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,173.76,581.14,206.78,8.07">Information Retrieval Baselines for the ResPubliQA Task</title>
		<author>
			<persName coords=""><forename type="first">Joaquín</forename><surname>Pérez-Iglesias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,398.52,581.14,66.14,8.07">CLEF 2009. LNCS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="8,138.10,602.62,342.49,8.07;8,146.52,613.54,334.19,8.07;8,146.52,624.46,220.52,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,295.80,602.62,184.79,8.07;8,146.52,613.54,187.06,8.07">Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,204.12,624.46,20.57,8.07">SIGIR</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM/Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.10,632.98,342.54,10.11;8,146.52,645.94,333.97,8.07;8,146.52,656.86,146.00,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,405.36,635.02,75.28,8.07;8,146.52,645.94,121.57,8.07">The Effect of Entity Recognition on Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,283.08,645.94,39.76,8.07">CLEF 2007</title>
		<title level="s" coord="8,384.96,645.94,95.53,8.07;8,146.52,656.86,25.69,8.07">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="483" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.10,117.82,342.46,9.99;9,146.52,130.78,334.16,8.07;9,146.52,141.70,60.68,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,146.52,130.78,246.37,8.07">Approaching Question Answering by means of Paragraph Validation</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joaquín</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,410.04,130.78,65.90,8.07">CLEF 2009. LNCS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
