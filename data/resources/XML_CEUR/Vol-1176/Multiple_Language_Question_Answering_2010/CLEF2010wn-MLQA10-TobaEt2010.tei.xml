<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.36,142.76,274.07,11.84;1,233.22,159.20,145.59,11.84">Contextual Approach for Paragraph Selection in Question Answering Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.28,195.28,47.91,8.48"><forename type="first">Hapnes</forename><surname>Toba</surname></persName>
							<email>hapnes.toba@ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<orgName type="institution">University of Indonesia Depok Campus</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.87,195.28,47.16,8.48"><forename type="first">Syandra</forename><surname>Sari</surname></persName>
							<email>syandra.sari@ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<orgName type="institution">University of Indonesia Depok Campus</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.94,195.28,52.58,8.48"><forename type="first">Mirna</forename><surname>Adriani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<orgName type="institution">University of Indonesia Depok Campus</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.68,195.28,57.76,8.48"><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<orgName type="institution">University of Indonesia Depok Campus</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.36,142.76,274.07,11.84;1,233.22,159.20,145.59,11.84">Contextual Approach for Paragraph Selection in Question Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99F534D1111EE0E8CFD90FD028AA7842</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>context supporting</term>
					<term>passage retrieval</term>
					<term>random projection</term>
					<term>n-gram overlapping</term>
					<term>textual containment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year we participated in the English monolingual paragraph selection task at ResPubliQA 2010. Our general strategy is to find the supporting word context from query and candidate passages during the paragraph selection. We use the techniques of state-of-the art publicly available question answering systems, i.e. Open Ephyra and JIRS, and the random projection implementation in the Semantic Vectors package to evaluate the word context. To strengthen the paragraph selection, besides the context evaluation, we also use n-gram overlapping and textual containment. Our approach has a c@1 measure of 0.73 for our pattern-based context configuration and 0.64 for our n-gram-based context configuration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) is a specific form of information retrieval (IR) that seeks to produce an exact answer given a natural language question. An automated QA system tries to retrieve explicit answers in the form of a single answer or snippets of text rather than a whole document or set of documents. The main techniques that mostly have been used in current QA research are semantic analysis using semantic role labeling <ref type="bibr" coords="1,198.65,511.72,14.46,8.48" target="#b19">[20]</ref>, named entity recognition <ref type="bibr" coords="1,326.80,511.72,9.98,8.48" target="#b4">[5]</ref>, path dependency <ref type="bibr" coords="1,416.77,511.72,10.05,8.48" target="#b4">[5]</ref>, semantic markup <ref type="bibr" coords="1,174.65,522.58,14.49,8.48" target="#b11">[12]</ref>, n-gram passages <ref type="bibr" coords="1,260.34,522.58,10.28,8.48" target="#b1">[2,</ref><ref type="bibr" coords="1,273.10,522.58,7.10,8.48" target="#b6">7,</ref><ref type="bibr" coords="1,282.70,522.58,11.22,8.48" target="#b10">11]</ref>, statistical methods <ref type="bibr" coords="1,372.62,522.58,14.93,8.48" target="#b12">[13,</ref><ref type="bibr" coords="1,390.00,522.58,11.22,8.48" target="#b13">14]</ref>, combinations of semantic structures and probabilistic approaches <ref type="bibr" coords="1,332.27,533.38,14.46,8.48" target="#b14">[15]</ref>, and combinations of semantic structures and automated reasoning <ref type="bibr" coords="1,280.42,544.18,14.92,8.48" target="#b15">[16,</ref><ref type="bibr" coords="1,298.12,544.18,11.78,8.48" target="#b16">17,</ref><ref type="bibr" coords="1,312.68,544.18,11.16,8.48" target="#b17">18]</ref>. There is no ultimate technique, each approach has its own role, application domain, and tasks <ref type="bibr" coords="1,358.26,555.04,14.45,8.48" target="#b18">[19]</ref>. This year's ResPubliQA<ref type="foot" coords="1,251.64,565.60,2.82,5.08" target="#foot_0">1</ref> evaluation campaign is a continuation from last year which tries to evaluate QA performance in a specific context, i.e. the legal domain <ref type="bibr" coords="1,143.34,587.50,14.46,8.48" target="#b20">[21]</ref>. Since this year is our first participation, we decided to compete in the English monolingual paragraph selection task. We experiment with various QA strategies that are supported by the notion of word context, which return passages that contain candidate answers. For this purpose, we develop a context supporting paragraph selection strategy which validates the passage retrieval mechanisms from two stateof-the-art publicly available QA systems, Open Ephyra <ref type="bibr" coords="2,353.32,151.66,11.02,8.48" target="#b0">[1]</ref> and JIRS <ref type="bibr" coords="2,403.21,151.66,10.23,8.48" target="#b1">[2,</ref><ref type="bibr" coords="2,415.74,151.66,6.82,8.48" target="#b6">7]</ref>.</p><p>Our general hypothesis in this experiment is that, in a specific domain, the context of a question is near to the context of candidate paragraphs that are suggested during the passage retrieval. This hypothesis is a generalization of the distributional hypothesis from the word space methodology, which says that words with similar meaning tend to occur in similar contexts <ref type="bibr" coords="2,304.13,205.84,10.04,8.48" target="#b2">[3]</ref>. We took the publicly available random projection implementation in Semantic Vectors <ref type="bibr" coords="2,328.25,216.58,11.03,8.48" target="#b3">[4]</ref> as the context supporting system of our approach. Our observation during this experiment will be: how to employ word context to support passage retrieval performed by language model and n-gram approaches. To answer this research question, our approach tries to boost the influence of word context during paragraph selection.</p><p>In this paper we describe the approach of our context supporting strategy and the results obtained in the monolingual English paragraph selection task. The rest of the paper is structured as follows: in section 2 we describe in general the related systems whose techniques were used in our approach, namely Open Ephyra, JIRS and Semantic Vectors. The details of our approach will be covered in section 3. Section 4 will show some results and analysis in the paragraph selection task of this year's ResPubliQA. Finally, some conclusions and suggestions for future work will be given in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QA Pipeline and Word Space Methodology</head><p>Open Ephyra<ref type="foot" coords="2,194.22,407.86,2.82,5.08" target="#foot_1">2</ref> and JIRS<ref type="foot" coords="2,235.62,407.86,2.82,5.08" target="#foot_2">3</ref> are two QA systems which offer comprehensive pipelines. They base their passage retrieval strategies on, respectively, language models and ngram structures of the passages. Both strategies rely on the probability and sequences of adjacent words from query-passage pairs, but do not really observe the word in context. Alternatively, Semantic Vectors<ref type="foot" coords="2,300.48,451.12,2.82,5.08" target="#foot_3">4</ref> offers the possibility to observe the word context in a domain by implementing the word space methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pattern-based Question Answering</head><p>Open Ephyra (OE) uses a pattern learning approach to categorize questions <ref type="bibr" coords="2,430.78,514.90,10.23,8.48" target="#b0">[1,</ref><ref type="bibr" coords="2,443.48,514.90,6.82,8.48" target="#b4">5]</ref>. OE can learn question-answer pairs and uses standard IR systems, such as Indri, to fetch text snippets that are suitable for pattern extraction. It consists of four main modules: a question analyzer, query generator, search engine, and answer extractor. Each module can be used independently and is thus suitable for experimenting with multiple approaches to question-answering as one pipeline system.</p><p>There are two main steps for the pattern-learning approach in OE: 1. The first is to learn the question patterns from question templates according to each question type. The aim of this step is to interpret the questions and transform them into queries. The question templates need to be manually developed according to various interrogative sentences that are independent for each natural language. 2. The second step is to learn the answer patterns from question-answer pairs. The aim of this second step is to extract answer candidates from relevant document snippets and to rank them. In our experiment, we only used the first step to develop an appropriate set of question patterns for each question type according to the training data from last year's ResPubliQA. A recognized question pattern for a given question is essential in order to extract important keywords (i.e. target, context and property) for querying purposes. The retrieval phase in OE is done within the Indri search engine that searches for passages in documents -based on the recognized query keywords -which could contain the answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">n-Gram-based Question Answering</head><p>The question answering module in JIRS is an extension of the JIRS passage retrieval system as described in <ref type="bibr" coords="3,233.28,334.18,9.97,8.48" target="#b6">[7]</ref>. The general architecture of JIRS is comparable to OE as described in the previous sub-section. A given question will be classified to an appropriate class that will be further used by the passage retrieval algorithm. JIRS's retrieval algorithm is based on the ordering of n-neighboring words extracted from a passage, which is called as Clustered Keyword Positional Distance (CKPD).</p><p>JIRS is based on the idea that in a large document collection, an n-gram related with a question will be found in the collection at least once <ref type="bibr" coords="3,373.63,399.10,14.42,8.48" target="#b10">[11]</ref>. Only passages with n-grams that contain question terms are returned. The weight of each passage is calculated according to the similarity between the question and the passage n-grams. The similarity of a passage with the question is greater if the passage shares longer structures with the question <ref type="bibr" coords="3,250.77,442.42,9.99,8.48" target="#b1">[2]</ref>. A brief introduction of JIRS can found in <ref type="bibr" coords="3,427.08,442.42,14.47,8.48" target="#b10">[11]</ref>, while the complete CKPD algorithm can be found in <ref type="bibr" coords="3,320.88,453.22,10.02,8.48" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Vectors</head><p>Semantic Vectors is an open source package that can be used to build context vectors of word concepts in a specific domain. It implements word space methodology <ref type="bibr" coords="3,445.77,519.04,11.02,8.48" target="#b2">[3]</ref> by applying random projections of words in the document collection. Words in Semantic Vectors are represented as vectors in a high dimensional space, where words or documents that have related meanings are in close proximity. By applying random projection, the computational resources that are usually required for computing semantic similarity, as for instance in Latent Semantic Analysis (LSA), can be minimized.</p><p>The random projection strategy can be summarized in two main steps <ref type="bibr" coords="3,420.17,594.82,10.23,8.48" target="#b2">[3,</ref><ref type="bibr" coords="3,432.66,594.82,7.08,8.48" target="#b3">4,</ref><ref type="bibr" coords="3,442.00,594.82,3.50,8.48" target="#b7">8</ref>]: 1. Step 1: Build the term vectors.</p><p>For each document, make an N-dimensional ternary index vector by placing a small number, k, of -1's and +1's (e.g., ten of each; k = 10) at random among the N-dimensional index vector, the rest will be 0's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Step 2: Build the document vectors Scan through the documents. Every time the word appears, add the index vector to row w of matrix G, where G is a (M x N) matrix, M is the number of words in the collection and N is the number of reduced columns. The Semantic Vectors package uses the Apache Lucene API <ref type="foot" coords="4,396.72,183.88,2.82,5.08" target="#foot_4">5</ref> to create a word space model from a term document matrix, using random projection to perform dimensionality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>For the data preparation, from the JRC-ACQUIS<ref type="foot" coords="4,330.90,298.06,2.82,5.08" target="#foot_5">6</ref> and EUROPARL<ref type="foot" coords="4,403.02,298.06,2.82,5.08" target="#foot_6">7</ref> corpus we have created a passage index based on the paragraph segmentations. In total we have around 1.5 million passages. We created separate indexes for each information retrieval system that we used in our approach, namely Indri, JIRS and Lucene.</p><p>Indri is a search engine that is specially designed for passage retrieval <ref type="bibr" coords="4,426.96,341.50,11.02,8.48" target="#b1">[2]</ref> such as JIRS. The difference between them lies in the retrieval model. Indri's retrieval model is based on a combination of language modeling and inference network retrieval frameworks <ref type="bibr" coords="4,190.60,373.96,10.00,8.48" target="#b5">[6]</ref>, while JIRS based its retrieval model on the CKPD algorithm <ref type="bibr" coords="4,436.71,373.96,10.03,8.48" target="#b1">[2]</ref>.</p><p>The Lucene index will be used by the Semantic Vectors package to form the terms and document vectors <ref type="bibr" coords="4,234.32,395.68,9.98,8.48" target="#b3">[4]</ref>. In our approach we choose the value of 2000 for the number of dimensions. This value is the reduced columns for documents projection that is considered as the word space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Architecture</head><p>The high level architecture of our approach can be seen in Fig. <ref type="figure" coords="4,386.19,472.30,3.54,8.48" target="#fig_0">1</ref>. This architecture is based on the general framework of question answering systems. The first component is the question analysis, which normalizes the question (i.e. removes the question word, stop-words, and performs word stemming), and determines the question type. We used manually developed question patterns to determine the question types.</p><p>After the question analysis step, the system delivers the normalized question to a query generator based on the techniques that were used in OE <ref type="bibr" coords="4,391.38,537.22,10.24,8.48" target="#b0">[1,</ref><ref type="bibr" coords="4,405.14,537.22,6.76,8.48" target="#b4">5]</ref>, JIRS <ref type="bibr" coords="4,440.58,537.22,11.02,8.48" target="#b1">[2]</ref> and Semantic Vectors <ref type="bibr" coords="4,218.31,548.08,10.03,8.48" target="#b3">[4]</ref>. The next step is the information retrieval phase. In our approach, it is done by the Indri, JIRS or Lucene search engine, which corresponds to the generated queries. The retrieval results from each search engine are collected in separated files that will be evaluated by the paragraph selection component.</p><p>The main idea during paragraph selection is to evaluate whether supporting word context can be found among the candidate passages. It checks first the word context that is present at the top-1 retrieval of the candidate passages. This is done by comparing candidate passages from Indri or JIRS with the one retrieved by Semantic Vectors. If the two systems that were compared returned the same document at the top-1 retrieval, then we believe that supporting word(s) context was found in the passages that were compared, or in other words the documents "shared" the same context. Otherwise, the paragraph selection will be determined by applying n-gram overlapping, textual containment <ref type="bibr" coords="5,270.59,205.84,10.16,8.48" target="#b8">[9,</ref><ref type="bibr" coords="5,283.34,205.84,11.23,8.48" target="#b9">10]</ref>, and answer type validation from the namedentity of the expected answer type. The above explained architecture was tested using the data from ResPubliQA 2009 for all 500 questions, the results of which can be seen in Table <ref type="table" coords="6,380.62,175.24,3.54,8.48" target="#tab_0">1</ref>. From this initial experiment, we can see that our proposed approach, as described in previous sub-section, outperformed the baseline accuracy of Indri and JIRS by 6.8% and 6.2% respectively. The graphical interpretation from the above experimental result is presented in Fig. <ref type="figure" coords="6,291.99,291.94,3.54,8.48">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2 Graphical Interpretation of ResPubliQA 2009 Test Questions</head><p>Fig. <ref type="figure" coords="6,172.06,464.62,3.54,8.48">2</ref>. gives the number of right answers for each question type, i.e.: factoid (F), definition (D), purpose (PU), reason (R), procedure (PR), and for all question types (All) from last year's ResPubliQA training data. There are a number of conclusions that can be drawn from this initial experiment: 1. JIRS' retrieval engine gives the best result when used as a standalone system. 2. D and PR question types have lower accuracy in comparison with F, PU, and R. 3. The combination of context support and JIRS gives the best accuracy. 4. Semantic Vectors is not suitable as a standalone system for this task, but it can give a supporting 'second opinion' when passages from Indri or JIRS are not supported enough as a final answer. These initial results increased our confidence that context supporting will improve the overall performance of our architecture. Our final scenarios for the submitted runs in ResPubliQA 2010 will be explained in the next two sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pattern Based -Context Support</head><p>For our first submitted run, we combined the technique of pattern-based question answering, as proposed in <ref type="bibr" coords="7,248.02,174.28,10.19,8.48" target="#b0">[1,</ref><ref type="bibr" coords="7,261.71,174.28,7.87,8.48" target="#b4">5]</ref> with the results from Semantic Vectors. There are five question patterns manually developed for each question type: factoid, definition, reason-purpose, procedure, and opinion. Each question pattern will be responsible for the expected answer type analysis during the paragraph selection phase. An example of the developed patterns is shown in Listing 1, for the opinion question type. The detailed strategy of the paragraph selection phase can be seen in Listing 2. In this scenario, the supporting context (the first step in listing 2), is used to select passages that we believed 'share' the same context, and hence they can be used confidently as the final answer. If the result of Indri and Semantic Vectors did not return the same passage, we evaluate the top-5 of Indri results by applying n-gram overlapping (step 2.a) If n-gram overlapping is unsuccessful, i.e. all passages have the same amount of n-gram overlapping, we use a textual containment strategy to calculate the best proportion of query terms that can be found in the passages (step 2.b), and at the same time try to find a named-entity that is expected to be present in the passages (step 2.c). 'No answer' will be returned if there is no passage that meets the conditions in the strategy (step 3). Note that by using containment, passages whose text is too long will be discarded. This step is used essentially to anticipate the definition question type, which usually only has one or two terms from the query that can be found back in the candidate passages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Listing 2 Detail Strategy of Scenario 1 (Pattern-based -Context Support)</head><p>Table <ref type="table" coords="8,178.78,140.92,4.71,8.48" target="#tab_2">2</ref> gives some examples of the system's decision according to this scenario for the test questions from ResPubliQA 2010. As an example from Table <ref type="table" coords="8,267.92,330.34,3.54,8.48" target="#tab_2">2</ref>, the Quest. # 0008 (Name a purpose of MEDIA Mundus), Indri and SV have both retrieved the same top-1 document (EP_CRE-20090507-EN_cl.xml). According to our strategy, the system should take step 1 in Listing 2 as the final decision for the paragraph returned.</p><p>In another example from Table <ref type="table" coords="8,280.42,373.66,3.55,8.48" target="#tab_2">2</ref>, the Quest. # 0027 (Define children footwear), Indri and SV have retrieved different documents. The next step is to determine the best n-gram overlapping (step 2.a) between the top-5 Indri retrieved passages and the question terms (children footwear). Since all of the top-5 Indri retrieved passages have only one term in common with the question (i.e. the term children), the system should now compute the value of textual containment <ref type="bibr" coords="8,352.04,427.72,10.05,8.48" target="#b8">[9]</ref>, to determine how well the n-gram overlapping is in comparison with the length of the passage (step 2.b). From last year's training data, we decided to use a minimum threshold value of 0.02 for textual containment. In the case of Quest. # 0027, all of the top-5 retrieved passages have a textual containment value lower than the threshold, hence the system will return 'NOA' as the final answer (step 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">n-Gram Passage Retrieval -Context Support</head><p>For our second submitted run, we combined the results of JIRS and Semantic Vectors to retrieve passages which could contain answer candidates. The detailed strategy of the paragraph selection phase can be seen in Listing 3.</p><p>1. Compare the retrieved passages from JIRS and Semantic Vectors. If the top-1 returned document is the same, then return the passage offered by JIRS as the final answer. The first paragraph selection strategy is to compare the top-1 retrieved passage from JIRS and the one retrieved by Semantic Vectors. If both retrieval systems suggested the same document, then we took the passage returned by JIRS as the final answer. Otherwise, we followed the same strategy as scenario 1 to select the best paragraph. But now, we used the top-1 suggested document from Semantic Vectors as the final paragraph decision. Table <ref type="table" coords="9,281.65,299.14,4.71,8.48" target="#tab_3">3</ref> gives some examples of the system's decision according to this second scenario for the test questions from ResPubliQA 2010. The explanation for Table <ref type="table" coords="9,228.58,320.74,4.71,8.48" target="#tab_3">3</ref> is similar to the explanation for Table <ref type="table" coords="9,384.62,320.74,4.71,8.48" target="#tab_2">2</ref> in the previous subsection. In Scenario 2, there was no strategy developed for the NOA-answer. The reason for this is considering the final decision rule that selects paragraphs from only one document, i.e. the Semantic Vectors top-1 retrieved document -with no textual containment threshold -and thus, there must be one paragraph returned as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>Table <ref type="table" coords="9,169.41,603.76,4.71,8.48" target="#tab_4">4</ref> gives the result summary of our submitted runs. Context support which combines the passage retrieval from Indri and Semantic Vectors (uiir101PSenen) gives better accuracy than the second configuration which uses the combination of JIRS-Semantic Vectors (uiir102PSenen). These two configurations differ in the way passages are refined during the paragraph selection phase (cf. Section 3). If we observe the accuracy of each question type in detail (Table <ref type="table" coords="10,441.33,256.66,3.41,8.48" target="#tab_5">5</ref>), the 'Definition' question type gives the lowest accuracy in each configuration, i.e. 0.43 (Scenario 1) and 0.37 (Scenario 2). This result indicates that the 'Definition' question type is the most difficult one to be answered, and our strategy is not suitable to answer this type of question properly. The best accuracy in both scenarios is achieved for the 'Reason-Purpose' question type, i.e. 0.87 (Scenario 1) and 0.77 (Scenario 2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RIGHT ANS. ACCURACY WRONG ANS JUDGMENTS</head><p>It is interesting to explore the accuracy of the 'Procedure' question type, which achieved 0.58 accuracy in Scenario 1, and 0.68 in Scenario 2. Most of the question types have question patterns and question terms that are almost identical from one question to another; see for example the 'Opinion' question pattern in Listing 1. This is also one of the reasons that Scenario 1 performed better than Scenario 2, except for the 'Procedure' question type that has richer patterns and more answer variations. Table <ref type="table" coords="11,180.26,140.92,4.71,8.48" target="#tab_6">6</ref> shows the judgments of the system's decision that was performed by step #1 in each scenario (the complete steps can be found in Listing 2 and 3), which is important in our word-space contextual supporting strategy. By using Scenario 1, there were 38 questions (19% of all questions) that conformed to step #1, and 24 of them were judged correctly (0.63 accuracy). By using Scenario 2, there were 34 questions (17%) that matched the rule in this step, and 24 of the answers were judged correctly (0.71 accuracy). We found that the accuracy is acceptable, but the number of questions that have been evaluated by this step is rather low, and needs to be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Works</head><p>In this paper we have described our context supporting approach in passage retrieval question answering for the English monolingual task. Our experimental results showed that in a specific domain, context supporting -as a 'second-opinion' decision system by using random document projection -can improve the performance of the passage retrieval (paragraph selection) component in a QA system.</p><p>It would be interesting to conduct a further study in how to relate word context in broader domains, to support open-domain and multilingual question answering. It would also be interesting to investigate the value of reduced dimensionality from a document collection that formed the word space during retrieval. In this experiment we have only used the number of dimensions of 2000, due to technical restrictions. Perhaps, if the value of the dimension is varied, the context supporting strategy could perform better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,209.10,590.86,193.75,7.62;5,143.34,224.90,342.90,363.82"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 High Level Architecture of Proposed Approach</figDesc><graphic coords="5,143.34,224.90,342.90,363.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,143.34,238.76,230.25,5.90;7,151.80,247.28,135.47,5.90;7,156.06,255.80,312.62,5.90;7,156.06,264.32,36.12,5.90;7,223.14,278.80,176.40,7.62"><head>QUESTION_TEMPLATE:</head><label></label><figDesc>What is the &lt;CO&gt; opinion of &lt;TO&gt; ANSWER_TEMPLATE: Opinion: &lt;PO&gt; what (a|the)? &lt;CO&gt; (position|opinion|feelings|ideas) (with respect to) &lt;TO&gt; Listing 1 Example of Question Pattern (Opinion)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,143.34,607.15,62.10,7.41;8,160.26,616.94,308.37,5.90;8,177.24,625.46,291.51,5.90;8,177.24,633.80,291.52,5.90;8,177.24,642.56,144.46,5.90;9,160.26,140.48,308.42,5.90;9,177.24,149.00,291.51,5.90;9,177.24,157.52,291.44,5.90;9,177.24,165.80,291.52,5.90;9,177.24,174.56,144.47,5.90;9,160.26,183.08,308.48,5.90;9,177.24,191.36,291.47,5.90;9,177.24,200.12,221.19,5.90;9,151.74,214.60,308.50,7.62;9,289.80,224.32,32.41,7.62"><head></head><label></label><figDesc>the top-5 passages from JIRS retrieval, count the querypassage n-gram overlapping (unigram, bigram and trigram) from each passage, return passage with the biggest total n-gram overlapping as the final answer. b. If all of the top-5 JIRS passages have the same total count of n-gram, then count the n-gram overlapping (unigram, bigram and trigram) of the query in each paragraph of the retrieved top-1 Semantic Vector document, return passage with the biggest n-gram overlapping as the final answer. c. If there is more than one paragraph from the SV top-1 document have the same total count of n-gram then return paragraph with the best textual containment as the final answer. Listing 3 (cont'd) Detail Strategy of Scenario 2 (n-Gram Passage Retrieval -Contexts Support)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,192.12,191.56,229.33,52.80"><head>Table 1 Test Session ResPubliQA 2009</head><label>1</label><figDesc></figDesc><table coords="6,192.12,209.00,229.33,35.35"><row><cell>IR Engine</cell><cell>Baseline c@1</cell><cell>+Context</cell><cell>Difference</cell></row><row><cell>Indri</cell><cell>29.6</cell><cell>36.4</cell><cell>(+) 6.8</cell></row><row><cell>JIRS</cell><cell>38.8</cell><cell>45</cell><cell>(+) 6.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,154.26,168.04,309.09,147.81"><head>Table 2 Examples of System's Response using Scenario 1</head><label>2</label><figDesc></figDesc><table coords="8,154.26,183.69,309.09,132.16"><row><cell>Quest. #</cell><cell>Top-1 Indri</cell><cell>Top-1 SV</cell><cell>Step (see also</cell><cell>Final Paragraph</cell></row><row><cell></cell><cell></cell><cell>Retrieval</cell><cell>Listing 2)</cell><cell>Returned</cell></row><row><cell></cell><cell>p_id=13</cell><cell>EP_CRE-</cell><cell></cell><cell>p_id="13"</cell></row><row><cell>0008</cell><cell>doc_id=EP_CRE-</cell><cell>20090507-</cell><cell>1</cell><cell>docid="EP_CRE-</cell></row><row><cell></cell><cell>20090507-EN_cl.xml</cell><cell>EN_cl.xml</cell><cell></cell><cell>20090507-EN_cl.xml"</cell></row><row><cell>0001</cell><cell>p_id=45 doc_id=EP_CRE-20091008-EN_cl.xml</cell><cell>jrc31989L0552-en.xml</cell><cell>2.a</cell><cell>p_id="45" docid="EP_CRE-20091008-EN_cl.xml"</cell></row><row><cell>0026</cell><cell>p_id=13 doc_id=jrc32002D0268 -en.xml</cell><cell>jrc31998D0512 -en.xml</cell><cell>2.b, 2.c</cell><cell>p_id="26" docid="jrc32006R158 3-en.xml"</cell></row><row><cell></cell><cell>p_id=164</cell><cell>EP_TA-</cell><cell></cell><cell></cell></row><row><cell>0027</cell><cell>doc_id=EP_TA-</cell><cell>20080116-</cell><cell>3</cell><cell>NOA</cell></row><row><cell></cell><cell>20090114-EN_cl.xml</cell><cell>EN_cl.xml</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,154.26,347.86,309.83,136.65"><head>Table 3 Examples of System's Response using Scenario 2</head><label>3</label><figDesc></figDesc><table coords="9,154.26,363.57,309.83,120.94"><row><cell>Quest. #</cell><cell>Top-1 JIRS</cell><cell>Top-1 SV</cell><cell>Step (see also</cell><cell>Final Paragraph</cell></row><row><cell></cell><cell></cell><cell>Retrieval</cell><cell>Listing 3)</cell><cell>Returned</cell></row><row><cell></cell><cell>p_id=34 doc_id=</cell><cell>EP_CRE-</cell><cell></cell><cell>p_id=34 doc_id=</cell></row><row><cell>0003</cell><cell>EP_CRE-20091008-</cell><cell>20091008-</cell><cell>1</cell><cell>EP_CRE-20091008-</cell></row><row><cell></cell><cell>EN_cl.xml</cell><cell>EN_cl.xml</cell><cell></cell><cell>EN_cl.xml</cell></row><row><cell>0059</cell><cell>p_id=11 doc_id= jrc31988R3498-en.xml</cell><cell>jrc32000R1609 -en.xml</cell><cell>2.a</cell><cell>p_id=11 doc_id= jrc31988R3498-en.xml</cell></row><row><cell></cell><cell>p_id=268 doc_id=</cell><cell>EP_TA-</cell><cell></cell><cell>p_id="283"</cell></row><row><cell>0012</cell><cell>EP_TA-20070201-</cell><cell>20080925-</cell><cell>2.b</cell><cell>docid="EP_TA-</cell></row><row><cell></cell><cell>EN_cl.xml</cell><cell>EN_cl.xml</cell><cell></cell><cell>20080925-EN_cl.xml</cell></row><row><cell>0017</cell><cell>p_id=1006 doc_id= jrc32006R0865-en.xml</cell><cell>EP_TA-20070710-EN_cl.xml</cell><cell>2.c</cell><cell>p_id="289" docid="EP_TA-20070710-EN_cl.xml</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,145.74,168.04,317.78,73.98"><head>Table 4 Submitted Runs Results</head><label>4</label><figDesc></figDesc><table coords="10,145.74,182.26,317.78,59.76"><row><cell></cell><cell></cell><cell>Ans.</cell><cell>Ans.</cell><cell>Unans-</cell><cell>Un-ans.</cell><cell>Un-ans.</cell><cell>Un-ans.</cell><cell>Overall</cell></row><row><cell>Run</cell><cell>Ans.</cell><cell>Right</cell><cell>Wrong</cell><cell>wered</cell><cell>Right</cell><cell>Wrong</cell><cell>Empty</cell><cell cols="2">Accuracy c@1</cell></row><row><cell cols="2">uiir101PSenen 197</cell><cell>143</cell><cell>54</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>0.72</cell><cell>0.73</cell></row><row><cell cols="2">uiir102PSenen 200</cell><cell>127</cell><cell>73</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.64</cell><cell>0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,150.24,327.34,308.81,105.74"><head>Table 5 Question Types Judgments</head><label>5</label><figDesc></figDesc><table coords="10,150.24,356.43,308.81,76.65"><row><cell>Q. Type</cell><cell cols="2"># of Quest. Sce. 1</cell><cell>Sce. 2</cell><cell>Sce. 1</cell><cell>Sce. 2</cell><cell>Sce. 1</cell><cell>Sce. 2</cell></row><row><cell>Factoid</cell><cell>69</cell><cell>53</cell><cell>47</cell><cell>16</cell><cell>22</cell><cell>0.77</cell><cell>0.68</cell></row><row><cell>Definition</cell><cell>29</cell><cell>13</cell><cell>11</cell><cell>17</cell><cell>19</cell><cell>0.45</cell><cell>0.38</cell></row><row><cell>Reason-Purpose</cell><cell>31</cell><cell>27</cell><cell>24</cell><cell>3</cell><cell>6</cell><cell>0.87</cell><cell>0.77</cell></row><row><cell>Procedure</cell><cell>38</cell><cell>22</cell><cell>26</cell><cell>16</cell><cell>12</cell><cell>0.58</cell><cell>0.68</cell></row><row><cell>Opinion</cell><cell>33</cell><cell>28</cell><cell>19</cell><cell>5</cell><cell>14</cell><cell>0.85</cell><cell>0.58</cell></row><row><cell>TOTAL</cell><cell>200</cell><cell>143</cell><cell>127</cell><cell>57</cell><cell>73</cell><cell>0.72</cell><cell>0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,212.16,518.38,198.35,102.75"><head>Table 6 Judgments of "Step #1" in each Scenario</head><label>6</label><figDesc></figDesc><table coords="10,227.10,533.40,172.45,87.74"><row><cell>Step 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Judgements</cell><cell cols="4">RIGHT ANS. WRONG ANS.</cell></row><row><cell>Q. Type</cell><cell cols="4">Sce. 1 Sce. 2 Sce. 1 Sce. 2</cell></row><row><cell>Factoid</cell><cell>13</cell><cell>12</cell><cell>3</cell><cell>3</cell></row><row><cell>Definition</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>2</cell></row><row><cell>Reason-Purpose</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>0</cell></row><row><cell>Procedure</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>2</cell></row><row><cell>Opinion</cell><cell>4</cell><cell>2</cell><cell>0</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.26,645.58,116.00,7.62"><p>http://celct.isti.cnr.it/ResPubliQA/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,148.26,625.72,79.49,7.62"><p>http://www.ephyra.info</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,148.26,635.68,117.23,7.62"><p>http://sourceforge.net/projects/jirs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,148.26,645.58,134.94,7.62"><p>http://semanticvectors.googlecode.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,148.26,625.72,83.80,7.62"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,148.26,635.68,82.27,7.62"><p>http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,148.26,645.58,106.55,7.62"><p>http://www.europarl.europa.eu/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,146.89,469.30,321.97,8.48;11,160.26,480.10,308.47,8.48;11,160.26,490.96,138.77,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,393.91,469.30,74.94,8.48;11,160.26,480.10,241.88,8.48">A Pattern Learning Approach to Question Answering within the Ephyra Framework</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gieselmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,422.46,480.10,22.47,8.48">LNAI</title>
		<imprint>
			<biblScope unit="volume">4188</biblScope>
			<biblScope unit="page" from="687" to="694" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.89,501.76,321.80,8.48;11,160.26,512.56,308.58,8.48;11,160.26,523.42,108.74,8.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,389.17,501.76,79.52,8.48;11,160.26,512.56,174.72,8.48">Answering questions with an n-gram based passage retrieval engine</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,341.52,512.56,127.32,8.48;11,160.26,523.42,27.80,8.48">Journal of Intelligent Information System</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.89,534.22,321.81,8.48;11,160.26,545.02,308.50,8.48;11,160.26,555.88,308.44,8.48;11,160.26,566.62,308.55,8.48;11,160.26,577.48,308.52,8.48;11,160.26,588.34,96.78,8.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,220.37,534.22,146.83,8.48">An introduction to random indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<ptr target="http://www.sics.se/˜mange/papers/RI_intro.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,392.54,534.22,76.16,8.48;11,160.26,545.02,308.50,8.48;11,160.26,555.88,303.97,8.48">Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE)</title>
		<meeting>the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">2005. March 2010</date>
		</imprint>
		<respStmt>
			<orgName>SICS, Swedish Institute of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.89,599.08,321.83,8.48;11,160.26,609.94,308.43,8.48;11,160.26,620.80,64.59,8.48" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,265.16,599.08,203.56,8.48;11,160.26,609.94,187.62,8.48">Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ferraro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MAYA Design</note>
</biblStruct>

<biblStruct coords="12,146.89,140.92,321.87,8.48;12,160.26,151.66,308.51,8.48;12,160.26,162.52,203.07,8.48" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,222.31,140.92,246.44,8.48;12,160.26,151.66,39.36,8.48">Deploying Semantic Resources for Open Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Language Technologies Institute School of Computer Science Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma Thesis</note>
</biblStruct>

<biblStruct coords="12,146.89,173.38,321.89,8.48;12,160.26,184.12,295.36,8.48" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="http://ciir.cs.umass.edu/~metzler/indriretmodel.html" />
		<title level="m" coord="12,217.34,173.38,132.96,8.48">Indri Retrieval Model Overview</title>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,146.89,194.98,321.95,8.48;12,160.26,205.84,308.48,8.48;12,160.26,216.58,308.60,8.48;12,160.26,227.44,21.22,8.48" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,404.49,194.98,64.35,8.48;12,160.26,205.84,234.76,8.48">JIRS Languageindependent Passage Retrieval system: A comparative study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,417.06,205.84,51.68,8.48;12,160.26,216.58,177.89,8.48">Proc. 5th int. conf. on natural language processing (ICON)</title>
		<meeting>5th int. conf. on natural language processing (ICON)<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01-06">4-6 January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,146.89,238.30,321.94,8.48;12,160.26,249.04,308.46,8.48;12,160.26,259.90,308.46,8.48;12,160.26,270.70,170.37,8.48" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,335.84,238.30,132.99,8.48;12,160.26,249.04,113.66,8.48">Random indexing of text samples for Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kristoferson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,449.23,249.04,19.50,8.48;12,160.26,259.90,294.83,8.48">Proc. 22nd Annual Conference of the Cognitive Science Society (U Pennsylvania)</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Gleitman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Josh</surname></persName>
		</editor>
		<meeting>22nd Annual Conference of the Cognitive Science Society (U Pennsylvania)<address><addrLine>Mahwah, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">1036</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,146.89,281.50,321.83,8.48;12,160.26,292.30,286.08,8.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,229.21,281.50,217.91,8.48">On the resemblance and containment of documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.26,292.30,162.74,8.48">Compression and Complexity of Sequences</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,303.16,317.57,8.48;12,160.26,313.90,308.51,8.48;12,160.26,324.76,132.98,8.48" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,217.92,303.16,250.88,8.48;12,160.26,313.90,39.63,8.48">Detecting short passages of similar text in large document collections</title>
		<author>
			<persName coords=""><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.73,313.90,240.03,8.48;12,160.26,324.76,54.41,8.48">Conference on Empirical Methods in Natural Language (EMNLP2001)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,335.56,317.44,8.48;12,160.26,346.36,131.17,8.48" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,308.04,335.56,141.29,8.48">NLEL-MAAT at CLEF-ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rossio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.26,346.36,107.66,8.48">Working Notes ResPubliQA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,357.16,317.52,8.48;12,160.26,368.02,308.52,8.48;12,160.26,378.76,308.61,8.48;12,160.26,389.62,121.50,8.48" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,257.31,357.16,211.44,8.48;12,160.26,368.02,111.60,8.48">AquaLog: An Ontology-Portable Question Answering System for the Semantic Web</title>
		<author>
			<persName coords=""><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,292.99,368.02,175.80,8.48;12,160.26,378.76,24.11,8.48">ESWC (European Semantic Web Conference) LNCS</title>
		<title level="s" coord="12,192.60,378.76,142.52,8.48">Lecture Notes on Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3532</biblScope>
			<biblScope unit="page" from="546" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,400.48,317.50,8.48;12,160.26,411.22,81.78,8.48;12,242.10,409.30,4.79,5.49;12,249.18,411.22,158.43,8.48" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,274.85,400.48,175.87,8.48">IBM&apos;s Statistical Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.26,411.22,81.78,8.48;12,242.10,409.30,4.79,5.49;12,249.18,411.22,130.15,8.48">Proceedings of the 10 th Text Retrieval Conference (TREC)</title>
		<meeting>the 10 th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,422.08,317.61,8.48;12,160.26,432.94,231.28,8.48" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,292.55,422.08,176.29,8.48;12,160.26,432.94,92.53,8.48">Analysis of Statistical Question Classification for Fact-based Questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bruce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,443.68,317.55,8.48;12,160.26,454.54,64.09,8.48" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,304.01,443.68,164.77,8.48;12,160.26,454.54,36.86,8.48">Question Answering Based on Semantic Structures</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,465.40,317.53,8.48;12,160.26,476.14,118.13,8.48;12,278.40,474.22,4.79,5.49;12,285.48,476.14,158.31,8.48" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,247.01,465.40,221.75,8.48;12,160.26,476.14,16.99,8.48">Employing Two Question Answering Systems in TREC-2005</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,196.62,476.14,81.77,8.48;12,278.40,474.22,4.79,5.49;12,285.48,476.14,130.13,8.48">Proceedings of the 14 th Text Retrieval Conference (TREC)</title>
		<meeting>the 14 th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,487.00,317.52,8.48;12,160.26,497.80,81.78,8.48;12,242.10,495.88,4.79,5.49;12,249.18,497.80,158.45,8.48" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,195.37,487.00,253.29,8.48">The &quot;La Sapienza&quot; Question Answering System at TREC-2006</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.26,497.80,81.78,8.48;12,242.10,495.88,4.79,5.49;12,249.18,497.80,130.17,8.48">Proceedings of the 15 th Text Retrieval Conference (TREC)</title>
		<meeting>the 15 th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,508.60,317.52,8.48;12,160.26,519.46,95.48,8.48" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,268.27,508.60,126.91,8.48">The LogAnswer Project at CLEF</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,435.39,508.60,33.36,8.48;12,160.26,519.46,71.94,8.48">Working Notes ResPubliQA</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,530.26,317.49,8.48;12,160.26,541.06,308.51,8.48;12,160.26,551.92,308.54,8.48;12,160.26,562.72,21.22,8.48" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,287.93,530.26,180.79,8.48;12,160.26,541.06,67.77,8.48">Automated Question Answering: Review of the Main Approaches</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrenucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sneiders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,252.57,541.06,216.20,8.48;12,160.26,551.92,204.13,8.48">Proceedings of the Third International Conference on Information Technology and Applications (ICITA)</title>
		<meeting>the Third International Conference on Information Technology and Applications (ICITA)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,573.52,317.53,8.48;12,160.26,584.38,308.60,8.48;12,160.26,595.12,270.74,8.48" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,313.21,573.52,155.56,8.48;12,160.26,584.38,193.66,8.48">Improving Text Retrieval Precision and Answer Accuracy in Question Answering Systems</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.06,584.38,93.80,8.48;12,160.26,595.12,242.92,8.48">Proceedings of the ACL 2nd Workshop on Information Retrieval for Question Answering</title>
		<meeting>the ACL 2nd Workshop on Information Retrieval for Question Answering</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.23,605.98,317.57,8.48;12,160.26,616.84,304.93,8.48" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,258.59,605.98,210.21,8.48;12,160.26,616.84,141.58,8.48">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.92,616.84,129.17,8.48">Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
