<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,212.13,116.95,191.15,12.62">NLEL at RespubliQA 2010</title>
				<funder ref="#_vrxgh7T">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.21,154.63,68.25,8.74"><forename type="first">Santiago</forename><surname>Correa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution">ELiRF Universidad Politécnica de Valencia</orgName>
								<address>
									<addrLine>Camino de Vera s/n</addrLine>
									<settlement>Valencia</settlement>
									<country key="ES">España</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.38,154.63,68.99,8.74"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution">ELiRF Universidad Politécnica de Valencia</orgName>
								<address>
									<addrLine>Camino de Vera s/n</addrLine>
									<settlement>Valencia</settlement>
									<country key="ES">España</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,367.49,154.63,52.68,8.74"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic.upv.esdbuscaldi</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution">ELiRF Universidad Politécnica de Valencia</orgName>
								<address>
									<addrLine>Camino de Vera s/n</addrLine>
									<settlement>Valencia</settlement>
									<country key="ES">España</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,212.13,116.95,191.15,12.62">NLEL at RespubliQA 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7863073E9C0ECE4F1323E6227DA72B35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Question Answering, n-gram based Passage Retrieval</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the participation of the NLEL Lab. from the Universidad Politécnica of Valencia to the RespubliQA task at CLEF 2010. The system designed for this participation is based on the one used in our previous participation, with some modifications required in order to adapt it to the new guidelines. The system participated to both the "Paragraph Selection" (PS) and "Answer Selection" (AS) subtasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The participation to the PS sub-task was centered around the JIRS n-gram based passage retrieval system <ref type="bibr" coords="1,270.07,416.07,9.97,8.74" target="#b5">[6]</ref>. In order to participate in the AS sub-task, it was necessary to integrate into the system an Answer Extraction module, which was developed originally for the QUASAR QA system <ref type="bibr" coords="1,372.16,439.98,9.97,8.74" target="#b4">[5]</ref>, which participated in past CLEF-QA editions, from 2005 to 2007. In the following sections we describe the characteristics of the QA system in both PS and AS configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">JIRS Passage Retrieval System</head><p>JIRS<ref type="foot" coords="1,156.35,517.55,3.97,6.12" target="#foot_0">1</ref> is an n-gram based passage retrieval system that has been developed specifically for the Question Answering task. An n-gram is a sequence of n adjacent terms extracted from a sentence or a question. JIRS is based on the premise that in a sufficiently large document collection, question n-grams should appear near the answer at least once. JIRS represents the core of the system, since it was used both in the PS and AS sub-tasks.</p><p>The architecture of JIRS is shown in Figure <ref type="figure" coords="1,348.01,590.86,3.88,8.74" target="#fig_0">1</ref>. The user question is passed to a search engine that returns relevant snippets of a documents collection in which relevant terms from the question occur. The n-gram extraction module will return all the n-grams of size 1 to n, where n is the number of terms of the question. This process is done both for the question and for each of the snippets retrieved by the search engine. Once obtained the n-grams of the question and the snippets, a comparison is made to calculate a similarity value between them. This similarity value is used to sort the list of passages that will eventually be returned to the user. The similarity between the question and the retrieved passages is defined in Equation <ref type="formula" coords="2,273.79,360.27,3.88,8.74">1</ref>.</p><formula xml:id="formula_0" coords="2,228.78,376.25,251.81,46.42">Sim(p, q) = ∑ ∀x∈Q h(x, P ) 1 d(x, x max ) ∑ n i=1 w i (1)</formula><p>Where, Sim(p, q) is the function that measures the similarity of n-grams sets of the question q with respect to the n-grams sets of the passage p. P is the n-gram set of the heaviest passage p (i.e., the one with most weight) whose terms are in the question; Q is the set of j-grams that are generated from the question q and n is the total number of terms in the question. There are three special and particular terms functions:</p><p>w i is the weight of the i -th term of the question which is determined by:</p><formula xml:id="formula_1" coords="2,270.95,527.99,209.64,22.31">w i = 1 - log(n i ) 1 + log(N )<label>(2)</label></formula><p>Where n i is the number of sentences in which the term t i occurs and N is the number of sentences in the collection; the function h(x, P ) measures the weight of each n-gram and is defined as:</p><formula xml:id="formula_2" coords="2,240.18,594.44,240.41,29.96">h(x, P j ) = { ∑ j k=1 w k if x ∈ P j 0 otherwise<label>(3)</label></formula><p>Where w k is the weight of the k-th term (see Equation <ref type="formula" coords="2,385.01,633.21,4.43,8.74" target="#formula_1">2</ref>) and j is the number of terms that compose the analyzed n-gram;</p><p>and the factor 1 d(x,xmax) that is a distance factor which reduces the weight of the n-grams that are far from the heaviest n-gram. The function d(x, x max ) determines numerically the value of the separation according to the number of words between a n-gram and the heaviest one. That function is defined as shown in Equation <ref type="formula" coords="3,248.83,169.51,4.98,8.74" target="#formula_3">4</ref>:</p><formula xml:id="formula_3" coords="3,252.14,193.32,224.21,9.65">d(x, x max ) = 1 + k• ln(1 + L) (<label>4</label></formula><formula xml:id="formula_4" coords="3,476.35,193.32,4.24,8.74">)</formula><p>Where k is a factor that determines the importance of the distance in the similarity calculation and L is the number of words between a n-gram and the heaviest one (see Equation <ref type="formula" coords="3,288.39,234.75,3.88,8.74" target="#formula_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answer Extraction System</head><p>In order to cope with the AS sub-task guidelines, which require that beyond retrieving a paragraph containing the answer to a question in natural language, systems are required to demarcate also the exact answer, we had to fit JIRS with an answer extraction system. This system is based on the QUASAR AE module described in <ref type="bibr" coords="3,226.11,337.55,9.97,8.74" target="#b1">[2]</ref>, which has been used to participate in previous CLEF-QA tasks. The system has been modified by the addition of two new categories of questions: PERCENTAGE and MODE, and a new question analysis module based on the extraction of constraints by means of idf weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Analysis Module</head><p>This module obtains both the expected answer type (or class) and some constraints from the question. The different answer types that can be treated by our system are shown in Table <ref type="table" coords="3,271.42,446.32,3.88,8.74" target="#tab_0">1</ref>. Each category is defined by one or more patterns written as regular expressions. For instance, the Italian patterns for the category "CITY" are: .*(che-quale) .*cittá .+ and (qual-quale) .*la capitale .+ . The questions that do not match any defined pattern are labeled with OTHER. If a question matches more than one pattern, it is assigned the label of the longest matching pattern (i.e., we consider longest patterns to be less generic than shorter ones).</p><p>The Question Analyzer has the purpose of identifying patterns that are used as constraints in the AE phase. In order to carry out this task, the set of different n-grams in which each input question can be segmented are extracted, after the removal of the initial quetsion stop-words. For instance consider the question: "Where is the Sea World aquatic park? ", then the following n-grams are generated: </p><formula xml:id="formula_5" coords="3,134.76,610.01,156.92,56.12">[Sea] [World] [aquatic] [park] [Sea World] [aquatic] [park] [Sea] [World aquatic] [park] [Sea] [World] [aquatic park] [Sea World] [aquatic park]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Sea] [World aquatic park] [Sea World aquatic] [park] [Sea World aquatic park]</head><p>The weight for each segmentation is calculated in the following way:</p><formula xml:id="formula_6" coords="4,249.78,454.76,226.57,30.20">∏ x∈Sq log 1 + N D -log f (x) log N D (<label>5</label></formula><formula xml:id="formula_7" coords="4,476.35,464.89,4.24,8.74">)</formula><p>where S q is the set of n-grams extracted from query q, f (x) is the frequency of n-gram x in the collection D, and N D is the total number of documents in the collection D.</p><p>The n-grams that compose the segmentation with the highest weight are the contextual constraints, which represent the information that has to be included in the retrieved passage in order to have a chance of success in extracting the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Extraction</head><p>The input of this module is constituted by the n passages returned by the PR module and the constraints (including the expected type of the answer) obtained through the Question Analysis module described in Section 3.1. The positions of the passages in which the constraints occur are marked before passing them to the text analyzers (we named them TextCrawlers since they move on text like a spider on its web). One of these analyzers is instantiated for each of the n passages with a set of patterns for the expected type of the answer and a pre-processed version of the passage text.</p><p>Each TextCrawler begins its work by searching all the passage's substrings matching the expected answer pattern. Let us define C the set of constrains extracted in the Question Analysis phase; then a weight w(s) is assigned to each found substring s, inversely proportional to the text distance of s with respect to the constraints c i ∈ C. The final weight w(s) is calculated as the product of the distance weights obtained for every constraint in the passage:</p><formula xml:id="formula_8" coords="5,134.76,231.40,105.36,19.30">w(s) = ∏ ci∈C 1/d(s, c i ).</formula><p>A Filter module is based on a set of patterns compiled by hand in order to discard the candidate answers which do not match an allowed pattern or that do match with a forbidden pattern. When the Filter module rejects a candidate, the TextCrawler provide it with the next best-weighted candidate, if there is one. Finally, when all TextCrawlers end their analysis of the text, the Answer Selection module selects the answer to be returned by the system. The following strategies apply: SV is used for NAME type questions, with DV as a backoff strategy in case of two candidates obtaining the same weight. WV is used for every other type of questions, with TOP as a backoff strategy.</p><formula xml:id="formula_9" coords="5,140.99,341.34,5.73,8.77">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head><p>For the RespubliQA 2010 competition, the NLE Lab.has decided to participate in five monolingual tasks for passages extraction, the distribution of these tasks with the respective approaches used is:</p><p>-English task : Monolingual and monolingual -Stem participation; introducing these two units is expected to determine whether use of the Stem technique improves the performance of JIRS or not. -Spanish task : Monolingual and monolingual -BM25 participation; introducing these two units is expected to determine whether the use of the BM25 technique improves the performance of JIRS or not. -French, Italian and German Tasks: We present monolingual and multilingual approaches.</p><p>The following sections explain each one of the approaches implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual approach</head><p>The data had to be preprocessed, due to the format of the collection employed in ResPubliQA competition, a subset of the JRC-ACQUIS and Europarl Multilingual Parallel corpus. The documents cover various subject domains: law, politics, economy, health, information technology, agriculture, food and more.</p><p>To be able to use the JIRS system in this task, the documents were analyzed and transformed for proper indexing. Since JIRS uses passages as basic indexing unit, it was necessary to extract passages from the documents. We consider any paragraph included between &lt;p&gt; tags as a passage. Therefore, each paragraph was labeled with the name of the containing document and its paragraph number.</p><p>Once the collection was indexed by JIRS, the system was ready to proceed with the search for the answers to the test questions. For each question, the system returned a list with the passages that most likely contained the answer to the question, according to the JIRS weighting scheme. The architecture of the monolingual JIRS -based system is illustrated in Fig. <ref type="figure" coords="6,385.28,303.54,3.88,8.74" target="#fig_2">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multilingual approach</head><p>According to the excellent results obtained in the competition RespubliQA 2009 we decided to implement the multilingual approach also in RespubliQA 2010. This approach used the parallel collection to obtain a list of answers in different languages (Spanish, English, Italian, French and German). The idea of this approach is based on the implementation of 5 monolingual JIRS -based systems, one for each language, which process the set of questions in the respective language. For this purpose, we used a parallel sets of questions provided by the competition organisers. The final answer is selected as the one obtaining the best score; if the answer is not in the target language, the identifier of each paragraph (answer) is used to retrieve the aligned paragraph in the target language. The architecture of the multilingual JIRS -based system is illustrated in Fig. <ref type="figure" coords="6,471.40,602.86,3.88,8.74" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Monolingual -Stem approach</head><p>The Monolingual -Stem approach was inspired by the competition of the year 2009 <ref type="bibr" coords="6,158.04,657.12,9.97,8.74" target="#b6">[7]</ref>, where the best baseline was established using, among others, a corpus  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Monolingual -BM25 approach</head><p>The Monolingual -BM25 approach, was inspired by the competition of 2009 <ref type="bibr" coords="7,467.36,478.78,9.97,8.74" target="#b6">[7]</ref>, where the best baseline was established through the implementation of, among others, the BM25 technique to find the passages which are expected to be the answer to each question; the scheme that approach can be seen in Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In Table <ref type="table" coords="8,174.61,142.62,4.98,8.74" target="#tab_1">2</ref> shows the results of the paragraph selection task. In Table <ref type="table" coords="8,189.56,453.88,4.98,8.74" target="#tab_2">3</ref> shows the results of the answer selection task.</p><p>As shown in Table <ref type="table" coords="8,235.64,465.84,3.88,8.74" target="#tab_1">2</ref>, the monolingual approach applied to each of the five languages returns acceptable results, especially in English and Italian. The implementation of the systems: multilingual, monolingual -stem and monolingual -BM25, decreased the overall performance of the system with respect to the monolingual approach that used the JIRS n-grams density weighting scheme. This result confirms that the n-grams density weighting scheme of JIRS fits particularly well the QA task, with respect to term-based weighting scheme, as observed in <ref type="bibr" coords="8,187.43,549.52,9.97,8.74" target="#b0">[1]</ref>.</p><p>It is important to note that the multilingual approach was not able to repeat the results obtained in the RespubliQA-2009 competition. An analysis of the results in RespubliQA-2010 showed that the provided corpus is not perfectly aligned, as it can be observed from Tables <ref type="table" coords="8,321.63,597.34,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="8,349.26,597.34,3.88,8.74" target="#tab_4">5</ref>: a passage with the same ID in the same document can be different for each of the studied languages. This problem is present in both the JRC-AQUIS and Europarl corpora.</p><p>Due to the scheme adopted for the multilingual approach it is necessary to work on a corpus with 100% accuracy in alignment; otherwise, the system is not able to obtain good results, as it can be seen in Table <ref type="table" coords="8,366.99,657.12,3.88,8.74" target="#tab_1">2</ref>. Due to the fact that the  answers for the Answer Selection task were extracted from the same passages retrieved in the basic multilingual approach, the results obtained for this task were also poor as shown in Table <ref type="table" coords="10,282.49,143.90,3.88,8.74" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>According to the experiments, the use of techniques such as BM25 and Stem, decrement the performance of JIRS tool for purposes of question answering tasks.</p><p>It is verified through analysis, that problems with the alignment of the corpus provided poor performance resulting in the multilingual approach used. Additionally, Due to the poor result obtained with the multi-lingual approach, the extraction experiment response has similarly low results. In future work, we plan to implement a filter able to determine the paragraphs alignment of the corpus to improve the performance of multilingual approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,197.56,279.20,220.20,7.89;2,199.38,116.83,216.45,147.60"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of JIRS Passage Retrieval system</figDesc><graphic coords="2,199.38,116.83,216.45,147.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,151.70,341.37,328.95,8.74;5,151.70,353.33,195.43,8.74;5,140.99,364.62,339.66,8.77;5,151.70,376.61,328.95,8.74;5,151.70,388.56,69.19,8.74;5,140.99,399.86,339.66,8.77;5,151.70,411.84,139.97,8.74;5,140.99,423.14,339.62,8.77"><head></head><label></label><figDesc>Simple voting (SV): The returned answer corresponds to the candidate that occurs most frequently as passage candidate. -Weighted voting (WV): Each vote is multiplied for the weight assigned to the candidate by the TextCrawler and for the passage weight as returned by the PR module. -Double voting (DV): As simple voting, but taking into account the second best candidates of each passage. -Top (TOP): The candidate elected by the best weighted passage is returned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,206.56,421.05,202.19,7.89;6,235.08,332.93,145.35,73.35"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of NLEL monolingual system</figDesc><graphic coords="6,235.08,332.93,145.35,73.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,206.95,237.60,201.41,7.89;7,147.43,116.70,320.63,106.13"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of NLEL multilingual system</figDesc><graphic coords="7,147.43,116.70,320.63,106.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,192.45,408.85,230.41,7.89;7,204.78,313.98,205.65,80.10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of NLEL monolingual -stem system</figDesc><graphic coords="7,204.78,313.98,205.65,80.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,445.97,514.65,4.98,8.74"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,189.86,633.69,235.60,7.89;7,207.48,545.57,200.25,73.35"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Architecture of NLEL monolingual -BM25 system</figDesc><graphic coords="7,207.48,545.57,200.25,73.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,138.91,569.38,15.22,7.86;9,254.30,569.38,52.43,7.86;9,138.91,580.74,294.62,7.86;9,138.91,592.09,331.11,7.86;9,138.91,603.45,318.81,7.86;9,138.91,614.81,332.87,7.86;9,138.91,626.16,99.30,7.86;9,263.00,626.16,200.76,7.86"><head></head><label></label><figDesc>20081218-FR cl.xml 127 4. souhaite vivement entamer des. . . EP TA-20081218-EN cl.xml 127 4. Expresses its strong willingness to enter. . . EP TA-20081218-ES cl.xml 127 3. Toma nota de la Comunicación de la. . . EP TA-20081218-DE cl.xml 127 5. fordert, dass die gegenwrtige Krise nicht. . . EP TA-20081218-IT.xml 127 4. esprime la sua forte volontá di avviare. . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,200.75,116.91,213.83,238.47"><head>Table 1 .</head><label>1</label><figDesc>QC pattern classification categories.</figDesc><table coords="4,200.75,137.71,213.83,217.67"><row><cell>L0</cell><cell>L1</cell><cell>L2</cell></row><row><cell>NAME</cell><cell>ACRONYM</cell><cell></cell></row><row><cell></cell><cell>PERSON</cell><cell></cell></row><row><cell></cell><cell>TITLE</cell><cell></cell></row><row><cell></cell><cell>FIRSTNAME</cell><cell></cell></row><row><cell></cell><cell>LOCATION</cell><cell>COUNTRY</cell></row><row><cell></cell><cell></cell><cell>CITY</cell></row><row><cell></cell><cell></cell><cell>GEOGRAPHICAL</cell></row><row><cell cols="2">DEFINITION PERSON</cell><cell></cell></row><row><cell></cell><cell>ORGANIZATION</cell><cell></cell></row><row><cell></cell><cell>OBJECT</cell><cell></cell></row><row><cell>DATE</cell><cell>DAY</cell><cell></cell></row><row><cell></cell><cell>MONTH</cell><cell></cell></row><row><cell></cell><cell>YEAR</cell><cell></cell></row><row><cell></cell><cell>WEEKDAY</cell><cell></cell></row><row><cell cols="2">QUANTITY MONEY</cell><cell></cell></row><row><cell></cell><cell>DIMENSION</cell><cell></cell></row><row><cell></cell><cell>AGE</cell><cell></cell></row><row><cell></cell><cell>PERCENTAGE</cell><cell></cell></row><row><cell>MODE</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.76,173.46,347.58,252.23"><head>Table 2 .</head><label>2</label><figDesc>PS Task Results, Mono: participation uses monolingual approach, Multi: participation uses multilingual approach, Stem: participation uses stem pre-processing, BM25: participation uses BM25 post-processing, ANSWERED: number of questions answered, UNANSWERED: number of questions unanswered, ANSWERED R.C.: number of questions answered with right candidate answer, ANSWERED W.C.: number of questions answered with wrong candidate answer, UNANSWERED R.C.: number of questions unanswered with right candidate answer, UNANSWERED W.C.: number of questions unanswered with wrong candidate answer, UNANSWERED E.: number of questions unanswered with empty candidate, P.A.C.D.: Portion of answers correctly discarded</figDesc><table coords="8,136.16,292.90,346.18,132.79"><row><cell></cell><cell>EN</cell><cell></cell><cell>ES</cell><cell></cell><cell>FR</cell><cell></cell><cell>IT</cell><cell></cell><cell></cell><cell>DE</cell></row><row><cell></cell><cell cols="10">Mono Stem Mono BM25 Mono Multi Mono Multi Mono Multi</cell></row><row><cell>ANSWERED</cell><cell cols="10">196 198 194 200 191 197 196 199 183 200</cell></row><row><cell>UNANSWERED</cell><cell>4</cell><cell>2</cell><cell>6</cell><cell>0</cell><cell>9</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>17</cell><cell>0</cell></row><row><cell>ANSWERED R.C.</cell><cell cols="3">128 122 108</cell><cell>39</cell><cell cols="4">105 109 124 105</cell><cell>90</cell><cell>88</cell></row><row><cell>ANSWERED W.C.</cell><cell>68</cell><cell>76</cell><cell>86</cell><cell>161</cell><cell>86</cell><cell>88</cell><cell>72</cell><cell>94</cell><cell>93</cell><cell>112</cell></row><row><cell>UNANSWERED R.C.</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>0</cell></row><row><cell cols="2">UNANSWERED W.C. 2</cell><cell>2</cell><cell>5</cell><cell>0</cell><cell>7</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>15</cell><cell>0</cell></row><row><cell>UNANSWERED E.</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>ACCURACY</cell><cell cols="10">0.65 0.61 0.55 0.20 0.54 0.55 0.63 0.53 0.46 0.44</cell></row><row><cell>P.A.C.D.</cell><cell cols="10">0.50 1.00 0.83 0.00 0.78 1.00 0.50 1.00 0.88 0.00</cell></row><row><cell>C@1 MEASURE</cell><cell cols="10">0.65 0.62 0.56 0.20 0.55 0.55 0.63 0.53 0.49 0.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.76,144.86,345.77,185.67"><head>Table 3 .</head><label>3</label><figDesc>AS Task Results, ANSWERED: number of questions answered, UNAN-SWERED: number of questions unanswered, ANSWERED R.C.: number of questions answered with right candidate answer, ANSWERED W.C.: number of questions answered with wrong candidate answer, ANSWERED M.: number of questions answered with missed candidate answer, ANSWERED I.: number of questions answered with inexact candidate answer, A.E.P.: Answer extraction performance</figDesc><table coords="9,228.25,220.45,155.78,110.08"><row><cell></cell><cell cols="3">EN ES FR IT</cell></row><row><cell>ANSWERED</cell><cell cols="3">107 150 136 145</cell></row><row><cell>UNANSWERED</cell><cell cols="3">67 28 40 30</cell></row><row><cell cols="3">ANSWERED R.C. 10 12 4</cell><cell>6</cell></row><row><cell cols="4">ANSWERED W.C. 97 138 132 139</cell></row><row><cell>ANSWERED M.</cell><cell cols="3">20 21 13 18</cell></row><row><cell>ANSWERED I.</cell><cell>6</cell><cell cols="2">1 11 7</cell></row><row><cell>ACCURACY</cell><cell cols="3">0.05 0.06 0.02 0.03</cell></row><row><cell>C@1 MEASURE</cell><cell cols="3">0.07 0.07 0.02 0.03</cell></row><row><cell>A.E.P.</cell><cell cols="3">0.28 0.35 0.14 0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,138.25,396.42,334.18,86.27"><head>Table 4 .</head><label>4</label><figDesc>Non parallel JRC-Aquis corpus example</figDesc><table coords="9,138.25,417.22,334.18,65.47"><row><cell>File</cell><cell>Passage Text</cell></row><row><cell cols="2">jrc31972L0199-de.xml 139 4. BESTIMMUNG DER PEPSINAKTIVITT</cell></row><row><cell>jrc31972L0199-it.xml</cell><cell>139 7.3 SE IL PALLONE DELL'APPARECCHIO DI. . .</cell></row><row><cell cols="2">jrc31972L0199-en.xml 139 7 . OBSERVATIONS</cell></row><row><cell cols="2">jrc31972L0199-es.xml 139 3.2 . Ácido clorhídrico 0,075 N .</cell></row><row><cell>jrc31972L0199-fr.xml</cell><cell>139 Dfinition : L'unité de pepsine est définie comme. . .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,210.89,548.58,193.53,7.89"><head>Table 5 .</head><label>5</label><figDesc>Non parallel Europarl corpus example</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.80,147.68,7.86"><p>http://sourceforge.net/projects/jirs/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the <rs type="programName">TEXT-ENTERPRISE 2</rs>.0, <rs type="projectName">MICINN (Plan I+D+i) research</rs> project (<rs type="grantNumber">TIN2009-13391-C04-03</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vrxgh7T">
					<idno type="grant-number">TIN2009-13391-C04-03</idno>
					<orgName type="project" subtype="full">MICINN (Plan I+D+i) research</orgName>
					<orgName type="program" subtype="full">TEXT-ENTERPRISE 2</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.34,406.60,342.15,7.86;10,146.91,417.56,333.59,7.86;10,146.91,428.52,333.60,7.86;10,146.91,439.48,130.95,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,397.46,406.60,83.03,7.86;10,146.91,417.56,155.25,7.86">Keyword-based Passage Retrieval for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N-Gram</forename><surname>Vs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,325.12,417.56,155.38,7.86;10,146.91,428.52,261.17,7.86">Evaluation of Multilingual and Multimodal Information Retrieval, Revised Selected Papers CLEF-2006</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,450.44,342.13,7.86;10,146.91,461.40,333.59,7.86;10,146.91,472.36,100.88,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,332.54,450.44,147.93,7.86;10,146.91,461.40,129.66,7.86">Answering Questions with an n-gram based Passage Retrieval Engine</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,301.02,461.40,175.09,7.86">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="134" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,483.31,342.13,7.86;10,146.91,494.27,214.66,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,292.67,483.31,187.80,7.86;10,146.91,494.27,45.22,7.86">Passage Retrieval and Intellectual Property in Legal Texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.37,494.27,56.32,7.86">FLACOS-2009</title>
		<meeting><address><addrLine>Toledo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,505.23,342.15,7.86;10,146.91,516.19,250.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,288.88,505.23,145.17,7.86">NLEL-MAAT at CLEF-ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.64,505.23,24.85,7.86;10,146.91,516.19,158.57,7.86">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,527.15,342.12,7.86;10,146.91,538.11,333.61,7.86;10,146.91,549.07,244.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,391.11,527.15,89.35,7.86;10,146.91,538.11,265.91,7.86">QUASAR: The Question Answering System of the Universidad Politecnica de Valencia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.21,538.11,46.31,7.86;10,146.91,549.07,46.40,7.86">CLEF 2005 Proceedings</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date>4022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,560.03,342.12,7.86;10,146.91,570.99,333.61,7.86;10,146.91,581.94,287.40,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,351.14,560.03,129.33,7.86;10,146.91,570.99,134.28,7.86">A Passage Retrieval System for Multilingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.50,570.99,175.02,7.86;10,146.91,581.94,78.92,7.86">Proc. 8th Int. Conf. on Text, Speech and Dialogue, TSD-2005</title>
		<meeting>8th Int. Conf. on Text, Speech and Dialogue, TSD-2005</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">3658. 2005</date>
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,592.90,342.14,7.86;10,146.91,603.86,333.61,7.86;10,146.91,614.82,110.47,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,393.06,592.90,87.42,7.86;10,146.91,603.86,141.93,7.86">Information Retrieval Baselines for the ResPubliQA Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,311.06,603.86,169.46,7.86;10,146.91,614.82,16.87,7.86">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
