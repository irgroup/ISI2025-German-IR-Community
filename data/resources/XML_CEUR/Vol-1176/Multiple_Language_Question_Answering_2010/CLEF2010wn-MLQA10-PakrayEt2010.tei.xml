<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.60,150.73,317.58,12.64;1,260.28,166.69,74.81,12.64">J U_CSE_TE: System Descr iption QA@CLEF 2010 -ResPubliQA</title>
				<funder ref="#_uwcjbCg">
					<orgName type="full">IFCPAR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.92,204.76,55.86,8.96"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research 2</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.64,204.76,60.93,8.96"><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research 2</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.64,204.76,47.41,8.96"><forename type="first">Santanu</forename><surname>Pal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research 2</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.08,204.76,55.12,8.96"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research 2</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.12,216.28,90.79,8.96"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research 2</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,312.00,216.28,78.54,8.96"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<email>gelbukh@gelbukh.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering 1</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.60,150.73,317.58,12.64;1,260.28,166.69,74.81,12.64">J U_CSE_TE: System Descr iption QA@CLEF 2010 -ResPubliQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0794CA1AEFCE1216D3D590CDACD0277B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lucene Index</term>
					<term>Chunk Boundary</term>
					<term>n-gram overlapping</term>
					<term>SRL Tool 1 Intr oduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The article presents the experiments carried out as part of the participation in the Paragraph Selection (PS) Task and Answer Selection (AS) Task of QA@CLEF 2010 -ResPubliQA. Our System use Apache Lucene for document retrieval system. All test documents are indexed using Apache Lucene. Stop words are removed from each question and query words are identified to retrieve the most relevant documents using Lucene. Relevant paragraphs are selected from the retrieved documents based on the TF-IDF of the matching query words along with n-gram overlap of the paragraph with the original question. Chunk boundaries are detected in the original question and key chunks are identified. Chunk boundaries are also detected in each sentence in a paragraph. The key chunks are matched in each sentence in a paragraph and relevant sentences are identified based on the key chunk matching score. Each question is analyzed to identify its possible answer type. The SRL Tool (Assert Tool Kit) [1] is applied on each sentence in a paragraph to assign semantic roles to each chunk. The Answer Extraction module identifies the appropriate chunk in a sentence as the exact answer whose semantic role matches with the possible answer type for the question. The tasks have been carried out for English. The Paragraph Selection task has been evaluated on the test data with an overall accuracy score of 0.37 and c@1 measure of 0.50. The Answer Extraction task has performed poorly with an overall accuracy score of 0.16 and c@1 measure of 0.26.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywor ds</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.2" lry="841.8"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of ResPubliQA 2010 <ref type="foot" coords="2,213.00,147.37,3.24,5.83" target="#foot_0">1</ref> is to find the appropriate single paragraph that contains the answer along with the exact answer of a given question from a collection of parallel documents in the European languages.</p><p>The aim of ResPubliQA 2010 <ref type="bibr" coords="2,260.04,184.13,11.71,8.96" target="#b2">[3]</ref> is to capitalize on what has been achieved in the previous evaluation campaign while at the same time adding a number of refinements:</p><p>• The addition of new question types and the refinement of old ones;</p><p>• The opportunity to return both paragraph and exact answer;</p><p>• The addition of a new document collection: EUROPARL. Two separate tasks are part of the ResPubliQA 2010 <ref type="bibr" coords="2,348.73,241.63,11.71,8.96" target="#b2">[3]</ref> evaluation campaign: i. PARAGRAPH SELECTION (PS) TASK: to retrieve one paragraph (Text+ID) containing the answer to a question in natural language. This task is very similar to the one performed in 2009. ii. ANSWER SELECTION (AS) TASK: beyond retrieving a paragraph, systems are required to retrieve also the exact answer (shorter string of text) answering a question in natural language. The parallel-aligned documents are available in 9 languages, i.e. Bulgarian, Dutch, English, French, German, Italian, Portuguese, Romanian and Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus Statistics</head><p>The ResPubliQA <ref type="bibr" coords="2,198.48,398.92,11.71,8.96" target="#b2">[3]</ref> collection is made up of a subset of two multilingual parallel aligned document collections. i. The J RC-ACQUIS Multilingual Par allel Cor pus<ref type="foot" coords="2,350.76,419.65,3.24,5.83" target="#foot_1">2</ref> : The JRC-ACQUIS corpus contains the complete EU legislation, including texts between the years 1950 to 2006. A sub-set of the JRC-ACQUIS has been created with roughly 10,700 parallel and aligned documents in each of the 9 languages involved in the track. ii. The Eur opar l collection<ref type="foot" coords="2,240.12,465.73,3.24,5.83" target="#foot_2">3</ref> : A (very small) subset of the Europarl corpus has been created with parallel documents in all the 9 languages involved in the track by crawling the web to get the data from the European Parliament's website. The sub-set includes 150 parallel and aligned documents per language.</p><p>The subject of the JRC-ACQUIS documents is European legislation while the EUROPARL collection deals with the parliamentary domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Fr amewor k</head><p>In this section, we describe our Information Retrieval (IR) based Question Answering (QA) system. The system is defined in three parts: documents selection from indexed documents in the collections, paragraph selection from documents and finally answer selection from the paragraph. The Apache Lucene <ref type="foot" coords="3,218.28,450.01,3.24,5.83" target="#foot_3">4</ref> IR system has been used for the present task. Lucene follows the standard IR model with Document parsing, Document Indexing, TF-IDF calculation, query parsing and finally searching/document retrieval. Some modules in Lucene have been upgraded for our present need as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Par sing</head><p>The web documents are full of noises mixed with the original content. In that case it is very difficult to identify and separate the noises from the actual content. ResPubliQA 2010 Corpus had many noise in the documents and the documents are in tagged format. So, first of all the documents had to be preprocessed. The document structure is checked and reformatted according to the system requirements. 3.1.2 Remove Noise and Symbols. The corpus has some noise as well as some special symbols that are not necessary for our system. The list of noise symbols and the special symbols is initially developed manually by looking at a number of documents and then the list is used to automatically remove such symbols from the documents. Table <ref type="table" coords="4,198.48,263.09,4.98,8.96" target="#tab_0">1</ref> lists some of the noisy tokens and their replacements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Pr ocessing for quer y wor d identification</head><p>After indexing has been done, the queries have to be processed to retrieve relevant documents. Each question is processed to identify the query words for submission to Lucene. The question processing steps are described below:</p><p>3.3.1 Key-char acter Removal. Certain key characters in the query cause implicit query handling during searching like dot character between two query words denotes AND of the two query words. Such key characters are thus removed from the question before submission to Lucene. For example, http://wt.jrc.it/ = "http wt jrc it", doug@nutch.org = "doug nutch org", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Stop Wor d Removal.</head><p>In this step the query words are identified from the question. The Stop words and question words (what, when, where, which etc.) are removed from each question and the words remaining in the question after the removal of such words are identified as the query words. The stop word list used in the present work can be found at http://members.unine.ch/jacques.savoy/clef/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>Stemming. Query words may appear in inflected forms in the question. For English, standard Porter Stemming algorithm <ref type="foot" coords="5,307.20,214.93,3.24,5.83" target="#foot_4">5</ref> has been used to stem the query words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Retr ieval</head><p>After searching each query into the Lucene index, a set of retrieved documents in ranked order for each query is received.</p><p>First of all, all queries were fired with AND operator. If at least one document is retrieved using the query with AND operator then the query is removed from the query list and need not be searched again. The rest of the queries are fired again with OR operator. OR searching retrieves at least one document for each query. Now, the top ranked relevant ten documents for each query are considered for Paragraph selection. In case of AND search only the top ranked document is considered. Document retrieval is the most crucial part of this system. We take only the top ranked relevant documents assuming that these are the most relevant documents for the query or the question from which the query had been generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relevant Par agr aph Selection</head><p>The selection of relevant paragraphs is one of the important activities of this system. We have used both "AND" and "OR" searching similar to document retrieval, to select relevant paragraphs from each retrieved relevant document. First those paragraph(s) are identified that contain all the query words. If at least one paragraph containing all the query words is found then the paragraph selection process for that document is stopped. Otherwise, we continue searching the paragraphs which contain at least one query word. Such relevant paragraphs are ranked using the n-gram overlap score between the paragraph and the original question. By the above process all the relevant paragraphs for each query are identified.</p><p>3.5.1 n-gr am Over lap. In this step the n-grams are identified from the question. These n-grams from the question are matched in the documents. If no match is found for a higher order n-gram then the search is repeated for the immediate lower order ngram. For each n-gram overlap, the score is calculated as the value of n plus n/100. The additional score of n/100 assures that higher order n-gram overlap will have a higher bonus in the score. The composite n-gram overlap score for a paragraph is the sum of the individual n-gram overlap scores. The paragraph that has the highest ngram overlap score is selected as the answer paragraph. If more than one paragraph has the same highest score then the paragraph that occurs earlier in the document is selected as the answer paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Question Analysis</head><p>The question sentences are pre-processed using Stanford Dependency parser <ref type="bibr" coords="6,437.75,228.04,10.69,8.96" target="#b3">[4]</ref>. The words along with their part of speech (POS) information are passed through a Conditional Random Field (CRF) based chunker <ref type="bibr" coords="6,324.23,251.08,11.72,8.96" target="#b4">[5]</ref> to extract phrase level chunks of the questions. A rule-based module is developed to identify the chunk boundaries. Key chunks are identified for each question. The chunks that are related by each prep relation constitute the key chunks corresponding to that prep relation. These key chunks are searched in the answer paragraph. We analyze each question to identify its possible answer type based on the question keyword as listed in Table <ref type="table" coords="6,407.52,308.57,3.76,8.96" target="#tab_1">2</ref> The sentences in the answer paragraph are detected. Each sentence is processed using Stanford Dependency parser and chunker as well. Our chunk boundary detector module detects every chunk as well as its boundary in each and every sentence. Each sentence is assigned a score based on the matching of question key chunks in the sentence. The top ranked sentence in each answer paragraph is identified as the answer sentence. Each such answer sentence in the answer paragraph is passed to the SRL Tool Kit <ref type="bibr" coords="6,124.79,555.66,11.71,8.96" target="#b0">[1]</ref> for appropriate labeling of the semantic roles to each chunk in the sentence. The semantic roles ARGM-TMP and ARGM-LOC associated to the chunks help to identify the DATE and LOCATION named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Answer Selection fr om par agr aph</head><p>If the question type is "who", the answer sentence is passed to the RASP parser <ref type="bibr" coords="6,458.88,634.60,11.71,8.96" target="#b5">[6]</ref> mainly to identify the occurrence of PERSON type named entities in the sentence. The PERSON type named entity is identified as the answer phrase. Answers to "when" type of questions are selected by looking for a chunk in the answer sentence that has been labeled with the semantic role ARGM-TMP. In case of "where type" questions, the chunk in the answer sentence that has been labeled with semantic role ARGM-LOC is identified as the answer phrase. Answers to "what" type questions are identified by looking for cue phrases such as "defined as", "means that" etc. and then selecting the part of the sentence after the cue phrase till the end of the sentence. In case of 'why" type questions, the answers are identified by looking for cue phrases like "reason of", "because of" etc. and then selecting the part of the sentence after the cue phrase till the end of the sentence.</p><p>In case of "How much" or "How many" question types, clause detection in the answer sentence becomes necessary as most often these sentences are complex in structure. The punctuation marks, discourse markers identified through mark type dependency relations, causal words (as, because) are used for clause detection. The dependency relations connected directly with each verb chunk are used in clause detection. Each verb chunk and the associated chunks whose head is directly linked with the verb chunk in any dependency relation identify a clause. If any chunk contains any word with POS category CD, the chunk is considered as the answer phrase for the specific question. Otherwise, the candidate phrases that contain capitalized words or Named Entities are considered as the answer to the question. Two examples of Answer Extraction are given below. Only the answer sentence has been shown in these examples and not the answer paragraph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Example of answer extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We submitted English monolingual run for one Paragraph selection Task and one Answer selection Task. The main measure used in this evaluation campaign is c@1 which is defined in equation 1.</p><p>(1) where, n R : the number of correctly answered questions, n U : number of unanswered questions and n: the total number of questions</p><p>In addition to computing the c@1 score, the answer extraction performance has also been measured as shown in equation 2.</p><p>Answer extr action per for mance= #R / (#R + #X + #M) <ref type="bibr" coords="8,459.00,435.28,11.72,8.96" target="#b1">(2)</ref> where, #R, #X and #M denote the number of answered questions identified as Right, inexact and Missed respectively.   The question answering system has been developed as part of the participation in the ResPubliQA 2010 track as part of the CLEF 2010 evaluation campaign. The system uses document retrieval using Lucene search engine, an n-gram based match for paragraph selection and combines various NLP tools for answer selection. The overall system has been evaluated using the evaluation metrics provided as part of the ResPubliQA 2010 track. The evaluation results are satisfactory considering that this is the first participation in the track. Future works will be motivated towards improving the performance of the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,248.52,429.16,109.70,8.96;3,141.24,147.84,323.88,266.88"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. IR based QA Model</figDesc><graphic coords="3,141.24,147.84,323.88,266.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,124.80,149.68,345.73,8.96;4,124.80,161.08,345.59,8.96;4,124.80,172.61,194.86,8.96"><head>3. 1 . 1 XML</head><label>11</label><figDesc>Par ser . The corpus was in XML format. All the XML test data has been parsed before indexing using our XML Parser. The XML Parser extracts the Title of the document along with the paragraphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,132.00,411.52,48.54,8.96;7,132.00,428.32,336.38,8.96;7,132.00,439.72,36.10,8.96;7,132.00,451.24,336.30,8.96;7,132.00,462.76,336.32,8.96;7,132.00,474.28,336.23,8.96;7,132.00,485.80,233.62,8.96;7,132.00,508.72,88.26,8.96;7,132.01,520.25,122.50,8.96;7,132.01,531.77,66.71,8.96;7,132.00,543.28,120.05,8.96;7,132.00,560.56,48.54,8.96;7,132.00,577.60,336.31,8.96;7,132.00,589.12,48.82,8.96;7,132.00,600.52,156.60,8.96;7,132.00,612.04,234.91,8.96;7,419.51,612.04,48.80,8.96;7,131.99,623.56,336.54,8.96;7,131.97,635.09,216.07,8.96;7,132.00,658.12,52.66,8.96;7,202.92,658.12,265.51,8.96;7,132.00,669.52,336.48,8.97;7,132.00,681.04,71.72,8.96;8,132.00,163.96,112.10,8.96;8,132.00,175.48,93.26,8.96;8,132.00,187.00,47.60,8.96;8,132.00,198.40,74.82,8.96;8,132.00,209.92,114.03,8.96;8,132.00,232.96,329.49,8.96"><head>Example: 1</head><label>1</label><figDesc>Question (Qid: 0025): When did Dow Chemical obtain the shares of Union Carbide? &lt;p_id="14"&gt; (3) However, since the Dow Chemical Company acquired on 6 Febr uar y 2001 all shares of Union Carbide Corporation, a company benefiting from an individual anti-dumping duty of EUR 59,25 per tonne, the Dow Chemical Company is still active in the ethanolamine business. &lt;/p&gt; Question Type: When Expected Answer type: DATE Parse: SRL Tool Answer: on 6 Febr uar y 2001 Example: 2 Question (qid: 0020): How many transactions can be covered in a DEPBS credit application? Chunked Sentence from Parsed output: (How/WRB/B-NP#many/JJ/I-NP#transactions/NNS/I-NP) (can/MD/B-VP#be/VB/I-VP#covered/VBN/I-VP) (in/IN/B-PP) (a/DT/B-NP#DEPBS/NNP/I-NP#credit/NN/I-NP#application/NN/I-NP) (?/./B-O#) &lt;p n="129"&gt; (55) An application for DEPBS cr edits can cover up to 25 export tr ansactions and, if electronically filed, an unlimited amount of export transactions. &lt;/p&gt; Capitalized Phrase: DEPBS Named Entity: DEPBS Verb: cover POS Tag (CD): 25 Cue Phrase: DEPBS cr edits Answer: An application for DEPBS credits can cover up to 25 export transactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,136.20,486.53,246.73,9.67;8,136.20,510.28,173.34,8.96;8,136.20,521.80,168.04,8.96;8,136.20,533.21,177.41,8.96;8,136.20,544.73,287.93,8.96;8,136.20,556.26,295.14,8.96;8,136.20,567.78,297.30,8.96;8,136.20,579.30,304.63,8.96;8,136.20,590.71,274.96,8.96;8,136.20,602.23,287.49,8.96"><head></head><label></label><figDesc>Accur acy Measur e of Par agr aph Selection Task (PS): Our PS file contains a total of 200 answers. -Number of questions ANSWERED: 125 -Number of questions UNANSWERED: 75 -Number of questions ANSWERED with RIGHT candidate answer: 73 -Number of questions ANSWERED with WRONG candidate answer: 52 -Number of questions UNANSWERED with RIGHT candidate answer: 0 -Number of questions UNANSWERED with WRONG candidate answer: 0 -Number of questions UNANSWERED with EMPTY candidate: 75 The statistics of Paragraph Selection Task (PS) task is given in figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,202.92,256.60,200.69,8.96;9,136.20,279.64,274.21,8.96;9,136.20,291.05,134.58,8.96;9,136.20,302.57,218.33,8.96;9,136.20,314.09,177.88,8.96;9,136.20,336.65,234.37,9.67;9,136.20,360.04,175.06,8.96;9,136.20,371.57,163.00,8.96;9,136.20,383.09,182.45,8.96;9,136.20,394.61,287.94,8.96;9,136.20,406.14,295.14,8.96;9,136.20,417.66,294.05,8.96;9,136.20,429.06,296.22,8.96;9,136.20,440.59,297.30,8.96;9,136.19,452.11,309.67,8.96;9,136.19,463.64,308.45,8.96;9,136.19,475.16,310.62,8.96;9,136.18,486.56,274.98,8.96;9,136.18,509.61,279.70,8.96;9,164.88,530.88,276.84,115.80"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Statistics of Paragraph Selection Task (PS) The accuracy of the answer selection process has been calculated as: Overall accuracy = 73/200 = 0.37 Proportion of answers correctly discarded: 0/75 = 0.00 c@1 measure = (73+75(73/200))/200 = 0.50 Accur acy Measur e of Answer Selection Task (AS): Our AS file contains a total of 200 answers. -Number of questions ANSWERED: 43 -Number of questions UNANSWERED: 115 -Number of questions ANSWERED with RIGHT candidate answer: 31 -Number of questions ANSWERED with WRONG candidate answer: 12 -Number of questions ANSWERED with MISSED candidate answer: 10 -Number of questions ANSWERED with INEXACT candidate answer: 8 -Number of questions UNANSWERED with RIGHT candidate answer: 0 -Number of questions UNANSWERED with WRONG candidate answer: 40 -Number of questions UNANSWERED with MISSED candidate answer: 24 -Number of questions UNANSWERED with INEXACT candidate answer: 0 -Number of questions UNANSWERED with EMPTY candidate: 75The statistics of Answer Selection Task (AS) task is given in figure3.</figDesc><graphic coords="9,164.88,530.88,276.84,115.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,231.48,660.88,143.49,8.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Answer Selection Task (AS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.79,297.45,345.72,165.87"><head>Table 1 .</head><label>1</label><figDesc>Token Replacement List</figDesc><table coords="4,124.79,321.45,345.72,141.87"><row><cell></cell><cell cols="2">Replace by Symbol</cell></row><row><cell>Replace by blank</cell><cell>Or iginal Token</cell><cell>Replaced</cell></row><row><cell></cell><cell></cell><cell>Token</cell></row><row><cell>. -</cell><cell>á</cell><cell>a</cell></row><row><cell>();</cell><cell>č</cell><cell>c</cell></row><row><cell>[...]</cell><cell>è</cell><cell>e</cell></row><row><cell>()</cell><cell>š</cell><cell>s</cell></row><row><cell>3.2 Document Indexing</cell><cell></cell><cell></cell></row><row><cell cols="3">After parsing the documents, they are indexed using Lucene, an open source full text</cell></row><row><cell>search tool.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,124.80,308.57,290.25,153.20"><head>Table 2 .</head><label>2</label><figDesc>. Question Keyword and Expected Answer</figDesc><table coords="6,124.80,356.01,253.68,105.75"><row><cell>Question Type</cell><cell>Expected Answer</cell></row><row><cell>Who</cell><cell>PERSON</cell></row><row><cell>When</cell><cell>DATE / TIME</cell></row><row><cell>Where</cell><cell>LOCATION</cell></row><row><cell>Why</cell><cell>REASON</cell></row><row><cell>What</cell><cell>OBJECT / DEFINITION</cell></row><row><cell>How</cell><cell>MEASURE</cell></row><row><cell cols="2">3.7 Answer Sentence Selection in a Par agraph</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,136.91,665.37,158.95,8.10"><p>http://celct.isti.cnr.it/ResPubliQA/index.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,137.87,675.81,87.48,8.10"><p>http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,137.87,686.13,113.27,8.10"><p>http://www.europarl.europa.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,133.32,686.13,89.04,8.10"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,132.35,686.13,179.74,8.10"><p>http://tartarus.org/~martin/PorterStemmer/java.txt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The work has been carried out with support from <rs type="funder">IFCPAR</rs> funded Project "<rs type="projectName">An Advanced platform for question answering systems</rs>" (Project No. <rs type="grantNumber">4200-IT-1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_uwcjbCg">
					<idno type="grant-number">4200-IT-1</idno>
					<orgName type="project" subtype="full">An Advanced platform for question answering systems</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,128.21,494.85,342.42,8.10;10,136.20,505.18,334.29,8.10;10,136.20,515.50,334.37,8.10;10,136.20,525.82,333.66,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,136.20,505.18,216.54,8.10">Shallow Semantic Parsing using Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kadri</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">H</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,371.17,505.18,99.32,8.10;10,136.20,515.50,334.37,8.10;10,136.20,525.82,227.63,8.10">Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL-2004)</title>
		<meeting>the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL-2004)<address><addrLine>Bosto, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">May 2-7, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.22,536.15,342.30,8.91;10,136.21,546.59,334.58,8.10;10,136.21,556.91,334.48,8.10;10,136.21,567.23,255.08,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,375.61,546.59,95.18,8.10;10,136.21,556.91,236.42,8.10">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,388.69,556.91,82.00,8.10;10,136.21,567.23,79.92,8.10">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 September-2 October, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.22,577.74,342.42,8.72;10,136.21,587.99,334.45,8.10;10,136.21,598.32,334.38,8.10;10,136.21,608.64,61.06,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,198.85,587.99,271.81,8.10;10,136.21,598.32,76.34,8.10">Overview of ResPubliQA 2010: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.97,598.32,165.47,8.10">Working Notes for the CLEF 2010 Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="20" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.22,618.96,342.51,8.10;10,136.22,629.40,334.40,8.10;10,136.22,639.73,188.77,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,430.82,618.96,39.91,8.10;10,136.22,629.40,202.52,8.10">Generating Typed Dependency Parses from Phrase Structure Parses</title>
		<author>
			<persName coords=""><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.98,629.40,115.63,8.10;10,136.22,639.73,162.49,8.10">5th International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.22,650.05,319.63,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,203.54,650.05,157.10,8.10">CRFChunker: CRF English Phrase Chunker</title>
		<author>
			<persName coords=""><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,366.98,650.05,31.92,8.10">PACLIC</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.22,660.37,342.29,8.10;10,136.22,670.81,266.99,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,292.82,660.37,161.53,8.10">The Second Release of the RASP System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,136.22,670.81,263.29,8.10">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
