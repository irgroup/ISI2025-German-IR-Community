<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,258.17,153.63,99.02,8.74"><forename type="first">David</forename><forename type="middle">Mark</forename><surname>Nemeskey</surname></persName>
							<email>@respubliqa2010</email>
							<affiliation key="aff0">
								<orgName type="institution">SZTAKI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer and Automation Research Institute</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F102467495EF6FD017458D7CA4F02F33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes the results of our first participation at ResPubliQA. Lacking a true question answering system, we relied on our traditional search engine to find the answers. We submitted two runs: a pure IR baseline and one where definition question identification and corpus-specific techniques were employed to improve IR performance. Both runs performed well, and our second run obtained better results than the baseline. However, the drawbacks of relying solely on an IR phase are clearly seen in our runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we present the SZTAKI team participation in Question Answering@CLEF 2010, also known as ResPubliQA. In this track, participants receive a set of questions they are expected to answer. A single answer may be returned for every question. This year, two separate tasks were proposed for ResPubliQA. In the Paragraph Selection (PS) task, participants had to return a numbered paragraph containing the answer to the query. In the Answer Selection (AS) task, systems were required to identify the exact answer within the paragraph text.</p><p>In our first time participation we only focused on the first part of the QA pipeline: question analysis and information retrieval. Although important for question answering <ref type="bibr" coords="1,220.14,460.14,9.96,8.74" target="#b2">[3]</ref>, we did not perform document analysis and answer selection.</p><p>We relied on our text search engine to find the paragraphs relevant to the question in the corpus. The paragraph with the highest score was returned as the answer. This required good precision from our IR phase. Question analysis was employed to formulate queries that can fulfill this requirement.</p><p>Due to the reasons mentioned above, we participated only in the Paragraph Selection task. Furthermore, we concentrated our efforts on the English Monolingual task, which consisted of a body of 200 questions.</p><p>The rest of the paper is organized as follows. Section 2 describes the components of our system and the techniques we used. Section 3 presents the submitted runs and a brief analysis of the results. We draw our conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The architecture and functions of our system can be seen on Figure <ref type="figure" coords="1,421.36,644.16,3.87,8.74" target="#fig_0">1</ref>. The system is built around the Hungarian Academy of Sciences search engine <ref type="bibr" coords="1,427.66,656.12,10.52,8.74" target="#b0">[1]</ref> based on Okapi BM25 ranking <ref type="bibr" coords="2,230.65,118.99,10.52,8.74" target="#b5">[6]</ref> with the proximity of query terms taken into account <ref type="bibr" coords="2,134.77,130.95,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,146.95,130.95,7.01,8.74" target="#b1">2]</ref>. The engine accepts queries that consist of words in an arbitrary tree of OR and AND relationships. Exact matching ("") is supported as well.</p><p>As already mentioned, the system is not a complete question answering system, as it lacks any form of document analysis, document validation or answer selection. The answer to a question is simply the first document returned by the IR module. The role of the question processing module is to formulate queries that maximize IR performance. These components are described in more detail in the next sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>Our indexer module generates a simple unigram index from the documents in the collection. We employ the Porter stemmer <ref type="bibr" coords="2,343.52,445.12,10.52,8.74" target="#b3">[4]</ref> both on the corpus and the topics. An unstemmed index is created as well to identify exact matches.</p><p>The selection of indexing units needed some effort. In the JRC-Acquis corpus, a file contains one document, which is divided into numbered paragraphs. In the EuroParl collection, an XML file consists of several chapters, which represent what can be safely considered a "document". These chapters are, in turn, segmented into numbered paragraphs. We considered two options. Document: the whole file (Acquis) or chapter (EuroParl) is indexed, including the title, the contents of all paragraphs and certain metadata, such as the executor id in EuroParl Texts Adopted documents. Paragraph: Only the paragraph contents are indexed. This is also the obvious choice for the Paragraph Selection task.</p><p>We tested both options with the questions in the 2009 training data. Our results are summarized in Table <ref type="table" coords="2,277.53,620.25,3.87,8.74" target="#tab_0">1</ref>.</p><p>Even if the numbers are not entirely accurate (the 2009 Gold Standard lists only one valid paragraph for a question, and it disregards duplicates), it can be seen that higher MAP can be achieved for document retrieval. However, due to the lack of an answer selection component, we had to resort to paragraphs as the indexing unit. To compensate for this loss of accuracy, the title of a document or chapter was added to the text of all its paragraphs, albeit with lower weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query List Creation</head><p>Instead of generating one query for every question, we decided to create a list of query variations. The first query formulates the strictest constraints of the documents it accepts, and it is followed by increasingly more permissive variants.</p><p>The default query consists of all non-stopword terms of the question in an OR relationship. The queries are sent to the IR module in this order. Processing of the list stops if at least one paragraph is found for a particular query. The idea behind using a list of queries is that we assume that for some question types, the passages we are looking for may exhibit certain peculiarities, which might be exploited with an adequately phrased query. We considered the following features: "Codes": words that contain both letters and digits (legal entries, etc.) We considered this feature important, so such words resulted in two query variants: one, where the word is in an AND relationship with the rest of the words, and the default one. Quotations: in the JRC-Acquis corpus, quotation marks (") are denoted by the %quot% string. This enabled us to include these in the search with regular IR techniques. Therefore, two queries were created for questions that contained quoted word(s): one with the "AND quot" string appended to the query, and one without.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Classification</head><p>Our system does not yet include a full-fledged question classification module. However, we assumed that in legal texts, such as the JRC-Acquis and the Eu-roParl corpora, great care is taken to define terms and expressions clearly and unambiguously. If this was the case, recognition of definition questions would benefit the precision of our system substantially. Therefore, an experimental component that identifies definition questions was implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern creation</head><p>We classified the questions in the 2009 training data manually, and collected a set of patterns that occurred in definition questions. Typical examples include "What is a XY?" and "What is meant by XY?". These informal patterns were then formalized as regular expressions, and matched against the questions in this year's evaluation set. Regular expressions allowed us to decide if the question was a definition question and retrieve the terms or expressions to be defined at the same time.</p><p>Out of the about 24 definition questions we found manually (some of these choices may be debatable), our method successfully identified 18, or 75%.</p><p>Exact matching With the expressions to be defined extracted, we needed a way to use them to enhance IR precision. We use question 66: "What is sports footwear? " as an example to present our method.</p><p>Our first assumption was that the passage where the expression (e.g. "sports footwear ") is defined may either include it in the same form as in the question, or at least contain all the terms that make up the expression ("sport" and "footwear "). If both of these assumptions are false, we need to fall back to the default OR query. Therefore, three query variations are added to the list for every definition question:</p><p>the whole expression in quotation marks, which denotes exact matching; the terms connected by an AND relation; the terms connected by an OR relation (the default).</p><p>Based on the formality of the language used, we also assumed that the words "mean", "definition" and their synonyms may actually occur in the paragraph where the term is defined. Hence, for every query in the list, we created an expanded variation that included these words in and AND relation with the rest of the query, and added these new, stricter queries to the list as well.</p><p>The listing below shows the query list created from our example question.</p><p>&lt;query id="66"&gt; &lt;subquery index="1"&gt;"sports footwear" AND (mean OR define OR ...) &lt;/subquery&gt; &lt;subquery index="2"&gt;"sports footwear"&lt;/subquery&gt; &lt;subquery index="3"&gt;sport AND footwear AND (mean OR define OR ...) &lt;/subquery&gt; &lt;subquery index="4"&gt;sport AND footwear&lt;/subquery&gt; &lt;subquery index="5"&gt;sport OR footwear&lt;/subquery&gt; &lt;/query&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Information Retrieval</head><p>In our system, the IR phase is responsible for selecting the paragraph that answers the question. We used our own search engine, which ranked the passages with Okapi-BM25 ranking function. Our implementation takes document structure into account and it is possible to assign different weights to the various fields (e.g. title, body, meta-data).</p><p>BM25 is a TF-IDF-like ranking, with two additional features. Firstly, it is possible to adjust the effect of term frequency on the score via the k 1 parameter. Secondly, a normalization based on document length is applied to the final score. The degree of normalization can be adjusted by the b parameter.</p><p>The following settings were used for passage retrieval:</p><p>-The b parameter was set to 0. While document length is usually an important feature in ad-hoc retrieval, our experiences with the 2009 training set indicate no correlation between paragraph length and relevancy. Therefore, we decided against length normalization. -The weight of the title field is set to 0.5, as it does not belong to the paragraph itself.</p><p>As mentioned earlier, the queries in the query list associated with a question are processed until a paragraph is retrieved. If more than one is found, as is usually the case, the one with the highest BM25 score is returned as the answer to the question. Our system did not tag any questions as NOA.</p><p>It is worth mentioning that out of the 23 questions that had a query list of at least two elements (in line with section 2.2 and 2.3), there was only one for which the first query did not return any paragraphs. On the one hand, this means that our assumptions about the corpus were right. On the other, since it would be difficult to add further, meaningful constraints to the queries, it also shows that perhaps not much improvement potential is left in this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Submitted</head><p>We submitted two runs for the English monolingual task.</p><p>Run 1 was our baseline run. Only the paragraph texts were indexed, and default Okapi parameters (k 1 = 2, b = 0.75) were used. However, quotation and code detection -and the resulting query variants -were included in this run as well, mostly by mistake. Run 2 featured all additional techniques described in the earlier chapters, namely:</p><p>the paragraphs were augmented with the document titles, -Okapi parameter b was set to 0, query expansion was performed and exact matches were preferred for definition questions.</p><p>Table <ref type="table" coords="5,177.60,566.62,4.98,8.74" target="#tab_1">2</ref> summarizes our results<ref type="foot" coords="5,287.34,565.05,3.97,6.12" target="#foot_0">1</ref> . The cells show the number of correct answers and average c@1 for the specified run and question type.</p><p>Regarding the results, we can observe that run 2 generally yielded a higher number of correct answers. However, only the improvements for definition questions can be considered significant. Nevertheless, it shows the potential of adapting parameters to the collection. The higher rate of correct answers for definition questions validates our assumptions about the nature of the corpus and proves that exact matching and using meta-information, such as including synonyms of "mean" and "define" in the query can indeed increase precision. However, this naïve implementation may also mislead the retrieval engine by giving too much importance to terms not actually part of the question, as shown by the following example. This paragraph was returned because the term "port facility" occurred in the document title.</p><p>Q. What is a port facility? A. 2.4 Terms not otherwise defined in this part shall have the same meaning as the meaning attributed to them in chapters I and XI-2.</p><p>This problem underlines the importance of paragraph validation and analysis, which could have filtered such a passage from the result list. Analysis of the returned paragraphs may also increase precision. Table <ref type="table" coords="6,397.73,392.13,4.98,8.74">3</ref> lists the recall at 1, 5 and 10 passages our system achieved on the 2009 English training data. (Gold standard for the 2010 evaluation data is not available yet.) An answer validator component that examines the top ten candidates may, in the optimal case, improve precision by as much as 77%.</p><p>Table <ref type="table" coords="6,234.57,470.33,4.13,7.89">3</ref>. Recall achieved on the 2009 training set. r@1 = p@1 r@5 r@10 Recall 0.40625 0.6458 0.71875</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper, we have presented our QA system and our results for the English monolingual task. Our system did not use any post-processing of answer candidates and relied completely on the IR module. Our efforts were mainly focused on improving IR performance by tuning retrieval settings, preferring exact matching for definition questions and including collection-specific elements in the queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,247.97,254.12,119.42,7.89;2,152.06,263.01,311.24,104.65"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A system architecture</figDesc><graphic coords="2,152.06,263.01,311.24,104.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,152.90,115.91,309.55,61.34"><head>Table 1 .</head><label>1</label><figDesc>MAP values achieved by document-and paragraph-based indexing</figDesc><table coords="3,168.62,146.65,278.11,30.61"><row><cell></cell><cell cols="2">EN-EN (95 questions) All-EN (500 questions)</cell></row><row><cell>Document</cell><cell>0.528</cell><cell>0.537</cell></row><row><cell>Paragraph</cell><cell>0.432</cell><cell>0.476</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,163.62,115.91,288.11,84.06"><head>Table 2 .</head><label>2</label><figDesc>Results for the English monolingual runs.</figDesc><table coords="6,163.62,146.65,288.11,53.32"><row><cell>Run</cell><cell>Regular</cell><cell cols="2">Quotation Definition</cell><cell>Sum</cell></row><row><cell># of questions</cell><cell>177</cell><cell>5</cell><cell>18</cell><cell>200</cell></row><row><cell>Run 1</cell><cell>116 (0.655)</cell><cell>4 (0.8)</cell><cell>9 (0.5)</cell><cell>129 (0.65)</cell></row><row><cell>Run 2</cell><cell>118 (0.666)</cell><cell>5 (1.0)</cell><cell>13 (0.72)</cell><cell>136 (0.68)</cell></row><row><cell>Difference</cell><cell>+2 (0.011)</cell><cell>+1 (0.2)</cell><cell>+4 (0.22)</cell><cell>+7 (0.035)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,645.84,335.86,7.86;5,144.73,656.80,266.30,7.86"><p>We believe that some of the questions were judged incorrectly. Hence, while the table shows the official results, the numbers may not be 100% accurate.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The results have proven the validity of our approach. We achieved an almost 50% increase in accuracy for definition questions, compared to our baseline run. Tuning of retrieval settings, however, has not yielded significant improvement.</p><p>Our results have also shown the fragility of depending on the IR phase alone for question answering. In the future, we intend to extend our system to a complete QA system by implementing question classification for all question types, paragraph validation and answer extraction. Several options to improve the IR phase, such as using thesauri for query expansion and fine tuning retrieval parameters are also yet to be explored.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,270.11,342.24,7.86;7,146.91,281.07,333.68,7.86;7,146.91,292.03,333.68,7.86;7,146.91,302.99,239.97,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,213.40,281.07,226.48,7.86">Searching a small national domain-preliminary report</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Csalogány</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fogaras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Windhager</surname></persName>
		</author>
		<ptr target="http://datamining.sztaki.hu/?q=en/en-publications" />
	</analytic>
	<monogr>
		<title level="m" coord="7,463.04,281.07,17.56,7.86;7,146.91,292.03,243.75,7.86">Proceedings of the 12th World Wide Web Conference (WWW)</title>
		<meeting>the 12th World Wide Web Conference (WWW)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,313.95,342.24,7.86;7,146.91,324.90,333.68,7.86;7,146.91,335.86,91.91,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,325.85,313.95,154.74,7.86;7,146.91,324.90,143.57,7.86">Term proximity scoring for ad-hoc retrieval on very large text collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,311.28,324.90,39.60,7.86">SIGIR &apos;06</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,346.82,342.24,7.86;7,146.91,357.78,86.08,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,189.38,346.82,188.16,7.86">From document retrieval to question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="7,138.35,368.74,319.06,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,194.70,368.74,130.19,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,331.81,368.74,34.98,7.86">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,379.70,342.24,7.86;7,146.91,390.66,304.76,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,240.57,379.70,235.81,7.86">Term proximity scoring for keyword-based retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,160.99,390.66,137.39,7.86">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,401.62,342.24,7.86;7,146.91,412.58,122.21,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,269.82,401.62,146.66,7.86">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,439.25,401.62,41.35,7.86;7,146.91,412.58,65.16,7.86">Document retrieval systems</title>
		<imprint>
			<biblScope unit="page" from="143" to="160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
