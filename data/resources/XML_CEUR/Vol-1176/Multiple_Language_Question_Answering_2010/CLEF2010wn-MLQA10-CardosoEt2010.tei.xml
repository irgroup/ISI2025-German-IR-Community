<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,176.11,115.96,263.14,12.62;1,149.00,133.89,317.35,12.62">Revamping Question Answering with a Semantic Approach over World Knowledge</title>
				<funder ref="#_kzK92Rx">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder ref="#_wBkFrYq">
					<orgName type="full">FCT</orgName>
				</funder>
				<funder ref="#_6yTrr4K">
					<orgName type="full">EU-</orgName>
				</funder>
				<funder ref="#_kbbg8NB">
					<orgName type="full">Centre for Next Generation Localisation</orgName>
					<orgName type="abbreviated">CNGL</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.95,171.56,62.35,8.74;1,202.30,169.98,1.83,6.12"><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
							<email>ncardoso@xldb.di.fc.ul.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Sciences</orgName>
								<orgName type="institution">University of Lisbon</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.54,171.56,68.91,8.74;1,281.45,169.98,1.83,6.12"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
							<email>i.dornescu2@wlv.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">RIILP</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.69,171.56,70.35,8.74"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<email>hartrumpf@gmx.net</email>
							<affiliation key="aff2">
								<orgName type="institution">SEMPRIA GmbH</orgName>
								<address>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.34,171.56,79.48,8.74;1,470.82,169.98,1.36,6.12"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<email>jleveling@computing.dcu.ie</email>
							<affiliation key="aff3">
								<orgName type="laboratory">CNGL</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,176.11,115.96,263.14,12.62;1,149.00,133.89,317.35,12.62">Revamping Question Answering with a Semantic Approach over World Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">272CA4CC7038E23D06B53391E7F224CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classic textual question answering (QA) approaches that rely on statistical keyword relevance scoring without exploiting semantic content are useful to a certain extent, but are limited to questions answered by a small text excerpt. With the maturation of Wikipedia and with upcoming projects like DBpedia, we feel that nowadays QA can adopt a deeper, semantic approach to the task, where answers can be inferred using knowledge bases to overcome the limitations of textual QA approaches. In GikiCLEF, a QA-flavoured evaluation task, the best performing systems followed a semantic approach. In this paper, we present our motivations for preferring semantic approaches to QA over textual approaches, with Wikipedia serving as a raw knowledge source.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a challenging task, requiring a good expertise in several natural language processing fields to properly understand information needs in the questions, to obtain a list of answer candidates from documents, and to filter them based on solid evidence that justifies each answer's correctness.</p><p>Most state-of-the-art QA approaches are textual QA systems built around a passage retrieval core, where questions (or their affirmative re-phrasings) are treated as bag-of-words or n-grams, ignoring their semantic content. These simplified question representations are fed into the information retrieval engine to obtain the paragraphs most likely to contain answers. The candidates are then extracted based on their type and ranked based on their frequency.</p><p>Passage-retrieval QA systems have their share of success in QA evaluation tracks such as QA@CLEF, which include a considerable amount of concrete questions (such as factoid or definition questions). Answers to these questions can usually be found in the collection within a paragraph containing a sentence similar to the question (e.g. "Where is X located ? " → "X is located in Y "), or by exploiting redundant information in large collections such as the Web.</p><p>As textual QA systems focus on selecting text excerpts from the collection, they cannot address structurally complex questions that require advanced reasoning over key concepts, nor questions whose answers are not explicitly represented in a text excerpt, but must be inferred from knowledge that may (or may not) be automatically derived from those text excerpts.</p><p>GikiCLEF <ref type="bibr" coords="2,197.34,118.99,10.52,8.74" target="#b0">[1]</ref> is a QA-flavoured evaluation track that challenges participating systems to produce answers and justifications from Wikipedia for geographicallybiased open list questions. A key aspect of GikiCLEF is that the answers to most topics are scattered across Wikipedia pages, and the answers and justifications have to be linked to an unambiguous real-world entity (represented by a Wikipedia URL), rather than being of pure textual nature. One of the main motivations of GikiCLEF is to refocus QA evaluation on its real challenge: prove the answer(s) from the question, just like humans do, but automatically; i.e., use world knowledge to reason over the entities that satisfy the criteria of the question. Although we participated in GikiCLEF with distinct systems from different projects and goals (our systems ranked 1st, 2nd, and 4th on the task <ref type="bibr" coords="2,457.35,238.55,7.75,8.74" target="#b1">[2]</ref><ref type="bibr" coords="2,465.10,238.55,3.87,8.74" target="#b2">[3]</ref><ref type="bibr" coords="2,468.97,238.55,7.75,8.74" target="#b3">[4]</ref>; the 3rd rank corresponds to manual experiments), we shared the same semantic vision on how to tackle the GikiCLEF task.  <ref type="table" coords="2,178.32,459.78,4.98,8.74" target="#tab_0">1</ref> summarizes what we think are the key differences of textual QA approaches, compared to semantic QA approaches like ours. Textual QA is typically based on retrieving candidate answers from textual snippets in a document collection, augmented by term-based statistical approaches such as query expansion on the level of words or conflated word forms (stems). In contrast, semantic QA employs processes that go beyond matching questions and documents: external knowledge in a formal representation (such as RDF) is used to reason over disambiguated concepts and entities, derive relations between them, and infer answers from the question representation. While textual QA approaches can be successful for finding explicitly stated answers from documents, semantic QA aims for complex questions where several information sources must be merged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Accessing the world knowledge</head><p>One of the obstacles for semantic QA approaches is the need for a large repository of "condensed world knowledge" derived from validated, well-structured and machine-accessible data to enable inferring information that is implicit in the text. While compiling such a repository is highly unfeasible for any single researcher or research group (the famous knowledge bottleneck), it is feasible for community-driven projects such as Wikipedia or Freebase. Along with upcoming projects like DBpedia <ref type="bibr" coords="3,232.64,154.86,9.96,8.74" target="#b4">[5]</ref>, YAGO <ref type="bibr" coords="3,282.05,154.86,10.52,8.74" target="#b5">[6]</ref> and the recent interest in Linked Data <ref type="bibr" coords="3,467.31,154.86,9.96,8.74" target="#b6">[7]</ref>, the QA community now has at its disposal large amounts of human knowledge in machine-readable format that is easily accessible, freely available, and constantly improving in quantity and quality. While such resources and services only cover certain domains and do not (yet) encompass all the knowledge required to answer all questions from either QA evaluation fora or from real users, the foundation is now laid and we can start developing QA systems with semantic strategies over this Linked Data layer of human knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From question analysis to answer reasoning</head><p>Our semantic QA systems dedicated special attention to these three tasks:</p><p>Grounding expected answer types to a workable category/classification. The expected answer type (EAT) for a question must be mapped to entities from the world knowledge, so that its meaning becomes unambiguously grounded. Take for instance the question "Which Romanian writers were born in Bucharest in the 20th century? "; the EAT can be grounded to DBpedia resource URL http://dbpedia.org/resource/Category:Romanian_writers which makes it easier to search and validate candidate answers.</p><p>Parse constraints from the question. A semantic question interpretation can be seen as an EAT and a composition of constraints. In the example above, the question has two restrictions: one geographical (born 'in Bucharest' ) and one temporal (born 'in the 20th century' ). By handling each constraint separately, systems can use its meaning to employ the right strategies for validating candidate answers <ref type="bibr" coords="3,193.35,465.41,9.96,8.74" target="#b2">[3]</ref>. This divide-and-conquer method is well illustrated by Cardoso et al. <ref type="bibr" coords="3,159.53,477.36,9.96,8.74" target="#b1">[2]</ref>, who mapped predicates in the conditions into DBpedia ontology properties, or by Hartrumpf et al. <ref type="bibr" coords="3,268.74,489.32,10.52,8.74" target="#b3">[4]</ref> who decompose a question into subquestions and uses subanswers to reformulate the original question.</p><p>Semantic analysis. The natural language question is transformed into a machinereadable representation, which can be rewritten or expanded in some way. One technique is logical question expansion by employing an inference engine with rules for paraphrases, entailments, and logical equivalence, as described in <ref type="bibr" coords="3,467.30,560.48,9.96,8.74" target="#b3">[4]</ref>. Another technique is to generate SPARQL queries from all grounded concepts in the question, and submit to DBpedia's SPARQL endpoint (http://www. dbpedia.org/sparql), as shown in <ref type="bibr" coords="3,292.92,596.34,9.96,8.74" target="#b1">[2]</ref>. Relying solely on knowledge bases such as DBpedia to obtain answers leads to very high precision at the cost of very low recall, but this will change as Wikipedia articles become more complete, and DBpedia's ontology coverage on extracted resources and grounded properties increases. A reference such as http://dbpedia.org/resource/Mihail Fȃrcȃşanu is not just a span of text of type Person -as is the case for textual QA, but an entity linked to various KB, both structured (databases, ontologies) and unstructured (textual). Disambiguated references allow merging relevant information sources as well as combining heterogeneous procedures. It is this background information that can help answer questions traditionally considered "complex".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Final Remarks</head><p>The QA community finally has access to a considerable amount of world knowledge, created and curated by a dedicated community, encoded in machinereadable formats such as RDF/OWL and freely available through Linked Data. We feel that the time has come to focus on semantic approaches for QA. The core strategy of semantic QA systems is to understand key entities, concepts and relations between them in the question (the information need), and solve it by exploiting multiple knowledge sources and selecting the best strategies to determine and validate the right answers. Classic textual QA approaches are still useful for simple questions, but are quite limited for elaborated questions, and depending on a collection limits their question range. We foresee that the performance of semantic QA systems will improve as the resources and services from associated projects evolve over time, and will be able to answer more complex, open list questions that have more resemblance to real-user information needs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,294.47,325.99,174.04"><head>Table 1 .</head><label>1</label><figDesc>Key differences between textual QA and semantic QA approaches</figDesc><table coords="2,134.77,294.47,323.83,174.04"><row><cell>2 Semantic QA approach</cell><cell></cell></row><row><cell>Textual QA</cell><cell>Semantic QA</cell></row><row><cell>[web of] documents</cell><cell>[web of] data</cell></row><row><cell>document retrieval (IR core)</cell><cell>search for and derive facts (IE core)</cell></row><row><cell>query expansion on word level</cell><cell>question expansion on entity level</cell></row><row><cell>keywords &amp; co-occurrence</cell><cell>concepts &amp; relations</cell></row><row><cell cols="2">ambiguous words (or even word forms) disambiguated concepts</cell></row><row><cell>textual snippets</cell><cell>graph patterns</cell></row><row><cell>gazetteers</cell><cell>RDF data</cell></row><row><cell cols="2">lexical semantics (thesaurus oriented) formal semantics</cell></row><row><cell>text with entities</cell><cell>linked entities with text</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is partially supported by <rs type="funder">FCT</rs> for its <rs type="programName">LASIGE Multi-</rs>annual support, <rs type="projectName">GREASE-II</rs> project (grant <rs type="grantNumber">PTDC/EIA/73614/2006</rs>) and <rs type="grantName">Nuno Cardoso</rs>'s scholarship (grant <rs type="grantNumber">SFRH/BD/45480/2008</rs>), partly supported by the <rs type="funder">EU-</rs>funded project <rs type="projectName">QALL-ME</rs> (<rs type="grantNumber">FP6 IST-033860</rs>), and supported by the <rs type="funder">Science Foundation Ireland</rs> (Grant <rs type="grantNumber">07/CE/I1142</rs>) as part of the <rs type="funder">Centre for Next Generation Localisation (CNGL)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_wBkFrYq">
					<idno type="grant-number">PTDC/EIA/73614/2006</idno>
					<orgName type="grant-name">Nuno Cardoso</orgName>
					<orgName type="project" subtype="full">GREASE-II</orgName>
					<orgName type="program" subtype="full">LASIGE Multi-</orgName>
				</org>
				<org type="funded-project" xml:id="_6yTrr4K">
					<idno type="grant-number">SFRH/BD/45480/2008</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
				</org>
				<org type="funding" xml:id="_kzK92Rx">
					<idno type="grant-number">FP6 IST-033860</idno>
				</org>
				<org type="funding" xml:id="_kbbg8NB">
					<idno type="grant-number">07/CE/I1142</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,474.91,342.24,7.86;4,146.91,485.00,333.68,7.86;4,146.91,495.08,172.40,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,248.61,474.91,231.98,7.86;4,146.91,485.00,207.51,7.86">GikiCLEF: Crosscultural issues in an international setting: asking non-English-centered questions to Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Cabral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,377.54,485.00,103.06,7.86;4,146.91,495.08,16.79,7.86">Working Notes for CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 Sept.-2 Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,504.18,342.24,7.86;4,146.91,514.26,333.68,7.86;4,146.91,524.34,172.40,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,384.78,504.18,95.81,7.86;4,146.91,514.26,212.62,7.86">Where in the Wikipedia is that answer? the XLDB at the GikiCLEF 2009 task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Baptista</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Lopez-Pellicer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,379.88,514.26,100.72,7.86;4,146.91,524.34,16.79,7.86">Working Notes for CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 Sept.-2 Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,533.44,342.24,7.86;4,146.91,543.52,240.18,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,203.79,533.44,218.79,7.86">EQUAL: Encyclopaedic QUestion Answering for Lists</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dornescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,445.76,533.44,34.83,7.86;4,146.91,543.52,84.57,7.86">Working Notes for CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 Sept.-2 Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,552.61,342.24,7.86;4,146.91,562.70,333.68,7.86;4,146.91,572.78,148.34,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,265.95,552.61,214.64,7.86;4,146.91,562.70,181.31,7.86">GIRSA-WP at GikiCLEF: Integration of Structured Information and Decomposition of Questions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,352.25,562.70,124.14,7.86">Working Notes for CLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 Sept.-2 Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,581.88,342.24,7.86;4,146.91,591.96,333.68,7.86;4,146.91,602.04,333.68,7.86;4,146.91,612.12,249.51,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,433.12,581.88,47.47,7.86;4,146.91,591.96,131.22,7.86">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,300.69,591.96,179.90,7.86;4,146.91,602.04,249.95,7.86">6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-11-15">Nov. 11-15, 2007. 2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,621.22,342.24,7.86;4,146.91,631.30,292.15,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,320.88,621.22,155.08,7.86">YAGO: A Core of Semantic Knowledge</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,160.99,631.30,114.16,7.86">Proceedings of the WWW&apos;07</title>
		<meeting>the WWW&apos;07<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-12">May 8-12 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,640.40,342.24,8.12;4,146.91,650.48,99.28,8.12" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berners-Lee</surname></persName>
		</author>
		<ptr target="http://www.w3.org/DesignIssues/LinkedData.html" />
		<title level="m" coord="4,215.78,640.40,111.30,7.86">Linked Data -Design Issues</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
