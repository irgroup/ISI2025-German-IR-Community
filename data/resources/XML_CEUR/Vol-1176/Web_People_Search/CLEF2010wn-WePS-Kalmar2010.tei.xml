<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.60,151.74,263.72,12.60;1,202.70,169.14,190.13,12.60">Bootstrapping Websites for Classification of Organization Names on Twitter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,278.30,207.77,50.23,9.00"><forename type="first">Paul</forename><surname>Kalmar</surname></persName>
							<email>paul@kalmarresearch.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Kalmar Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.60,151.74,263.72,12.60;1,202.70,169.14,190.13,12.60">Bootstrapping Websites for Classification of Organization Names on Twitter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0555C18789A7697B8E5EB204D845729</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>bootstrap</term>
					<term>unsupervised</term>
					<term>Twitter</term>
					<term>disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a growing interest in monitoring the social media presence of companies for improved marketing. Many public APIs are available for tapping into the data, and there are companies that will collect all posts related to a given set of keywords. But with so much data, who is to say that all of the posts are relevant, especially when so many company and product names are highly ambiguous? In the context of the WePS Task 2, we aim to reduce noise by collecting only the relevant tweets about a company given the company's website and set of Twitter data. In a real world situation, any company who wanted to identify tweets about themselves could provide a short list of labeled tweets and use this as a base set for training data. Given that for this task we were given a large list of companies with no such training data, it would have been unrealistic to create such data for each company. We chose to use the company's website as surrogate training data. Because the websites come from a different register than Twitter, we used the initial model to bootstrap a model from the actual tweets. As it is the most simple data to acquire, the features we chose to use were the co-occurring words in each tweet. To compute the relevance of each word to a given company, we computed the pointwise mutual information between the word and the target's label. The results show that our approach was successful, yet with room for improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a growing interest in monitoring the social media presence of companies for improved marketing. Many public APIs are available for tapping into the data, and there are companies that will collect all posts related to a given set of keywords. But with so much data, who is to say that all of the posts are relevant, especially when so many company and product names are highly ambiguous? In the context of the WePS Task 2, we aim to reduce noise by collecting only the relevant tweets about a company given the company's website and set of Twitter data.</p><p>For the task of classification, there needs to be at least one well defined class. In a real world situation, any company who wanted to identify tweets about themselves could provide a short list of labeled tweets and use this as a base set for training data. Given that for this task we were given a large list of companies with no such training data, it would have been unrealistic to create such data for each company. We chose to use the company's website as surrogate training data. All text was extracted from the site and used to create an initial model. Meta tags such as keywords and description were heavily weighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data collection</head><p>To collect the data, we used Python to query and cache each webpage listed for the data sets. The cached page was then processed using BeautifulSoup, extracting all text and meta tags. Some website addresses were modified from the original data set to potentially get a more correct or textual site. This was done manually, but in future research a process that warns about pages with low amounts of text would be preferred. One of the main changes was correcting Wikipedia page addresses to the English form of the page and collecting the raw text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>As it is the most simple data to acquire, the features we chose to use were the cooccurring words in each tweet. To compute the relevance of each word to a given company, we computed the pointwise mutual information between the word and the target's label. Pointwise mutual information is defined as the log of the probability of the word occurring with the label divided by the product of the probability of the word and the probability of the label. PMI = log( p(word,label) / (p(word)*p(label)) )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Normalization</head><p>Originally we planned to use the data completely unnormalized for various reasons. The primary reason is that the less is done to alter the starting data, the more reliably the system would perform in a completely unsupervised setting with unknown data. Twitter has many terms and spellings which distinguish the language used there from standard English. Also, the original specifications for the task included Spanish language data, which would have either required an additional Spanish system or, as we opted for, a language agnostic system.</p><p>Given that the task eliminated non-English data, we added some simple normalization to the text. All words were converted to lowercase and passed through a stemmer from NLTK. <ref type="bibr" coords="3,222.40,173.07,11.63,9.00" target="#b0">[1]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bootstrapping</head><p>Because the company websites come from a different register than Twitter, we used the initial model to bootstrap a model from the actual tweets. We examined multiple approaches to bootstrapping the model. To build the bootstrapped model, we took the initial model built from the company website and applied it to label the company's tweets. The set of tweets that the initial model was most confident in labeling were then taken and used to build subsequent models. This bootstrapped model was then incremented over several iterations by repeating the process and retaining a larger set of tweets. There are three variables that had to be determined: size of initial bootstrapped model, method of incrementing model, and number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size of model.</head><p>Optimally, the initial bootstrapped model should have all correctly labeled tweets and none of the incorrectly labeled tweets. As that is highly unlikely, it is best to err on the side of precision as opposed to recall. To determine the size of the model, two possible options are thresholding and limiting the set to a fixed number or percentage of tweets. We developed models using two different methods: 1. Keep all tweets with confidence greater than or equal to the median confidence. 2. Keep a specific percentage of best confidence labeled tweets. For future research, it might be interesting to use machine learning to determine the optimal confidence threshold and use this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method of incrementing the model.</head><p>Once the initial model is created, we iteratively increment the model on the set of tweets. To do so it is necessary to again determine how much more data to add, and whether to include the previous data in the model. With the initial model which relied on the median confidence, we stuck with this method as we iterated through the tweets. For the model that kept a specific percentage of best confidence tweets, we gradually increased this percentage by a small amount each iteration up to a maximum size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of iterations.</head><p>The last variable to choose for bootstrapping was the number of iterations to perform. For future research, the optimal stopping point could be computed with machine learning. Due to time constraints, we tried varying the number and looking at the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted configurations.</head><p>For the WePS task, we submitted results from the following configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>This task consisted of a binary classification of tweets, and therefore can be evaluated using standard classification metrics. Some such metrics are accuracy, precision, recall, and f-measure which is the harmonic mean of precision and recall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CP = #Correct Positives, FP = # False Positives, CN = #Correct Negatives, FN = #False Negatives</head><p>Of these, accuracy reflects performance across all classes whereas the others reflect performance with regard to a specific class. Accuracy is the only one of the above metrics that utilizes the number of correct negatives.</p><p>Although the evaluation metric used for the WePS task was accuracy, we chose to focus on the f-measure for the positive class. When using a system such as this, finding negative results is not in any way helpful --what matters are the results that are actually about the company. Recall is important so that all possible results are returned about the company, and precision is important so there are few to no false positives in the results. We realized early on that accuracy would be of little actual use to us, so we did not attempt for high accuracy results. Instead, we focus on results for the harmonic mean of precision and recall of positive results.</p><p>The following table is the official results from the WePS task for our systems. The results show that our approach was successful, yet with room for improvement. Many sites contained little or no text, which caused our approach to be ineffective for these companies. As expected, the system which used the smallest initial model and a large number of iterations generally performed the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Because our approach was based on co-occurring words, the best results appear when the keyword is disambiguated to its correct sense. Many of the companies used the keyword in the same sense as non-company related tweets, however, and this caused a high error rate in our system. An example of this is sports teams, where the name of the city is used in the same sense in the name of the team and when referring to the city in general. For keywords which had a completely different meaning than the company name, results were much more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Although our approach achieved lower results than expected, this seems to be a good initial pass which can be improved by automatically setting variables with machine learning. This system achieves high results on labeled training data, and is therefore a suitable approach for normal supervised scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.90,149.65,320.20,155.82"><head>Table 1 .</head><label>1</label><figDesc>Submitted configurations.</figDesc><table coords="4,127.60,169.07,305.73,106.10"><row><cell>Configuration</cell><cell cols="2">Initial model size Increment</cell><cell>Number of</cell></row><row><cell>Name</cell><cell></cell><cell>percentage</cell><cell>iterations</cell></row><row><cell>KalmarResearch_1*</cell><cell>Median</cell><cell>Median</cell><cell>40</cell></row><row><cell>KalmarResearch_2</cell><cell>Median</cell><cell>Median</cell><cell>40</cell></row><row><cell>KalmarResearch_3</cell><cell>Median</cell><cell>Median</cell><cell>35</cell></row><row><cell>KalmarResearch_4</cell><cell>10.00%</cell><cell>2.00%</cell><cell>30</cell></row><row><cell>KalmarResearch_5</cell><cell>15.00%</cell><cell>1.00%</cell><cell>25</cell></row></table><note coords="4,124.90,296.47,320.20,9.00"><p>*KalmarResearch_1 was a slight variation in the code from KalmarResearch_2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,124.90,419.75,320.88,57.02"><head>Table 2 .</head><label>2</label><figDesc>Metrics.    </figDesc><table coords="4,127.60,439.17,318.18,37.60"><row><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-Measure</cell></row><row><cell>(CP+CN)/</cell><cell>CP/(CP+FP)</cell><cell>CP/(CP+FN)</cell><cell>(2*Precision*Recall)/</cell></row><row><cell>(CP+FP+CN+FN)</cell><cell></cell><cell></cell><cell>(Precision + Recall)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,128.28,657.35,342.31,8.10;5,136.20,667.75,62.98,8.10" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,275.09,657.35,157.37,8.10">Natural Language Processing with Python</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.28,149.65,341.66,8.10;6,136.20,160.05,272.57,8.10" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,181.08,149.65,288.86,8.10;6,136.20,160.05,75.20,8.10">Less is More: Advantages of Using Local Homogenous Data Sets in Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kalmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>San Diego State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis at</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
