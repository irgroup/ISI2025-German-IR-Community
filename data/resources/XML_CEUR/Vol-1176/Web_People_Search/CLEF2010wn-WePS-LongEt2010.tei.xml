<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.83,113.78,297.65,15.06;1,198.08,131.72,219.16,15.06">Web Person Name Disambiguation by Relevance Weighting of Extended Feature Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,258.42,170.85,50.08,10.46"><forename type="first">Chong</forename><surname>Long</surname></persName>
							<email>chongl@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo! Global R&amp;D</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.87,170.85,29.06,10.46"><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<email>lshi@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo! Global R&amp;D</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.83,113.78,297.65,15.06;1,198.08,131.72,219.16,15.06">Web Person Name Disambiguation by Relevance Weighting of Extended Feature Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E18255C75425B7B60F547A9AEEDF747A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach to the Person Name Disambiguation clustering task in the Third Web People Search Evaluation Campaign(WePS-3). The method focuses on two aspects: the extended feature sets, and feature relevance weighting. Bag-of-words and named entities are most commonly used features in many existing web entity disambiguation algorithms and we further extend this basic feature set with Wikipedia concepts. Then two feature weighting models are employed. One is the feature relevance to the target person name(or "query name"), and the other is the feature relevance to the text content. Similarity score is calculated according to the feature weights for clustering documents of the same person. Experiments show that the system based on our approach has generated the best results among all the WePS-3's submissions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person name disambiguation has long been an important problem in natural language processing and text mining. Due to prevalent occurrences on the web that identical person names (or surface names) on different web pages refer to distinct people, being able to resolve the referees of person names on web content is essential for many applications. For instance, <ref type="bibr" coords="1,149.71,462.58,11.62,10.46" target="#b0">(1)</ref> In web search, 15-21% of the queries contain person names (11-17% of the queries are composed of a person name in web search, with aditional terms and 4% are identified simply as person names) <ref type="bibr" coords="1,274.27,486.49,15.27,10.46" target="#b13">[14]</ref>. If we are able to retrieve documents that match the user's intended person instead of the surface name, the relevance of search results for people related queries can be substantially improved. <ref type="bibr" coords="1,352.82,510.40,11.62,10.46" target="#b1">(2)</ref> Many online social network applications rely on person name as one of the major identities of their users. Resolution of person name ambiguity is hence crucial for many online SNS services. <ref type="bibr" coords="1,440.68,534.31,11.62,10.46" target="#b2">(3)</ref> Along with ambiguity of word sense, entity name ambiguity has been a major impediment for many natural language processing tasks, such as text classification, clustering etc.</p><p>WePS(http://nlp.uned.es/weps/) is a public evaluation campaign for web entity disambiguation, providing annotated datasets for training and testing <ref type="bibr" coords="1,399.98,582.91,10.58,10.46" target="#b0">[1]</ref>. In 2010, we Yahoo! Software R&amp;D Beijing participated the Person Name Disambiguation Task in the third workshop, namely WePS-3. In this task, 300 person names (or query names) are provided along with the top 200 documents retrieved from the search engine for each of the person name. The target is to cluster documents based on the identity of the person, such that documents with names referring the same person are converged into the same cluster.</p><p>Our method for the task focuses on two aspects: the feature set and feature weighting. Bag-of-words and named entities are most commonly used features in many existing web entity disambiguation algorithms. Although they have been reported as effective <ref type="bibr" coords="2,151.75,153.38,10.58,10.46" target="#b5">[6]</ref>, we further extend the basic feature set with Wikipedia concepts. Wikipedia offers a large repository of a wide range of concepts. Compared with conventional named entities consisting of person, location and organization, Wikipedia concepts have many advantages such as well-organized, clean and accurate. For instance, "Support Vector Machine" is better treated as a single coherent conceptual unit rather than three individual words as it is an entry in Wikipedia. Concepts like "Support Vector Machine" are also unable to be recognized by current NER tools because they do not belong to person, location or organization. Since Wikipedia entries are edited by human, they are very accurate compared with entities automatically recognized by NER tools. Previous attempts to leverage Wikipedia for entity disambiguation concentrated on using Wikipedia entries as referees for resolution instead of features. They tried to map a surface name in the text to a Wikipedia entry. However, due to limited coverage of Wikipedia on people, majority of the person names are actually out of Wikipedia except some famous people and therefore this method does not apply to most of the people on the web.</p><p>To assign weights to the features that indicate their contribution in resolving the person name's identity, we employed two weighting models. Most of the existing methods use TFIDF as feature weights. Though simple, TFIDF may not well represent the feature's relevance to the query name as well as the content of the text. Some researchers use information extraction method to extract all the related entities. For example, if the sentence "George Bush is the former president of the U.S." fits a pattern, "the former president" will be extracted as the profession of George Bush. But, pattern-based methods normally lead to low recall due to its difficulty to enumerate all the highly accurate patterns between elements. Our method views a feature's contribution to person name disambiguation from two different perspectives. First, the feature should be relevant or related to the query name; second, the feature should represent the content of the text. And accordingly, we employ two weighting models to measure feature relevance in these two regards.</p><p>In this paper, we first introduce the related works in Section 2; then we describe complementing the conventional bag-of-words or named entity based features with Wikipedia concepts in Section 3.1. And in Section 3.2 two feature weighting models are introduced. One is the model to measure feature relevance to the query name and the other is the relevance to the text content. Section 3.3 and Section 3.4 present our method to calculate similarity measure and our clustering algorithm, respectively. The experiment results on the WePS datasets are shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Web person name disambiguation is also viewed as cross-document co-reference problem in the many previous work. Bagga et al. <ref type="bibr" coords="2,324.77,630.72,16.60,10.46" target="#b11">[12]</ref> employed co-occuring word vectors to calculate similarity between entity names. Niu et al. <ref type="bibr" coords="2,387.89,642.68,16.60,10.46" target="#b10">[11]</ref> extended Bagga's method through information extraction. Mann and Yarowsky <ref type="bibr" coords="2,385.77,654.64,16.60,10.46" target="#b9">[10]</ref> proposed the clus-tering method based on extracted biographic data. However, Niu and Mann's methods were only evaluated on manually generated test data and mainly focus on person name disambiguation. Wan et al. <ref type="bibr" coords="3,248.18,141.42,16.60,10.46" target="#b14">[15]</ref> took the assumption that a query entity of a person usually omits the middle name and implemented a person name disambiguation system called "WebHawk". Our approach will be able to deal with more general situations.</p><p>Bekkerman and McCallum <ref type="bibr" coords="3,263.00,177.33,11.62,10.46" target="#b2">[3]</ref> focused on social network to find documents that refer to a particular person through two methods: one is based on a link structure and the other used agglomerative/conglomerate double clustering. Bunescu and Pasca <ref type="bibr" coords="3,468.98,201.24,11.62,10.46" target="#b3">[4]</ref> and Cucerzan <ref type="bibr" coords="3,194.41,213.19,11.62,10.46" target="#b4">[5]</ref> used Wikipedia knowledge to disambiguate named entities. However, different from our approach, they try to "map" the surface names in the text to a Wikipedia entry. Due to the limitation of the coverage of the wikipedia entries of people, this method cannot be applied to resolve the majority of the people who are not famous enough to be included in Wikipedia.</p><p>Recently Yoshida et al. <ref type="bibr" coords="3,245.29,273.01,16.60,10.46" target="#b15">[16]</ref> proposed a two-stage clustering algorithm and further used the bootstrapping algorithm in the second stage <ref type="bibr" coords="3,348.49,284.97,15.27,10.46" target="#b16">[17]</ref>. Their method relies heavily on named entity extraction. In Section 4 we will show that our approach that incorporates Wikipedia concepts outperforms those based on entities identified by conventional named entity recognition modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section we present our proposed web person name disambiguation approach, which consists of four main steps. The overview of our approach will be provided first, followed by detailed steps.</p><p>1. First, Wikipedia concepts are extracted as features of a web page, together with other conventional features such as bag-of-words and named entities. The web page is converted into a feature vector based on the three types of features extracted from the text.</p><p>2. Then the weight of each feature in the feature vector is estimated by two weighting models: one is the feature's relevance to the query name, and the other is the relevance to the text content. Each feature in a vector is measured by its TFIDF score and weights under two models.</p><p>3. After that the similarity score between two different pages containing the same query name is calculated through their feature vectors based on two similarity measures: cosine similarity and overlap similarity.</p><p>4. Finally, web pages referred to the same entity are clustered according to the pairwise similarity score calculated in the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wikipedia Concept Extraction</head><p>As mentioned in Section 2, much of the existing work takes named entities as important features. We, in addition, include Wikipedia concepts(or called "Wikipedia elements") extracted from the text in our feature set. We first extract all the manually edited entries from Wikipedia and build a Wikipedia concept dictionary. Given a web page (with html tags removed), the FSA (Finite State Automata) is used to extract string sequences in the text that match the Wikipedia concepts in the dictionary. In order to avoid overlapping, we use the maximum matching. For instance, both "People's Republic of China" and "China" are Wikipedia concepts. Since "People's Republic of China" contains the string "China" in it, only the maximum match "People's Republic of China" is extracted as a Wikipedia concept feature. These features together with the bag-of-words and named entities of person, location and organization names recognized with the Stanford NER tool (http://nlp.stanford.edu/ner/index.shtml) form a feature vector that represents the content of the web page.</p><p>Therefore, our extended feature set has three types of features in all: Wikipedia concepts, bag-of-words and named entities.</p><p>Compared with bag-of-words and named entities, using Wikipedia concepts offers the following merits:</p><p>1. Wikipedia is a large, well-organized dictionary for named entities. For example, "Support Vector Machines" is treated as three different words under the bag-of-words model. However, with Wikipedia, this term is rather recognized as a single concept since Wikipedia has a manually edited entry for it.</p><p>2. Wikipedia's redirect pages can help find other alternative names for an entity <ref type="bibr" coords="4,466.49,308.79,10.58,10.46" target="#b3">[4]</ref>. For example, the redirect pages of "United States" correspond to acronyms (U.S.A., U.S., USA, US), Spanish translations (Los Estados Unidos, Estados Unidos), misspellings (Untied States) or synonyms (Yankee land).</p><p>3. Wikipedia's disambiguation pages can guide the system to disambiguate a number of entities. For example, the disambiguation page for the name "Michael Jordan" lists 8 associated entities(people). If there is a name "Michael Jordan" in a web page and it is closely related to one of the 8 people, it can help the system to make a decision.</p><p>Our experiments on the WePS dataset shows in Section 4 that our system with Wikipedia features outperforms the ones with only bag-of-words and named entity features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Weighting Model</head><p>After the web page is converted into a feature vector, every feature in the vector is assigned a weight measuring its importance in recognizing the identity of the query name. Each word, named entity and Wikipedia concept, is called a "unit", denoted as u in this paper. These units vary from each other according to their corresponding feature weights. At the beginning, each unit is assigned a TFIDF score</p><formula xml:id="formula_0" coords="4,236.44,545.04,244.15,10.46">T F IDF (u) = tf (u) • -log df (u)<label>(1)</label></formula><p>where tf (u) is u's term frequency on the web page, and df (u) is u's document frequency on a large corpus. We use the Yahoo! search engine to collect the statistics of df (u). Then we propose two feature weighting models: the query relevance model and the content relevance model, to assign each unit a proper weight.</p><p>Query Relevance Weighting Model Query relevance weighting is to measure how relevant a feature is to the query name. Intuitively, relevant concepts of the query name can better represent its identity. In our method, we base our weighting model on the assumption that words or concepts that appear close to the query name in the text are more relevant than distant ones. The distance d(u) is measured by the minimum number of sentences between those contain query q and those contain u. d(u) = 0 if u and q co-exist in the same sentence. All 'u's with 0 ≤ d(u) ≤ d max are considered. We get d max = 11 from the training sets. Three polynomial functions are used:</p><formula xml:id="formula_1" coords="5,134.77,165.33,345.83,23.32">f 1 (u), f 2 (u) and f 3 (u). If d(u) &gt; d max , f 1 (u) = f 2 (u) = f 3 (u) = 0; if 0 ≤ d(u) ≤ d max ,</formula><p>they can be computed as Equation <ref type="formula" coords="5,254.01,189.24,4.98,10.46" target="#formula_2">2</ref>to Equation <ref type="formula" coords="5,310.19,189.24,3.74,10.46" target="#formula_5">4</ref>.</p><formula xml:id="formula_2" coords="5,269.07,210.04,207.65,24.94">f 1 (u) = 1 - d(u) d max (<label>2</label></formula><formula xml:id="formula_3" coords="5,476.72,216.78,3.87,10.46">)</formula><formula xml:id="formula_4" coords="5,262.95,243.85,217.63,24.94">f 2 (u) = 1 -( d(u) d max ) 2<label>(3)</label></formula><formula xml:id="formula_5" coords="5,262.95,277.66,217.63,24.94">f 3 (u) = (1 - d(u) d max ) 2<label>(4)</label></formula><p>Here we give an example. The following passage comes from a Wikipedia's article about Michael Jordan (http://en.wikipedia.org/wiki/Michael Jordan). This passage has the following ten sentences, which are numbered from one to ten.</p><p>1. In the 1990 -91 season, Jordan won his second MVP award after averaging 31.5 ppg on 53.9% shooting, 6.0 rpg, and 5.5 apg for the regular season.</p><p>2. The Bulls finished in first place in their division for the first time in 16 years and set a franchise record with 61 wins in the regular season.</p><p>3. With Scottie Pippen developing into an All-Star, the Bulls elevated their play. 4. The Bulls defeated the New York Knicks and the Philadelphia 76ers in the opening two rounds of the playoffs.</p><p>5. They advanced to the Eastern Conference Finals where their rival, the Detroit Pistons, awaited them. 6. However, this time the Bulls beat the Pistons in a surprising sweep. <ref type="bibr" coords="5,149.71,462.97,3.74,10.46" target="#b6">7</ref>. In an unusual ending to the fourth and final game, Isiah Thomas led his team off the court before the final minute had concluded.</p><p>8. Most of the Pistons went directly to their locker room instead of shaking hands with the Bulls. 9. The Bulls compiled an outstanding 15 -2 record during the playoffs, and advanced to the NBA Finals for the first time in franchise history, where they beat the Los Angeles Lakers four games to one.</p><p>10. Perhaps the best known moment of the series came in Game 2 when, attempting a dunk, Jordan avoided a potential Sam Perkins block by switching the ball from his right hand to his left in mid-air to lay the shot in.</p><p>The query name is "Michael Jordan", or "Jordan". Each sentence's distance weight is shown in Table <ref type="table" coords="5,210.09,594.86,3.74,10.46" target="#tab_2">3</ref>.2. Two sentences in the passage above contains the query name: No.1 and No.10, therefore, their units' d(u) is 0. The units in the 2nd and the 9th sentences get d(u) = 1 because sentences No.1 and No.10 are close to them, respectively. With this method we can get the other sentences' distances to the query name. The distance weights under three weighting functions are listed in the last three rows, respectively. A unit u's score under query relevance weighting model v p (u) is its TFIDF score T F IDF (u) multiplied by a distance weight function f (u). f (u) can either be f 1 (u), f 2 (u) or f 3 (u).</p><p>If u appears for more than one times within a document, all us' weights in different positions are accumulated together.</p><formula xml:id="formula_6" coords="6,243.31,301.88,233.41,21.85">v p (u) = T F IDF (u) • u∈d f (u) (<label>5</label></formula><formula xml:id="formula_7" coords="6,476.72,301.88,3.87,10.46">)</formula><p>where d is a web page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Relevance Weighting Model</head><p>The content relevance weighting model measures the feature's relevance to the content of the web page. It originates from the observation that query names in the web pages of similar content tend to refer to the same person. Specifically:</p><p>1. If the query name is the main topic of the page (say, the home page of a person), all units in the web page are related to the query. Then their relevance to the page content is similar to their relevance to the query name.</p><p>2. If the query name is not the main topic of the content, for example, a query name "David Beckham" mentioned in some news article talking about soccer games, though the query names like "David Beckham" are not the main topic of the articles, they tend to refer to the same person in the text with words like "soccer", and "game" etc..</p><p>Here we use a machine learning model to learn the unit(including the query name)'s relevance score to the text content. The machine learned model incorporates both content and some page structural features:</p><p>1 for i = 1 to N do 4:</p><p>compute the negative gradient r ik = gi -h k-1 (xi) 5:</p><p>end for 6:</p><p>Fit a regression tree to {r ik }, i = 1, . . . , N 7:</p><p>giving terminal regions R jk , j = 1, ..., J k , J k is the number of regions 8:</p><p>for j = 1 to J k do 9: compute</p><formula xml:id="formula_8" coords="7,134.77,236.17,345.82,34.71">τ jk = xi ∈ R jk (gi = h k-1 (xi)) |i : x i ∈ R jk | ,<label>(7) 10:</label></formula><p>average the residual in each terminal region. 11:</p><p>end for 12:</p><p>Update</p><formula xml:id="formula_9" coords="7,238.56,293.93,242.04,30.71">h k (x) = h k-1 (x) + η( J k j=1 τ jk I(x ∈ R jk )),<label>(8)</label></formula><p>where η is the shrinkage factor and I(.) in the indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13: end for</head><p>The Gradient Boosted Decision Tree (GBDT) <ref type="bibr" coords="7,338.53,379.17,10.79,10.46" target="#b7">[8,</ref><ref type="bibr" coords="7,350.98,379.17,8.30,10.46" target="#b6">7]</ref> is used for content weighting with the above features. To train the machine learning relevance model, 1.3 million popular web pages are collected as the content weighting training data to learn the features and compute the frequencies. In addition, 400,000 query-url pairs are collected for manual annotation. We can get a web page from each url, and a query can be viewed as one of its features (or units). Annotators judge the relevance of a query to a web page on a 5-point scale (Perfect, Excellent, Good, Fair, Bad). Then the 5-point scales are scored as 1.0(Perfect) to 0(Bad), respectively.</p><p>Each element in the training data is written as (x i , g i ) N i=1 (N is the size of training data) and we need to fit a function such that g i ≈ h(x i ), i = 1, . . . , N . The lost function between g and h is</p><formula xml:id="formula_10" coords="7,274.02,526.03,206.57,31.54">N i=1 |g i -h(x i )| 2<label>(6)</label></formula><p>We apply the gradient descent in functional space to minimize the discrepancy <ref type="bibr" coords="7,461.51,567.31,15.27,10.46" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The algorithm of GBDT regression is as Algorithm 1:</head><p>There are two parameters, M and η. They are estimated with cross validation on the content weighting training set.</p><p>Here we have the relevance score 0 ≤ r(u) ≤ 1 of a unit u (including the query name q). The higher value r(u) has, the more relevant the unit u is to the text content.</p><p>Since r(u) is the relevance of u to the text content, we can use it to estimate v r (u), which is the relevance of the unit u to the query name q.</p><formula xml:id="formula_11" coords="8,214.12,118.51,266.47,42.74">v r (u) =    1 if q = u 0 if r(q) &lt; θ and r(u) &lt; θ r(q)r(u) if r(q) ≥ θ and r(u) ≥ θ (9)</formula><p>where θ is a parameter and we get θ = 0.5 from the training set.</p><p>Here we have two weight models v p (u) and v r (u), and they are both used to estimate the relevance of u to the query name q. We set the weights v(u) to be the maximum of v p (u) and v r (u).</p><formula xml:id="formula_12" coords="8,252.46,227.59,228.13,11.36">v(u) = max{v p (u), v r (u)}<label>(10)</label></formula><p>For each unit u, we can get its relevance score v(u). All unit scores of a web page form a vector V . We can compute similarity between a pair of web pages through their vectors V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity Measures</head><p>Let V and V be two the feature vectors of two web pages with the same entity name. Two types of similarity measures are used: (1) cosine similarity</p><formula xml:id="formula_13" coords="8,254.89,349.41,225.70,24.02">Sim cos (V, V ) = V • V |V ||V |<label>(11)</label></formula><p>and ( <ref type="formula" coords="8,155.51,382.47,3.87,10.46" target="#formula_2">2</ref>) overlap similarity</p><formula xml:id="formula_14" coords="8,198.35,402.33,278.09,27.92">Sim overlap (V, V ) = u∈V,u∈V (v(u) + v (u)) w∈V v(w) + w ∈V v (w ) , (<label>12</label></formula><formula xml:id="formula_15" coords="8,476.44,410.56,4.15,10.46">)</formula><p>where each u is one of the common units shared by V and V ; w and w are all units in V and V , respectively. v(u), v (u), v(w) and v (w ) are the scores of u. w and w are computed on one of the two weighting models. The performance of these two similarity measures is compared and presented in our experiment section. The results show that the disambiguation result based on overlap similarity is better than that of cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Clustering</head><p>We employed the Hierarchical Agglomerative Clustering <ref type="bibr" coords="8,363.65,547.00,11.62,10.46" target="#b8">[9]</ref> algorithm to cluster documents with the same person name. Suppose C i and C j are two clusters. If there are two web pages V and V in C i and C j , respectively,</p><formula xml:id="formula_16" coords="8,271.61,592.08,72.15,11.35">V ∈ C i , V ∈ C j ,</formula><p>and they satisfy</p><formula xml:id="formula_17" coords="8,272.69,625.20,203.75,10.46">Sim(V, V ) &gt; γ, (<label>13</label></formula><formula xml:id="formula_18" coords="8,476.44,625.20,4.15,10.46">)</formula><p>C i and C j will be merged into one cluster. Where Sim(V, V ) can be computed with either Equation 11 or Equation <ref type="formula" coords="8,260.11,654.64,8.30,10.46" target="#formula_14">12</ref>. γ = 0.25 is tuned in the training sets.</p><p>Algorithm 2 pseudo-code of the clustering algorithm 1: C = {{1}, {2}, ..., {n}} (n is the number of web pages) 2: m ← n (m is the number of clusters) 3: while m &gt; 1 do 4:</p><formula xml:id="formula_19" coords="9,159.97,164.19,159.47,12.13">(C i , C j ) ← arg max C i ,C j ∈C Sim(C i , C j ),</formula><p>where Sim(C i , C j ) = max x∈C i ,y∈C j Sim(V x , V y ) 5:</p><p>if Sim(Ci, Cj) &lt;= γ then goto 10 6:</p><formula xml:id="formula_20" coords="9,138.55,198.30,75.11,31.34">C i ← C i ∪ C j 7: C ← C \ C j 8:</formula><p>m ← m -1 9: end while 10: Output the clustering results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we will evaluate our approach on the WePS datasets. The WePS-1 and WePS-2 datasets are used as the training and the test data for evaluation first. And our system's performance on the WePS-3 campaign is also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>There are totally 76 query names in WePS-1. They are randomly selected from US Census, ambiguous person names in the English Wikipedia and program committee listing of a Computer Science conference <ref type="bibr" coords="9,272.76,408.85,10.58,10.46" target="#b0">[1]</ref>. For each query name, at most top 100 web pages returned by Yahoo! search engine are collected for disambiguation, so there are 6445 pages in total. In WePS-2, 30 query names are selected. Each of the query name has at most 150 pages from top search results, and there are 3444 web pages in total. In WePS-3 there are 300 query names, with top 200 web pages returned by Yahoo! for each query name, yielding 57355 evaluation pages in total. The WePS program committee asked annotators to manually label document clustering for each query name. The system's performance is measured by comparing the clustering generated from the algorithm with human labeled gold-standard test data. Two evaluation metrics are used in WePS: the Purity F-score and the B-Cubed F-score <ref type="bibr" coords="9,350.63,516.45,10.58,10.46" target="#b1">[2]</ref>.</p><p>The WePS-1 datasets are used as the training data in our experiments to learn the similarity metrics and tune the parameters. The WePS-2 datasets are used as the test data. We submit our system's outputs on the WePS-3 datasets for this campaign, without any modification on the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>In this sub-section, we will evaluate our approach through three aspects. First of all, the experimental results of the system under different feature sets and similarity measures are provided. Then our proposed two relevance weighting models will be evaluated. Finally we will show our system's performance on the WePS-3 datasets. Results on the WePS-3 datasets In WePS-3 campaign, we evaluated our system on the WePS-3 test datasets. Table <ref type="table" coords="11,247.89,412.42,4.98,10.46" target="#tab_2">3</ref> shows our results. "Best" and "median" are the best and the median F Cubed scores among all submissions, respectively. We have submitted three groups of results named "YHBJ-1", "YHBJ-2" and "YHBJ-3". YHBJ-1 and YHBJ-2 are both based on the extended feature sets including bag-of-words and Wikipedia concepts, with query relevance and content relevance weighting models. YHBJ-1 sets γ = 0.3 as the clustering threshold and YHBJ-2 sets γ = 0.25(see Equation <ref type="formula" coords="11,434.60,472.20,7.89,10.46" target="#formula_17">13</ref>). YHBJ-3 does not use content relevance weighting model so its performance is lower than the other two submissions. From the table we can see that YHBJ-2 is the best result among all WePS-3 submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Our approach to web person name disambiguation extends existing bag-of-words features with Wikipedia concepts. In order to measure feature weights for calculating document clustering similarity, we employ two weighting models that take into account feature relevance to the query name and text content. Experiment results on the WePS-3 task 1 confirms the effectiveness of our method which outperforms all other competing algorithms.</p><p>In the future, we can make the following further improvement on this method: 1. There are no more than 200 top ranked web pages for each query name of the WePS datasets, but the large number of the rest of the search results contain a great deal of information which can help to get better clusters. We plan to build up a model to use the information returned by search engines as much as possible;</p><p>2. Currently, our distance weighting functions are applied to entities and Wikipedia concepts. We in the future plan to leverage semantic information in our weighting function;</p><p>3. Machine learning models can be used to calculate similarity scores in order to get more accurate estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,210.73,288.59,193.82,9.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparisons of different functions and models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,186.18,114.58,242.89,76.21"><head>Table 1 .</head><label>1</label><figDesc>Distance Weighting for the Query Name "Michael Jordan"</figDesc><table coords="6,195.72,135.54,223.89,55.25"><row><cell cols="2">Sentence # 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell cols="2">9 10</cell></row><row><cell>d(u)</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell cols="11">f 1 (u) 1.00 0.90 0.80 0.70 0.60 0.60 0.70 0.80 0.90 1.00</cell></row><row><cell cols="11">f2(u) 1.00 0.99 0.96 0.91 0.84 0.84 0.91 0.96 0.99 1.00</cell></row><row><cell cols="11">f 3 (u) 1.00 0.81 0.64 0.49 0.36 0.36 0.49 0.64 0.81 1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,532.95,345.84,132.14"><head></head><label></label><figDesc>. Structural characteristics of unit u in a web page d, including the frequency of u in d's title, meta key words, outgoing another text, headings, table headers, table body, lists, etc; 2. Visual characteristics of u in d, including the frequency of u in d's visual title, relative font size, and when u is in bolding, italics, a changed color with respect to the</figDesc><table coords="6,134.77,593.26,345.84,71.84"><row><cell>Algorithm 1 pseudo-code of GBDT regression</cell></row><row><cell>1: Initialize h 0 (x) = 2: for k = 1 to M (number of trees in gradient boosting) do N g i i=1 N</cell></row><row><cell>3:</cell></row><row><cell>context/background, etc;</cell></row><row><cell>3. General characteristics of d, including d's category, domains, document struc-</cell></row><row><cell>tures, language, length, url depth, etc;</cell></row><row><cell>4. Non-structural characteristics, including TFIDF score, offset, etc;</cell></row><row><cell>5. Corpus level characteristics, including document frequency, dominant category,</cell></row><row><cell>etc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,235.11,541.45,145.10,85.35"><head>Table 3 .</head><label>3</label><figDesc>Results on the WePS-3 datasets</figDesc><table coords="11,265.94,560.60,82.98,66.20"><row><cell cols="2">Submission ID F Cubed</cell></row><row><cell>Best</cell><cell>0.55</cell></row><row><cell>Median</cell><cell>0.40</cell></row><row><cell>YHBJ-1</cell><cell>0.52</cell></row><row><cell>YHBJ-2</cell><cell>0.55</cell></row><row><cell>YHBJ-3</cell><cell>0.50</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features and Similarity Measures</head><p>We have introduced in Section 3.1 that there are three types of features (or units): bag-of-words, named entities and Wikipedia concepts. Two similarity measures are also described in Section 3.3. Evaluation results about these two aspects are first presented, with TFIDF as the weight on each feature. The results with different feature sets and similarity measure are shown in Table <ref type="table" coords="10,438.37,165.33,3.74,10.46">2</ref>. F Cubed and F P urity refer to B-Cubed F-score and Purity F-score, respectively. "Cosine" and "Overlap" mean cosine similarity and overlap similarity. "Bag-of-words&amp;Named Entity" means both bag-of-words and named entity features are used. "All" means all the three types of features are used. From this table we can get four observations:</p><p>1. If we only use bag-of-words features, the results are not satisfactory. While combined with named entities and Wikipedia concept features, we can get better results;</p><p>2. Using all three types of features does not yield much better results than two types because there are many overlappings between named entities and Wikipedia concepts (a Wikipedia concept can also be viewed as a named entity); 3. The system based on overlap similarity outperforms the one based on cosine similarity; 4. The system can get the best results with overlap similarity and under bag-ofwords and Wikipedia features(F Cubed = 0.74 and F P urity = 0.81). Therefore, they are used to do the experiments in the following sub-sections.</p><p>Evaluation of the Weighting Models We will evaluate our proposed two relevance weighting models in this sub-section.</p><p>First, we evaluate the query relevance weighting model. There are three weighting functions: f 1 , f 2 and f 3 (see Equation <ref type="formula" coords="10,288.84,558.48,4.98,10.46">2</ref>to Equation <ref type="formula" coords="10,344.96,558.48,3.60,10.46">4</ref>). The experimental results using these three functions as well as without using weighting functions are shown on the lefthand-side of Figure <ref type="figure" coords="10,216.18,582.40,3.74,10.46">1</ref>. "1","2" and "3" are query relevance weighting functions f 1 , f 2 and f 3 , respectively. "0" means no weighting functions (only TFIDF weights). We can see from this figure that the performance of the system has been substantially improved with query relevance weighting functions. The system performs almost equally well under three functions. We compare the system's performance under different weighting models. On the right-hand-side of Figure <ref type="figure" coords="10,235.62,654.64,3.74,10.46">1</ref>, "No Models" means we only use the TFIDF weight. "Query"</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,373.36,337.83,9.41;12,150.95,384.31,329.59,9.41;12,150.95,395.27,215.03,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,336.00,373.36,144.44,9.41;12,150.95,384.31,190.85,9.41">The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task</title>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,357.10,384.31,123.44,9.41;12,150.95,395.27,147.88,9.41">the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,406.00,337.84,9.41;12,150.95,416.96,226.82,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,336.09,406.00,144.36,9.41;12,150.95,416.96,142.27,9.41">Weps 2 evaluation campaign: Overview of the web people search clustering task</title>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,309.56,416.96,18.49,9.41">WWW</title>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,427.68,337.83,9.41;12,150.95,438.65,191.85,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,305.41,427.68,175.03,9.41;12,150.95,438.65,50.74,9.41">Disambiguating web appearances of people in a social network</title>
		<author>
			<persName coords=""><forename type="first">Ron</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,218.59,438.65,18.49,9.41">WWW</title>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,449.37,337.83,9.41;12,150.95,460.32,166.05,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,280.83,449.37,199.61,9.41;12,150.95,460.32,32.49,9.41">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,200.01,460.32,19.12,9.41">EACL</title>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,471.06,337.83,9.41;12,150.95,482.01,131.20,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,218.88,471.06,243.65,9.41">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName coords=""><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,482.01,26.37,9.41">EMNLP</title>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,492.74,337.84,9.41;12,150.95,503.69,329.61,9.41;12,150.95,514.66,275.97,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,437.87,492.74,42.58,9.41;12,150.95,503.69,243.27,9.41">Psnus: Web people name disambiguation by simple clustering with rich features</title>
		<author>
			<persName coords=""><forename type="first">Ergin</forename><surname>Elmacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yee</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Su</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongwon</forename><surname>Lee1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,411.44,503.69,69.13,9.41;12,150.95,514.66,208.81,9.41">the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,525.38,194.88,9.41" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,218.93,525.38,114.58,9.41">Greedy Function Approximation</title>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,536.10,281.77,9.41" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,218.93,536.10,103.77,9.41">Stochastic Gradient Boosting</title>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,546.83,337.93,9.41;12,150.95,557.79,310.07,9.41" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,357.16,546.83,123.38,9.41;12,150.95,557.79,158.23,9.41">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct coords="12,142.24,568.51,338.21,9.41;12,150.95,579.48,138.83,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,300.14,568.51,162.51,9.41">Unsupervised personal name disambiguation</title>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,579.48,43.40,9.41">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,590.20,338.21,9.41;12,150.95,601.16,329.61,9.41;12,150.95,612.12,37.35,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,308.47,590.20,171.97,9.41;12,150.95,601.16,234.66,9.41">Weakly supervised learning for cross-document person name disambiguation supported by information extraction</title>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><forename type="middle">K</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,403.15,601.16,13.81,9.41">ACL</title>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,622.84,338.21,9.41;12,150.95,633.80,164.06,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,214.07,622.84,262.27,9.41">Entity-based cross-document coreferencing using the vector space model</title>
		<author>
			<persName coords=""><forename type="first">Deepa</forename><surname>Paranjpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.66,633.80,49.35,9.41">COLING-ACL</title>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,644.52,338.21,9.41;12,150.95,655.49,133.42,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,214.77,644.52,265.68,9.41;12,150.95,655.49,30.24,9.41">Learning document aboutness from implicit user feedback and document structure</title>
		<author>
			<persName coords=""><forename type="first">Deepa</forename><surname>Paranjpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,197.51,655.49,19.73,9.41">CIKM</title>
		<imprint>
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,118.36,338.21,9.41;13,150.95,129.32,205.20,9.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,349.41,118.36,131.03,9.41;13,150.95,129.32,25.92,9.41">Searching for people on web search engines</title>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,183.81,129.32,92.55,9.41">Journal of Documentation</title>
		<imprint>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,140.28,338.21,9.41;13,150.95,151.24,215.49,9.41" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,357.65,140.28,122.80,9.41;13,150.95,151.24,61.15,9.41">Person resolution in person search results: Webhawk</title>
		<author>
			<persName coords=""><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binggong</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,229.77,151.24,19.73,9.41">CIKM</title>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,162.20,338.21,9.41;13,150.95,173.15,266.10,9.41" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,434.84,162.20,45.60,9.41;13,150.95,173.15,181.45,9.41">Person name disambiguation on the web by two-stage clustering</title>
		<author>
			<persName coords=""><forename type="first">Minoru</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masaki</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,348.84,173.15,18.49,9.41">WWW</title>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,184.12,338.21,9.41;13,150.95,195.07,194.98,9.41" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,434.84,184.12,45.60,9.41;13,150.95,195.07,113.99,9.41">Person name disambiguation by boostrapping</title>
		<author>
			<persName coords=""><forename type="first">Minoru</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masaki</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,281.70,195.07,20.54,9.41">SIGIR</title>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
