<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.82,116.95,335.73,12.62;1,174.98,134.89,265.39,12.62">WePS-3 Evaluation Campaign: Overview of the Online Reputation Management Task</title>
				<funder ref="#_WguxuAY">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.11,172.98,66.03,8.74"><forename type="first">Enrique</forename><surname>Amig√≥</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.69,172.98,58.65,8.74"><forename type="first">Javier</forename><surname>Artiles</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.89,172.98,59.99,8.74"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.43,172.98,66.83,8.74"><forename type="first">Damiano</forename><surname>Spina</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.82,172.98,38.20,8.74"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.26,184.93,61.74,8.74"><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Llorente &amp; Cuenca</orgName>
								<orgName type="institution" key="instit2">Communication Consultants Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.82,116.95,335.73,12.62;1,174.98,134.89,265.39,12.62">WePS-3 Evaluation Campaign: Overview of the Online Reputation Management Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C919A729788E6C1AF186C6D265C82FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes the definition, resources, evaluation methodology and metrics, participation and comparative results for the second task of the WEPS-3 evaluation campaign. The so-called Online-Reputation Management task consists of filtering Twitter posts containing a given company name depending of whether the post is actually related with the company or not. Five research groups submitted results for the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>People share opinions about products, people and organizations by means of web sites such as blogs, social networks and product comparison sites <ref type="bibr" coords="1,457.89,476.94,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="1,470.07,476.94,7.01,8.74" target="#b5">6]</ref>. Online reputation management (ORM) consists of monitoring media, detecting relevant contents, analyzing what people say about an entity and, if necessary, interact with costumers. Negative comments in online media can seriously affect the reputation of a company, and therefore online reputation management is an increasingly important area of corporate communication.</p><p>Perhaps the most important bottleneck for reputation management experts is the ambiguity of entity names. For instance, a popular brand requires monitoring hundreds of relevant blog posts and tweets per day; when the entity name is ambiguous, filtering out spurious name matches is essential to keep the task manageable.</p><p>WePS-3 ORM task consists of automatically filter out tweets that do not refer to a certain company. In particular, we focus on the Twitter social network because (a) it is a critical source for real time reputation management and (b) also because ambiguity resolution is particularly challenging: tweets are minimal and little context is available for resolving name ambiguity. This task is a natural extension of WePS evaluation campaigns, which have been previously focused on person name ambiguity in Web Search results; with the ORM task, WePS-3 extends its scope to cover other relevant type of named entity. Our task is related to the TREC Blog Track <ref type="bibr" coords="2,366.38,155.86,9.96,8.74" target="#b6">[7]</ref>, which focused on blog posts. However, in that case, systems dealt with information needs expressed by queries, rather than focusing on a name disambiguation problem.</p><p>2 Task definition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Twitter</head><p>Twitter is a relatively new social networking site <ref type="bibr" coords="2,341.77,251.70,10.52,8.74" target="#b3">[4]</ref> referred to as a microblogging service. Its particularity is that posts do not exceed 140 characters and there are no privacy conditions. Therefore, Twitter reflects opinions in real time and it is very sensitive to burstiness phenomena.</p><p>Tweets are particularly challenging for disambiguation tasks given that the ambiguity must be sorted out using a very small textual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ambiguity</head><p>The idea of ambiguity is actually quite fuzzy. For instance, suppose that we are interested in a certain car brand. If the brand name is common, of course, occurrences that refer the common word sense are not related to the brand. But let us suppose that the brand sponsors a football team. We could think that the referred organization is actually the football team, but not the brand. But experts could be interested on monitoring these occurrence given that they have spend money to be mentioned in this way. In addition, experts might be interested on mentions to the brand generically, but not on specific products (which might be handled separately). In short, the ambiguity is closely related with the concept of relevance, which is inherently fuzzy.</p><p>For evaluation purposes, one option consists of defining the relevance criteria for each entity just like in other competitive tasks as TREC <ref type="bibr" coords="2,397.87,490.27,9.96,8.74" target="#b6">[7]</ref>. However, interpreting the relevance criteria can be difficult even for humans. Probably, systems will not able to tackle this issue. Indeed, interpreting the relevance criteria can be difficult even for humans.</p><p>In this competition we opt for a lax interpretation of relevance, considering ambiguity at a lexical level: the sense of the name must be derived from the company, even if the sentence does not explicitly talk about the company. Table <ref type="table" coords="2,475.61,562.00,4.98,8.74" target="#tab_0">1</ref> illustrates this idea for the Apple company.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Input and output data</head><p>The first decision when defining system input and output is whether systems should be able to use a training set for each of the companies included in the test set. There are two possible scenarios: an ORM company that provides individualized services to a limited number of clients, or an online system that accepts any company name as input. In the first scenario, the system will probably be trained for each of the clients. In the second scenario, this is not viable, as the system must immediately react to any imaginable company name. We have decided to focus on the second scenario, which is obviously the most challenging. Therefore, the set of organization names in the training and test corpora are different.</p><p>For each organization in the dataset, systems are provided with the company name and its homepage URL. This web page contain textual information that allows systems to model the vocabulary associated to the company. The input information per tweet consists of a tuple containing: the tweet identifier, the entity name, the query used to retrieve the tweet, the author identifier and the tweet content.</p><p>3 Data set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trial Corpus</head><p>The trial corpus consists of 100 tweets per organization name. 24 companies were selected; 18 from English speaking countries and 6 from Spanish speaking countries. Most of these entities were extracted from a Twitter Brand Index that appears in the blog "Fluent Simplicity"<ref type="foot" coords="3,306.64,457.76,3.97,6.12" target="#foot_0">4</ref> . Table <ref type="table" coords="3,344.29,459.33,4.98,8.74" target="#tab_1">2</ref> enumerates these entities and the category associated in the brand index.</p><p>The first observation was that identifying companies for our purposes was not a trivial task. The first reason is that many companies are not usually mentioned in Twitter. Tweets tend to focus on certain issues. For instance, some frequents issues are entertainment technologies, movies, travel, politics, etc. Therefore, most companies do not have enough presence in Twitter to be included in our test bed. In addition, many company names are either too ambiguous or not ambiguous at all. For instance, "British Airways" is not ambiguous. However, in order to ensure a high recall, we should use the query term "British" (e.g. "I fly with British"). But in this case, 100 tweets would not be enough to obtain true samples. Notice that this does not imply that our systems would not be useful to monitor British Airways. The key issue is that we need reasonably ambiguous company names in order to make the annotation task feasible. In short, the company selection is very costly, given that it requires to retrieve and check tweets manually to analyze their ambiguity. For each company, the first 100 tweets retrieved by the corresponding query have been annotated directly by the task organizers. During the annotation, we observed that the best approach consisted of detecting key terms associated to the company. In some cases these key terms were related with a certain event that happens just before the retrieval process (such as, for instance, a new product launched by Palm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and test corpus</head><p>The initial purpose was to define a methodology for the company selection. Fluent Simplicity was not enough to pick them. The next attempt consisted of filtering automatically the companies included in DBpedia<ref type="foot" coords="4,395.69,553.91,3.97,6.12" target="#foot_1">5</ref> which is a knowledge base that extracts structured information from Wikipedia. The automatic filter consisted of detecting company names that match common names. This should ensure the ambiguity of names. However, the presence in Twitter was less frequent than companies from the Twitter Brand Index. In addition, again, some company names were either too much ambiguous or not ambiguous at all. Finally, the list was expanded with a few entities that are not exactly a company, such as sport teams or music bands, which are very common in Twitter.</p><p>Although the original plan was to annotate around 500 entities, the training and test corpus finally contains 100 company names. We have discarded Spanish companies given that, for now, Twitter is still far less popular than in English speaking countries. Table <ref type="table" coords="5,248.57,155.86,4.98,8.74" target="#tab_4">5</ref> shows the entities selected for the training and test corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mechanical Turk</head><p>The training and test corpora have been annotated by means of Mechanical Turk services. The advantages of using this service for annotation have been reported in previous work <ref type="bibr" coords="5,248.44,268.70,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="5,260.61,268.70,3.87,8.74" target="#b2">3</ref>] Figure <ref type="figure" coords="5,302.03,268.70,4.98,8.74" target="#fig_0">1</ref> shows an example of our formularies for Mechanical Turk. Each hit contains five tweets from the same company name to be annotated. It also includes a brief description for the company and the annotator can access the company web page. In order to ensure that tweets have been annotated, there is no default value for the annotation. The annotation options for each tweet were "related", "non related" or "undecidable". Each hit has been redundantly annotated by five Mecahnical Turk workers. The form includes the following instructions to annotators:</p><p>The next table contains tweets that apparently mention a company. The task consists of determining whether each tweet mentions the company (button "related"), does not mention the company (button "non related") or there is not enough information to decide it (button "undecidable"). This page provides the company name and its URLs. For each tweet the table includes the tweet author and content. Notice that most tweets contain links that can help you make this decision. Find below some examples for the Apple company.</p><p>902 annotators participated in the annotation of 43730 tweets. Given that not all company names had the same presence in Twitter and some tweets have been discarded, the number of annotated tweets per entity is variable; between 334 and 465 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agreement analysis</head><p>Figure <ref type="figure" coords="5,165.58,534.73,4.98,8.74" target="#fig_1">2</ref> shows the relationship between the average agreement for single turkers versus the number of annotated tweets. The averaged agreement of single turkers is computed as the number of annotators that have taken the same decision (related, non related or undecidable). As the figure shows, most annotators have an average agreement with other annotators between 3 and 4.5. That means that in most cases at least 3 annotators have taken the same decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ground truth</head><p>We have followed the following criteria to decide the final annotation (related or non related) for each tweet:  -If four or five annotators take the same decision, then this corresponds with the ground truth. This set represents 58% of tweets. -If three or more annotators agree and there is no more than one disagreeing annotator, then we also consider that it is the ground truth. We consider that two annotators disagree when one says "related" and the other says "non related". This sample set represents the 21% of cases. -The most controversial case is when three annotators are contradicted by two annotators. These are 14% of cases. We analyzed manually 100 samples and we found that the three votes corresponded with the ground truth in around 80% of cases. At the risk of introducing a bit of noise in the corpus, we have considered the majority of votes as the ground truth. In any case, system evaluation results did not change substantially when considering these cases. -In a 0.1% of cases, there were less than 2 related and non related votes, in favor of undecidable votes. We have directly discarded these cases. -In 7% of cases there were two related votes and two, related votes and one undecidable. These cases have been meta-evaluated manually by the task organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Entity ambiguity</head><p>Figure <ref type="figure" coords="7,167.00,361.72,4.98,8.74" target="#fig_2">3</ref> shows the distribution of ambiguity across company names. That is, the ratio of related tweets for each entity. The company names have been sorted according to their ratio. As the figure shows, although we have tried to select names with medium ambiguity, there is a great variability of ambiguity in the corpus and there is an important amount of companies with low occurrence in tweets. This has important implications in the evaluation metric definition. It is desirable to check to what extent systems are able to detect the ratio of related tweets for each single company name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation metrics</head><p>Basically, this task can be considered as a classification task. The most natural way of evaluating is the accuracy measure. That is, in how many cases the system output matches the annotation. However, this metric does not consider the distribution of related and non related tweets within the correct outputs. That is, for a high ambiguous company name, even only a few related tweets appeared in the corpus, the decisions taken in these cases are crucial. This issue has relevance in this corpus given that most of company names have a very high or low ambiguity. We consider this aspect by computing also the precision and recall over both the related and non related classes. In addition, the F measure of precision and recall is computed for each company name and class. Another important aspect is how to consider the cases in which the system does not return any results. In the case of accuracy, these are fails. In term of precision and recall measures, these cases affect by decreasing the recall for the corresponding class (related or non related). The accuracy metric assigns a relative weight to the related and non related classes depending on the distribution of both classes in each company name. That is, the more the tweets are related to the company, the more this class is considered in the evaluation process. However, this weighting criterion is arbitrary. In addition, the combination of precision and recall measures by means of the F measure over each class assumes that both precision and recall have the same weight. The final ranking could change if we employed a different metric weighting criterion.</p><p>For this reason, for each system pair we check to what extent the improvement is robust across potential metric weighting schemes by applying the UIR measure <ref type="bibr" coords="9,134.77,203.82,9.96,8.74" target="#b0">[1]</ref>. This measure was also employed in WEPS2 campaign <ref type="bibr" coords="9,385.86,203.82,9.96,8.74" target="#b1">[2]</ref>. Being T ‚àÄm.a&gt;b the number of company names such us System a improves System b for all the four metrics, and being T the total number of company name (test cases), UIR is defined as follows:</p><formula xml:id="formula_0" coords="9,238.23,260.02,137.20,22.31">U IR(a, b) = T ‚àÄm.a&gt;b -T ‚àÄm.b&gt;a T</formula><p>The more System b improves System a for all metrics (or there are contradictory results between metrics), the more U IR(a, b) increases (decreases). We have combined the four precision and recall metrics with UIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Participation and evaluation results</head><p>16 runs have participated in the task. Table <ref type="table" coords="9,323.84,369.76,4.98,8.74" target="#tab_2">3</ref> shows the evaluation results sorted by accuracy. Two baseline systems have been added to the ranking, consisting of tagging all tweets as related (Baseline R ) or non related(Baseline N R ). The first observation is that the ranking discriminates participating groups.</p><p>The top system is LSIR-EPFL. The main particularity of this system is the use of additional resources for classification which include Wordnet, meta-data from the web page, Google results, and user feedback (just some words). Their experiments showed that even excluding the user feedback, they obtained high accuracy. According to their experiment description, using the same approach but considering just the company web page, the evaluation results would descend to the middle of the ranking.</p><p>The system SINAI (located in the middle of the ranking) also employs additional resources, but they basically consist of named entities extracted from the tweets while LSIR-EPFL employs all the tweet content. A deeper analysis showed that there is a great variability of evaluation results for this system across company names. For some company names, the system improves the top ranked system, while for other names, it achieves very low results. This variability is not related with the ratio of related tweets for the company name. Therefore it is not due to classification thresholds. In short, the SINAI evaluation results suggest that considering the named entities appearing in tweets is appropriate for certain company names.</p><p>The second best system is ITC-UT, which uses an initial classification step to predicting the ambiguity of the company name, according to some evidences. The classification step consisted of a set of rules based on Part of Speech tagging and Named Entity recognition. Given that the system variants do not differ from each other substantially, it is difficult to know what aspect lead the system to get ahead other systems. However, this result shows that it is possible to obtain an acceptable accuracy just considering linguistic aspects of the company mention.</p><p>The system UVA makes a relevant contribution to the task results. This system does not employ any resource related with the company, such as the web page or Google results. Although the accuracy results are not very high, the Related Ratio Deviation is as low as the systems located at the top of the ranking. This result suggests that a general classifier can be employed to predict the presence of any company in Twitter.</p><p>Finally, the Kalmar system employs a bootstrapping method starting from the vocabulary of the web page. The global accuracy results are not very high, but a deeper analysis shows that this approach improves the best system in terms of F measure over the related class when just a few tweets are relevant in the collection. In general, systems tend to achieve low F measure over the related class when the related class is not frequent. This does not happen in the case of Kalmar system. In other words, Kalmar results suggests that bootstrapping is appropriate for company names with high ambiguity.</p><p>Table <ref type="table" coords="10,177.53,585.38,4.98,8.74" target="#tab_3">4</ref> shows the UIR results. The third column represents the set of systems that are improved by the corresponding system with no dependence on metric evaluation weightings. As the table shows, the top system, in addition to achieve higher Accuracy, improves robustly most of the other systems. Of course, although a baseline system (all tweets are non related) appears in the middle of the ranking, it does not improve robustly any other system: it is just an effect of the metric combination used to rank systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This competition is the first attempt to define a shared task to solve the problem of company name disambiguation in social networks (Twitter in our case). Our conclusion is that it is a task feasible to evaluate, given that we have obtained an acceptable agreement between Mechanical Turk annotators. A corpus with around 20,000 annotated tweets is now available for future benchmarking.</p><p>The evaluation results have shed some light on how to solve the task: (i) Considering additional sources like Google results or wordnet seems to be useful; (ii) linguistic aspects of the company mention are also very indicative (iv) It is possible to define a general approach to estimate approximately the presence of a company name in Twitter (v) Finally, bootstrapping methods seems to be useful, specially for highly ambiguous company names. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,192.78,401.39,229.80,7.89;6,163.68,129.28,288.00,257.35"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of form for Mechanical Turk annotation</figDesc><graphic coords="6,163.68,129.28,288.00,257.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,145.17,643.58,325.02,7.89;6,163.68,442.15,288.00,186.66"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average agreement for single turkers versus number of annotated tweets.</figDesc><graphic coords="6,163.68,442.15,288.00,186.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,198.64,304.39,218.07,7.89;8,181.68,116.83,252.00,172.79"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ambiguity distribution across company names</figDesc><graphic coords="8,181.68,116.83,252.00,172.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,147.81,118.78,319.75,62.61"><head>Table 1 .</head><label>1</label><figDesc>...you can install 3rd-party apps that haven't been approved by Apple.. TRUE ...RUMOR: Apple Tablet to Have Webcam, 3G... TRUE ...featuring me on vocals: http://itunes.apple.com/us/album/... TRUE ...Snack Attack: Warm Apple Toast... FALSE ...okay maybe i shouldn't have made that apple crumble... FALSE Examples of tweet disambiguation for the company Apple</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,176.15,118.78,263.06,271.23"><head>Table 2 .</head><label>2</label><figDesc>Selected tweets for trial corpus</figDesc><table coords="4,176.15,118.78,263.06,260.32"><row><cell>Entity name</cell><cell cols="2">Query Language</cell><cell>Category</cell></row><row><cell>Best Buy</cell><cell cols="2">best buy English</cell><cell>Online-shopping</cell></row><row><cell>Leap frog</cell><cell>leapfrog</cell><cell>English</cell><cell>toys</cell></row><row><cell>Overstock</cell><cell cols="2">overstock English</cell><cell>Online-shopping</cell></row><row><cell>Palm</cell><cell>palm</cell><cell>English</cell><cell>Mobile products</cell></row><row><cell>Lennar</cell><cell>lennar</cell><cell>English</cell><cell>home builder</cell></row><row><cell>Opera</cell><cell>opera</cell><cell>English</cell><cell>Sofstware</cell></row><row><cell>Research in motion</cell><cell>rim</cell><cell>English</cell><cell>Mobile products</cell></row><row><cell>TAM airlines</cell><cell>tam</cell><cell>English</cell><cell>Airline</cell></row><row><cell>Warner Bros</cell><cell>warner</cell><cell>English</cell><cell>Films</cell></row><row><cell cols="3">Southwest Arilines southwest English</cell><cell>Airline</cell></row><row><cell>Dunkin Donuts</cell><cell>dunkin</cell><cell>English</cell><cell>Food</cell></row><row><cell>Delta Airlines</cell><cell>delta</cell><cell>English</cell><cell>Airline</cell></row><row><cell>CME group</cell><cell>cme</cell><cell>English</cell><cell>Financial group</cell></row><row><cell cols="2">Borders bookstore borders</cell><cell>English</cell><cell>bookstore</cell></row><row><cell>Ford Motor</cell><cell>ford</cell><cell>English</cell><cell>Motor</cell></row><row><cell>Sprint</cell><cell>sprint</cell><cell>English</cell><cell>Mobile products</cell></row><row><cell>GAP</cell><cell>gap</cell><cell>English</cell><cell>Clothing store</cell></row><row><cell cols="3">El hormiguero hormiguero Spanish</cell><cell>TV program</cell></row><row><cell>Renfe Cercanas</cell><cell cols="3">cercanias Spanish commuter train service</cell></row><row><cell>El Pas</cell><cell>pais</cell><cell>Spanish</cell><cell>Newspaper</cell></row><row><cell>El Pozo</cell><cell>pozo</cell><cell>Spanish</cell><cell>Food</cell></row><row><cell>Real madrid</cell><cell>madrid</cell><cell>Spanish</cell><cell>Soccer team</cell></row><row><cell>Cuatro</cell><cell>cuatro</cell><cell>Spanish</cell><cell>TV chanel</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,136.16,118.05,371.87,176.32"><head>Table 3 .</head><label>3</label><figDesc>Final ranking</figDesc><table coords="10,136.16,118.05,371.87,165.92"><row><cell>Run</cell><cell>Non</cell><cell cols="8">precision Recall F measure precision recall F measure Accuracy Related</cell></row><row><cell></cell><cell cols="4">processed (related) (related) (related)</cell><cell>(non</cell><cell>(non</cell><cell>(non</cell><cell></cell><cell>Ratio</cell></row><row><cell></cell><cell>tweets</cell><cell></cell><cell></cell><cell></cell><cell cols="3">related) related) related )</cell><cell></cell><cell>Deviation</cell></row><row><cell>LSIR.EPFL 1</cell><cell>0</cell><cell>0.71</cell><cell>0.74</cell><cell>0.63</cell><cell>0.84</cell><cell>0.52</cell><cell>0.56</cell><cell>0.83</cell><cell>0.15</cell></row><row><cell>ITC-UT 1</cell><cell>0</cell><cell>0.75</cell><cell>0.54</cell><cell>0.49</cell><cell>0.74</cell><cell>0.6</cell><cell>0.57</cell><cell>0.75</cell><cell>0.18</cell></row><row><cell>ITC-UT 2</cell><cell>0</cell><cell>0.74</cell><cell>0.62</cell><cell>0.51</cell><cell>0.74</cell><cell>0.49</cell><cell>0.47</cell><cell>0.73</cell><cell>0.23</cell></row><row><cell>ITC-UT 3</cell><cell>0</cell><cell>0.7</cell><cell>0.47</cell><cell>0.41</cell><cell>0.71</cell><cell>0.65</cell><cell>0.56</cell><cell>0.67</cell><cell>0.26</cell></row><row><cell>ITC-UT 4</cell><cell>0</cell><cell>0.69</cell><cell>0.55</cell><cell>0.43</cell><cell>0.7</cell><cell>0.55</cell><cell>0.46</cell><cell>0.64</cell><cell>0.32</cell></row><row><cell>SINAI 1</cell><cell>449</cell><cell>0.84</cell><cell>0.37</cell><cell>0.29</cell><cell>0.68</cell><cell>0.71</cell><cell>0.53</cell><cell>0.63</cell><cell>0.36</cell></row><row><cell>SINAI 4</cell><cell>449</cell><cell>0.9</cell><cell>0.26</cell><cell>0.17</cell><cell>0.73</cell><cell>0.72</cell><cell>0.53</cell><cell>0.61</cell><cell>0.38</cell></row><row><cell>BASELINE N R</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.57</cell><cell>1</cell><cell>0.66</cell><cell>0.57</cell><cell>0.43</cell></row><row><cell>SINAI 2</cell><cell>449</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.58</cell><cell>0.98</cell><cell>0.65</cell><cell>0.56</cell><cell>0.43</cell></row><row><cell>UVA 1</cell><cell>409</cell><cell>0.47</cell><cell>0.41</cell><cell>0.36</cell><cell>0.6</cell><cell>0.64</cell><cell>0.55</cell><cell>0.56</cell><cell>0.27</cell></row><row><cell>SINAI 5</cell><cell>449</cell><cell>0.72</cell><cell>0.51</cell><cell>0.28</cell><cell>0.75</cell><cell>0.47</cell><cell>0.33</cell><cell>0.51</cell><cell>0.48</cell></row><row><cell>KALMAR R. 4</cell><cell>874</cell><cell>0.48</cell><cell>0.75</cell><cell>0.47</cell><cell>0.65</cell><cell>0.25</cell><cell>0.28</cell><cell>0.46</cell><cell>0.43</cell></row><row><cell>SINAI 3</cell><cell>449</cell><cell>0.6</cell><cell>0.7</cell><cell>0.36</cell><cell>0.86</cell><cell>0.28</cell><cell>0.19</cell><cell>0.46</cell><cell>0.54</cell></row><row><cell>KALMAR R. 2</cell><cell>874</cell><cell>0.47</cell><cell>0.7</cell><cell>0.43</cell><cell>0.61</cell><cell>0.27</cell><cell>0.28</cell><cell>0.44</cell><cell>0.43</cell></row><row><cell>KALMAR R. 5</cell><cell>874</cell><cell>0.48</cell><cell>0.77</cell><cell>0.47</cell><cell>0.65</cell><cell>0.21</cell><cell>0.23</cell><cell>0.44</cell><cell>0.45</cell></row><row><cell>BASELINE R</cell><cell>0</cell><cell>0.43</cell><cell>1</cell><cell>0.53</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.43</cell><cell>0.56</cell></row><row><cell>ALMAR R. 1</cell><cell>2207</cell><cell>0.51</cell><cell>0.7</cell><cell>0.42</cell><cell>0.59</cell><cell>0.19</cell><cell>0.21</cell><cell>0.4</cell><cell>0.39</cell></row><row><cell>KALMAR R. 3</cell><cell>2202</cell><cell>0.49</cell><cell>0.66</cell><cell>0.39</cell><cell>0.66</cell><cell>0.25</cell><cell>0.27</cell><cell>0.4</cell><cell>0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,136.16,118.78,362.41,227.39"><head>Table 4 .</head><label>4</label><figDesc>UIR results. UIR threshold = 0.1</figDesc><table coords="11,136.16,118.78,362.41,216.48"><row><cell>Run</cell><cell cols="2">Accuracy Improved systems</cell></row><row><cell>LSIR.EPFL 1</cell><cell cols="2">0.83 KALMAR R. 1 KALMAR R. 5 ITC-UT 2 KALMAR R. 2</cell></row><row><cell></cell><cell></cell><cell>KALMAR R. 3 ITC-UT 4 KALMAR R. 4 UVA 1 BASELINER</cell></row><row><cell>ITC-UT 1</cell><cell cols="2">0.75 SINAI 4 UVA 1</cell></row><row><cell>ITC-UT 2</cell><cell cols="2">0.73 SINAI 4, UVA 1</cell></row><row><cell>ITC-UT 3</cell><cell cols="2">0.67 KALMAR R. 2, KALMAR R. 3, UVA 1,</cell></row><row><cell>ITC-UT 4</cell><cell cols="2">0.64 SINAI 4, UVA 1</cell></row><row><cell>SINAI 1</cell><cell cols="2">0.63 SINAI 4, SINAI 2, UVA 1, BASELINENR</cell></row><row><cell>SINAI 4</cell><cell>0.61</cell><cell></cell></row><row><cell>BASELINENR</cell><cell>0.57</cell><cell></cell></row><row><cell>SINAI 2</cell><cell>0.56</cell><cell></cell></row><row><cell>UVA 1</cell><cell cols="2">0.56 KALMAR R. 1, KALMAR R. 2, KALMAR R. 3</cell></row><row><cell>SINAI 5</cell><cell>0.51</cell><cell></cell></row><row><cell cols="3">KALMAR R. 4 0.46 KALMAR R. 1, KALMAR R. 5</cell></row><row><cell>SINAI 3</cell><cell>0.46</cell><cell></cell></row><row><cell cols="2">KALMAR R. 2 0.44</cell><cell></cell></row><row><cell cols="2">KALMAR R. 5 0.44</cell><cell></cell></row><row><cell>BASELINER</cell><cell>0.43</cell><cell></cell></row><row><cell>KALMAR R. 1</cell><cell>0.4</cell><cell>BASELINENR</cell></row><row><cell>KALMAR R. 3</cell><cell>0.4</cell><cell>KALMAR R. 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,136.16,166.13,345.23,439.73"><head>Table 5 .</head><label>5</label><figDesc>Selected tweets for test and training corpora</figDesc><table coords="13,136.16,166.13,345.23,429.33"><row><cell>Test set</cell><cell></cell><cell>Training set</cell><cell></cell></row><row><cell>Entity name</cell><cell>Query</cell><cell>Entity name</cell><cell>Query</cell></row><row><cell>Fluent SimplicityAmazon.com</cell><cell>Amazon</cell><cell>alcatel</cell><cell>alcatel</cell></row><row><cell>apache</cell><cell>apache</cell><cell>Amadeus IT Group</cell><cell>Amadeus</cell></row><row><cell>Apple</cell><cell>Apple</cell><cell>Apollo Hospitals</cell><cell>Apollo</cell></row><row><cell>Blizzard Entertainment</cell><cell>Blizzard</cell><cell>armani</cell><cell>armani</cell></row><row><cell>camel</cell><cell>camel</cell><cell>barclays</cell><cell>barclays</cell></row><row><cell>Canon inc.</cell><cell>canon</cell><cell>BART</cell><cell>BART</cell></row><row><cell>Cisco Systems</cell><cell>Cisco</cell><cell>bayer</cell><cell>bayer</cell></row><row><cell>CVS/pharmacy</cell><cell>CVS</cell><cell>Blockbuster Inc.</cell><cell>Blockbuster</cell></row><row><cell>Denver Nuggets</cell><cell>Denver</cell><cell>Boingo (Wifi for travelers)</cell><cell>Boingo</cell></row><row><cell>Deutsche Bank</cell><cell>Deutsche</cell><cell>Bulldog Solutions</cell><cell>bulldog</cell></row><row><cell>Emory University</cell><cell>emory</cell><cell>cadillac</cell><cell>cadillac</cell></row><row><cell>Ford Motor Company</cell><cell>ford</cell><cell>Craft Magazine</cell><cell>Craft</cell></row><row><cell>fox channel</cell><cell>fox</cell><cell>Delta Holding</cell><cell>Delta</cell></row><row><cell>friday's</cell><cell>friday's</cell><cell>dunlop</cell><cell>dunlop</cell></row><row><cell>Gibson</cell><cell>Gibson</cell><cell>Edmunds.com</cell><cell>Edmunds</cell></row><row><cell>General Motors</cell><cell>GM</cell><cell>Elf corporation</cell><cell>elf</cell></row><row><cell>Jaguar Cars Ltd.</cell><cell>jaguar</cell><cell>Emperor Entertainment Group</cell><cell>Emperor</cell></row><row><cell>John F. Kennedy International Airport</cell><cell>jfk</cell><cell>fender</cell><cell>fender</cell></row><row><cell>Johnnie Walker</cell><cell>johnnie</cell><cell>Folio Corporation</cell><cell>folio</cell></row><row><cell>kiss band</cell><cell>kiss</cell><cell>Foxtel</cell><cell>Foxtel</cell></row><row><cell>Lexus</cell><cell>Lexus</cell><cell>Fujitsu</cell><cell>Fujitsu</cell></row><row><cell>Liverpool FC</cell><cell>Liverpool</cell><cell>Harpers</cell><cell>Harpers</cell></row><row><cell>Lloyds Banking Group</cell><cell>Lloyd</cell><cell>Impulse (Records )</cell><cell>Impulse</cell></row><row><cell>macintosh</cell><cell>mac</cell><cell>lamborghini</cell><cell>lamborghini</cell></row><row><cell>McDonald's</cell><cell>McDonald's</cell><cell>linux</cell><cell>linux</cell></row><row><cell>McLaren Group</cell><cell>McLaren</cell><cell>Liquid Entertainment</cell><cell>Liquid</cell></row><row><cell>Metro supermarket</cell><cell>Metro</cell><cell>Lufthansa</cell><cell>Lufthansa</cell></row><row><cell>A.C. Milan</cell><cell>Milan</cell><cell>Luxor Hotel and Casino</cell><cell>Luxor</cell></row><row><cell>MTV</cell><cell>MTV</cell><cell>LYNX Express</cell><cell>Lynx</cell></row><row><cell>muse band</cell><cell>muse</cell><cell>Mack Group</cell><cell>Mack</cell></row><row><cell>Oracle</cell><cell>oracle</cell><cell>Magnum Research</cell><cell>Magnum</cell></row><row><cell>Orange</cell><cell>Orange</cell><cell cols="2">Mandalay Bay Resort and Casino Mandalay</cell></row><row><cell>Paramount Group</cell><cell>Paramount</cell><cell>Marriott International</cell><cell>Marriott</cell></row><row><cell>A.S. Roma</cell><cell>Roma</cell><cell>Marvel comics</cell><cell>Marvel</cell></row><row><cell>scorpions</cell><cell>scorpions</cell><cell>mdm (Event agency)</cell><cell>mdm</cell></row><row><cell>seat</cell><cell>seat</cell><cell>MEP</cell><cell>MEP</cell></row><row><cell>Sharp Corporation</cell><cell>sharp</cell><cell>Mercedes-Benz</cell><cell>Mercedes</cell></row><row><cell>sonic.net</cell><cell>sonic</cell><cell>Mercer consulting</cell><cell>Mercer</cell></row><row><cell>sony</cell><cell>sony</cell><cell>MGM Grand Hotel and Casino</cell><cell>MGM</cell></row><row><cell>Stanford Junior University</cell><cell>stanford</cell><cell>MTA Bike Plus (NYC)</cell><cell>MTA</cell></row><row><cell>Starbucks</cell><cell>Starbucks</cell><cell>nikon</cell><cell>nikon</cell></row><row><cell>subway</cell><cell>subway</cell><cell>Nordic Airways</cell><cell>nordic</cell></row><row><cell>Tesla Motors</cell><cell>tesla</cell><cell>philips</cell><cell>philips</cell></row><row><cell>US Airways</cell><cell>US</cell><cell>pierce manufacturing</cell><cell>pierce</cell></row><row><cell>Virgin Media</cell><cell>Virgin</cell><cell>Pioner Company</cell><cell>pioneer</cell></row><row><cell>Yale University</cell><cell>Yale</cell><cell>Renaissance Technologies</cell><cell>Renaissance</cell></row><row><cell>Zoo Entertainment</cell><cell>zoo</cell><cell>Renault</cell><cell>Renault</cell></row><row><cell></cell><cell></cell><cell>Land Rover</cell><cell>Rover</cell></row><row><cell></cell><cell></cell><cell>shin corporation</cell><cell>shin</cell></row><row><cell></cell><cell></cell><cell>Smarter Travel</cell><cell>Smarter</cell></row><row><cell></cell><cell></cell><cell>Southwest Airlines</cell><cell>Southwest</cell></row><row><cell></cell><cell></cell><cell>Yamaha</cell><cell>Yamaha</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,657.79,216.41,7.86"><p>http://blog.fluentsimplicity.com/twitter-brand-index/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="4,144.73,657.79,106.51,7.86"><p>http://dbpedia.org/About</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the project <rs type="projectName">QEAVis-Catiex</rs> (<rs type="grantNumber">TIN2007-67581-C02-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_WguxuAY">
					<idno type="grant-number">TIN2007-67581-C02-01</idno>
					<orgName type="project" subtype="full">QEAVis-Catiex</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,138.35,646.84,342.24,7.86;11,146.91,657.79,333.67,7.86;12,146.91,120.67,333.68,7.86;12,146.91,131.63,81.10,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,301.02,646.84,179.57,7.86;11,146.91,657.79,333.67,7.86;12,146.91,120.67,32.93,7.86">Combining Evaluation Metrics with a Unanimous Improvement Ratio and its Application to the Web People Search Cluster-ing Task</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amig√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,213.43,120.67,267.16,7.86">Proceedings Of The 2nd Web People Search Evaluation Workshop</title>
		<meeting>Of The 2nd Web People Search Evaluation Workshop<address><addrLine>WePS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,142.59,342.25,7.86;12,146.91,153.55,333.68,7.86;12,146.91,164.51,199.82,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,304.62,142.59,175.98,7.86;12,146.91,153.55,157.00,7.86">WePS 2 Evaluation Campaign: overview of the Web People Search Clustering Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,335.29,153.55,145.30,7.86;12,146.91,164.51,115.41,7.86">Proceedings Of The 2nd Web People Search Evaluation Workshop</title>
		<meeting>Of The 2nd Web People Search Evaluation Workshop<address><addrLine>WePS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,175.46,342.24,7.86;12,146.91,186.42,333.68,7.86;12,146.91,197.38,333.67,7.86;12,146.91,208.34,292.03,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,336.73,175.46,143.87,7.86;12,146.91,186.42,126.29,7.86">Using mechanical turk to build machine translation evaluation sets</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bloodgood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,290.14,186.42,190.46,7.86;12,146.91,197.38,283.60,7.86">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,219.30,342.24,7.86;12,146.91,230.26,333.68,7.86;12,146.91,241.22,191.80,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,402.61,219.30,77.98,7.86;12,146.91,230.26,26.22,7.86">A few chirps about twitter</title>
		<author>
			<persName coords=""><forename type="first">Balachander</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillipa</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Arlitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,192.00,230.26,284.39,7.86">WOSP &apos;08: Proceedings of the first workshop on Online social networks</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,252.18,342.24,7.86;12,146.91,263.14,333.68,7.86;12,146.91,274.09,333.68,7.86;12,146.91,285.05,326.38,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,413.23,252.18,67.36,7.86;12,146.91,263.14,166.10,7.86">Document image collection using amazon&apos;s mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">Audrey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Ajot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,331.18,263.14,149.41,7.86;12,146.91,274.09,329.38,7.86">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,296.01,342.24,7.86;12,146.91,306.97,333.68,7.86;12,146.91,317.93,333.68,7.86;12,146.91,328.89,128.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,218.85,306.97,261.74,7.86;12,146.91,317.93,262.40,7.86">Exploring the value of online reviews to organizations: Implications for revenue forecasting and planning chrysanthos dellarocas</title>
		<author>
			<persName coords=""><forename type="first">Dell</forename><surname>Management</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chrysanthos</forename><surname>Mit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neveen</forename><surname>Dellarocas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoquan (michael</forename><surname>Farag Awad</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,429.29,317.93,51.30,7.86;12,146.91,328.89,28.17,7.86">Management Science</title>
		<imprint>
			<biblScope unit="page" from="1407" to="1424" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,339.85,342.25,7.86;12,146.91,350.81,333.68,7.86;12,146.91,361.77,52.97,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,350.75,339.85,90.81,7.86">On the trec blog track</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,462.94,339.85,17.66,7.86;12,146.91,350.81,329.44,7.86">Proceedings of International Conference on Weblogs and Social Media (ICWSM 2008)</title>
		<meeting>International Conference on Weblogs and Social Media (ICWSM 2008)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,372.73,342.25,7.86;12,146.91,383.68,333.68,7.86;12,146.91,394.64,236.49,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,189.63,372.73,290.97,7.86;12,146.91,383.68,101.75,7.86">Electronic Word of Mouth: A Genre Analysis of Product Reviews on Consumer Opinion Web Sites</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Pollach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.21,383.68,212.38,7.86;12,146.91,394.64,208.79,7.86">Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS&apos;06) Track 3</title>
		<meeting>the 39th Annual Hawaii International Conference on System Sciences (HICSS&apos;06) Track 3</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
