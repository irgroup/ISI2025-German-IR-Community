<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.77,116.95,217.81,12.62;1,141.57,134.89,332.22,12.62;1,196.36,152.82,222.63,12.62">WePS-3 Evaluation Campaign: Overview of the Web People Search Clustering and Attribute Extraction Tasks</title>
				<funder ref="#_3RvPFhC">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.87,190.67,58.64,8.74"><forename type="first">Javier</forename><surname>Artiles</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.07,190.67,81.83,8.74"><forename type="first">Andrew</forename><surname>Borthwick</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intelius, Inc. Bellevue</orgName>
								<address>
									<region>WA</region>
									<country>USA http</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.46,190.67,59.99,8.74"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.00,190.67,62.87,8.74"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">CS Dept</orgName>
								<orgName type="institution">New York University New York</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.43,202.63,66.03,8.74"><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED University</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.77,116.95,217.81,12.62;1,141.57,134.89,332.22,12.62;1,196.36,152.82,222.63,12.62">WePS-3 Evaluation Campaign: Overview of the Web People Search Clustering and Attribute Extraction Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB75CC3A7B60B126CB5100C07F25DA7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Web Search</term>
					<term>Web People Search</term>
					<term>Text Clustering</term>
					<term>Attribute Extraction</term>
					<term>Meta-search Engines</term>
					<term>Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The third WePS (Web People Search) Evaluation campaign took place in 2009-2010 and attracted the participation of 13 research groups from Europe, Asia and North America. Given the top web search results for a person name, two tasks were addressed: a clustering task, which consists of grouping together web pages referring to the same person, and an extraction task, which consists of extracting salient attributes for each of the persons sharing the same name. Continuing the path of previous campaigns, this third evaluation aimed at merging both problems into one single task, where the system must return both the documents and the attributes for each of the different people sharing a given name. This is not a trivial step from the point of view of evaluation: a system may correctly extract attribute profiles from different URLs but then incorrectly merge profiles. This campaign also featured a larger testbed and the participation of a state-of-the-art commercial WePS system in the attribute extraction task. This paper presents the definition, resources, evaluation methodology and results for the clustering and attribute extraction tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Web People Search task has been defined in WePS campaigns as the problem of organizing web search results for a given person name. The most frequently used web search engines return a ranked list of URLs which typically refer to various people sharing the same name. Ideally, the user would obtain groups of documents that refer to the same individual, possibly with a list of person attributes that help the user choosing the cluster that represents the person she is looking for.</p><p>From a practical point of view, the people search task is highly relevant: between 11 and 17% of web queries include a person name, 4% of web queries are just a person name, and person names are highly ambiguous: according to the US Census Bureau, only 90,000 different names are shared by more than 100,000,000 people. An indirect proof of the relevance of the problem is the fact that, since 2005, a number of web startups have been created precisely to address it (Spock.com -now Intelius -and Zoominfo.com being the best known).</p><p>From a research point of view, the task is challenging (the number of clusters is not known a priori; the degree of ambiguity does not seem to follow a normal distribution; and web pages are noisy sources from which attributes and other indexes are difficult to extract) and has connections with Natural Language Processing and Information Retrieval tasks (Text Clustering, Information Extraction, Word Sense Discrimination) in the context of the WWW as data source.</p><p>WePS-1 <ref type="bibr" coords="2,187.40,325.47,10.52,8.74" target="#b2">[3]</ref> was run as a Semeval 1 task in 2007, receiving submissions from 16 teams (being one of the largest tasks in Semeval) and WePS-2 <ref type="bibr" coords="2,420.91,337.42,10.52,8.74" target="#b3">[4]</ref> was run as a workshop of the WWW 2009 Conference, with the participation of 19 research teams. In the first campaign we addressed only the name co-reference problem, defining the task as clustering of web search results for a given person name.</p><p>In the second campaign we refined the evaluation metrics <ref type="bibr" coords="2,392.16,385.24,10.52,8.74" target="#b1">[2]</ref>  <ref type="bibr" coords="2,406.38,385.24,10.51,8.74" target="#b0">[1]</ref> and added an attribute extraction task <ref type="bibr" coords="2,245.98,397.20,15.50,8.74" target="#b14">[15]</ref> for web documents returned by the search engine for a given person name.</p><p>For this third campaign we aimed at merging both problems into one single task, where the system must return both the documents and the attributes for each of the different people sharing a given name. This is not a trivial step from the point of view of evaluation: a system may correctly extract attribute profiles from different URLs but then incorrectly merge profiles 4  WePS-1 and WePS-2 focused on consolidating a research community around the problem and an optimal evaluation methodology. In WePS-3 the focus was on implicating industrial stakeholders in the evaluation campaign, as providers of input to the task design phase and also as providers of realistic scale datasets. To reach this goal, we have incorporated as co-coordinator Andrew Borthwick, principal scientist at Intelius, Inc. -one of the main Web People Search services -which provides advanced people attribute extraction and profile matching from web pages.</p><p>This paper presents an overview of the WePS-3 clustering and attribute extraction tasks. The task definition is provided in Section 2, the WePS-3 testbed is described in Section 3, the methodology to produce our gold-standard is ex-plained in Section 4 and Section 5 includes the evaluation metrics and the campaign design. We also provide an overview of the participating systems and the results of the evaluation in Section 6. Finally, we end with some concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Given a set of web search results obtained using a person name as query, the proposed tasks are to cluster these search results according to the different people sharing the name and to extract certain biographical attributes for each person (i.e., for each cluster of documents). Groups were allowed to perform only the clustering task, or both tasks together.</p><p>Compared to previous WePS campaigns, the clustering task is defined in the same way, but the testbed is larger and more diverse (see Section 3). Also there is a closer relation between the clustering and attribute extraction tasks. The WePS-3 Attribute Extraction task is different from WePS-2 in that systems are requested to relate each attribute to a person (cluster of documents) instead of just listing the attributes obtained from each document. This is the reason why participants in the AE task are required to participate in the Clustering task too. Systems are expected to output one attribute of each type in each cluster of documents (i.e. only one affiliation, only one occupation, etc. for each person).</p><p>All attributes listed in Table <ref type="table" coords="3,272.80,387.80,4.98,8.74" target="#tab_0">1</ref> were included in the attribute extraction task Compared to the WePS-2 Attribute Extraction there were two main modifications: (i) WePS-2 training data had an attribute "education", which was separated into three attributes "school", "degree" and "major" in the test data. WePS-3 use school/degree/major as independent attributes; (ii) The annotated data in WePS-2 included "work" and "location", but these were not used in the WePS-2 evaluation and were not considered in WePS-3.</p><p>3 Data sets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clustering Training Dataset</head><p>Participants used the WePS-1 and WePS-2 public clustering testbeds to develop their systems. These datasets consist of the top web search results for a number different ambiguous person names, and contain human assessments of the correct way to group these documents according the different people mentioned with the same name (see <ref type="bibr" coords="4,207.28,296.51,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="4,219.46,296.51,7.75,8.74" target="#b3">4]</ref> for a detailed explanation of the corpus creation and annotation guidelines). The output format remained as in WePS-2 (a "clustering" root element and "entity" elements for each cluster of documents), except for a slight change in the XML format: now "doc" elements associated to a person are enclosed in a "documents" element, as in the Figure <ref type="figure" coords="4,382.53,344.33,3.87,8.74">1</ref>.</p><p>&lt;entity id="1"&gt; &lt;documents&gt; &lt;doc rank="99" /&gt; &lt;doc rank="104" /&gt; &lt;/documents&gt; &lt;/entity&gt; Fig. <ref type="figure" coords="4,231.12,459.63,4.13,7.89">1</ref>. Sample output from the clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute Extraction Training Dataset</head><p>A training dataset was provided for the Attribute Extraction Task based on the WePS-2 clustering and attribute extraction test datasets (see <ref type="bibr" coords="4,410.97,537.56,14.76,8.74" target="#b14">[15]</ref>). Given the clustering gold standard and attributes extracted for documents in the WePS-2 corpus, we generated a view of the extracted attributes grouped by cluster instead of documents. This provided the participants with the kind of output expected from their systems in WePS-3.</p><p>Both the clustering and attribute extraction output were provided in the same XML file (see Figure <ref type="figure" coords="4,253.95,609.29,3.87,8.74">2</ref>). In this file each cluster of documents is specified by the element entity, which contains the list of grouped documents and the list of extracted attributes. For each attribute it's required to indicate the type of attribute (date of birth, occupation, etc.), the source from which it was extracted (document ranking) and the value.</p><p>&lt;clustering searchString="AMANDA LENTZ"&gt; &lt;entity id="16"&gt; &lt;documents&gt; &lt;doc rank="17" /&gt; &lt;doc rank="66"/&gt; &lt;doc rank="73"/&gt; &lt;doc rank="51" notes= "from Huron" /&gt; &lt;/documents&gt; &lt;attributes&gt; &lt;attr type="date\_of\_birth" source="17"&gt;4th August 1979&lt;/attr&gt; &lt;attr type="occupation" source="17"&gt;Painter&lt;/attr&gt; &lt;/attributes&gt; &lt;/entity&gt; [...] &lt;/clustering&gt; Fig. <ref type="figure" coords="5,158.37,300.47,4.13,7.89">2</ref>. Sample output from the combined clustering and attribute extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Test Dataset</head><p>In WePS-3 we decided to substantially increase the amount of test data both in number of documents and person names. The same dataset was used both for the clustering and the attribute extraction tasks. A total of 300 person names were used, compared to 30 names used in WePS-2. As we did in WePS-2, we obtained names randomly from the US Census, Wikipedia and computer science conference program committees. In addition to that, we included names for which at least one person has one of the following occupations: attorney, corporate executive or realtor. 50 names were extracted from each one of these sources to make a total of 300 names.</p><p>In order to obtain person names where at least one person in the results sets has a particular occupation we designed a simple algorithm. Given a small set of keywords related to the occupation we are interested in (e.g. "real estate" or "housing" for realtor) we launch a query to a web search engine and randomly show documents to an annotator until she finds one that refers to a person with the intended occupation. Then we formulate a search query with that person's name. If the reference document is present in the top 200 search results we add this name and these documents to our dataset.</p><p>For each name the top 200 web search results from the Yahoo! API<ref type="foot" coords="5,452.70,551.13,3.97,6.12" target="#foot_0">6</ref> were downloaded and archived with their corresponding search metadata (search snippet, title, URL and position in the results ranking).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head><p>Systems are requested to make clusters as accurate as possible over the whole set of documents. However, given the annotation load required to manually cluster this amount of information, the evaluation was performed only on two people per person name. This allowed us to simplify the annotation task from grouping a large set of documents in an unknown number of people clusters to a classification task where only two people are considered when examining each document in the results. Even for this simplified annotation task a large amount of human resources and time is required. To leverage this problem we used the services of Mechanical Turk, distributing the task among many non-expert workers around the world (see Section 4.1).</p><p>Before handing the test data to the annotators, we had to select the two people ("person a" and "person b") that would be considered for each person name. In each case, we chose a document in the search results as a reference to classify other documents about that particular person. In general "person a" is related to the source from which the name was selected (e.g. a Wikipedia person when the source is Wikipedia, a realtor when the source is realtor names, etc.), while "person b" can be any other person in the search results.</p><p>To select a "person a" reference document, we randomly iterate through the search results until one of the following conditions is satisfied: (i) for Wikipedia names we select one of the Wikipedia articles within the search results for that name; (ii) for computer researchers we select a page that mentions the researcher<ref type="foot" coords="6,182.39,346.78,3.97,6.12" target="#foot_1">7</ref> (iii) occupation-related names already have a reference document that we obtained as described in Section 3.3.</p><p>"person b" can be anybody mentioned with the ambiguous name in the results that does not share the distinctive feature of the first person (not a Wikipedia entity or not having the lawyer, executive or realtor occupation). This second person is also selected by randomly iterating through the results until the conditions are satisfied by a certain web page. In the case of the Census names the only requirement for the two selected people is that they be different, but no constraints are set regarding the characteristics of the person. Finally, for researchers names we found that most of them monopolize search results and hence we did not extract a second person for these names. Still, a name disambiguation system has to be able to recognize that most of these documents belong to one individual and so we kept these names in the dataset.</p><p>Once we have the reference documents for all the names in the collection we can proceed to the annotation process. Each worker will receive a set of ten search results for a person name and two reference documents (one describing "person a" and other describing "person b"). The task for the annotator is to classify each of the ten documents as referring to either one of the two selected people or to a generic "someone else".</p><p>In Table <ref type="table" coords="6,189.46,577.33,4.98,8.74" target="#tab_1">2</ref> we show the average number of pages assigned to each person on each source of name. The main result to highlight in this table is that people in the conference source tend to monopolize the search results, followed by people that appear in Wikipedia articles. Note that the average number of pages is well below the total pages in the test dataset. The reason for this is that this table only considers pages for which at least three annotators out of five agreed in the assignment. For the evaluation of the Attribute Extraction task we didn't rely on a previously generated gold standard. Instead we pooled the output of the participating systems and submitted this for annotation in Mechanical Turk. We only evaluated the extraction of attributes for the same people annotated in the clustering task. For this reason, we only added to the pool attributes whose source is one of the documents annotated as mentioning "person a" or "person b" according to the Clustering gold standard. The annotators were given one webpage and a set of up to ten &lt; attribute, value &gt; pairs and were asked to decide whether each attribute fell into one of the following categories:</p><p>-Correct (this is a correct attribute that describes the person in the page).</p><p>-Incorrect for any reason other than being too long or too short. For instance: The type of attribute is incorrect (e.g. gardener is incorrectly identified as a date of birth); the attribute is not attached to this person (e.g. this attribute describes some other person describe on the page); or the attribute was simply not found in the text describing the subject person. -Correct, but too long or too short. The attribute is correct, but has one of the following problems:</p><p>• Too short. The attribute is incomplete (e.g. "director" when it should say "director of marketing"). • Too long. The attribute contains a correct value but includes irrelevant information (e.g. "CEO in 1982" when it should say "CEO" ). -Impossible to tell because the web page is unreadable.</p><p>-The web page is readable, but the specified person is not on this page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mechanical Turk Methodology</head><p>For the annotation of the test data we used Mechanical Turk 8 . Mechanical Turk (MTurk) offers a web portal to post tasks known as HITs (Human Intelligence Tasks). Workers (known as "Turkers") can choose among the available tasks and complete them for a small fee for each task. The use of MTurk for NLP tasks has been studied before <ref type="bibr" coords="8,221.14,143.90,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="8,238.30,143.90,7.75,8.74" target="#b5">6]</ref> and has been found to be effective, but this evaluation forced us to focus on the problem of detecting "spam" annotations by Turkers and to focus on "employee relations" issues of how best to motivate and interact with Turkers.</p><p>Spam annotations occur when a Turker attempts to rapidly do a large number of HITs without making a serious attempt at doing quality annotations or by simply writing an automated script to do the HITs. In the following paragraphs, we describe efforts we undertook to discourage spam annotations and to encourage the highest quality workers.</p><p>The primary guard that MTurk has to encourage high-quality annotations is the Turker's "HIT approval rate" (HAR), which is the percentage of each Turker's HITs that have been approved divided by the number of approved + rejected HITs. Amazon's instructions on the web site recommend that HITs be posted requiring that Turker's HITs have a minimum HAR of 95%. For our initial annotations, we adopted this recommendation. This succeeded initially, but we found some batches that showed obvious signs of spamming. Table <ref type="table" coords="8,475.61,323.23,4.98,8.74" target="#tab_2">3</ref> shows average inter-annotator agreement obtained in different portions of the corpus according to the source of the name. For each of the annotated web pages, agreement is measured as the percentage of annotators (five in each case) that selected the most voted annotation. The table also shows the number of HITs generated in each case (each HIT contains 10 pages related to an ambiguous name) and the average number of seconds spent by the annotators working on each HIT. Specifically, as can be seen in Table <ref type="table" coords="8,339.84,406.91,3.87,8.74" target="#tab_2">3</ref>, the census batch had 14/4325 HITs done by workers with average inter-annotator agreement of 50% or higher, while the realtor batch, which had the highest rate of IA agreement and which took the longest time per HIT, had Concerned that the some of our batches might have suffered from spam, we instituted a number of changes in our MTurk methodology for the attribute annotation task.</p><p>1. We raised the minimum HAR to 97%.</p><p>2. We added an additional requirement that the Turker must have had at least 500 approved HITs before doing our HITs. 3. On each HIT, we added at least one attribute which we knew to be very likely false (it was an attribute drawn from a response for a different person.</p><p>Hence it would only be true in the very unlikely case that the two people had the same attribute by chance). 4. We monitored the accuracy of the Turkers on each batch of HITs. Turkers who marked too many of the "trick attributes" as correct had their work "rejected" (not paid for and resubmitted to other Turkers to be redone) and were "blocked". Blocked workers are barred from ever performing work on the account that blocked them. 5. In addition, we instituted a bonus program whereby we paid cash bonuses to those workers who got the best score on the "trick" attributes. The workers with the best score received a 100% bonus. At the discretion of the manager of the Turk project (Dr. Borthwick), Turkers with a score close to the best were sometimes given 50% bonuses. 6. Finally, we established a dialog with the Turkers as described below.</p><p>We established a dialog with the Turkers in two ways. First of all, we took care to notify the Turkers that we were monitoring them and would reward good workers with bonuses and punish evil-doers with rejection of their HITs and by blocking them from doing future work. We put this notice on every HIT and we also alluded to it on the tag line of the HIT, where we put "Bonus available!" after the HIT description. Note that the rejection of an entire batch of HITs can have severe consequences for a Turker as it can push his/her HAR below the 95% threshold required to get work.</p><p>The more interesting initiative, though, was the establishment of a two-way dialog with the Turkers by posting on the Turker Nation bulletin board <ref type="foot" coords="9,473.36,443.69,3.97,6.12" target="#foot_3">9</ref> . We understand Turker Nation to be the most popular venue for this kind of discussion <ref type="bibr" coords="9,181.02,469.18,14.61,8.74" target="#b9">[10]</ref>. As per the convention on Turker Nation, we used a single thread as a point of discussion for all of the attribute extraction HITs. We posted notices there every time a new batch of HITs was posted to Mechanical Turk and we listed the ID's of those workers who received bonuses so as to communicate to Turkers that we were following through on our bonus commitment. Furthermore, we used this board to field queries about how best to judge the HITs. In all, we made 39 posts to this thread and fielded 47 questions and comments from Turkers. Anecdotally, we believe that this dialog had a strong positive effective, when taken in conjunction with our other initiatives. We could see from the questions we got that at least some Turkers were taking this task very seriously. One Turker, for instance, went to the trouble of collecting all of the Q and A on the whole thread into one consolidated FAQ. He also commented "Dr. Borthwick is by far the best requester I've ever worked with. Interesting HITs, fair pay + bonuses and good communication. Not sure what else I could ask for." Anecdotally, we noticed a strong correlation between workers who did a lot of HITs with high accuracy and workers who were frequent posters on Turker Nation.</p><p>Finally, a word on the financial model we used for this project. As can be seen from Table <ref type="table" coords="10,207.28,156.93,3.87,8.74" target="#tab_3">4</ref>, we devoted about 20% of our budget to worker bonuses. We also strove to fulfill the philosophy of "equal pay for equal work" by dividing the HITs into batches according to how many attributes workers had to judge (ranging from 2 -10, although we omit the "10 attribute" row from this table), so we decreased the pay as the number of attributes to score decreased. Finally, we strove to keep pay between $3 and $4 per hour, based on the advice of Amazon salesmen that this was the maximum hourly rate that yielded significant benefit on MTurk. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Methodology</head><p>For the evaluation of the clustering task we used the B-Cubed metrics <ref type="bibr" coords="10,438.95,511.50,9.96,8.74" target="#b4">[5]</ref>. These metrics were introduced in our task in WePS-2 and have been proved to be the only ones, among the different families of clustering metrics, that satisfy the intuitive formal constraints for this problem <ref type="bibr" coords="10,329.86,547.37,9.96,8.74" target="#b1">[2]</ref>. B-Cubed metrics independently compute the precision and recall associated to each item in the distribution. The precision of one item represents the amount of items in the same cluster that belong to its category. Analogously, the recall of one item represents how many items from its category appear in its cluster.</p><p>In WePS-2 an extended version of B-Cubed <ref type="bibr" coords="10,341.70,609.29,10.52,8.74" target="#b1">[2]</ref> was used to handle the problem of evaluating overlapping clustering (a clustering task where an element can belong to more than one cluster, in our case, when document mentions multiple people with the same ambiguous name). Due to the choices made in the design of the WePS-3 testbed, we excluded the possibility of an overlapping clustering (a document can only belong to one of the reference people or to someone else) and hence we used the original version of the metric <ref type="foot" coords="11,365.76,130.37,7.94,6.12" target="#foot_4">10</ref> .</p><p>The harmonic mean (F measure α=0,5 ) of B-Cubed Precision and B-Cubed Recall was used for the ranking of systems. For each query we have evaluated the clustering of documents mentioning two different people <ref type="foot" coords="11,396.45,166.37,7.94,6.12" target="#foot_5">11</ref> .</p><p>In the clustering annotation, a document with 3 or more votes (as explained in Section 4, each document was annotated by five Mechanical Turk workers) for person A, person B or "someone else" was considered as a positive document for the corresponding class. The system's output was evaluated and averaged the B-Cubed Precision and Recall values considering each element classified as person A or person B, over the set of elements classified as person A, person B or "someone else". Note that B-Cubed allows us evaluate the system's clustering solutions even though we do not have a full clustering assessment for each person name. The reason for this is that B-Cubed evaluates on the element level, and unlike Purity/Inverse Purity metrics, we do not have to choose a representative class for each cluster in the output.</p><p>For the attribute extraction task participating systems were evaluated based on the attributes they attached to the most representative cluster for each of the people annotated in the clustering gold standard. The cluster with the best recall of attributes for a person in the system output was considered its representative <ref type="foot" coords="11,472.15,345.98,7.94,6.12" target="#foot_6">12</ref>For instance, if we are evaluating "person A" of the name "Tiffany Hopkins", first we rank the clusters in the system output by their attributes recall to "person A" and then we evaluate precision and recall of the attributes in the best ranked cluster. The rationale for using attributes recall as selection criterion is the following: a user confronted with the system output is likely to choose the cluster that exposes the more attributes that identify the person.</p><p>Since the method used for the attribute extraction evaluation was pooling the system outputs, recall is not guaranteed to be representative on the attribute annotations: there might be attribute values which are not detected by any system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Participations and evaluation results</head><p>The WePS-3 organization was contacted by 34 teams expressing their interest in the clustering task. Out of these, 8 teams submitted a total of 27 different runs. Two baseline systems were included in the evaluation: "all-in-one" which places all documents in a single cluster, and "one-in-one" which places each document in a separate cluster.</p><p>Many systems (YHBJ, AXIS, TALP, WOLVES) <ref type="bibr" coords="12,363.55,119.99,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="12,380.71,119.99,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="12,390.12,119.99,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="12,399.53,119.99,12.73,8.74" target="#b10">11]</ref> include Hierarchical Agglomerative Clustering (HAC) as part of their system pipeline. DAEDALUS <ref type="bibr" coords="12,499.01,131.95,15.50,8.74" target="#b13">[14]</ref> intentionally departs from the usage of HAC and experiments with the k-Medioids clustering method. In TALP <ref type="bibr" coords="12,263.20,155.86,10.52,8.74" target="#b8">[9]</ref> three clustering methods (Lingo, HAC, and 2steps HAC) where compared using basic features extracted from the web pages.</p><p>WOLVES <ref type="bibr" coords="12,196.51,180.54,15.50,8.74" target="#b10">[11]</ref> trained a pairwise model to predict the likelihood that two documents refer to the same person. A variety of document features were used (words, named entities, Wikipedia topics, person attributes) along with different pairwise features that measure the similarity between documents (cosine, overlap, Jaccard index, etc). Then a clustering algorithm used these predictions to group the documents. The clustering methods used include HAC and Markov Clustering.</p><p>YHBJ <ref type="bibr" coords="12,179.90,265.01,15.50,8.74" target="#b11">[12]</ref> concentrates on the document representation and feature weighting. It uses Wikipedia entries to extend a feature set based on bag-of-words and named entities. The assignment of weights to the different features goes beyond the widely used TFIDF metrics, considering the relevance of the features to the name query and how representative it is of the main text of the page.</p><p>AXIS <ref type="bibr" coords="12,177.90,325.56,10.52,8.74" target="#b6">[7]</ref> analyzed patterns of Web graph structure as part of a two-stage clustering algorithm that also incorporates content-based features. The detection of related web pages is used to overcome the lack of information about Web graph structure.</p><p>RGAI <ref type="bibr" coords="12,177.91,374.16,15.50,8.74" target="#b12">[13]</ref> represented every document as vector of extracted person attribute values and proceed to apply a clustering algorithm (their experiments include bottom-up clustering and the Xmeans algorithm).</p><p>Table <ref type="table" coords="12,176.42,410.81,4.98,8.74" target="#tab_4">5</ref> presents the results of the 8 participants and the 2 baseline clustering systems. B-Cubed Precision, Recall and F-measure values are macro-averaged over each person name <ref type="foot" coords="12,237.34,433.14,7.94,6.12" target="#foot_7">13</ref> . In the cases where a team submitted multiple run we have chosen the run with the best score as the team representative in the ranking. The table of results shows that:</p><p>-The best scoring system obtains balanced results in both precision and recall, while the rest of the participants have biased scores towards one or other metric. Note that the macro-averaged F-measured scores are lower compared to the F-measure that would be obtained using directly the macro-averaged Precision and Recall values. This indicates that, even though Precion or Recall may obtain a high average value, it is usually at the cost of a low score in the other metric. The Unanimous Improvement Ratio results 14 confirmed that only the top two systems in the ranking make a robust improvements (independent of the weighting of Precision and Recall). According to UIR YHBJ 2 makes a robust improvement of RGAI AE 1, BYU and TALP 5; and AXIS 2 show a robust improvement compared to BYU. -As in the previous WePS campaigns, the correct selection of a cluster stopping criterion is a key factor in the performance of systems. The unbalance of Precion and Recall highlighted in the previous point shows how this affects the performance of the clustering systems in WePS. -Unlike previous WePS campaigns almost all the systems obtained scores above the baselines. It is likely that the one-in-one baseline is obtaining lower scores given that we are only considering two people for each name and that these two people are generally well represented in the dataset. This procedure excludes many people with only one document in the Web, which usually rewards the one-in-one approach.</p><p>Table <ref type="table" coords="13,177.71,274.05,4.98,8.74" target="#tab_5">6</ref> presents the results for the Attribute Extraction task. In this task Intelius (http://www.intelius.com) provided a baseline system that was evaluated along with the participants. Both RGAI <ref type="bibr" coords="13,338.02,297.96,15.50,8.74" target="#b12">[13]</ref> and WOLVES <ref type="bibr" coords="13,423.33,297.96,15.50,8.74" target="#b10">[11]</ref> relied on a rule base approaches, tailoring different heuristics for each attribute type. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The WePS-3 campaign has continued the research effort on people search by offering a larger testbed, integrating the clustering and attribute extraction task and including the participation of experts from companies. The evaluation has featured the use of Mechanical Turk to achieve a large amount of annotated data, and in this process we have learnt about the oportunities and dangers of such powerful tool. Participant teams in the campaign have further expanded the variety of approaches to the people search problem by including external </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,137.50,386.23,343.09,279.43"><head>Table 1 .</head><label>1</label><figDesc>Table 1 Definition of 16 attributes of Person at WePS-2</figDesc><table coords="3,137.50,420.69,343.08,244.96"><row><cell cols="2">Attribute class Example of attribute value</cell></row><row><cell cols="2">Date of birth 4 February 1888</cell></row><row><cell>Birth place</cell><cell>Brookline, Massachusetts</cell></row><row><cell>Other name</cell><cell>JFK</cell></row><row><cell>Occupation</cell><cell>Politician</cell></row><row><cell>Affiliation</cell><cell>University of California, Los Angeles</cell></row><row><cell>Award</cell><cell>Pulitzer Prize</cell></row><row><cell>School</cell><cell>Stanford University</cell></row><row><cell>Major</cell><cell>Mathematics</cell></row><row><cell>Degree</cell><cell>Ph.D.</cell></row><row><cell>Mentor</cell><cell>Tony Visconti</cell></row><row><cell>Nationality</cell><cell>American</cell></row><row><cell>Relatives</cell><cell>Jacqueline Bouvier</cell></row><row><cell>Phone</cell><cell>+1 (111) 111-1111</cell></row><row><cell>FAX</cell><cell>(111) 111-1111</cell></row><row><cell>Email</cell><cell>xxx@yyy.com</cell></row><row><cell>Web site</cell><cell>http://nlp.cs.nyu.edu</cell></row><row><cell cols="2">5 Please refer to the WePS-3 Attribute Extraction Task Guidelines in the WePS web-</cell></row><row><cell cols="2">site (http://nlp.uned.es/weps) for a detailed definition of each attribute.</cell></row></table><note coords="3,473.36,386.23,3.97,6.12;3,477.82,387.80,2.77,8.74"><p><p>5 </p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,171.46,165.04,272.44,95.88"><head>Table 2 .</head><label>2</label><figDesc>Average number classified of pages by source of the name</figDesc><table coords="7,205.07,165.04,199.07,84.97"><row><cell>name</cell><cell cols="3">Avg. number of pages classified as:</cell></row><row><cell>source</cell><cell cols="3">person a person b someone else total</cell></row><row><cell>attorney</cell><cell>5.34</cell><cell>5.66</cell><cell>44.64 55.64</cell></row><row><cell>realtor</cell><cell>6.36</cell><cell>4.96</cell><cell>119.56 130.88</cell></row><row><cell>executive</cell><cell>7.48</cell><cell>4.20</cell><cell>58.12 69.80</cell></row><row><cell>census</cell><cell>4.58</cell><cell>3.00</cell><cell>19.64 27.22</cell></row><row><cell cols="2">conference 28.94</cell><cell>-</cell><cell>26.12 55.06</cell></row><row><cell>wikipedia</cell><cell>9.32</cell><cell>2.82</cell><cell>23.38 35.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,442.78,345.82,128.73"><head>Table 3 .</head><label>3</label><figDesc>Annotation statistics for the clustering task</figDesc><table coords="8,289.00,442.78,191.59,8.74"><row><cell>1,386/4,695 HITs done by workers with IAA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,138.44,274.19,338.49,133.89"><head>Table 4 .</head><label>4</label><figDesc>Annotation statistics for the clustering task</figDesc><table coords="10,138.44,274.19,338.49,118.25"><row><cell cols="6"># attrs. # HITs pay per HIT pay for work bonus Amazon fee total cost</cell><cell>effective</cell></row><row><cell>to score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hourly pay</cell></row><row><cell>9</cell><cell>123</cell><cell>0.22</cell><cell>135.30 27.72</cell><cell>16.30</cell><cell>179.32</cell><cell>2.57</cell></row><row><cell>8</cell><cell>188</cell><cell>0.21</cell><cell>197.40 77.17</cell><cell>27.45</cell><cell>302.02</cell><cell>3.26</cell></row><row><cell>7</cell><cell>264</cell><cell>0.20</cell><cell>264.00 69.30</cell><cell>33.33</cell><cell>366.63</cell><cell>3.19</cell></row><row><cell>6</cell><cell>330</cell><cell>0.18</cell><cell>297.00 114.93</cell><cell>41.19</cell><cell>453.12</cell><cell>2.55</cell></row><row><cell>5</cell><cell>423</cell><cell>0.17</cell><cell>359.55 84.15</cell><cell>44.37</cell><cell>488.07</cell><cell>3.69</cell></row><row><cell>4</cell><cell>563</cell><cell>0.15</cell><cell>422.25 36.45</cell><cell>45.87</cell><cell>504.57</cell><cell>4.86</cell></row><row><cell>3</cell><cell>600</cell><cell>0.12</cell><cell>360.00 129.60</cell><cell>48.96</cell><cell>538.56</cell><cell>5.33</cell></row><row><cell>2</cell><cell>704</cell><cell>0.07</cell><cell>246.40 59.89</cell><cell>30.62</cell><cell>336.91</cell><cell>3.45</cell></row><row><cell>Total</cell><cell>3195</cell><cell></cell><cell>2281.9 599.21</cell><cell cols="2">288.111 3169.221</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,207.78,343.49,199.61,151.08"><head>Table 5 .</head><label>5</label><figDesc>Clustering results: official team ranking</figDesc><table coords="13,207.78,343.49,193.65,140.17"><row><cell></cell><cell cols="2">Macro-averaged Scores</cell></row><row><cell></cell><cell cols="2">F-measure B-Cubed</cell></row><row><cell>rank run</cell><cell cols="2">α =,5 Pre. Rec.</cell></row><row><cell>1 YHBJ 2 unofficial</cell><cell>0.55</cell><cell>0.61 0.60</cell></row><row><cell>2 AXIS 2</cell><cell>0.50</cell><cell>0.69 0.46</cell></row><row><cell>3 TALP 5</cell><cell>0.44</cell><cell>0.40 0.66</cell></row><row><cell>4 RGAI AE 1</cell><cell>0.40</cell><cell>0.38 0.61</cell></row><row><cell>5 WOLVES 1</cell><cell>0.40</cell><cell>0.31 0.80</cell></row><row><cell>6 DAEDALUS 3</cell><cell>0.39</cell><cell>0.29 0.84</cell></row><row><cell>7 BYU</cell><cell>0.38</cell><cell>0.52 0.39</cell></row><row><cell>one in one baseline</cell><cell>0.35</cell><cell>1.00 0.23</cell></row><row><cell>8 HITSGS</cell><cell>0.35</cell><cell>0.26 0.81</cell></row><row><cell>all in one baseline</cell><cell>0.32</cell><cell>0.22 1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,134.77,118.78,345.83,203.75"><head>Table 6 .</head><label>6</label><figDesc>Attribute Extraction results: official team ranking sources of knowledge (Wikipedia), applying new clustering methods to the task and new feature weighting schemes.</figDesc><table coords="14,214.37,118.78,180.46,128.81"><row><cell></cell><cell cols="2">Macro-averaged Scores</cell></row><row><cell></cell><cell>F-measure</cell><cell></cell></row><row><cell>run</cell><cell cols="2">α =,5 Pre. Rec.</cell></row><row><cell>RGAI AE 3</cell><cell>0.18</cell><cell>0.22 0.24</cell></row><row><cell>RGAI AE 1</cell><cell>0.15</cell><cell>0.18 0.19</cell></row><row><cell>Intelius AE unofficial</cell><cell>0.13</cell><cell>0.16 0.17</cell></row><row><cell>RGAI AE 2</cell><cell>0.12</cell><cell>0.16 0.15</cell></row><row><cell>RGAI AE 4</cell><cell>0.12</cell><cell>0.15 0.16</cell></row><row><cell>RGAI AE 5</cell><cell>0.12</cell><cell>0.15 0.15</cell></row><row><cell>BYU</cell><cell>0.10</cell><cell>0.11 0.14</cell></row><row><cell>WOLVES AE 1</cell><cell>0.10</cell><cell>0.18 0.09</cell></row><row><cell>WOLVES AE 2</cell><cell>0.06</cell><cell>0.08 0.07</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="5,144.73,657.79,146.53,7.86"><p>http://developer.yahoo.com/search/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="6,144.73,646.84,335.87,7.86;6,144.73,657.79,264.52,7.86"><p>Note that computer scientist names were obtained from a list of conference program committee members, so we already know the researcher's identity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="7,144.73,657.79,165.97,7.86"><p>https://www.mturk.com/mturk/welcome</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="9,144.73,657.79,159.16,7.86"><p>http://turkers.proboards.com/index.cgi</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4" coords="11,144.73,592.04,335.86,7.86;11,144.73,603.00,108.89,7.86"><p>Note that results for B-Cubed extended and regular B-Cubed are identical on a non-overlapping clustering.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5" coords="11,144.73,613.96,335.87,7.86;11,144.73,624.92,234.21,7.86"><p>With the exception of 50 names from the computer science conference names, for which only documents about one person where considered</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6" coords="11,144.73,635.88,335.87,7.86;11,144.73,646.84,335.86,7.86;11,144.73,657.79,226.09,7.86"><p>We also considered the cluster F-measure as an criterion for choosing the representative cluster. We found that this method often missed the cluster with more relevant attributes, resulting in extremely low evaluation results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7" coords="12,144.73,603.00,335.86,7.86;12,144.73,613.96,335.87,7.86;12,144.73,624.92,108.53,7.86"><p>Note that these tables reflect the scores obtained taking into account only two people for each person name. For this reason this table should not be directly compared to previous WePS campaigns.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8" coords="12,144.73,635.88,335.86,7.86;12,144.73,646.84,335.87,7.86;12,144.73,657.79,233.18,7.86"><p>The Unanimous Improvement Ratio (UIR) checks, for each system pair, to what extent the improvement is robust across potential metric weighting schemes (see<ref type="bibr" coords="12,464.72,646.84,9.52,7.86" target="#b0">[1]</ref>). This measure was also employed in WEPS2 campaign<ref type="bibr" coords="12,365.62,657.79,9.22,7.86" target="#b3">[4]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the project <rs type="projectName">QEAVis-Catiex</rs> (<rs type="grantNumber">TIN2007-67581-C02-01</rs>).</p><p>We would like to thank <rs type="person">Intelius</rs> for sharing their expertise in the field of people search and for their collaboration on the task design and the gold standard creation. The economic support made possible such a large scale annotation effort.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_3RvPFhC">
					<idno type="grant-number">TIN2007-67581-C02-01</idno>
					<orgName type="project" subtype="full">QEAVis-Catiex</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,474.52,337.64,7.86;14,151.52,485.48,329.07,7.86;14,151.52,496.44,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,304.31,474.52,176.29,7.86;14,151.52,485.48,273.67,7.86">Combining evaluation metrics via the unanimous improvement ratio and its application in weps clustering task</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,444.98,485.48,35.62,7.86;14,151.52,496.44,300.96,7.86">2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,506.80,337.63,7.86;14,151.52,517.75,329.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,355.24,506.80,125.34,7.86;14,151.52,517.75,209.51,7.86">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,367.98,517.75,85.34,7.86">Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,528.11,337.64,7.86;14,151.52,539.07,329.07,7.86;14,151.52,550.02,317.90,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,308.70,528.11,171.90,7.86;14,151.52,539.07,206.61,7.86">The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,378.25,539.07,102.33,7.86;14,151.52,550.02,264.69,7.86">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,560.38,337.64,7.86;14,151.52,571.34,329.07,7.86;14,151.52,582.29,182.91,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,301.27,560.38,179.32,7.86;14,151.52,571.34,130.57,7.86">Weps 2 evaluation campaign: overview of the web people search clustering task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,299.91,571.34,180.69,7.86;14,151.52,582.29,154.58,7.86">2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,592.65,337.63,7.86;14,151.52,603.61,329.07,7.86;14,151.52,614.57,134.83,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,266.26,592.65,214.33,7.86;14,151.52,603.61,75.23,7.86">Entity-based cross-document coreferencing using the vector space model</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,247.89,603.61,232.71,7.86;14,151.52,614.57,82.52,7.86">Proceedings of the 17th international conference on Computational linguistics</title>
		<meeting>the 17th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,624.92,337.64,7.86;14,151.52,635.88,329.07,7.86;14,151.52,646.84,329.07,7.86;14,151.52,657.79,196.69,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,230.64,624.92,249.95,7.86;14,151.52,635.88,110.04,7.86">Fast, cheap, and creative: evaluating translation quality using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,285.57,635.88,195.03,7.86;14,151.52,646.84,176.63,7.86">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,120.67,337.63,7.86;15,151.52,131.63,329.07,7.86;15,151.52,142.59,45.44,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,307.49,120.67,173.10,7.86;15,151.52,131.63,59.54,7.86">Using web graph structure for person name disambiguation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Elena Smirnova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Trousse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,230.23,131.63,217.90,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,153.55,337.64,7.86;15,151.52,164.51,329.07,7.86;15,151.52,175.46,195.72,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,364.40,153.55,116.19,7.86;15,151.52,164.51,209.73,7.86">Weps-3 evaluation campaign: Overview of the on-line reputation management task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G D S</forename><surname>Enrique Amig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,379.78,164.51,100.81,7.86;15,151.52,175.46,114.28,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,186.42,337.63,7.86;15,151.52,197.38,195.72,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,269.90,186.42,82.33,7.86">Talp at weps-3 2010</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodrguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,376.04,186.42,104.54,7.86;15,151.52,197.38,114.28,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,208.34,320.45,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,199.87,208.34,235.22,7.86">personal communication from Amazon sales representative</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoskins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,219.30,337.98,7.86;15,151.52,230.26,300.78,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,317.90,219.30,146.10,7.86">Cross-document coreference for weps</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">O</forename><surname>Iustin Dornescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lesnikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,151.52,230.26,219.33,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,241.22,337.98,7.86;15,151.52,252.18,329.07,7.86;15,151.52,263.14,73.34,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,237.76,241.22,242.83,7.86;15,151.52,252.18,84.47,7.86">Web person name disambiguation by relevance weighting of extended feature sets</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,255.67,252.18,220.12,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,274.09,337.98,7.86;15,151.52,285.05,329.07,7.86;15,151.52,296.01,20.99,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,263.94,274.09,216.66,7.86;15,151.52,285.05,38.61,7.86">Person attribute extraction from the textual parts of web pages</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,208.27,285.05,216.04,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,306.97,337.98,7.86;15,151.52,317.93,329.07,7.86;15,151.52,328.89,225.00,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,375.03,306.97,105.56,7.86;15,151.52,317.93,230.37,7.86">Daedalus at webps-3 2010: k-medoids clustering using a cost function minimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>-R. Sara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lana-Serrano</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Gonzlez-Cristbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,406.10,317.93,74.49,7.86;15,151.52,328.89,143.56,7.86">Third Web People Search Evaluation Forum (WePS-3)</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,339.85,337.98,7.86;15,151.52,350.81,301.61,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,260.09,339.85,130.38,7.86">Weps2 attribute extraction task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,413.91,339.85,66.68,7.86;15,151.52,350.81,273.28,7.86">2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,361.77,337.97,7.86;15,151.52,372.73,329.07,7.86;15,151.52,383.68,329.07,7.86;15,151.52,394.64,196.69,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,348.42,361.77,132.17,7.86;15,151.52,372.73,240.78,7.86">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,409.88,372.73,70.71,7.86;15,151.52,383.68,262.68,7.86">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
