<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.16,115.96,271.03,12.62">Cross-document coreference for WePS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.80,154.85,66.88,8.74"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
							<email>i.dornescu2@wlv.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">RIILP</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.80,154.85,82.33,8.74"><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
							<email>c.orasan@wlv.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">RIILP</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.82,154.85,79.07,8.74"><forename type="first">Tatiana</forename><surname>Lesnikova</surname></persName>
							<email>tatiana.lesnikova@wlv.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">RIILP</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.16,115.96,271.03,12.62">Cross-document coreference for WePS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D55BE4B1C76D73F2A0223AF68F6EEFBE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A good clustering performance depends on the quality of the distance function used to asses similarity. In this paper we propose a pairwise document coreference model to improve performance over a wordvector similarity approach for the WePS 3 clustering task. We identify a simple criterion which discriminates between highly ambiguous queries, i.e. many small clusters, and balanced queries, i.e. fewer, larger clusters. A document clustering framework was developed facilitating direct comparison between different parameters, features and algorithms. It uses a unified feature representation to afford a wide variety of clustering pipelines. Using the predicted coreference likelihood and a simple clustering algorithm, we achieve comparable results on the WePS 2 dataset, and competitive performance on the WePS 3 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Disambiguating people names in Web search results is an active research area, combining several Natural Language Processing challenges such as cross-document coreference, information extraction and document clustering. A good clustering performance depends upon the quality of the similarity function used. Most previous work uses a combination of content-based features, e.g. word, bigrams, NEs, and person attributes, e.g. email, date of birth, title, to compute document similarity <ref type="bibr" coords="1,179.12,498.26,9.96,8.74" target="#b0">[1]</ref>.</p><p>The main aim of this study is to use a supervised cross-document coreference approach <ref type="bibr" coords="1,178.06,523.39,10.52,8.74" target="#b1">[2]</ref> to improve performance for the WePS clustering task. A pairwise model is used to predict the likelihood that two documents refer to the same person. A clustering algorithm will then use these predictions to cluster the documents. In order to have a better understanding of what works best and why, we developed a generic framework for document clustering which allows complex pipelines to be built. By sharing the same feature extraction base, direct comparison between different parameters and algorithms is straightforward.</p><p>This paper is structured as follows: the generic framework is presented in Section 2. In Section 3 the elements of the processing pipeline are detailed, and a criterion distinguishing the ambiguity of a query is proposed. The three clustering algorithms are briefly presented in Section 4, and in Section 5 we detail the experiments and discuss the results.</p><p>In the recent WePS literature, two main approaches can be distinguished which need to be accommodated by the framework:</p><p>vector-space clustering -documents are represented as a weighted feature vectors -points in a high-dimensional space, which are clustered using a pairwise distance function. Usually the weighting scheme is tf • idf , the distance function is either cosine or euclidean, and the algorithm is single-link hierarchical agglomerative clustering (HAC). The stopping criterion most commonly used is a threshold limiting the link distance between the two nearest clusters. This value is learnt from training data <ref type="bibr" coords="2,282.96,249.93,9.96,8.74" target="#b0">[1]</ref>.</p><p>feature-graph clustering -the document×feature occurrence matrix is used to build a support graph which is used to compute a better document similarity. Usually a bipartite graph is built in which document node d is connected to feature node f if the feature f is extracted from document d. Afterwards, either a document×document graph is built, with the edges' weights reflecting the number of shared features (e.g. number of paths of length 2 between the two documents), or, conversely, a feature×feature graph is built using the common documents as support. Based on the clusters identified in this derived graph, the solution to the initial problem is built <ref type="bibr" coords="2,303.16,369.19,9.96,8.74" target="#b2">[3]</ref>.</p><p>While conceptually different, both these approaches can be abstracted using a unified graph representation: features extracted from the documents are used to build a derived graph, its nodes are then clustered and the documents associated with each of the clusters are returned. By sharing the same feature extraction algorithms, weighting schemes, and distance functions, various approaches can be directly compared, to gain insights into how efficient solutions to the problem can be built. In the context of Web search, users expect results to be available in seconds. For this reason, the ultimate aim of this framework is to analyse the trade-off between computational cost and performance benefit of different approaches to the WePS clustering task.</p><p>The architecture of the framework is presented in Figure <ref type="figure" coords="2,401.28,500.70,3.87,8.74" target="#fig_0">1</ref>. In the first step of the processing pipeline (a), plain text document views are extracted from the HTML files. A view can also employ NLP techniques to extract only some of the contents of another view (b), e.g. a set of snippets mentioning the target person, or meta data such as keywords, title, author and so on. The second step is the feature extraction stage (c) when vectors are extracted from document views. For each different feature F , a frequency matrix is built (m(d i , f j ) = how many times feature value f j occurred in document d i ). The simplest such feature consists of tokenizing and extracting content words (tokenization, stop word removal, indexing), while more involved features employ off-the shelf NLP tools and person-data information extraction. Rows from these matrices can be merged and/or weighted to create feature vectors. Composite features aggregate feature vectors from other features. Pairwise features reflect the similarity between document pairs, computed as a distance between their feature vectors.</p><p>In the next step a derived graph is built either directly from frequency data (feature-graph clustering), or induced by the pairwise distance matrix (vectorspace clustering). This graph is clustered using generic algorithms, and then the solution is built. In this paper we investigate whether ML can be used to provide better compatibility scores rather than afforded by the standard approach which uses cosine similarity in a tf • idf weighted vector space. The intuition is that a machine learning algorithm can employ both content-based similarity and semantic rules to make better predictions regarding coreference likelihood. For example, sharing the same email address is predictive for the coreference relation, while having different dates of birth entails two distinct persons. Such rules can have priority over the more generic token based similarity measures. However, we need to take into account the complexity of the IE task: the attribute values will be noisy (low precision) and sparse (low recall). Further more, they are not necessarily unique per document, e.g. several job titles, and could need specialised semantic similarity measures to be compared, e.g. email addresses are easier to compare than job titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>View</head><p>The IE framework employed is described in Section 3.2, while the clustering algorithms are briefly presented in Section 4. The processing steps are summarised in Table <ref type="table" coords="4,210.96,142.90,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Extraction Framework</head><p>The plain text extracted from the documents (dom.view ) was tokenised and indexed using lucene. To extract named entities, a view (NER.view ) was implemented wrapping the Stanford NER tool <ref type="bibr" coords="4,320.05,225.00,9.96,8.74" target="#b5">[6]</ref>. In was suggested <ref type="bibr" coords="4,415.48,225.00,10.52,8.74" target="#b6">[7]</ref> that simply using capitalisation yields better results, because generic NER tools are usually trained on news wire corpora and do not perform as well on noisy web data, therefore we also used this as an alternative feature (cap.view ).</p><p>To combine both named entities and terminology, a complex analysis tool based on Wikipedia-Miner<ref type="foot" coords="4,249.10,283.44,3.97,6.12" target="#foot_0">2</ref> was used <ref type="bibr" coords="4,297.49,285.02,9.96,8.74" target="#b7">[8]</ref>. The tool examines the text of the page and detects the most relevant Wikipedia articles, based on information such as the probability of a span s to be a link to an article a and the probability of two articles to co-occur in the same Wikipedia document <ref type="bibr" coords="4,368.73,320.88,9.96,8.74" target="#b3">[4]</ref>. The process is related to topic indexing and to explicit semantic analysis: each document is represented as a vector in a high dimensional space, but instead of words, the dimensions are unambiguous Wikipedia topics, ranked by their relevance score (wiki.view ).</p><p>A novel pairwise feature is the longest common substring (LCS) between two textual views. Documents describing the same person, tend to share some phrases and sentences, sometimes entire paragraphs. While naively parallel, LCS takes too much time to be computed at query time on the full text of the documents, but it performed quite well using the smaller snippet.view instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token-feature weighting</head><p>The system which achieved the best official result on WePS 2 <ref type="bibr" coords="4,412.92,468.97,10.52,8.74" target="#b8">[9]</ref> showed that using a web-scale corpus to have more accurate IDF values helped boost performance significantly. Therefore we compared two ways of computing IDF : local -only the set of documents for the current query are used (less than 200 documents), and global -all the documents in all WePS corpora are used (around 70K documents). To speed up computation, words with DF = 1 were removed. We also considered increasing the DF threshold to see if this yields better results. Figure <ref type="figure" coords="4,193.00,552.66,4.98,8.74">2</ref> shows that the difference between the two weighting schemes is significant.</p><p>When DF threshold is increased: document vectors become nil, which is expected. The immediate implication is that the cosine similarity becomes undefined. We observed an interesting effect if global IDF is used: frequent words such as home and contact have very low IDF values, thus considerably more feature vectors become very small (norm less than 10 -6 ) and are practically nil. Table <ref type="table" coords="4,162.92,636.58,4.98,8.74" target="#tab_1">2</ref> shows the percentage of undefined pairwise cosine values. Usually log  smoothing is used to avoid representation errors (we use the default similarity implementation from Lucene<ref type="foot" coords="6,260.08,295.73,3.97,6.12" target="#foot_1">3</ref> ), but we discovered that queries with many clusters tend to have considerably more undefined values than queries with fewer clusters. Previous work suggests that a criterion discriminating between different types of sets is beneficial. In <ref type="bibr" coords="6,236.76,478.64,9.96,8.74" target="#b6">[7]</ref>, a simple heuristic that achieves good results is proposed: if at least 3 documents from a random sample of 10 documents are coreferent use all-in-one, otherwise use one-in-one. The criterion is manually evaluated and the clustering is not informative, therefore this is yet another baseline. The performance is F h = 0.71, compared to the two baselines: F aio = 0.60, F oio = 0.39. In <ref type="bibr" coords="6,208.80,538.42,10.52,8.74" target="#b0">[1]</ref> it is acknowledged that choosing an individual clustering threshold for each set instead of a global value for the entire data achieves a high performance F = 0.85. To pick the parameter value for each set an oracle method is used which exploits gold standard information that is not available to a normal system. The performance reported serves as a theoretical upper bound.</p><p>We used gold data to plot each set as a point in a bi-dimensional space (Figure <ref type="figure" coords="6,169.06,610.74,3.87,8.74" target="#fig_2">3</ref>): the number of clusters vs. the ratio coref (the proportion of all pairwise relations that are coreferent). These points were then clustered using k-means (k=3) yielding 3 types of sets: type I -few clusters and predominantly coreferent relations, type II -average number of clusters and ratio coref , and type III -large number of clusters, predominantly distinct relations. Using as features the ratio of missing values for different DF thresholds, a ML model was trained on the train dataset, and its predictions were used to split development and test data into three parts -experiment 3P. This problem is rather difficult because of the very little amount of data available (15 of the training queries were deemed outliers because they have less than 40 documents). In the alternative experiment 2P, only the ratio coref was used to partition the training data into two sets: Type A (ratio &lt; 0.3) -ambiguous sets, and Type B (ratio &gt; 0.3) -balanced sets. We used decision trees and jrip models from Weka <ref type="bibr" coords="7,354.85,385.92,14.61,8.74" target="#b9">[10]</ref>. The best performance is achieved by the J48 classifier: F = 0.63 for the first partition, and F = 0.82 for the second. These partitions of the WePS data enable us to train models and to use distinct parameters for each type.</p><p>Unlike the cosine similarity, euclidean distance is always defined, with the drawback that values are not restricted to the interval [0, 1]. Using the same feature vectors, euclidean vs. cosine measures were compared in terms of their predictive power regarding the pairwise coreference relation. The euclidean pairwise features had a much better ranking than their cosine counterparts when using χ 2 , however this does not translate to better performance for the ML classifiers: while performance on the training data is indeed better, on the development set it is considerably worse. The over-fitting behaviour suggests that the euclidean distance has poor generalization power for our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute Extraction Framework</head><p>We employed a simple IE approach based on hand-crafted regular expressions. Based on their values, there are three types of attributes <ref type="bibr" coords="7,392.36,596.34,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="7,404.54,596.34,7.01,8.74" target="#b8">9]</ref>. The first type have values which can be matched by regular expressions, e.g. email address, dates, telephone, url address, post code. The second type have values which can be looked-up in gazetteers, e.g. degree, occupation, major, nationality. The third type have named entities as values, e.g. place of birth, place of death, address, family, mentor, affiliation, school, and therefore rely on NER and capitalization to detect possible candidates. The attribute extractors usually check a small window of text around a target entity mention, and use trigger words and phrases as well as candidate values matcher, i.e. gazetteer, regex, NER. One of the drawbacks of this approach is that it does not make use of automatic bootstrapping algorithms and that it relies on textual occurrences of attribute values. In the document collection more often than not, these attributes are presented in tables. For example, contact details -address, email, url, telephone, fax, and so on, tend to be displayed without mentioning the trigger words we rely on. In this case, special extractors are needed which are able to detect several fields at the same time <ref type="bibr" coords="8,199.58,226.59,9.96,8.74" target="#b8">[9]</ref>.</p><p>Our extraction framework is ill suited for the noisy Web documents. We allowed several values to be extracted for the same attribute, and we experimented with overlap measures: Jaccard index, a weighted version which uses value frequencies, and a simple non-void intersection criterion, i.e. the two documents must share at least one attribute value. To our disappointment, the attribute features are very sparse.</p><p>The attribute features yield mixed results: they are ranked highest using Information Gain, but lowest using GainRatio, χ 2 ranks them lowest of all. This suggests they are too sparse to have a big impact on the overall result, and that noise due to inaccurate extraction further limits their utility.</p><p>The drawbacks of the IE framework employed make it the biggest limiting factor for our approach. At the time of writing the final results for Task1b -Attribute Extraction are not yet available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Clustering Algorithms</head><p>To perform both vector-space clustering and feature-space clustering, our framework represents the pairwise similarity function as a matrix, which also corresponds to a weighted graph. This matrix can be computed using arbitrarily complex methods, e.g. latent semantic analysis, explicit semantic analysis, but in this case it can only be used with 'stable' clustering algorithms (e.g. k-medoids is stable, while k-means is not).</p><p>One of the challenges in WePS is that the number of clusters is unknown. Depending on the clustering algorithm employed, the stopping criterion is controlled via a parameter which is observed on training data. A quality function is evaluated at each step of the algorithm. Once this function reaches a critical point the clustering algorithm is stopped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchical agglomerative clustering (HAC)</head><p>HAC is one of the most commonly used algorithms due to its simplicity and ability to control granularity. The algorithm starts with singleton clusters and, at every step, merges the two nearest clusters. The link distance between two clusters can be computed in several ways. We investigated five aggregation functions (see Table <ref type="table" coords="8,208.34,656.12,3.87,8.74" target="#tab_2">3</ref>). The algorithm stops when only k clusters remain (k is an the maximum distance between documents ADJCOMPLETE adjusted complete link using the largest within cluster distance input parameter) or when the link distance between the two nearest clusters is greater than a threshold delta (another input parameter). In WePS, systems mainly use the second criterion, selecting the value delta which maximises training performance. We used an implementation based on Weka <ref type="bibr" coords="9,404.48,280.01,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Markov clustering</head><p>Markov clustering (MCL) <ref type="bibr" coords="9,253.34,329.31,10.52,8.74" target="#b4">[5]</ref> is a general purpose graph clustering algorithm commonly used in biology. It simulates network flow via two algebraic operations on stochastic matrices. The algorithm stops when convergence is achieved, but the clustering granularity can be controlled via an inflation I parameter. <ref type="foot" coords="9,134.77,375.56,3.97,6.12" target="#foot_2">4</ref> . Network clustering (community detection) algorithms can benefit from preprocessing the input graph by e.g. removing edges with low similarity <ref type="bibr" coords="9,433.61,389.09,14.61,8.74" target="#b10">[11]</ref>. In our experiments, removing edges longer than a threshold delta did not have much impact on clustering outcome: the output is usually one large cluster. A filtering technique which worked well was to limit node degree by keeping the k best neighbours per node (knn). This technique is common in spectral clustering <ref type="bibr" coords="9,462.33,436.91,14.61,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Connected components</head><p>The success of single link HAC is intriguing. For the single link delta HAC, none of the edges with a weight above the threshold are considered by the algorithm, and can thus be removed. Intuitively, the clusters found will correspond to the connected components of the rest of the graph. We also used a connected components algorithm (CC) to investigate how it compares to the single link HAC implementation. To our surprise, the results of the two algorithms were different, with the CC algorithm achieving the best performance in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Findings</head><p>To build the coreference models, WePS 1 data was used for training (6346 documents, 70 queries), WePS 2 data was used for parameter tuning -development data (3444 documents, 30 queries) and the WePS 3 data was used for test (57357 documents, 300 queries). When the gold standard for WePS 3 became available, it was used to search for the best clustering parameters on the WePS 3 data.</p><p>To compare the content similarity approach with the ML coreference approach, we ran three main experiments. In the first experiment (wiki), we used the cosine similarity between vectors produced by Wikipedia Miner (wiki.view ). In the second experiment (3P) we split the data into 3 parts as described in Section 3.1. A pairwise coreference model was trained for each part, using a set of over 50 pairwise features. The third experiment is similar, only this time we use the second partitioning method to split data into 2 parts. The same set of features was used to train the pairwise coreference models, but this time more pruning was employed, to avoid over-fitting and to obtain better confidence scores for the rules learnt. The pairwise features use a combination of parameters: document view, IDF type, DF limit and similarity measure, as well as the overlap between person attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Using Wikipedia topics for semantic similarity (wiki)</head><p>There are significant differences between the different WePS corpora and their respective evaluation methodologies. This explains to a certain extent the drop in performance for all WePS 3 participants. Results in figures 4 and 5 show that HAC has different behaviour on the test data than on the development data: single-link HAC drops from best to worst, while for three of the link types performance is almost constant in WePS3, regardless of the delta threshold. This is most likely due to the larger number of documents in each query: up to four times more document pairs per query in WePs 3 compared to WePS 1. The simple CC clustering is the best performer on both data sets, and has consistent behaviour, in both cases reaching maximum F score for delta ∈ [0.7, 0.8]. The first official run, Wolves1, used single link HAC with delta = 77, but only achieved average performance in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pairwise coreference model (3P)</head><p>The number of clusters and their sizes vary greatly from query to query. While the pairwise similarity values are usually within the [0, 1] interval, their distribution seems to be very query-specific. Training a ML model on the entire dataset yielded low performance due to the noisy features. For this reason we applied the criterion from Section 3.1.</p><p>When training the decision trees classifiers, while good classification accuracy was achieved for types I and III, the kappa statistic was rather low. Our initial experiments with selecting the delta threshold individually for each set did not improve performance much. The fact that the maximum score remains almost constant for most threshold values suggests that the evaluation measures need to be adjusted to account for chance, in order to better reflect performance.</p><p>The size difference between test, 200 documents per query, and development, 100-150 documents per query, affects the performance of MCL: in development, for low knn it creates many singleton clusters while for values over 90 it creates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>At the time of the WePS 3 competition, the framework was still under development. Only single and average link HAC runs were available, for two experiments: wiki and 3P. The first run submitted, Wolves1, used average link HAC on wiki with delta = 0.77. The experiments carried out after the competition revealed that the choice for delta does not affect much the performance of this first system (Figure <ref type="figure" coords="14,169.75,343.72,3.87,8.74">5</ref>), with the official result on WePS 3 F = 0.40. The second run submitted, Wolves2, used the pairwise predictions of the 3P model and average HAC clustering. Figure <ref type="figure" coords="14,216.43,367.63,4.98,8.74">6</ref> shows that the algorithm plateaus on WePS 2, achieving F 0.5 = 0.66 for delta ∈ [0.06, 0.66] on development. The threshold used for the official run achieved a poor performance F 0.5 = 0.36. Using the gold-standard for WePS 3, we found that delta = 0.5 yields better performance F 0.5 = 0.45; only one team obtained a better official result.</p><p>The MCL algorithm performed well. To achieve the best performance, both the knn limit and the inflation parameter need to be increased compared to the development set. It seems that the value for the knn filter is best chosen relative to the number of documents.</p><p>Average and single link perform better than the other link types, but neither stands out as a clear choice, perhaps due to the differences between the two data sets. The surprising result was the performance achieved by connected components clustering which is a naive algorithm exploiting the network topology. While it does not improve the state-of-the-art, it achieves competitive results: best result on WePS3 is F 0.5 = 0.54. Table <ref type="table" coords="14,320.87,536.04,4.98,8.74" target="#tab_3">4</ref> shows the best results found on the WePS3 test data, after the release of the gold standard.</p><p>One issue that became obvious during these experiments is that the standard evaluation measures used in WePS make comparisons across datasets difficult. This is because only the upper-bound is normalised: a perfect clustering will have score 1.00, but the performance of a random uninformative clustering, instead of being 0.00, varies significantly from query to query. The official BCubed measure is good for relative comparison of two clustering solutions for the same query, but the overall average is difficult to be interpreted as absolute performance score. Perhaps using measures adjusted for chance <ref type="bibr" coords="14,387.67,644.16,14.61,8.74" target="#b11">[12]</ref>, such as adjusted Rand index, adjusted mutual information or kappa, is one way to achieve better understanding of what works for WePS and how well it works. Another concern is that the WePS3 evaluation methodology, by considering documents split into three clusters -person A, person B and other, differs significantly from the initial task formulation. This makes it even more difficult to determine if a system with very high performance on WePS 3 data will perform similarly on real life data, when more then two clusters, if not all, are evaluated. Future work will focus on extending the IE framework and on adding featurespace clustering algorithms which achieve state-of-the-art performance on WePS2. Mining high precision rules exploiting semantic attributes will also be investigated, as recall seems to be less important. Another model which will be investigated is to consider pairwise relation as coreferent, undecided and distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper reports the experiments carried out for the WePS3 clustering task. A generic framework was developed allowing the implementation and comparison of varied clustering pipelines. We compared a similarity-based approach in a Wikipedia-topic space representation with two rule-based ML coreference models trained on pairwise document similarity measures. We showed that a simple clustering algorithm can exploit high precision predictions of the pairwise coreference model, achieving comparable performance on WePS 2 dataset and competitive performance on the WePS 3 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,209.54,465.25,196.28,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: General architecture of the framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,236.00,345.83,8.74;6,134.77,247.95,343.83,8.74"><head></head><label></label><figDesc>Fig. 2: Word-vector weighting and pairwise distance; impact of global IDF and DF threshold on cosine and euclidean measures (red-coreferent, blue-distinct)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,145.88,241.15,323.60,9.65"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Relation between the number of clusters (y) and the Ratio coref (x )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,136.26,128.52,332.22,529.19"><head>Table 1 :</head><label>1</label><figDesc>Main components of the processing pipeline</figDesc><table coords="5,137.13,144.04,331.35,220.00"><row><cell></cell><cell>dom.text</cell><cell>Jericho HTML parser is used to clean HTML and extract</cell></row><row><cell></cell><cell></cell><cell>text from the DOM document, then openNLP 1 is used to</cell></row><row><cell>document views</cell><cell>plain.text xhtml.view snippet.view</cell><cell>split the text into sentences the w3m text browser is used to render the files and dump the the textual content as displayed on screen returns a cleaned xhtml version of the HTML file extracts all snippets spanning ws=300 characters before and after each target mention; overlapping windows are merged</cell></row><row><cell></cell><cell>NER.view</cell><cell>employs Stanford NER tool to extract named entities from</cell></row><row><cell></cell><cell></cell><cell>the underlying view</cell></row><row><cell></cell><cell>capitalization</cell><cell>extracts all the capitalised words and/or sequences, a high-</cell></row><row><cell></cell><cell></cell><cell>recall NER baseline</cell></row><row><cell>document features</cell><cell></cell><cell></cell></row></table><note coords="5,154.45,280.55,23.36,7.86;5,226.40,280.55,242.08,7.86;5,226.40,291.51,242.08,7.86;5,226.40,302.47,39.43,7.86;5,154.45,313.43,25.65,7.86;5,226.40,313.43,215.08,7.86;5,154.45,324.39,50.23,7.86;5,226.40,324.39,242.08,7.86;5,226.40,335.34,242.07,7.86;5,226.40,346.30,141.21,7.86;5,154.45,357.26,14.72,7.86;5,226.40,357.26,242.08,7.86;5,226.40,368.22,242.08,7.86;5,226.40,379.18,169.37,7.86;5,136.26,445.79,7.86,32.84;5,136.26,411.16,7.86,31.57;5,154.45,395.14,314.04,7.86;5,226.40,406.10,240.80,7.86;5,226.40,417.06,41.08,7.86;5,154.45,428.02,236.47,7.86;5,154.45,438.98,42.80,7.86;5,226.40,438.98,141.63,7.86;5,368.03,437.21,3.65,5.24;5,375.26,438.98,80.44,7.86;5,154.45,451.08,55.70,7.86;5,226.40,451.08,193.68,7.86;5,423.84,448.79,20.87,5.24;5,423.84,456.17,20.87,5.24;5,154.45,463.29,24.83,7.86;5,226.40,463.29,242.08,7.86;5,154.45,480.38,28.95,7.86;5,226.40,480.38,113.63,7.86;5,354.82,479.76,23.68,4.37;5,380.50,475.10,47.68,6.39;5,354.82,491.92,23.68,4.37;5,380.50,487.25,47.68,6.39;5,137.08,512.13,7.86,14.21;5,154.45,502.37,51.45,7.86;5,226.41,502.37,242.07,7.86;5,226.40,513.33,242.08,7.86;5,226.40,524.29,234.77,7.86;5,136.26,579.09,7.86,38.99;5,154.45,540.25,20.22,7.86;5,226.40,540.25,242.08,7.86;5,226.40,551.21,242.07,7.86;5,226.40,562.17,242.08,7.86;5,226.40,573.13,171.68,7.86"><p>words standard tokenization and stop word filtering (using apache lucene library) to create a word vector representation for each document tokens uses only the marked-up entities, e.g. from NER.view densification detects most relevant Wikipedia topics using a wikification service based on Wikipedia Miner [4]; the document is represented as a weighted topic vector I.E. RegEx-based framework for extracting person-attributes required in Task1b (see Section 3.2); for each attribute, the most likely candidate values are extracted pairwise features tf • idf weighting the standard weighting scheme for token-based vectors; we experimented with two different IDF scopes and several DF thresholds cosine similarity dot product of length normalised vectors Minkowski most experiments carried out for L 2 (euclidean distance) Jaccard index the overlap between attribute values: J(A, B) = |A∩B| |A∪B| match if the sets share attribute values: m(A, B) = 1 ⇐⇒ A∩B = φ overlap weighted version of Jaccard: x∈A∩B w A (x)+w B (x) x∈A∪B w A (x)+w B (x) ML Weka toolkit we experimented mainly with rule based classifiers because they are fast, granularity can be controlled by pruning and they also give insights into what works and what does not clustering HAC standard hierarchical agglomerative clustering, using a pairwise distance matrix and any of the following link types: single, average, mean, complete, adjcomplete; the maximum link threshold delta is observed in training</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,146.50,363.40,322.35,60.27"><head>Table 2 :</head><label>2</label><figDesc>Proportion of undefined cosine similarities for different DF limits</figDesc><table coords="6,151.28,388.88,312.80,34.78"><row><cell>IDF</cell><cell>minDF=2</cell><cell>minDF=5</cell><cell>minDF=10</cell><cell cols="2">minDF=15 minDF=20</cell></row><row><cell>local</cell><cell>3055 1%</cell><cell>3840 1%</cell><cell>4396 1%</cell><cell>4994 1%</cell><cell>6653 2%</cell></row><row><cell cols="6">global 51943 14% 161604 44% 198633 55% 212035 58% 224715 62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,142.55,127.36,246.60,66.23"><head>Table 3 :</head><label>3</label><figDesc>HAC: link types</figDesc><table coords="9,142.55,152.85,246.60,40.74"><row><cell>SINGLE</cell><cell>the minimum distance between documents</cell></row><row><cell>AVERAGE</cell><cell>the average distance between documents</cell></row><row><cell>MEAN</cell><cell>the mean distance between documents</cell></row><row><cell>COMPLETE</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,12.07,148.87,468.44,478.77"><head>Table 4 :</head><label>4</label><figDesc>Fig.5: wiki experiment: performance on the test set one large cluster; on test data, MCL only reaches the best performance for higher thresholds. The second official run, Wolves2, used group average HAC, but for the selected delta threshold, it performed poorly on test data. Again, CC clustering outperforms both group average HAC and single link HAC on test data. This suggests that using a set of high precision rules can achieve competitive performance. Theoretically, for a query with n documents and c clusters, a set of n -c coreferent edges is enough to build the complete clustering solution. A future direction of research is to use sampling and weighting approaches to train high precision-low recall models. The best results avhieved on WePS3 by each algorithm and experiment HACsingle 0.25 0.88 0.36 δ=0.76 0.45 0.70 0.48 δ=0.11 0.54 0.55 0.48 δ=0.16 HACaverage 0.39 0.72 0.40 δ=0.71 0.51 0.50 0.45 δ=0.46 0.55 0.56 0.50 δ=0.61</figDesc><table coords="11,12.07,148.87,468.44,465.34"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">delta HAC (wiki)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CC clustering (wiki)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F0.5</cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AVERAGE SINGLE COMPLETE ADJCOMPLETE MEAN</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP BER F0.5 RI</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell cols="2">0.8</cell><cell>0.9</cell><cell>1</cell><cell></cell><cell></cell><cell>0</cell><cell>0.2</cell><cell cols="2">0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>delta HAC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">delta HAC -SINGLE delta HAC -SINGLE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta HAC -AVERAGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">delta HAC -ADJCOMPLETE (wiki)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F0.5</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AVERAGE SINGLE COMPLETE ADJCOMPLETE MEAN</cell><cell>0.6 0.5 0.4</cell><cell></cell><cell>0.6 0.5 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP BER F0.5 RI</cell><cell>BEP BER F0.5 RI</cell><cell></cell><cell>0.6 0.5 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP BER F0.5 RI</cell><cell>0.6 0.4</cell><cell></cell><cell></cell><cell>BEP BER F0.5 RI</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.2</cell><cell>0.4</cell><cell>0.4</cell><cell>0.6</cell><cell>0.6</cell><cell>0.8</cell><cell>0.8</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>delta threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="21">Fig. 4: wiki experiment: performance on the development set</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">HAC (wiki)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CC clustering (wiki)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SINGLE AVERAGE</cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>COMPLETE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BER</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADJCOMPLETE MEAN</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0.2</cell><cell cols="2">0.4</cell><cell cols="2">0.6</cell><cell cols="2">0.8</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">delta threshold</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">delta HAC -SINGLE (wiki)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">delta HAC -AVERAGE (wiki)</cell><cell></cell><cell></cell><cell cols="3">delta HAC -ADJCOMPLETE (wiki)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BEP</cell><cell></cell><cell></cell><cell></cell><cell>BEP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BER</cell><cell></cell><cell></cell><cell></cell><cell>BER</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F0.5</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F0.5</cell><cell>0.4</cell><cell></cell><cell></cell><cell>F0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">0.4</cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">delta threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>delta threshold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,150.37,127.36,314.61,104.10"><head>Table 5 :</head><label>5</label><figDesc>WePS3 official results</figDesc><table coords="15,150.37,152.85,314.61,78.62"><row><cell>System</cell><cell cols="3">avg. BCubed precision avg. BCubed recall avg. F-measure</cell></row><row><cell>YHBJ 2 unofficial</cell><cell>0.61</cell><cell>0.6</cell><cell>0.55</cell></row><row><cell>AXIS 2</cell><cell>0.69</cell><cell>0.46</cell><cell>0.5</cell></row><row><cell>WOLVES 1</cell><cell>0.31</cell><cell>0.8</cell><cell>0.4</cell></row><row><cell>WOLVES 2</cell><cell>0.26</cell><cell>0.88</cell><cell>0.36</cell></row><row><cell>one in one baseline</cell><cell>1</cell><cell>0.23</cell><cell>0.35</cell></row><row><cell>all in one baseline</cell><cell>0.22</cell><cell>1</cell><cell>0.32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,144.73,657.44,179.37,7.47"><p>http://wikipedia-miner.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,144.73,657.44,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="9,144.73,656.80,202.63,8.12"><p>mcl version 10-324, http://www.micans.org/mcl/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.96,580.59,337.63,7.86;15,151.52,591.54,329.07,7.86;15,151.52,602.50,186.51,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,296.81,580.59,183.78,7.86;15,151.52,591.54,129.39,7.86">Weps 2 evaluation campaign: overview of the web people search clustering task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,300.51,591.54,180.09,7.86;15,151.52,602.50,51.33,7.86">2nd Web People Search Evaluation Workshop (WePS 2009)</title>
		<imprint>
			<publisher>WWW Conference</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,613.21,337.64,7.86;15,151.52,624.17,201.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,250.79,613.21,229.80,7.86;15,151.52,624.17,61.90,7.86">Entity-based cross-document coreferencing using the vector space model</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,236.07,624.17,56.13,7.86">COLING-ACL</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,634.88,337.64,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,201.61,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,389.56,634.88,91.03,7.86;15,151.52,645.84,261.45,7.86">Grape: A graph-based framework for disambiguating people appearances in web search</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,422.57,645.84,58.02,7.86;15,151.52,656.80,137.62,7.86">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,119.67,337.64,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,154.62,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,247.01,119.67,124.74,7.86">Learning to link with Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,391.90,119.67,88.70,7.86;16,151.52,130.63,302.28,7.86">CIKM &apos;08: Proceeding of the 17th ACM conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,152.55,337.64,7.86;16,151.52,163.51,59.15,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="16,219.59,152.55,150.77,7.86">Graph Clustering by Flow Simulation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Utrecht</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="16,142.96,174.47,337.64,7.86;16,151.52,185.43,329.07,7.86;16,151.52,196.39,329.07,7.86;16,151.52,207.34,275.91,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,317.92,174.47,162.68,7.86;16,151.52,185.43,196.28,7.86">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,368.79,185.43,111.80,7.86;16,151.52,196.39,273.08,7.86">ACL &apos;05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,218.30,337.64,7.86;16,151.52,229.26,329.07,7.86;16,151.52,240.22,305.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,361.49,218.30,119.10,7.86;16,151.52,229.26,244.15,7.86">Which who are they? people attribute extraction and disambiguation in web search results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,416.15,229.26,64.44,7.86;16,151.52,240.22,170.58,7.86">2nd Web People Search Evaluation Workshop (WePS 2009)</title>
		<imprint>
			<publisher>WWW Conference</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,251.18,337.64,7.86;16,151.52,262.14,194.44,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,347.08,251.18,129.14,7.86">Mining Meaning from Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CoRR)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct coords="16,142.96,273.10,337.63,7.86;16,151.52,284.06,329.07,7.86;16,151.52,295.02,186.51,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,309.56,273.10,171.03,7.86;16,151.52,284.06,120.68,7.86">Polyuhk: A robust information extraction system for web personalnames</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,295.27,284.06,185.32,7.86;16,151.52,295.02,51.33,7.86">2nd Web People Search Evaluation Workshop (WePS 2009)</title>
		<imprint>
			<publisher>WWW Conference</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,305.98,337.98,7.86;16,151.52,316.91,329.07,7.89;16,151.52,327.89,23.04,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,464.72,305.98,15.88,7.86;16,151.52,316.93,158.38,7.86">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,322.20,316.93,101.66,7.86">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,338.83,337.98,7.89" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="16,223.86,338.85,127.37,7.86">A tutorial on spectral clustering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<idno>CoRR abs/0711.0189</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,349.81,337.98,7.86;16,151.52,360.77,329.07,7.86;16,151.52,371.73,329.07,7.86;16,151.52,382.69,94.96,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,291.51,349.81,189.09,7.86;16,151.52,360.77,194.75,7.86">Information theoretic measures for clusterings comparison: is a correction for chance necessary?</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,363.27,360.77,117.32,7.86;16,151.52,371.73,236.91,7.86">ICML &apos;09: Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
