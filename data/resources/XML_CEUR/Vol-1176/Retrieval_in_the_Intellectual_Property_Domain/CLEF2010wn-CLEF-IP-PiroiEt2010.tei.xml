<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.68,117.92,309.96,10.51;1,205.62,135.85,204.07,10.51">CLEF-IP 2010: Retrieval Experiments in the Intellectual Property Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,279.71,173.41,55.95,6.65"><forename type="first">Florina</forename><surname>Piroi</surname></persName>
							<email>f.piroi@ir-facility.org</email>
							<affiliation key="aff0">
								<orgName type="department">The Information Retrieval Facility (IRF)</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.68,117.92,309.96,10.51;1,205.62,135.85,204.07,10.51">CLEF-IP 2010: Retrieval Experiments in the Intellectual Property Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45C9DC722A8EC5D2C62078DF59721D30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the recent decade that research in IR methods for Intellectual Property domain has increased. The rst eorts in observing how information retrieval is done in patent domain were done with the series of Nist workshops (see for example <ref type="bibr" coords="1,173.68,318.37,10.30,6.65" target="#b1">[2]</ref>). Lately, more workshops and conferences are dedicated to bringing together IR and IP specialists <ref type="bibr" coords="1,268.33,330.33,10.51,6.65" target="#b2">[3,</ref><ref type="bibr" coords="1,278.84,330.33,7.01,6.65" target="#b6">7]</ref>.</p><p>In 2008, the Irf obtained the agreement to coordinate two evaluation campaigns with emphasis on patent documents and prior art retrieval: ClefIp and TrecChem.</p><p>The ClefIp track was launched in 2009 to investigate IR techniques for patent retrieval and it was part of the CLEF 2009 evaluation campaign. In 2010, the track continued as a benchmarking activity of the Clef 2010 conference.</p><p>The track utilizes a collection of more than 1.3 million patent documents derived from Epo (European Patent Oce) sources. The collection covers English, French and German with at least 150,000 documents in each language.</p><p>There were two tasks in the 2010's track. The rst one is to nd patent documents that are candidates to constitute prior art for a given document. The second task is to classify a given document according to the International Patent Classication system (Ipc). Relevance judgements will be produced using the patent citations for the Prior Art Candidates search task and using the recorded classication codes for the Classication task.</p><p>This notebook gives a report on the ClefIp activity in 2010. The paper is structured as follows: Section 2 describes the test collection used this year, section 3 presents the participating teams and gives an overview of the methods the teams involved. In the same section we also present the main measurements done in this track. <ref type="bibr" coords="1,134.76,596.64,6.72,13.44" target="#b1">2</ref> The 2010 ClefIp Data Collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Objects in the Collection</head><p>The ClefIp collection contains patents, physically stored as a collection of patent documents. A patent document may be an application document, a search report, or a granted patent document. We describe in the following some of the key terms and steps in a patent's lifecycle.</p><p>A patent is a set of exclusive legal rights for the use and exploitation of an invention in exchange for its public disclosure. The exclusive rights are given by a governing authority and are limited in time. The requirements for granting patents vary widely among patent oces, but a common rst step is to le a patent application request with a patent oce. For this, the applicant must supply a written specication of the inventionalso called an application documentwhere the background of the invention, a description of the invention, and a set of claims which dene the scope of protection, should the patent be granted, are given. The application date, or ling date of a patent refers to the date when the patent application was led.</p><p>In order to be granted, a patent application is examined by professionals who will analyze wether it meets certain patentability criteria and wether the application complies with the relevant patent law. When a patent application is found to meet all the necessary legal and patentability requirements, a decision to grant the patent is made and, after further fees and procedural steps, the granted patent is published. An important procedural step at the Epo is that a translation of the claims in all three ocial Epo languages (English, German, French) is provided <ref type="bibr" coords="2,373.50,485.94,9.95,6.65" target="#b0">[1]</ref>.</p><p>Patent documents generated at the dierent stages of the patent's life-cycle are identied by a country code (denoting the patent oce analyzing/granting the patent), a unique numeric identier, and by a kind code together with a version number . In the case of Epo the A in the kind code denote a patent document published in the application phase, the B kind code marks a granted patent document.</p><p>It is possible to le a patent application at more than one patent oce. When the same invention is granted a patent by dierent patent oces, the two patents are said to belong to the same patent family.</p><p>For the EP patents, documents at dierent stages have the same numeric identier.</p><p>For other patent oces this is not always the case. For example, the patent document US-6689545-B2 represents a US granted patent with its application document publication number US-2003011722-A1</p><p>An important tool in organizing the large amount of patent data which patent oces regulate is the classication system. A patent classication system `sorts' the patents according to the technical area they belong to, and it is a basis for a quick investigation of the state of the art in a eld . There are several patent classication systems, the most used being the International Patent Classication system (Ipc), the European Classication System (Ecla), the US Classication System.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Technical Elements</head><p>Compared to the ClefIp 2009 data collection, this year there has been an increase in the number of patent documents to be included in the ClefIp data collection. The total number of patent documents is over 3.5 million, almost one million more documents than in 2009.</p><p>The documents in the ClefIp 2010 collection are extracted from the Marec ! data corpus, and are patent documents published by the Epo.</p><p>Following the same procedure as last year, we split the available data into two parts 1. the collection corpus (or target data set) contains documents with application date prior to 2002. This set contains over 2.6 million documents, representing over 1.9 million patents. " http://www.wipo.int/pct/en/ in which case, the Epo does not republish the whole patent application, but only a bibliographic entry which refers to the original application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tasks and Topics</head><p>There were two tasks in ClefIp 2010. A Prior Art Candidates Search task (Pac) and a Classication task (Cls).</p><p>The rst task in this track (Pac) consisted in nding patent documents in the target collection that may invalidate a given patent application. The participants were provided with two sets of patents from the topic pool (a small set of 500 topics and a large set of 2000 topics). The task didn't restrict the language used for retrieving the documents, but participants were encouraged to use the multilingual characteristic of the collection (namely, that claims in granted patent documents are provided in three languages).</p><p>The second task in the ClefIp track (Cls) is a newly introduced one, and required to classify a given patent document according to the Ipc system. The classication was to be given at the subclass level. The set of classication topics contained 2000 patent documents, a dierent set than the one used in the Pac task.</p><p>Dierently from the last year's topics, where a virtual patent document was composed with a description and claims in German, English and French, this year we have used patent application documents as topics. This means that the topic documents contain claims in only one of the three languages, with about 67% of the documents having English content, 26% German content, and 7% French content. We have placed no constraints on the choice of topics, other than one: the application documents must have content in the abstract, description and claims sections of the Xml document. The patent documents released as topics had the citation records (for the Pac task) and classication records (for the Cls task) removed from the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Assessments</head><p>The relevance assessments used to evaluate the Pac submissions were obtained automatically from the patent citations stored in the collection documents. Since the average number of citations per patent in the ClefIp collection is low (below 4), we have looked for methods to extend the set of relevant documents per topic. For this we used an extended list of citations, where to the patents listed in the patent's search report (the direct citations), we added also the patent citations listed in the family members of the topic patent, as well as the family members of the cited patents. For a detailed explanation of the citation extraction procedure, we point the reader to the last year's track overview article <ref type="bibr" coords="4,134.76,610.11,9.97,6.65" target="#b5">[6]</ref>.</p><p>The relevance assessments used to evaluate the Cls submissions were also obtained automatically from the documents that originated the Cls topics. We have extracted the Ipc codes, restricted to the subclass level, from the patent documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions and Results</head><p>For both tasks, a submission consisted of a single text le with at most 1,000 answers per topic, in the standard format used for the Trec submissions. 12 participating groups have submitted a total of 25 runs to the Pac task and 27 runs to the Cls task (see Table <ref type="table" coords="5,279.81,179.48,3.88,6.65" target="#tab_1">1</ref>). The submissions were sent to us via a ftp server. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of the Submitted Runs</head><p>This section is based on the descriptions provided by the participants. We present here which Xml elds were used in document processing, what kind of pre and postprocessing was done, the retrieval and ranking system that was used to obtain the results, crosslanguage techniques involved.</p><p>⋆ The bitem participant has submitted runs to both Pac and Cls tasks. For both tasks, the Porter stemmer was applied, and stopwords were eliminated in a document preprocessing step.</p><p>In the Pac task, the participant has used the following elds both for index creation and query generation: title, abstract, claims, Ipc codes, applicants and inventors information. Using the Terrier platform, only one English index was created, and retrieval results were ranked using Terrier's PL scheme. Fields in other language than English were translated into English before adding them to the index using the Google translator. Topic documents in a dierent language than English were also translated into English with the Google translator. For the run that simulated the examiner search a postprocessing step was applied where the citations provided by the applicant in the text of the document were used. The participant also experimented with using the geographical location of the applicant in the postprocessing phase.</p><p>The document elds used for indexing within the Cls task are title, abstract, claims, description, applicant and citations. First a retrieval step is done, where the Google translator is used as in the Pac task. The retrieved document are given as input to the kNN algorithm which maps them to Ipc codes, which are then reranked.</p><p>⋆ The dcu group submitted runs to the Pac task. The English index used in the retrieval was created from the following elds: title, abstract, description, claims, and classication tags. The document preprocessing phase included stopwords removal, stemming and number removal. The nonEnglish topics were translated into English using the Google translator, the IR system used for retrieving results was Indri which ranked the results using a language model and inferred networks. The postprocessing step for one of the runs added the citations extracted from the topic document descriptions (i.e. applicant citations) to the list of results.</p><p>⋆ The University of Hildesheim group (hild) participated in the Pac task and experimented with various types of queries in the frame of an Apache Lucene based system. One English language index was created based on the patent number, title, abstract and Ipc Xml elds. Stopwords were removed and a Porter stemmer was also applied on both corpus and topic documents. Phrase queries are extracted from various elds in the topic document, like phrases from the title only or from title, main claim, rst part of the description. The Ipc codes are also used in ltering the results, by looking for results that share at least one Ipc code with the topic document.</p><p>⋆ The humb group took part in both track's tasks with the same custommade system (PATATRAS). The preprocessing step included citation identication in the patent's text, cleaning the inventor and applicant names, languagebased tokenizations, POStagging, concepttagging, keyterm extraction and lematization. All patent document elds were used in the index creation. One lemma index per language and a concept index based on a selfdeveloped terminological database GRISP were used in the retrieval experiments. The PATATRAS system combines the Lemur, Okapi BM25, and Indri retrieval engines, each acting on certain index les. Result ranking is done by BM25, Indri and SVM.</p><p>The classication results were obtained with the same system, but involving the KNN classier, and the conceptual tagging is eliminated from the preprocessing phase.</p><p>⋆ All classication results sent in by the insa group were obtain using the LCS2 # classication system, using a balanced Winnow method. The text fed into the classier was considered as a bag of words or as a bag of linguistic triples obtained by preprocessing selected elds with AGFL $ builtin linguistic parsers (EP4IR for English, and FR4IR for French). The various training experiments were done by choosing dierent document elds to be considered in the training process: a)abstract and titles, b) abstracts, titles, names and addresses, c) description.</p><p>⋆ jve participated in the Cls task with three runs. Both in training and in the test phase the title, claims, description, and abstract (when available) were used. The rst run was obtained with a SVM classier, where documents were preprocessed by tokenization, POStagging, lemmatization, and a keyphrase tagging step, which in a patentoriented context detects the terms that best explain the subject of the patent application. WordNet (a lexical resource for the English language) was also used in this step. The second run submitted by JSI Jouve made use of the Lemur system to index the data corpus, generating one index per language. Lemur was also used to retrieve relevant documents to the given topics. From the returned patent documents, only the classication codes were kept. The third run combined the two methods used for the rst two runs.</p><p>⋆ The Radboud University group (run) participated in both Pac and Cls tasks. The retrieval system used for the Pac task was Lemur/Indri based. The index was created out of the title, abstract, claims, description and Ipc code elds, in English only. Per topic, one hundred documents were retrieved, which were then reranked using regression models. The system used for the Cls task is LCS Winnow with a Lucene analyzer. Only the English abstracts were fed into the classier. The abstracts were rst processed to remove punctuation, numbers and to put all letters into lowercase, then a simple tokenizer was applied. Experiments with dependency triples in the abstract were done using the AEGIR hybrid parser. ⋆ The retrieval system used by Spinque (spq) is an inhouse retrieval system that contains a graphical search strategy builder, and an own indexer based on MonetDB. The same system, with the same generic index was used in both Pac and Cls tasks. In both tasks, the topic documents were passed through a Snowball stemmer, with English stopwords removed. The rst 26 terms given by the tf-idf algorithm were considered to be part of the query. Also, the Ipc codes were used in the query creation. The retrieval step returned a list of ranked patent documents, for the Pac task, and, for the Cls task, the Ipc codes attached to the ranked patent documents. ⋆ The run submitted by the uaic participant to the Pac task was obtained by a Lucene based system. The English only index was created using the invention title, claims, and abstract or description (when the abstract was missing) document elds. The data corpus was split into 20 and the indexing was done in parallel on 20 machines, one split per machine. Lucene was used to extract a query from the topic document using the same document elds as for the index creation. Various boost factors were applied to the document elds used in the query.</p><p>⋆ In the Pac task, the University of Indonesia ui participant used a simple Lemur Indri setting. A large number of Xml elds were used in the English index creation, among which we list abstract, applicant, applicant's address, abstract, claims, classication codes, descriptions. The three submitted runs extracted the query from dierent document elds in the topic documents: invention title and description; claims; invention title, description, and claims. The query extraction is done by the tf-idf term weighting algorithm, keeping the rst 10 terms.</p><p>⋆ The uned group participated in the Pac task with a retrieval system based on BM25. Before indexing or retrieving, the patent documents in the collection have been joined (at Xml eld level) into one patent document. Four Xml elds, to which stemming and stopword removal was applied, have been considered for the indexing process: title, abstract, description and claims. A separate index per language was created. The query terms are extracted from the topic documents by computing the Kullback-Leibler divergence (KLD) between the language model of the topic document and the language model of the patent collection. Experiments with eld boost values were also made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Results</head><p>We have evaluated the submitted experiments using the most common metrics in IR. Before we ran the evaluation software, some simple cleanup of the data was done. A further important data correction was done on the experiments submitted to the Cls task. Here, we noticed that several participants have made use of Ipc versions that were not used in the ClefIp data corpus. (The data feeds that originated the ClefIp documents did not carry classication symbols that were eliminated when the Ipc system got revised over the time.) For this reason, we removed all entries in the result les where classication codes not occurring in the ClefIp 2010 corpus were listed.</p><p>For each submitted Pac experiment we computed the following measures:</p><p>% Collocations are concept expressed by more than one word.</p><p>Precision, Precision@5, Precision@10, Precision@50, Precision@100 Recall, Recall@5, Recall@10, Recall@50, Recall@100 Map Ndcg Pres</p><p>For each submitted Cls experiment we computed the following measures: Precision@1, Precision@5, Precision@10, Precision@25, Precision@50 Recall@5, Recall@25, Recall@50, Map F 1 at 5, 25 and 50.</p><p>All computations were done using the trec_eval 9.0 software provided by Nist, with the exception of the Pres measure, which we computed using a script provided to us by the measure's authors <ref type="bibr" coords="9,314.97,288.18,9.98,6.65" target="#b7">[8]</ref>. Figures <ref type="figure" coords="9,367.44,288.18,51.43,6.65">1 through 5</ref> show some of the calculated measures. Detailed values for each of the mentioned measures are</p><p>given in <ref type="bibr" coords="9,172.40,312.09,10.51,6.65" target="#b4">[5]</ref> and <ref type="bibr" coords="9,205.59,312.09,9.97,6.65" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Final Observations</head><p>We have presented here an account of the benchmarking activities done within the ClefIp lab, organized in the frame of the Clef 2010 conference. The time was, unfortunately, too short to be able to do an indepth analysis of the found results, we leave this as future work. Lack of resources was also an impediment in following some of the lines drawn at the end of the 2009 track. One of such lines to follow is intensifying contacts with IP professionals. However, we were not able to pursue this goal. We didn't forget about the conclusions drawn at the end of ClefIp      </p><formula xml:id="formula_0" coords="11,177.25,564.55,54.97,43.33">p q - b m 2 5 - R u n 1 - 2 0 0 0 h u m b - p a t a t r a s - r u n 1 b it e m - F R E Q - R u n</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,138.97,375.40,341.64,11.45;3,151.70,390.95,328.94,6.65;3,151.70,402.91,83.92,6.65;3,149.71,422.02,330.90,6.65;3,134.76,433.98,345.89,6.65;3,134.76,445.94,345.87,6.65;3,134.76,457.89,345.87,6.65;3,134.76,469.85,345.88,6.65;3,134.76,481.80,113.16,6.65;3,149.71,493.76,330.88,6.65;3,134.76,505.71,293.80,6.65;3,134.76,519.79,26.16,10.46;3,134.76,531.74,83.67,10.46;3,134.76,543.70,20.93,10.46;3,134.76,555.65,277.30,10.46;3,149.71,567.20,330.88,11.02;3,134.76,582.42,345.92,6.65;3,134.76,591.11,345.84,11.02;3,134.76,606.33,340.18,6.65;3,474.92,601.75,3.97,7.94;3,479.39,606.33,3.88,6.65;3,144.73,622.99,218.64,9.41;3,137.51,631.73,3.65,7.30;3,144.73,634.09,15.88,9.22;3,163.29,630.06,30.56,14.59;3,196.53,634.09,262.51,9.22;3,461.65,630.06,18.95,14.59;3,144.73,645.05,120.60,9.22;3,268.37,641.02,14.06,14.59;3,285.50,645.05,87.55,9.22"><head>2 .</head><label>2</label><figDesc>the topic pool contains documents with application date between 2002 and 2009. This set contains over 0.8 million patent documents, representing over 0.6 million patents. The same as in 2009, the Test Collection Corpus was delivered to the participants as is, without merging the documents related to the same patent into one document. Each patent is identied by a unique patent number-a string starting with EP and followed by 7 digits. Corresponding to each patent is a directory containing the patent documents related to that patent. The layout is nnnnnn/nn/nn/nn/*.xml. For example, to patent EP 0981201 corresponds the directory containing les EP-0981201-A2.xml, EP-0981201-A3.xml, and EP-0981201-B1.xml: &gt; pwd /000000/98/12/01 &gt; ls EP-0981201-A2.xml EP-0981201-A3.xml EP-0981201-B1.xml All documents in the ClefIp collection contain the following main Xml elds: bibliographic data, abstract, description, and claims. Not all documents actually have content in these elds. This happens because certain Epo patent applications are internationally led under the Patent Cooperation Treaty (Pct " ) See http://www.wipo.int/classifications/ipc/en/ ! The Marec data corpus is a collection of over 19 million patent documents, in Xml format, made available by the Irf for research purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.76,593.22,345.84,8.74;7,134.76,606.98,345.87,6.65;7,134.76,618.94,345.92,6.65;7,137.51,642.68,3.65,7.30;7,144.73,644.90,145.82,9.41;7,137.51,653.64,3.65,7.30;7,144.73,655.86,112.89,9.41;8,134.76,117.21,345.86,11.45;8,134.76,132.76,345.89,6.65;8,134.76,144.72,345.87,6.65;8,134.76,156.67,142.87,6.65;8,277.60,152.09,3.97,7.94;8,285.15,156.67,195.45,6.65;8,134.76,168.63,27.17,6.65"><head>⋆</head><label></label><figDesc>The classier used by the Simple Shift participant (ssft), myClass, is a winnow like inhouse implementation. The elds used in the classication process are: inventor, applicant, title, abstract, claims, and description reduced to a size of # http://www.phasar.cs.ru.nl/LCS/ $ http://www.agfl.cs.ru.nl maximum 4k. During the training phase ssft made use of additional patent corpora to increase the classication precision. In other training experiments, over sampling (copying the documents in the respective category a certain number of times) was used. A collocation % extraction step was done during the indexing phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,219.82,360.44,175.71,11.45"><head></head><label></label><figDesc>Fig. 1. MAP measures for the Pac runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,171.84,637.96,271.66,11.45"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Precision and Recall at 100 measures for the Pac runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,177.18,355.97,260.99,11.45"><head>2 bFig. 5 .</head><label>25</label><figDesc>Fig. 5. Precision and Recall at 5 measures for the Cls runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,147.48,216.84,320.40,240.47"><head>Table 1 .</head><label>1</label><figDesc>List of participants and runs submitted</figDesc><table coords="5,147.48,234.20,320.40,223.11"><row><cell>ID</cell><cell>Institution</cell><cell cols="2">Cls Pac</cell></row><row><cell>bitem</cell><cell>BiTeM, Service of Medical Informatics, Geneva</cell><cell>CH 7</cell><cell>2</cell></row><row><cell></cell><cell>University Hospitals</cell><cell></cell><cell></cell></row><row><cell>dcu</cell><cell>Dublin City Univ. -School of Computing</cell><cell>IE</cell><cell>3</cell></row><row><cell>hild</cell><cell>Hildesheim Univ. -Information Science</cell><cell>DE</cell><cell>4</cell></row><row><cell>humb</cell><cell>Humboldt Univ. -Dept. of German Language and</cell><cell>DE 1</cell><cell>1</cell></row><row><cell>insa</cell><cell>Linguistics Lci Institut National des Sciences Appliquées de</cell><cell>FR 5</cell><cell></cell></row><row><cell></cell><cell>Lyon</cell><cell></cell><cell></cell></row><row><cell>jve</cell><cell>Industrial Property Documentation Department, JSI</cell><cell>FR 3</cell><cell></cell></row><row><cell></cell><cell>Jouve</cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>Information Foraging Lab, Radboud University</cell><cell>NL 2</cell><cell>2</cell></row><row><cell></cell><cell>Nijmegen</cell><cell></cell><cell></cell></row><row><cell>spq</cell><cell>Spinque</cell><cell>NL 1</cell><cell>1</cell></row><row><cell>ssft</cell><cell>Simple Shift</cell><cell>CH 8</cell><cell></cell></row><row><cell>uaic</cell><cell>Al. I. Cuza University of Ia³i -Natural Language</cell><cell>RO</cell><cell>1</cell></row><row><cell></cell><cell>Processing</cell><cell></cell><cell></cell></row><row><cell>ui</cell><cell cols="2">Information Retrieval Group, Universitas Indonesia ID</cell><cell>3</cell></row><row><cell>uned</cell><cell>UNED -E.T.S.I. Informatica, Dpto. Lenguajes y</cell><cell>ES</cell><cell>8</cell></row><row><cell></cell><cell>Sistemas Informaticos</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,224.33,451.57,182.74,6.65"><head></head><label></label><figDesc>2009, we only postponed their realization.</figDesc><table coords="10,161.77,125.49,131.17,211.31"><row><cell>0.3</cell></row><row><cell>0.25</cell></row><row><cell>0.2</cell></row><row><cell>0.15</cell></row><row><cell>0.1</cell></row><row><cell>0.05</cell></row><row><cell>0</cell></row><row><cell>u a ic -S r u n -1 -s m a ll r h u n -2 -s m a h il d -1 -s m a h il d -3 -s m a u il d -4 -s m a u i-1 -s m a ll h il i-d 2 --2 s -m a s m ll a ll ll ll ll ll</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,200.15,408.72,261.50,240.41"><head></head><label></label><figDesc>Fig. 4. Map and F_1 measures for the Cls runs</figDesc><table coords="11,204.18,408.72,257.47,207.32"><row><cell>0.9</cell></row><row><cell>0.8</cell></row><row><cell>0.7</cell></row><row><cell>0.6</cell></row><row><cell>0.5</cell></row><row><cell>0.4</cell></row><row><cell>0.3</cell></row><row><cell>0.2</cell></row><row><cell>s p q -b m 2 5 -R u n 1 -2 0 0 0 h u m b -p a t a t r a s -r u n 1 b it e m -F R E Q -R u n 1 in s a -W in n o w R u n 2 jv e -s e m -R u n 1 r u n -a b s t r a c t s B O W -u n 1 R in s a -W in n o w R u n 1 in s a -W in n o w R u n 4 in s a -W in n o w R u n 3 in s a -W in n o w R u n 5 r b u n it e -a m -b s t r F R a c t E Q s B --u n R R u O T W n</cell></row><row><cell>1</cell></row><row><cell>MAP</cell></row><row><cell>F1 at 5</cell></row><row><cell>1 o n in -s in a W w R u n 2 jv e -s e m -R u n 1 r u n -a b s t r a c t s B O W -R u n 1 in s a -W in n o w R u n 1 in s a -W in n o w R u n 4 in s a -W in n o w R u n 3 in s a -W in n o w R u n 5 r u n -a b s t r a c t s B O W T -u n R b it e m -F R E Q -R u n 2 b it e m -F R E Q -R u n 3 b it e m -M U L T I -R u n 2 b it e m -L I S T -M U L T I -u n 2 u 1 n R b it e m -L I S T -M U L T I -R b it e m -M U L T I -R u n 1 jv e -c o m b -R u n 3 s s jv e s f t s f s t s f s t s f s t s f s t s f s t s f s t s f t -s -C -C -C -C -C -C -C -C im -R u n 2 4 5 0 0 -r u n 2 E 6 8 0 0 -r u n 4 C 4 5 0 0 -r u 6 n 0 -r u n 1 E 0 -r u n 3 C 0 -r E C 6 E C 0 n 8 u u n 5 -8 0 r u 0 n -r 7</cell></row><row><cell>1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.34,425.10,173.99,9.22;12,334.70,424.95,145.82,9.41;12,146.91,435.91,54.83,9.41" xml:id="b0">
	<monogr>
		<ptr target="http://www.epo.org/patents/law/legal-texts" />
		<title level="m" coord="12,146.91,426.36,127.35,7.54">European Patent Convention</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,447.02,342.31,9.22;12,146.91,457.98,333.69,9.22;12,146.91,468.93,333.70,9.22;12,146.91,481.15,333.72,7.54;12,146.91,490.85,333.73,9.22;12,146.91,501.81,249.58,9.22" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,366.79,447.02,113.87,9.22;12,146.91,457.98,158.27,9.22">Overview of the Patent Retrieval Task at the NTCIR-6 Workshop</title>
		<author>
			<persName coords=""><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Makoto</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,179.53,470.19,301.08,7.54;12,146.91,481.15,333.72,7.54;12,146.91,492.11,108.46,7.54">Proceedings of the Sixth NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access</title>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">Kirk</forename><surname>Evans</surname></persName>
		</editor>
		<meeting>the Sixth NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access<address><addrLine>Hitotsubashi, Chiyoda-ku, Tokyo; Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="101" to="8430" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,512.77,342.33,9.22;12,146.91,523.73,221.30,9.22" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,307.12,512.77,173.56,9.22;12,146.91,523.73,111.96,9.22">1st international workshop on advances in patent information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.31,523.73,43.77,9.22">AsPIRe&apos;10)</title>
		<imprint>
			<date type="published" when="2010-03">March 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,534.69,342.26,9.22" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,203.80,534.69,216.27,9.22">CLEF-IP 2010: Classication task evaluation summary</title>
		<author>
			<persName coords=""><forename type="first">Florina</forename><surname>Piroi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,545.65,342.23,9.22;12,146.91,556.61,20.99,9.22" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,203.88,545.65,251.35,9.22">CLEF-IP 2010: Prior art candidates search evaluation summary</title>
		<author>
			<persName coords=""><forename type="first">Florina</forename><surname>Piroi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,567.56,342.21,9.22;12,146.91,578.52,333.70,9.22;12,146.91,589.48,93.13,9.22" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,314.55,567.56,166.01,9.22;12,146.91,578.52,133.16,9.22">CLEF-IP 2009: Retrieval Experiments in the Intellectual Property Domain</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,349.21,579.78,131.40,7.54;12,146.91,590.74,25.39,7.54">Proc. of CLEF, Revised Selected Papers</title>
		<meeting>of CLEF, Revised Selected Papers</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="12,138.34,600.44,342.31,9.22;12,146.91,611.40,179.99,9.22" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,284.56,600.44,196.10,9.22;12,146.91,611.40,64.30,9.22">The 3rd international workshop on patent information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-10">2010. October 2010</date>
			<pubPlace>PaIR</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.34,622.36,342.28,9.22;12,146.91,633.32,203.15,9.22" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,273.73,622.36,206.90,9.22;12,146.91,633.32,132.91,9.22">PRES: A score metric for evaluating recall-oriented information retrieval applications</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,299.29,634.58,46.49,7.54">SIGIR 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
