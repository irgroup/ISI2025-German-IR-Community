<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.71,151.72,320.04,13.06;1,267.06,169.18,77.96,13.06">Automatic prior art searching and patent encoding at CLEF-IP &apos;10</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.39,207.54,67.66,9.15"><forename type="first">Douglas</forename><surname>Teodoro</surname></persName>
							<email>douglas.teodoro@hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics Service</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>4 rue Gabrielle-Perret-Gentil</addrLine>
									<postCode>1211</postCode>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Library and Information Sciences Department</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<addrLine>7 route de Drize</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.10,207.54,54.81,9.15"><forename type="first">Julien</forename><surname>Gobeill</surname></persName>
							<email>julien.gobeill@hesge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics Service</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>4 rue Gabrielle-Perret-Gentil</addrLine>
									<postCode>1211</postCode>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.48,207.54,55.11,9.15"><forename type="first">Emilie</forename><surname>Pasche</surname></persName>
							<email>emilie.pasche@hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics Service</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>4 rue Gabrielle-Perret-Gentil</addrLine>
									<postCode>1211</postCode>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.41,207.54,72.60,9.15"><forename type="first">Dina</forename><surname>Vishnyakova</surname></persName>
							<email>dina.vishnyakova@hcuge.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Library and Information Sciences Department</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<addrLine>7 route de Drize</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.13,207.54,51.92,9.15"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
							<email>patrick.ruch@hesge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics Service</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>4 rue Gabrielle-Perret-Gentil</addrLine>
									<postCode>1211</postCode>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.80,219.06,60.22,9.15"><forename type="first">Christian</forename><surname>Lovis</surname></persName>
							<email>christian.lovis@hcuge.ch</email>
						</author>
						<title level="a" type="main" coord="1,151.71,151.72,320.04,13.06;1,267.06,169.18,77.96,13.06">Automatic prior art searching and patent encoding at CLEF-IP &apos;10</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A6429048DCF372280BEBE13220F577E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>Prior art search</term>
					<term>IPC encoding</term>
					<term>Patent classification</term>
					<term>k-NN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the intellectual property field two tasks are of high relevance: prior art searching and patent classification. Prior art search is fundamental for many strategic issues such as patent granting, freedom to operate and opposition. Accurate classification of patent documents according to the IPC code system is vital for the interoperability between different patent offices and for the prior art search task involved in a patent application procedure. In this paper, we report our experiments with prior art searching and patent classification in the context of CLEF-IP '10 evaluation track. In the Prior Art Candidates search task, we strongly improved our last year's model based on our experiments on training data (MAP 0.22), but official results, alas, were far from the expected ones (MAP 0.14). Regarding multilingual issues, our simple Google translator strategy achieved a 10% improvement. Nevertheless we think that the multilingual aspects in CLEF-IP'10 were less clear than for CLEF-IP'09. Finally, exploiting applicant's citations led to a 30% improvement, but their visibility depends on who (the applicant or the examiner) performs the prior art search in the simulated task. This issue needs clarification by the organizers for the forthcoming campaigns. In the Classification task, we apply the k-NN algorithm in the categorisation process and explore different retrieval models, ranking combinations and languages features in order to enhance our results. Using multi-collection in the classification process improved the results by 2%. Both the prior art search and classification systems are in the top three rank among the participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to EPO, it is estimated that 80% of the knowledge is found in patent documents. Due to its importance as source of knowledge and to the delay in patent analysis caused the growth of applications, new areas of knowledge and size of patent databases, new tools to automate patent searching and classification processes have become a hot topic in the last decades. As example, we can cite the challenges CLEF 2009, TREC-CHEM 2009-2010 and the workshops SIGIR 2000, ACL 2003 and NTCIR 3-8 which all have tasks dedicated to patent retrieval. In that context, the CLEF-IP 2010 evaluation track proposes two tasks for automation of prior art searching and of patent classification.</p><p>Prior art candidates search (PAC) is a fundamental task in patent processing, since many of the strategic issues in intellectual property rely upon retrieving patents that deal with a given invention. The most usual example is prior art search that applicants and examiners have to provide in order to grant an application. PAC may also be performed for invalidating another patent, for freedom to operate or for patent landscape. PAC primarily is an information retrieval task, in which recall is the most important measure, as one single document can invalidate a patent.</p><p>Automating the attribution of IPC codes to patent applications is important for several reasons: it assists patent officers in the patent classification task, aids inventors with the prior art search and helps referees to validate or refute a given application. When a patent application is considered or submitted, the search for previous inventions in the field relies crucially on accurate patent classification. The use of the assigned IPC code is also key information for searching patents across nations because of its language independence.</p><p>In this paper, we report the experience of the BiTeM group<ref type="foot" coords="2,390.42,442.87,3.00,5.51" target="#foot_0">1</ref> in the CLEF-IP 2010 evaluation track. The challenge is divided into two tasks: Prior Art Candidates search (PAC) and Classification (CLS). In the PAC task, participants have to re-build the citations section of the 2000 applications belonging to the test set, mainly written in English. In the CLS task, patent applications written in English, French and German are automatically encoded using the IPC subclass descriptors.</p><p>We use an EPO patent collection composed by 2.7M documents and a set of 300 patent applications written in English, French and German to train the system. The assessments of our approaches are performed using 2000 documents in the PAC and CLS tasks. In order to improve classification we develop several re-ranking techniques that are further described.</p><p>The rest of this paper is organised as follows. In Section 2, the corpus and training data are depicted. Moreover, we describe the methods used to retrieve documents for the PAC task and the classification system. In Section 3, the results obtained are presented and remarks are discussed. In Section 4, the paper is concluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training and test data</head><p>In our experiments with the PAC and CLS tasks, we use a patent collection provided by EPO containing 2.7M patent documents, including A and B files. In total, the collection contains 1.3M patents. The distribution of patent documents according to their sections for the three different languages -English, French and German -is described in Table <ref type="table" coords="3,479.80,242.76,3.74,9.15" target="#tab_0">1</ref>. The organisers also provide two sets of training (300 applications) and testing (2000 applications) documents.</p><p>In the CLS task, the fields title, abstract, claim, description, applicant and citation are used for indexing the collection. The average number of subclass codes per patent document (A and B) in the corpus is 8491 while the median is 2927. The majority of the codes (95%) are found in 100 or more documents. Six classes, A61K, A61P, C07D, H01L, G06F and G01N, are presented 100K in or more documents.</p><p>In the PAC task, organizers decided this year that the gold file would contain patent documents instead of patent families. Yet, we decided for time reasons to continue to work at the level of patent family. Hence, we continue to concatenate all documents relative to a given patent family in a unique virtual file. Once the run is computed, we simply split each virtual document in all its parts. We use Terrier<ref type="foot" coords="3,198.84,520.15,3.00,5.51" target="#foot_1">2</ref> as our information retrieval (IR) engine. Terrier implements several methods to calculate the similarity between documents: BM25, BB2 (Bose-Einstein model for randomness), InL2 (inverse document frequency model for randomness), among others and it is optimised to work with large collections. It is based on JAVA and freely available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification system</head><p>In the classification experiments, we choose a classifier based on the k-NN algorithm. Some authors <ref type="bibr" coords="4,189.60,183.84,11.65,9.15" target="#b0">[1]</ref> have shown that k-NN, together with SVM, outperforms other approaches such as neural networks, Rocchio and Na√Øve Bayes. Compared to SVM, k-NN scales much better to larger systems that contain many features and classes, which is the case of the proposed task.</p><p>The classification system architecture is presented in Fig. <ref type="figure" coords="4,370.64,230.04,3.74,9.15" target="#fig_0">1</ref>. A query is provided to the IR engine, which ranks the first k documents d j according to ranking model. The documents are mapped to their respective codes c i and the codes are further re-ranked using the methods described in the next subsection. A ranked list of n codes is then created. Depending on the multi-lingual strategy, the topics are first translated using Google Language Tools<ref type="foot" coords="4,221.46,287.41,3.00,5.51" target="#foot_2">3</ref> before being used as input to the IR engine.</p><p>We have tuned the number of neighbours k so that it maximises the precision at the top rank code. It happens to be 31 according to our experiments. The slope of the ranking models was suggested by the Terrier experiments with the .GOV collection and set to 0.2381 for the BM25 and DFR_BM25 models to 26.04 for the PL2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Ranking strategies</head><p>In our attempt to improve the precision of the top n ranked codes, we have experimented several re-ranking algorithms as described in Methods 1 to 7. First, in Method 1, 2 and 3 we compare the use of a single index containing all the three language documents against the use of a monolingual indexes and of query translation. Further, in Method 4 we experiment the combination of different ranking models (BM25, BM25_DFR and PL2) and the combination of patent collections (derived from the different language in the documents). As previously demonstrated <ref type="bibr" coords="5,297.08,149.76,10.62,9.15" target="#b1">[2]</ref>, the combination of patent collections can enhance the classification results. Finally, analogously to the work of Xiao et al. <ref type="bibr" coords="5,461.80,161.28,10.62,9.15" target="#b2">[3]</ref>, in Method 5 we apply some simple re-ranking algorithms to the lists obtained in the Methods 1 and 2. These methods are implemented as follow: Method 1. The collection containing English documents is indexed. Queries in French and German are translated to English before being submitted against this index. The model BM25, DFR_BM25 and PL2 are used to retrieve the documents. The codes are mapped and ranked using their frequency in the top k retrieved documents, as showed in Eq. (1) (see <ref type="bibr" coords="5,143.42,258.12,10.66,9.15" target="#b2">[3]</ref>): <ref type="bibr" coords="5,458.97,275.70,11.65,9.15" target="#b0">(1)</ref> where f is defined by:</p><p>(2) Method 2. An index is created using the whole collection. Queries in the three original languages are submitted against this index. The model DFR_BM25 is used to calculate the document/query similarity. The codes are mapped and ranked using Eq. (1). Method 3. Three different indexes are created from the English, French and German patent documents. Each index contains only sections from one language plus application and citation sections. Queries are translated to all the three languages and submitted against their respective index (DE-&gt;DE, EN-&gt;EN and FR-&gt;FR). The model DFR_BM25 is used to fetch the documents. The codes are mapped and ranked using their frequency [Eq. <ref type="bibr" coords="5,144.24,502.38,11.23,9.15" target="#b0">(1)</ref>] in the top k retrieved documents. Method 4. In this method, the results of Method 3 are combined linearly in order to see how the combination of different collections can improve the results. Since the language indexes have different performances, they receive different weights in the combination: 1.00 for English, 0.25 for German and 0.15 for French. In the same line of thought, the results of Method 1 are combined. As in the language combination, the models receive different weights with 0.05 for BM25, 1.00 for DFR_BM25 and 0.01 for PL2. The weights were obtained from the training phase.</p><p>Method 5. In this method, the results obtained in Methods 1 and 3 are re-ranked using the rank list combination (rank combination) method described in Xiao et al. <ref type="bibr" coords="6,422.74,161.28,10.82,9.15" target="#b2">[3]</ref>:</p><formula xml:id="formula_0" coords="6,458.97,178.86,11.65,9.15">(3)</formula><p>where r cij is the code's rank in the ranked list j and varies between 1 and n. The lists used are original (Eq. ( <ref type="formula" coords="6,204.09,248.76,3.50,9.15">1</ref>)), sum, listweak, which are all described in Xiao et al. <ref type="bibr" coords="6,453.91,248.76,10.60,9.15" target="#b2">[3]</ref>, and citation, which is derived as follow: citation -It has been shown in previous experiments that citation is an important source of information for patent retrieval <ref type="bibr" coords="6,274.75,294.96,11.00,9.15" target="#b4">[5,</ref><ref type="bibr" coords="6,285.75,294.96,7.33,9.15" target="#b5">6,</ref><ref type="bibr" coords="6,293.08,294.96,7.33,9.15" target="#b6">7,</ref><ref type="bibr" coords="6,300.41,294.96,7.33,9.15" target="#b7">8]</ref>. Eq. ( <ref type="formula" coords="6,334.36,294.96,3.88,9.15" target="#formula_1">4</ref>) shows how the codes c i are ordered according to this method:</p><p>where f is defined by:</p><p>(</p><p>and d c is a document cited by d j .</p><p>is the weight of each ranking method, original, sum, listweak and citation, respectively set to 1.15, 1.00, 0.75 and 0.30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prior art search system</head><p>The prior art search used for CLEF-IP'10 system largely relies upon our last year's system <ref type="bibr" coords="6,155.12,505.62,10.61,9.15" target="#b5">[6]</ref>. Several additional strategies were evaluated throughout the pre-processing, the retrieval, and the post-processing steps. For this purpose, we worked with training data and simply computed a baseline run, and then tried to optimize the Mean Average Precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.1</head><p>Pre-Processing strategies Document Representation. In the framework of CLEF-IP 09 evaluation <ref type="bibr" coords="7,454.33,184.32,10.62,9.15" target="#b5">[6]</ref>, we established that the best Document Representation for our system included Title, Abstract, Claims, and IPC codes (in both subclass and subgroup forms), but not Description. This year, we evaluated the contribution of other unexploited fields that are Applicants and Inventors. From the Applicants field contained in a patent document, we try to split the information and to extract three different fields that are the Applicants' names, the Applicants' countries, and the Applicant's address. The same strategy is used with the Inventors field.</p><p>Query Representation. Last year, we established that the best Query Representation for our system was the same we used for the collection plus Description. No further experiments were conducted regarding the Query Representation, unless including applicants and/or inventors information as for the collection.</p><p>Multilingual issues. This year, the collection includes documents in which English, French and/or German versions of each field can be present. Our strategy was to exclusively work in English and was simple: for each patent document, when the English version of a given field amongst Title, Abstract and Claims is available, we use this English version. Otherwise, if a French or a German version is available, we simply apply Google Translator on it. The same strategy is used for both documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Retrieval strategies</head><p>The Information Retrieval step is performed with Terrier. Last year, we conducted a set of experiments in order to determine the best tuning, that was using Terrier BM25 with b=1.15 for weighting scheme, and Terrier Bose-Einstein for Query Expansion model. The same parameters are kept for CLEF-IP 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Post-Processing strategies</head><p>Applicants' Countries. We investigated the hypothesis that the country of origin of the applicants, or the inventors, brings information, since citations are more likely to come from the same area due to a geographical bias [10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussions</head><p>In this section, we present the official results in the CLEF-IP '10 challenge for the PAC and CLS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification results</head><p>In our experiments in the CLS task, we submitted seven official runs, which are listed in Table <ref type="table" coords="8,150.84,256.80,3.75,9.15" target="#tab_1">2</ref>. Comparing the baseline run FREQ_Run1, obtained from Method 1 using BM25 model, with FREQ_Run2, which is also obtained from Method 1 but based in the divergence from randomness (DFR_DM25) model, we see a relevant improvement of 15% in the classifier performance.</p><p>When comparing Method 1 (FREQ_Run2), which uses an English collection for indexing and translates the topics from other languages to English, with Method 2 (FREQ_Run3), which uses indexes and queries from the three original languages, the results are very similar. From these results, we conclude that translation of the topics is not necessary if documents of the same topic's language are presented in the index. Otherwise, translation does not affect the classification results in the case of inexistent original topic's language in the index. This corroborates with our result in NTCIR-8 <ref type="bibr" coords="8,463.64,372.24,10.61,9.15" target="#b1">[2]</ref>.</p><p>Our best run (MULTI_Run1) obtains 0.7281 of performance (MAP) and it uses Method 4, with the combination of the different language indexes obtained in Method 3. It shows an improvement of 1.9% over the best model of Method 1 and 1.2% improvement over Method 2. We obtained similar results in <ref type="bibr" coords="8,396.72,418.44,11.65,9.15" target="#b1">[2]</ref> combining patent collections from different offices (JPO and USPTO). In MULTI_Run2, the combination of models obtained from Method 1 also improves the results slightly (1.0%). Finally, the results of Method 5 do not show any improvement when compared to their counterparts (MULTI_Run1 vs LIST_MULTI_Run1 and MULTI_Run2 vs LIST_-MULTI_Run2) from Method 4. These results were not expected from the training results, where we saw an improvement of 1.5% in Methods 5. We believe that it may have been due to overfitting.</p><p>In our attempt to analyse the reasons for the classification errors we try to correlate 1) the code classes, 2) the code document frequency (CDF) and 3) the size of queries with the query average precision. For 1) and 2) we do not find any clear correlation. The 50 best and 50 worst code classes have random distributions of codes with an overlap of approximately 30% between them. For the CDF correlation, the 50 best and 50 worst have also similar CDF. However, for hypotheses 3) we notice (Fig. <ref type="figure" coords="9,382.56,265.20,4.16,9.15" target="#fig_1">2</ref>) a linear increase in the average query precision with the size of the topic. We believe that this can give us some indications of where we should improve the classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior Art Candidates search results</head><p>All the reported experiments were conducted with training data. In order to evaluate a strategy, we compute a baseline run, using last year's best features, and then try to increase the Mean Average Precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pre-Processing strategies</head><p>Multilingual issues. We start with a Baseline run, for which only original English is used, i.e. no translations. MAP for this baseline run is 0.106. Our simple translation strategy leads to a +8% improvement for MAP when applied for the collection, +10% when applied for both the collection and the queries (see Table <ref type="table" coords="10,361.94,161.28,3.61,9.15" target="#tab_2">3</ref>). This improvement needs to be compared with more sophisticated strategies evaluated within this benchmark.  <ref type="table" coords="10,438.97,325.14,4.16,9.15" target="#tab_3">4</ref>) that the information contained in both fields is relevant, and helpful for the Information Retrieval.</p><p>Including applicants and inventors names respectively both leads to a +3% improvement.</p><p>Including the country of origin seems to be ineffective. Addresses are noisy information in the patent. Yet, using them leads to +6% improvement. Our strategy was to split information contained in the Applicants or Inventors fields, in order to avoid what seems to be noise. The fact remains that the best results are obtained with all the fields, without any splitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Post-Processing strategies</head><p>Applicant's country. Closer analysis on training data reveals that, in the gold file, 50% of the cited patents share the same country of origin of the applicant than the patent used as query. Moreover, there seems to be clear patterns depending on the country. For Japanese patents, 70% of the cited patents come from Japan, while 10% come from USA. For French patents, 31% of the cited patents come from France, while 19% come from Germany. We can hypothesize that rules inferred from these patterns can improve the model in a re-ranking way. Unfortunately, we tried several boosting or filtering strategies, but never obtained better results than the baseline.</p><p>Applicant's citation. Last year, a CLEF-IP'09 participant took benefit from the citations that the applicant provides in the Description field. In our report <ref type="bibr" coords="11,387.92,207.48,10.56,9.15" target="#b5">[6]</ref>, we raised objections regarding this strategy, because this information may be not visible for the person who accomplishes the Prior Art, depending whether he is the applicant or the examiner. This year, since nothing forbids it, we chose to extract these applicants' citations contained in Description. Evaluated on training data, from a baseline run which achieves a MAP of 0.153, using applicant's citations leads to a +39% improvement (MAP of 0.213). Therefore, two different official runs were submitted, one called "Applicant's view" which simulates the Prior Art Search for the applicant, and another one called "Examiner's view" which simulates the Prior Art Search for the examiner and which includes the Applicant's citations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Official runs</head><p>We hence submitted two runs, depending on the use of the applicant's citation. Final tuning on training data led to a MAP of 0.153 for the Applicant's View, but the official run only achieved a MAP of 0.106. For the Examiner's view (including applicants citations), we achieved a MAP of 0.213 for training data, but only 0.14 for official results (+32% compared to the Applicant's View).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we report our work in the Prior Art Candidates search and Classification in the CLEF-IP 2010 evaluation track. A corpus of 2.7M patents documents is used during the IR stage. The systems are evaluated with 2000 patent applications on both tasks.</p><p>In the CLS task, our system was ranked top three among the 7 participants, reaching 73% of mean average precision in the best run. The use of the multi-patent collections improved slightly the performance of the classification system. Moreover, the use of a multi-lingual collection or monolingual plus query translation showed to be equivalent concerning their classification performances. We plan to use the Catchword Index provided by WIPO to see if we can further improve our classification results. Moreover, we want to exercise the classification system using n-grams.</p><p>In the PAC task, our system, which largely relies on last year's system, was ranked top three among the 9 participants, while official results are disappointing regarding to the results obtained with training data. Further analysis needs to reveal the reason of such a bias. Our translation strategy was simple, but regarding to the weak amount of multilingual data, this +10% improvement is encouraging. We think that the multilingual aspects in CLEF-IP'10 were less clear than for CLEF-IP'09. Including inventors and applicants information is effective, but splitting them in different parts in order to reduce the noise is not. Finally, including applicants provided citations leads to a +35% improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,124.80,483.21,362.47,9.30;4,124.80,494.88,196.34,9.15;4,126.81,354.71,362.50,120.30"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Classification system: information retrieval engine (left box), k-NN algorithm (middle box) and re-ranking methods (right box).</figDesc><graphic coords="4,126.81,354.71,362.50,120.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,136.15,447.45,351.17,9.30;9,124.80,459.12,342.68,9.15;9,195.31,297.52,234.86,141.73"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Query size vs Query precision in the official run. Notice an almost linear relation between the average precision and the average query size for the 2000 topics.</figDesc><graphic coords="9,195.31,297.52,234.86,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.80,404.54,362.57,100.51"><head>Table 1 .</head><label>1</label><figDesc>Section</figDesc><table coords="3,124.80,404.68,362.57,100.37"><row><cell cols="4">distribution of patent documents (A and B) for the 3 three languages: German</cell></row><row><cell>(DE), English (EN) and French (FR).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Section</cell><cell>DE</cell><cell>EN</cell><cell>FR</cell></row><row><cell>Title</cell><cell>93.2%</cell><cell>99.6%</cell><cell>93.2%</cell></row><row><cell>Abstract</cell><cell>11.1%</cell><cell>28.1%</cell><cell>3.2%</cell></row><row><cell>Claim</cell><cell>14.0%</cell><cell>36.3%</cell><cell>4.7%</cell></row><row><cell>Description</cell><cell>14.0%</cell><cell>36.3%</cell><cell>4.7%</cell></row><row><cell>Applicant</cell><cell>96.3%</cell><cell>96.3%</cell><cell>96.3%</cell></row><row><cell>Citation</cell><cell>19.1%</cell><cell>19.1%</cell><cell>19.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,124.80,464.72,353.06,132.01"><head>Table 2 .</head><label>2</label><figDesc>BiTeM official results in CLS task.</figDesc><table coords="8,133.08,481.63,344.78,115.10"><row><cell>Run 4</cell><cell>Language</cell><cell>Index</cell><cell>Model</cell><cell>Ranking</cell><cell>MAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>method</cell><cell></cell></row><row><cell>FREQ_Run1</cell><cell>EN</cell><cell>1</cell><cell>BM25</cell><cell>codefreq</cell><cell>0.6194</cell></row><row><cell>FREQ_Run2</cell><cell>EN</cell><cell>1</cell><cell>DFR_BM25</cell><cell>codefreq</cell><cell>0.7145</cell></row><row><cell>FREQ_Run3</cell><cell>DE+EN+FR</cell><cell>1</cell><cell>DFR_BM25</cell><cell>codefreq</cell><cell>0.7195</cell></row><row><cell>MULTI_Run1</cell><cell>DE+EN+FR</cell><cell>3</cell><cell>DFR_BM25</cell><cell>codefreq</cell><cell>0.7281</cell></row><row><cell>MULTI_Run2</cell><cell>EN</cell><cell>1</cell><cell>BM25+DFR_BM25 +PL2</cell><cell>codefreq</cell><cell>0.7216</cell></row><row><cell>LIST_MULTI_Run1</cell><cell>DE+EN+FR</cell><cell>3</cell><cell>DFR_BM25</cell><cell>list</cell><cell>0.7259</cell></row><row><cell>LIST_MULTI_Run2</cell><cell>EN</cell><cell>1</cell><cell>BM25+DFR_BM25 +PL2</cell><cell>list</cell><cell>0.7227</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,124.80,195.98,362.55,138.31"><head>Table 3 .</head><label>3</label><figDesc>Evaluating translation strategies in PAC task. EN means that only English fields are used, while EN+TR means that both English and translated fields are used. We start with a Baseline run, which was computed using Titles, Abstracts, Claims, IPC codes for both collection and queries, and also Description for queries. We aim at evaluating the contribution of the different information contained in the Applicants and Inventors fields. Experiments show (see Table</figDesc><table coords="10,124.80,235.20,309.04,64.47"><row><cell>Strategy</cell><cell>MAP</cell></row><row><cell>Baseline</cell><cell>0.106</cell></row><row><cell>Translation strategy applied to the collection</cell><cell>0.114</cell></row><row><cell>Translation strategy applied to the collection and the queries</cell><cell>0.117</cell></row><row><cell>Document Representation.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,124.80,429.14,362.50,111.25"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of the different strategies for Document and Query Representation. App stands for Applicants and inv does for Inventors.</figDesc><table coords="10,159.93,468.36,284.49,72.03"><row><cell>Strategy</cell><cell>MAP</cell></row><row><cell>Baseline</cell><cell>0.117</cell></row><row><cell>Including app names</cell><cell>0.120</cell></row><row><cell>Including app names and countries</cell><cell>0.120</cell></row><row><cell>Including app names and inv names</cell><cell>0.124</cell></row><row><cell>Including app names, plus inv names and countries</cell><cell>0.124</cell></row><row><cell>Including app names and addresses, plus inv names and addresses</cell><cell>0.131</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,130.30,635.40,102.66,9.15"><p>http://eagl.unige.ch/bitem</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,130.30,635.40,111.54,9.15"><p>http://ir.dcs.gla.ac.uk/terrier</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,130.30,635.40,107.13,9.15"><p>http://translate.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="7,124.80,612.90,362.49,9.15;7,124.80,624.42,129.41,9.15"><p>Applicants' proposed Citations. Citations are extracted from the query Description field with simple regular expressions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="8,130.33,623.82,357.01,9.15;8,133.30,635.50,19.72,9.05"><p>Official run ids are prefixed by the group name, bitem, and suffixed by the task acronym, CLS.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,128.18,261.10,359.17,8.27;12,136.15,271.48,45.77,8.27" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,188.13,261.10,181.72,8.27">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,376.00,261.10,93.58,8.27">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,281.86,359.09,8.27;12,136.15,292.24,254.98,8.27" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,274.86,281.86,212.41,8.27;12,136.15,292.24,48.04,8.27">Automatic IPC encoding and novelty tracking for effective patent mining</title>
		<author>
			<persName coords=""><forename type="first">Teodoro</forename><forename type="middle">D</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,200.15,292.24,162.49,8.27">Proceedings of NTCIR-8 Workshop Meeting</title>
		<meeting>NTCIR-8 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,302.62,359.13,8.27;12,136.15,313.06,326.50,8.27" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,373.60,302.62,113.70,8.27;12,136.15,313.06,122.66,8.27">Knn and re-ranking models for English patent mining at NTCIR-7</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.40,313.06,160.72,8.27">Proceedings of NTCIR-7 Workshop Meeting</title>
		<meeting>NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,323.44,359.15,8.27;12,136.15,333.82,277.51,8.27" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,332.62,323.44,154.71,8.27;12,136.15,333.82,70.32,8.27">Overview of the patent mining task at the NTCIR-7 workshop</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nanba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,222.66,333.82,162.50,8.27">Proceedings of NTCIR-7 Workshop Meeting</title>
		<meeting>NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,344.20,359.15,8.27;12,136.15,354.64,303.43,8.27" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,332.56,344.20,154.76,8.27;12,136.15,354.64,64.86,8.27">Report on the TREC 2009 experiments: Chemical IR track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruch</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.39,354.64,194.70,8.27">the Eighteenth Text REtrieval Conference (TREC-18)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,365.02,359.12,8.27;12,136.15,375.40,351.16,8.27;12,136.15,385.78,170.22,8.27" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,319.72,365.02,167.58,8.27;12,136.15,375.40,210.52,8.27">Simple pre and post processing strategies for patent searching in the CLEF intellectual property track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruch</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.35,375.40,101.96,8.27;12,136.15,385.78,131.98,8.27">CLEF 2009 Proceedings in Lecture Notes in Computer Sciences</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="12,128.18,396.16,359.21,8.27;12,136.15,406.60,334.93,8.27" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,351.98,396.16,135.40,8.27;12,136.15,406.60,149.80,8.27">Using taxonomy, discriminants, and signatures for navigating in text databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raghavan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,301.62,406.60,140.97,8.27">Proceedings of 23rd VLDB conference</title>
		<meeting>23rd VLDB conference</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.18,416.98,359.13,8.27;12,136.15,427.36,351.17,8.27;12,136.15,437.74,142.26,8.27" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,341.37,416.98,145.94,8.27;12,136.15,427.36,347.19,8.27">Scalable feature selection, classification and signature generation for organizing large text databases into hierarchical topic taxonomies</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raghavan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,136.15,437.74,68.15,8.27">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.17,448.18,359.17,8.27;12,136.15,458.56,351.15,8.27;12,136.15,468.94,23.99,8.27" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,255.49,448.18,231.86,8.27;12,136.15,458.56,142.92,8.27">Does it matter where patent citations come from? Inventor vs. examiner citations in European patents</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Criscuolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verspagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,296.60,458.56,57.92,8.27">Research Policy</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1892" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
