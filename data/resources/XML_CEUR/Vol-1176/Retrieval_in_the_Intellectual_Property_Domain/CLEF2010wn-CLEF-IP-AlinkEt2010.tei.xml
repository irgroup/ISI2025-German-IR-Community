<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.08,115.96,335.21,12.62">CLEF-IP 2010; Building strategies, a year later</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.29,153.63,38.46,8.74"><forename type="first">W</forename><surname>Alink</surname></persName>
							<email>alink@spinque.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde &amp; Informatica</orgName>
								<address>
									<addrLine>Science Park 123</addrLine>
									<postCode>1098 XG</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.50,153.63,60.00,8.74"><forename type="first">R</forename><surname>Cornacchia</surname></persName>
							<email>cornacchia@spinque.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde &amp; Informatica</orgName>
								<address>
									<addrLine>Science Park 123</addrLine>
									<postCode>1098 XG</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.84,153.63,57.23,8.74"><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde &amp; Informatica</orgName>
								<address>
									<addrLine>Science Park 123</addrLine>
									<postCode>1098 XG</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.08,115.96,335.21,12.62">CLEF-IP 2010; Building strategies, a year later</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92C9D6542A86660DEB165EAA340A849F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>After participating in last year's CLEF IP (2009) evaluation benchmark, our scores were rather low. The CLEF IP 2010 PAC task enabled us to correct some experiments and obtain better results, basically using the same techniques (almost the same BM25-category strategy as used last year) and improved strategy builder software, and less computing hardware at our disposal. The results are now comparable with other participants. Similar to last year, no feature extraction techniques have been applied; and queries only used the structural information provided in the XML-format of the patent-documents. Furthermore we participated in the new CLS task, which, although scores were rather low, shows again the flexibility of our approach. The low scores can be explained by the straight-forward method applied searching the patent-document collection using keywords from the topic-patent, and using the IPCR-classifications extracted from the documents as results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main objective of this research is to demonstrate the importance of flexibility in expressing strategies for patent-document retrieval. While last year's submission focussed on flexibility, this year also scalability, and retrieval quality have been taken into account. The results of our system are comparable with other participants, while it can also be operated interactively, which makes it a powerful tool. The paper gives an overview of the system (Section 2) and the techniques used to generate the runs (Section 3). Afterwards the results are evaluated (Section 4). Finally a conclusion is drawn (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System overview</head><p>We created our submission for the CLEF-IP 2010 evaluation benchmark using Spinque's strategy builder interface. The setup is similar to last year's setup, only the system has matured. The hardware requirements have lowered from a supercomputer to a single server, and querying could be performed on a desktop machine, whereas query-speed has even increased. Last year the strategies were still optimized by hand; this year, there was no performance tweaking done between strategy definition and performing the benchmark runs. The reader is pointed to last year's paper <ref type="bibr" coords="1,258.09,656.12,10.51,8.74" target="#b0">[1]</ref> for more details about the setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Index creation</head><p>The index was created on a high-end server: 2.4Ghz 4 core processor, 36GB RAM, 5x 2TB SATA disks in RAID-5 configuration. All querying was done on a 3-year old desktop with average computing specs: 2.4Ghz processor, 4 cores, 8GB ram, 2x500GB SATA disks in RAID-0. Creating a generic index for the whole collection took about 3 days. Creating a full run over 2000 topics took about 12 hours. A SQL dump of the resulting database is roughly 300GB uncompressed in size (81GB bzipped).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLEF-IP Experiments</head><p>This Section reports on the experiments conducted for the official submission. Fine-tuning of all parameters used for the PAC task was performed on the training set provided. The parameters for the CLS task haven't been trained.</p><p>Instead of merging patent-documents belonging to the same patent into a single document (suggested in the CLEF-IP instructions), we have indexed (similar to last year's submission) the original documents, and, aggregate scores from different patent-documents into patents as part of the search strategies.</p><p>Two runs have been submitted for CLEF-IP 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prior-Art Candidate Search Task</head><p>The strategy can be explained as follows: first make a selection of patents in the corpus that have at least one classification code in common with the topic patent, or have the same assignee. Search this selection using 26 keywords from topic-patent using the BM25 model. As a last step in the strategy, and due to the evaluation measures that are used for CLEF-IP 2010, all patent-documents within the same simple-family have been given the same score as the best scoring patent-document of that family. See figure <ref type="figure" coords="2,322.70,462.34,3.87,8.74">1</ref>.</p><p>Difference with last year is the searching for patents by same assignee, and the hard selection before keyword search based on classification and assignee, instead of a mixture between keywords, classifactions, and assignees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification Task</head><p>Used same strategy builder as for the PAC run, no additional coding, or reconfiguring. The strategy was to search documents using 26 keywords from the topic-patent, and then extract the classifications of these documents as results. See figure <ref type="figure" coords="2,180.18,580.15,3.87,8.74">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and analysis</head><p>We learned from the problems found during last year's participation. Similar to last year's contribution, the strategy building contribution, shows flexibility without re-programming, re-indexing, or re-configuring a system. Last year Fig. <ref type="figure" coords="3,232.56,521.35,3.87,8.74">1</ref>: Prior-Art Candidate search strategy (CLEF-IP 2009) the system was still under heavy development. This caused the software to perform far from optimal. This year, most of the issues have been resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prior-Art Candidate Search Task</head><p>The results are more in line with the expected results than last year. It looks like the participants with similar strategies perform similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: IPCR Classification strategy</head><p>Our run did not use structural information outside the given structural information in the XML documents in which the patent-documents were provided. It was shown by Lopez and Romary <ref type="bibr" coords="4,283.45,499.15,10.52,8.74" target="#b1">[2]</ref> that an increase of MAP by .10 (absolute) can be achieved when using citations extracted from the topic-patent.</p><p>Notice that the MAP, recall, and PRES are all quite stable over the different languages of the topic-patent, while some other participants seem to have much higher fluctuations between them. A possible explanation for this phenomenon is the lack of language specific optimizations.</p><p>In contrast to last year, we did not merge the results of the classification search, with the results of the keyword search, but filtered the results of the classification search with the results of the keyword search. This had a major consequence; documents not having any classification in common with the topic document were not extracted. Recall is therefore likely to be lower than when the results would have been mixed; notice that the average number of results per topic is not the maximum (1000), but a little lower. Results in <ref type="bibr" coords="4,433.66,644.16,10.52,8.74" target="#b2">[3]</ref> confirm this effect of hard (facet) selections. However, it has been much easier to define a balance between the weight of the classification and the weight of the keywords, which took a lot of time in last year's submission. More work on automatically tuning the weights for multiple components of the search to a specific task has to be done, and would likely yield better results, both in terms of MAP and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Task</head><p>Our classification run got low scores for both precision and MAP. The recall (at 25 and at 50) was reasonable to high compared to other participants. This is most probably due to the fact that for each topic we tried to provide the full 1000 results. Other dedicated systems do clearly outperform the strategy we have build. It would be interesting to see what happens to the precision scores if a cut-off was applied to the results based on the computed probabilities (and using a fixed cut-off value for all topics). Other improvements could probably be made by using more aspects of the patent, perhaps using the patent citations used in the topic patent, and information on the inventor and assignee. Due to the limited time available, such runs have not been created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Participation in the CLEF-IP 2010 evaluation track has been easier than last year. Compared to last year's results seem to have improved relative to other participants for the PAC task. The results for the CLS task show that there is still a lot of room for improvement. Also, more work is needed on the automatic tuning of strategy parameters.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements: We would like to thank the MonetDB kernel developers and Spinque for their support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,507.22,342.24,7.86;5,146.91,518.18,333.68,7.86;5,146.91,529.14,56.38,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,377.29,507.22,103.30,7.86;5,146.91,518.18,155.11,7.86">Running CLEF-IP experiments using a graphical query builder</title>
		<author>
			<persName coords=""><forename type="first">Wouter</forename><surname>Alink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Cornacchia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arjen</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vries</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,322.54,518.18,142.98,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="5,138.35,540.10,342.24,7.86;5,146.91,551.06,209.27,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,291.75,540.10,188.84,7.86;5,146.91,551.06,75.08,7.86">Multiple retrieval models and regression models for prior art search</title>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,241.52,551.06,86.41,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,562.02,342.24,7.86;5,146.91,572.97,221.21,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,267.95,562.02,194.69,7.86">Interactive Retrieval based on Faceted Feedback</title>
		<author>
			<persName coords=""><forename type="first">Lanbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,146.91,572.97,192.87,7.86">Proceedings of the 33rd ACM SIGIR Conference</title>
		<meeting>the 33rd ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
