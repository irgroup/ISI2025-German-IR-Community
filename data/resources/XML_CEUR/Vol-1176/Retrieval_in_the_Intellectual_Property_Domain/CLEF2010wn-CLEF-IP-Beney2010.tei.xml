<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.02,115.73,229.28,12.94;1,193.65,133.66,228.04,12.94">LCI-INSA Linguistic Experiment for CLEF-IP Classification Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,282.64,171.87,50.07,9.62"><forename type="first">Jean</forename><surname>Beney</surname></persName>
							<email>jean.beney@insa-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Département Informatique</orgName>
								<orgName type="laboratory">LCI</orgName>
								<orgName type="institution">INSA de Lyon F69621 Villeurbanne</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lyon</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.02,115.73,229.28,12.94;1,193.65,133.66,228.04,12.94">LCI-INSA Linguistic Experiment for CLEF-IP Classification Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43DCF60B9135735AF7D4E1F159B145AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Supervised Classification</term>
					<term>Document Cathegorization</term>
					<term>Winnow</term>
					<term>Natural Language Processing</term>
					<term>Affix grammars</term>
					<term>Linguistic triples</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the experiment the LCI group has performed to prepare our submission to CLEF-IP Classification Track. In this preliminary experiment we used a part of the available target documents as test set and the rest as train set. We describe the systems AGFL used for extracting these triples and the LCS used for classification by the Winnow algorithm. We show that the use of linguistic triples in place of bags of words improves the accuracy, as well as using the names and addresses of the applicants. we found that using the complete descriptions as bags of words does not really perform better than using only abstracts and titles. Some simple mathematics show that the official measures are redundant and that R@N should be used to evaluate a ranking, P@1 to evaluate routing and that the usual precision, recall and F1 should be used on the results of a real classification, that is a selection of the classes performed internally by the classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF-IP competition gave us the opportunity to repeat on a widely available document set some experiments that we have done on other less widely accessible data. The goal of these experiments is to investigate the effect of compound linguistic terms on document classification quality.</p><p>Most automatic classifications are based on the Bag of Words representation of the texts: the words<ref type="foot" coords="1,229.09,548.53,3.97,4.85" target="#foot_0">1</ref> are just counted but there order is not taken into account. It seems obvious that this order, so important for human understanding of the sentences, can have an effect in the classification, at least on the precision by disambiguating homonyms.</p><p>The linguistic system AGFL allows us to parse texts efficiently and to build dependency parse trees. We unnested them to dependency triples (two words and a relation) that we use as classification features. This Bag of Triples is supposed to better represent the aboutness <ref type="bibr" coords="1,276.15,634.16,13.14,9.07" target="#b0">[1]</ref> of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The classification system LCS and the Winnow algorithm</head><p>The system used is the Linguistic Classification System <ref type="bibr" coords="2,377.19,159.59,10.51,9.62" target="#b4">[5]</ref> which was developed by the University of Nijmegen (RU) in the course of the DORO and PEKING Esprit Projects<ref type="foot" coords="2,200.70,182.31,3.97,4.85" target="#foot_1">2</ref> . LCS is a generical system for classification of documents after training on a corpus of known documents (supervised classification). The terms (features) managed by LCS may be either simple words or linguistic triples The classifiers were trained with balanced Winnow <ref type="bibr" coords="2,373.24,219.99,9.96,9.62" target="#b7">[8]</ref>, a heuristical learning algorithm with nice mathematical convergence properties. It can be seen as a Perceptron with multiplicative updates. The balanced version <ref type="bibr" coords="2,404.11,243.89,10.51,9.62" target="#b1">[2]</ref> can cope withlarge numbers of features and can tolerate large variations in document length, because it uses positive and negative weights.</p><p>According to previous experiments, the Winnow parameters received the following values: promotion 1.02, demotion 0.92, 10 iterations over the documents, thick threshold [0.6,2]. The general parameters are: term strength LTC, term selection based on χ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The linguistic system AGFL and the associated grammars</head><p>The linguistic system AGFL<ref type="foot" coords="2,260.94,390.95,3.97,4.85" target="#foot_2">3</ref> (see <ref type="bibr" coords="2,290.21,392.14,10.79,9.62" target="#b5">[6]</ref>) reads the grammatical description of a natural language and builds a parser for this language.</p><p>The input is an affix grammar <ref type="bibr" coords="2,287.46,416.68,10.51,9.62" target="#b2">[3]</ref> that is a context-free grammar, endowed with enumerated parameters, which has been found well-suited for linguistic needs. There are no restrictions on left or right recursion, the huge lexicon is efficiently managed as a trie. The output of the generated transducer is described as a compositional transduction inside the grammar.</p><p>AGFL and the associated grammars are under GNU GPL license. The English grammar NPX<ref type="foot" coords="2,267.94,488.44,3.97,4.85" target="#foot_3">4</ref> used in the present experiment has been written by C.H.A. Koster in Nijmegen, the French grammar FR4IRhas been written by J. Beney in Lyon. Both output dependency trees, where the subtrees (including the leaves, the words) are linked by a grammatical relation.</p><p>The trees are unnested to lists of head-modifier pairs that can easily be interpreted as linguistic triples because, if the head is a simple word, the modifier is composed of a relation (or a preposition) and a word. Examples of triples: We have selected the documents that have at least one abstract (usually an A file, some have 2 abstracts) and an IPC code <ref type="foot" coords="3,327.02,177.11,3.97,4.85" target="#foot_4">5</ref> (to be found in a corresponding B file). To avoid a bias, we have kept only 1 file per patent (sometimes, there are 2 versions of the abstract(s)). We experimented on the 3 languages separately, but also on the concatenation of the abstracts in different languages. Table <ref type="table" coords="3,475.60,214.16,4.98,9.62" target="#tab_0">1</ref> gives the number of documents we have used, the average document length, the average number of classes and subclasses per document and the number of classes and subclasses that have at least 1 document. The last line gives the number of unique words for the Bag of Words representation of the abstracts and titles. The number of words in the complete set of documents is larger than the sum of the 3 language sets separately because most of the documents have only 1 abstract but 3 titles. Therefore we have more text (English and French title added to a German abstract, and so on).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental process</head><p>To prepare the CLEF-IP sumission, we have experimented on the training documents we have selected, using 20% of them as a test set. The documents were represented as Bag of Words or as bags of linguistic triplets obtained by parsing the abstracts and titles by the corresponding AGFL parser, concatenating the different unnested tree in case several abstracts are present. As we have no German analyser yet, the German abstracts were kept as bags of words.</p><p>For some experiments, we had time to repeat the classification test on 10 different shuffles 80%/20% of the document set (a form of cross-validation). In these cases, we give the mean and (between parentheses) the standard deviation over the 10 runs.</p><p>The full results are given for class level, then we will have a quick look at subclass level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accuracy measures</head><p>LCS has been designed to perform a classification, that is, do decide whether a given unknown document belongs to a given class or not. In fact, Winnow computes a score <ref type="foot" coords="4,190.83,226.11,3.97,4.85" target="#foot_5">6</ref> and the documents whose score are greater than 1 are retrieved <ref type="foot" coords="4,473.35,226.11,3.97,4.85" target="#foot_6">7</ref> .</p><p>During training, the measure F1 (harmonic mean between precision and recall, see <ref type="bibr" coords="4,171.22,251.22,10.79,9.62" target="#b8">[9]</ref>) is optimized. The Winnow score allows us to ra the documents, but we can only hope that the best F1 leads to the best ranking accuracy measures.</p><p>The value given in the following tables are micro-averaged F1 values on all documents. The macro-averaged F1 on all classes is generally lower because the accuracy is very bad for the small classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We have compared the words and triples representation, expecting that the triples will improve the accuracy. We then studied the effect of the document number on this improvement.</p><p>We also compared the use of different XML elements (abstracts, names, addresses and description) to see to what extent the full description performs better than the abstracts. And we briefly compared the accuracy at class and subclass level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Triples vs words</head><p>The main purpose of this experiment is to find whether linguistic triples can help the classifier or not. In table 2, we compare the classification accuracy (F1) at class level, with words only, with triples only and with both, for French abstracts, English abstracts and all abstracts including German.</p><p>We see that the triples-only representation give an accuracy lower than the words-only but that words and triples together give a better accuracy. The same result was found by us on other document collections <ref type="bibr" coords="4,370.16,546.22,9.96,9.62" target="#b3">[4]</ref>.</p><p>The English abstracts show a larger gain than the French abstracts, but this difference can be explained by the number of available training documents (see 5.3). The fact that the German abstracts give a poor result (lower than for French, with more documents) could be explained by this hypothesis: in German, composed words can be built as in English, but the components are glued together <ref type="foot" coords="5,198.04,248.51,3.97,4.85" target="#foot_7">8</ref> In the table 1, we see that the german word forms are much more numerous, while the average document length is smaller. Each word form is then more rare and statistically less discriminative.</p><p>The result obtained with all the documents is hard to interpret: the accuracy is better because we have more documents and more titles, but why is the gain so small?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Abstracts, names, descriptions</head><p>The patent documents in the CLEF-IP collection are composed of mayny other XML elements besides the abstracts and /or the description. We investigated the use of (words and triples from) the abstracts, titles, descriptions and applicant names and addresses.</p><p>The table <ref type="table" coords="5,195.07,403.97,4.98,9.62" target="#tab_2">3</ref> show the results for different parts of the XML documents The names and addresses of the applicants bear information because most companies work in a restricted domain. It is not surprising that using these parts of the patent helps a bit.</p><p>We can note that the improvement is almost the same for the two representations, which means that the information brought by the names and addresses is independent from the information brought by the triples.</p><p>The result obtained with the descriptions is disappointing especially when we consider the time needed to train a classifier with the huge vocabulary of this representation: 9 days when the training for the best result was obtained in 9 hours only.</p><p>We did not find time to parse the descriptions and to train with the triples obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The number of documents</head><p>On the English abstracts, which are the more numerous, also looked at the influence of the number of training documents. Keeping the test set fixed (20% of the available English abstracts), we computed F1 for words, triples, triples+words and for different number of training document randomly choosen in the training set. The results is shown in the training graph for subclass level: figure <ref type="figure" coords="6,162.45,290.93,3.87,9.62" target="#fig_1">1</ref>. We clearly see that the classification power of triples is lower than that of words, but it grows faster. We do not know whether, with a much larger number of document, the triples alone could give a better classifier than the words alone.</p><p>The effect of the triples on the accuracy of the combination words+triples (gain) also grows with the number of training documents.</p><p>These results can be explained as follows: because the triples are necessarily more rare than the words they are composed of, they need more documents in order to appear a number of times large enough for them to have a statistical effect.</p><p>The class size In figure <ref type="figure" coords="7,245.75,157.13,3.87,9.62" target="#fig_2">2</ref>, the vertical axe shows the gain obtained when using the words and the triples together compared to the use of the words alone. We have excluded the 202 classes that have less than hundred training documents for which the following effect is even more pronounced: the classification into small classes can gain or loose very much from the use of triples. The few train documents available are not enough to get a sample that is representative of the class. Therefore the result is rather uncertain: the gain can be negative or very large in the cases where the words gave a very low accuracy.</p><p>However, for classes that have more than 1500 training documents, the gain is always positive, even if it is very small for some very large classes that were already well classified with the words only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The subclass level</head><p>The following table can be compared with table 2, which concerns the class level.</p><p>As expected, the accuracy with more than 600 subclasses (level for CLEF-IP competition) is always lower than with 120 classes.</p><p>The fact that the words and triples together do not perform better than the words is probably due to the use of the subclasses, which are in average 5 times smaller than the classes. In the final runs, we have submitted rankings obtained from the Winnow scores with the abstracts in the three languages together. We expected that the differences to appear on the ranking measures would be in agreement with those found on F1 in our preliminary experiment. But most of the measures give a lower value for triples and words together than for the words alone; for the other measures, the difference is very low (less than 0.2%).</p><p>It turned out that the official results were computed using IPC-R codes, so that several subclasses (2.4% of the occurences) never appeared in our training set, making them impossible to be found by us.</p><p>Furthermore, the trec_eval program introduces a bias so that teams that do not submit full ranking get better scores. Therefore, it is hard to compare different run results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Note on the trec_eval measures</head><p>We can easily find a link between precision and recall at the same Rcut (Rank cut-off, see <ref type="bibr" coords="8,184.88,458.15,14.75,9.62" target="#b9">[10]</ref>).</p><p>Let us define M as the number of documents, T i as the number of relevant classes for document d i , S i @N as the number of relevant classes in the first N classes selected for document d i , T = i T i and S@N = i S i @N .</p><p>Then the micro-averaged precision at N is: P @N = S@N N ×M the micro averaged recall at N is: R@N = S@N T and F1 at N is: F 1@N = 2P @N ×R@N P @N +R@N = 2S@N T +N ×M . It follows that:</p><formula xml:id="formula_0" coords="8,196.35,563.78,221.56,23.54">N × M × P @N = T × R@N = T + N × M 2 F 1@N</formula><p>The 3 values are linearly dependent and monotically increasing one with the other. This is a situation completely different from the usual P/R graphs, where the differents pairs of values are obtained with the same classification method by varying the number of selected classes. Here, the number of selected classes is fixed; the different points are obtained by different methods (or the same method with different parameters).</p><p>Figure <ref type="figure" coords="9,181.26,118.66,4.98,9.62" target="#fig_3">3</ref> has been built using the values of P@25 and R@25 for all the submited runs. It clearly shows the linearity for complete rankings and the bias for incomplete rankings. The linearity is not perfect (the straight line is the theoretical line) and we can see in the data some cases where there is obviously an error, or at least a rounding error. We also clearly see on this graph the bias introduced by trec_eval for results that do not contain a complete ranking.</p><p>The linear dependency means that we do not need the 3 values to compare the results of 2 teams, 2 methods or 2 sets of parameter values: one is enough. The next section will help us to choose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ideal values</head><p>In some of cases, it is obvious to know what would be the value of given measure if the classifier was perfect.</p><p>Let P M @N and R M @N be the largest possible value of precision and recall when selecting N classes per document. You cannot select more than N classes, you cannot have in your selection for document d i more than T i relevant classes, therefore the largest possible value of relevant and selected classes is:</p><formula xml:id="formula_1" coords="9,252.83,647.68,109.69,19.37">S M @N = i min(T i , N ) T hen P M @N = i min(T i , N ) N × M and R M @N = i min(T i , N ) T</formula><p>For small values of N, these values are computed by looking at the number of classes of each document in the qrel file, but:</p><formula xml:id="formula_2" coords="10,173.30,200.11,268.76,19.37">when N ≥ max(T i ) : S M @N = i T i = T, then : R M @N = 1</formula><p>Which allows us to see if a method retrieves all the classes (R=1) or almost all the classes with a decent number of selections.</p><p>when N ≤ min(T i ) :</p><formula xml:id="formula_3" coords="10,255.08,262.29,198.00,19.37">S M @N = i N = N × M, then : P M @N = 1</formula><p>This correspond to another practical situation: routing to a single examiner who will, if necessary, forward the patent application to others examiners. For this task, we also have a human reference: at EPO, human routers reached a of the first choice of 81.2% (see <ref type="bibr" coords="10,274.20,327.62,10.29,9.62" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ranking versus classification</head><p>The Rcut method is often used is Information Retrieval: "give me the first 10 results and, if I do not find what I want, I will modify my request".</p><p>A classifier should be able to decide if a given document belongs to a given class or not. The number of classes for a given document can vary very much. Then, a Scut (score cut-off) is generally used: a document is selected for a class when its score (computed by the classifier) is larger than a given threshold (1 for us). This can be combined by an interval Rcut: assign to each document at least x classes and at most y classes (we used 1-4). This can also be combined by a Pcut (proportionnal cut-off): assign to each class a number of document proportionnal to the size of the class in the training set.</p><p>To evaluate the quality of these strategies, we need to know which documentclass pairs were selected, and the complete ranking does not help. Then, precision, recall and F1 could be computed on that selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and further works</head><p>The results we have obtained in these experiments confirm those that we have found on other patent sets (from EPO and WIPO): when using as terms besides the words the linguistic triples, there is a statistically significant improvement. The gain is not large but very stable in cross-validation. Furthermore this gain grows with the number of training documents, when we work on a single language.</p><p>When we used all the available abstracts and titles together (3 languages) the accuracy was much larger than with the languages separated, probably because of the much larger number of available train documents and maybe, for a small amount, of the additional titles.</p><p>In this situation, the applicant names and addresses bring extra information that seems independent of the information brought by the triples. The huge document representation by the description words does not perform better after a very long training. We therefore recommend using the triples (from the abstracts and titles) plus the names and addresses.</p><p>The French and English grammars are still under improvement and we hope to get even larger improvements with the new versions.</p><p>Using the MAREC collection, we plan to repeat the experiment with more documents to see whether the triples alone can perform better than the words with many more occurences. The new LCS3, that is much faster than the previous version, should allow to train on more than 1 million documents in a reasonable time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,149.71,574.55,150.73,9.62;2,149.71,587.13,109.14,9.62;2,149.71,599.70,159.18,9.62;2,149.71,612.27,119.88,9.62;3,134.77,117.42,135.30,10.77;3,134.77,144.67,124.52,10.02"><head></head><label></label><figDesc>[ N:rouleau, ATTR A:cylindrique ] [ N:machine, P:à N:café ] [ N:apparatus, SUBJ V:eliminating ] [ N:flocks, through N:duct ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,222.54,539.30,167.19,8.67"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Training curve, English abstracts,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,246.50,522.86,122.34,8.67"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Effect of the class size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,257.00,415.29,98.29,8.67"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. P/R graph at 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,169.52,296.15,276.32,120.36"><head>Table 1 .</head><label>1</label><figDesc>statistics on the documents.</figDesc><table coords="3,169.52,296.15,276.32,99.53"><row><cell></cell><cell>All languages</cell><cell cols="3">English French German</cell></row><row><cell>documents</cell><cell>544,126</cell><cell cols="3">366,804 55,876 148,631</cell></row><row><cell>number of classes</cell><cell>121</cell><cell>118</cell><cell>118</cell><cell>120</cell></row><row><cell>number of subclasses</cell><cell>631</cell><cell>617</cell><cell>617</cell><cell>631</cell></row><row><cell>words/document</cell><cell></cell><cell>132</cell><cell>121</cell><cell>99</cell></row><row><cell>classes/document</cell><cell>1.33</cell><cell>1.32</cell><cell>1.32</cell><cell>1.32</cell></row><row><cell>subclasses/document</cell><cell>1.45</cell><cell>1.45</cell><cell>1.42</cell><cell>1.44</cell></row><row><cell></cell><cell>Abstracts Descriptions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>unique words</cell><cell cols="4">1,337,592 4,635,879 288,471 91,734 781,308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,156.19,117.47,299.92,74.94"><head>Table 2 .</head><label>2</label><figDesc>F1 for the different document representations.</figDesc><table coords="5,156.19,117.47,299.92,54.10"><row><cell></cell><cell>words only</cell><cell>triples only</cell><cell>words+triples</cell><cell>gain</cell></row><row><cell cols="5">English 69.94%(0.15%) 66.11%(0.11%) 73.05%(0.12%) 3.11%(0.11%)</cell></row><row><cell cols="5">French 68.41%(0.19%) 50.02%(0.26%) 70.07%(0.20%) 1.66%(0.20%)</cell></row><row><cell>German</cell><cell>62.10%</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell>All</cell><cell>75.38%</cell><cell>missing</cell><cell>76.02%</cell><cell>.64%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,138.25,402.77,338.82,97.38"><head>Table 3 .</head><label>3</label><figDesc>F1 for different parts of the documents, all languages together, class level.</figDesc><table coords="5,192.39,436.58,230.56,42.74"><row><cell></cell><cell cols="2">words words+triples</cell></row><row><cell>abstracts+titles</cell><cell>75.38%</cell><cell>76.02%</cell></row><row><cell cols="2">abstracts+titles+names+addresses 76.50%</cell><cell>77.15%</cell></row><row><cell>descriptions</cell><cell>75.52%</cell><cell>missing</cell></row></table><note coords="5,458.17,402.77,3.97,4.85;5,462.64,403.97,2.77,9.62"><p><p>9 </p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,117.47,266.46,118.03"><head>Table 4 .</head><label>4</label><figDesc>Subclass level</figDesc><table coords="8,134.77,117.47,266.46,118.03"><row><cell cols="4">Subclass words only triples only words+triples</cell></row><row><cell>All</cell><cell>68.26%</cell><cell>missing</cell><cell>67.80%</cell></row><row><cell cols="2">English 61.88%</cell><cell>56.86%</cell><cell>65.41%</cell></row><row><cell>French</cell><cell>59.93%</cell><cell>41.46%</cell><cell>61.31%</cell></row><row><cell cols="2">German 60.38%</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell cols="2">6 Ranking for CLEF-IP</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.49,131.48,8.67"><p>wordforms or lemmatized words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,634.58,111.89,8.67"><p>http://www.cs.ru.nl/peking</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,645.53,102.18,8.67"><p>http://www.agfl.cs.ru.nl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,656.49,322.77,8.67"><p>This grammar has been improved and is now available under the name AEGIR.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.73,645.53,335.86,8.67;3,144.73,656.49,80.20,8.67"><p>we have used the &lt;main-classification&gt; and &lt;further-classification&gt; subelements of &lt;classification-ipc&gt;</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,144.73,601.70,335.85,8.67;4,144.73,612.65,206.86,8.67"><p>This scores allow us to output rankings of the documents in each class and vice versa. These ranking is the result sent to CLEF-IP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,144.73,623.62,335.86,8.67;4,144.73,634.58,335.83,8.67;4,144.73,645.53,32.36,8.67"><p>In addition to this Scut, we also use a Rcut, that is each document is retrieved in at least 1 class (which is very useful) and at most 7 classes (4 classes gives very similar results).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,144.73,612.65,269.02,8.67"><p>For example software engineering is translated by Softwaretechnik.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="5,144.73,634.58,335.85,8.67;5,144.73,645.53,44.86,8.67"><p>For the name and adresses, we used the following XML elements: name, last-name, street, city.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,329.58,337.63,8.67;11,151.52,340.54,170.86,8.67" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,277.92,329.58,182.29,8.67">A study of aboutness in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">W C</forename><surname>Huibers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,467.01,329.58,13.58,8.67;11,151.52,340.54,105.81,8.67">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,351.50,337.65,8.67;11,151.52,362.45,329.09,8.67;11,151.52,373.42,25.61,8.67" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,280.15,351.50,182.58,8.67">Mistake-driven learning in text categorization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Karov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,362.45,280.72,8.67">Proceedings of the Second Conference on Empirical Methods in NLP</title>
		<meeting>the Second Conference on Empirical Methods in NLP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,384.38,337.64,8.67;11,151.52,395.33,329.07,8.67;11,151.52,406.29,110.66,8.67" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,220.12,384.38,154.91,8.67">Affix grammars for natural languages</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,412.04,384.38,68.55,8.67;11,151.52,395.33,284.34,8.67">Attribute Grammars, Applications and Systems, International Summer School SAGA</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="469" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,417.26,337.62,8.67;11,151.52,428.21,252.38,8.67" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,265.84,417.26,195.48,8.67">Phrase-based document categorization revisited</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,428.21,177.22,8.67">Proceedings CIKM 2009, PAIR&apos;09 workshop</title>
		<meeting>CIKM 2009, PAIR&apos;09 workshop</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,439.17,337.65,8.67;11,151.52,450.13,329.07,8.67;11,151.52,461.09,54.30,8.67" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,312.96,439.17,167.65,8.67;11,151.52,450.13,52.55,8.67">Multi-classification of patent applications with Winnow</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Seutter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,228.05,450.13,98.79,8.67">Proceedings of PSI 2003</title>
		<meeting>PSI 2003</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2890</biblScope>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,472.05,337.63,8.67;11,151.52,483.01,168.98,8.67" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,286.45,472.05,123.75,8.67">The AGFL grammar work lab</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Verbruggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,432.53,472.05,48.06,8.67;11,151.52,483.01,94.11,8.67">Proceedings FREENIX/Usenix 2002</title>
		<meeting>FREENIX/Usenix 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,493.96,337.67,8.67;11,151.52,504.92,210.08,8.67" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,235.37,493.96,245.26,8.67;11,151.52,504.92,19.74,8.67">Automatic categorisation applications at the european patent office</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zaccà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,177.62,504.92,105.12,8.67">World Patent Information</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="187" to="196" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,515.89,337.63,8.67;11,151.52,526.84,231.65,8.67" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,214.84,515.89,265.75,8.67;11,151.52,526.84,77.85,8.67">Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,236.59,526.84,72.33,8.67">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="318" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,537.80,305.25,8.67" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,239.10,537.80,82.73,8.67">Information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Butterworths</publisher>
			<pubPlace>Londres</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,548.76,338.01,8.67;11,151.52,559.72,131.99,8.67" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,192.96,548.76,245.69,8.67">An evaluation of statistical approaches to text categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,445.78,548.76,34.84,8.67;11,151.52,559.72,55.16,8.67">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
