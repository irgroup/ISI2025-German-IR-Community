<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.13,115.96,321.10,12.62;1,285.08,133.89,45.20,12.62">Bird sound classification using a bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,240.85,171.66,57.23,8.74"><forename type="first">Lukas</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">ZHAW Zurich University of Applied Sciences</orgName>
								<address>
									<settlement>Winterthur</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,320.77,171.66,53.74,8.74"><forename type="first">Mario</forename><surname>Marti</surname></persName>
							<email>mario.marti@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">ZHAW Zurich University of Applied Sciences</orgName>
								<address>
									<settlement>Winterthur</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.13,115.96,321.10,12.62;1,285.08,133.89,45.20,12.62">Bird sound classification using a bidirectional LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47F3B5480AF614EEFF0EAF81E4137F20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LSTM</term>
					<term>Bi-directional</term>
					<term>RNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While RNN's are widely applied to many similar tasks, such as automatic speech recognition and transcription, nobody succeeded yet using them for classification in the BirdCLEF challenge. Recent work at ZHAW on the topic of speaker clustering has yielded very promising results using a Bidirectional LSTM. We have set out to apply the LSTM-Network mentioned above to the BirdCLEF challenge. By doing so, we hope to make a valuable contribution to the ongoing research in this field. Unfortunately, our approach did not perform as well as we hoped for. In the following, we detail the approach taken and some issues a bachelor student with little experience in the field of machine learning will encounter when participating in such a challenging competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introdruction</head><p>Recognising birds by their song in the wild is a challenging task. One reason for this is that according to recent research by <ref type="bibr" coords="1,329.20,469.46,10.52,8.74" target="#b1">[2]</ref> there are approximately 18'043 different bird species. Since birds do not just vocalise in bird songs but also various types of calls such as alarm calls or contact calls <ref type="bibr" coords="1,380.34,493.37,9.96,8.74" target="#b0">[1]</ref>, this leads to a huge classification problem.</p><p>The Cross-Language Evaluation Forum (CLEF) hosts an annual challenge for bird classification. This BirdCLEF challenge <ref type="bibr" coords="1,350.11,529.34,10.52,8.74" target="#b4">[5]</ref> allows researchers from all over the world to test their approaches against state of the art for bird voice recognition. The organisers have built a large dataset based on contributions of Xeno Canto 1 , a network for sharing bird songs from all over the world.</p><p>The Datalab at Zurich University of Applied Sciences had great success over the last years in applying recurrent neural networks, specifically LSTMs <ref type="bibr" coords="1,454.89,589.21,9.96,8.74" target="#b3">[4]</ref>, to speaker recognition tasks, such as speaker clustering.</p><p>Since the BirdCLEF challenge provides an exciting opportunity to test those approaches on a big dataset, we have set out to show that an LSTM can perform as well on the task as a CNN based approach. We participated in the challenge in the context of our bachelor's thesis. As such we did not have much experience in machine learning, let alone signal processing. For that reason, we decided to try and bring established preprocessing methods, and the LSTM mentioned earlier together. This way we would not need to build up the full knowledge required when building a different approach from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>When the baseline <ref type="bibr" coords="2,219.92,256.92,10.52,8.74" target="#b6">[7]</ref>  <ref type="bibr" coords="2,234.17,256.92,10.52,8.74" target="#b5">[6]</ref> was released, we had already started to implement a pipeline in Python 3.5, matching the software versions used in <ref type="bibr" coords="2,399.80,268.88,14.61,8.74" target="#b9">[10]</ref>. We integrated and subsequently used the preprocessing and augmentation steps found in the baseline <ref type="bibr" coords="2,172.45,292.79,10.52,8.74" target="#b6">[7]</ref>  <ref type="bibr" coords="2,186.30,292.79,9.96,8.74" target="#b5">[6]</ref>.</p><p>Differences to the baseline As a signal to noise threshold value, we used 1e-3, as that value was mentioned in the readme in <ref type="bibr" coords="2,347.88,340.09,9.96,8.74" target="#b5">[6]</ref>. We did not experimentally validate that threshold.</p><p>The LSTM Model was originally designed to work on spectrograms of 0.5 second long segments and a resolution of 50 pixels wide by 128 pixels wide. With the intentions to keep the input similar, we changed the corresponding settings compared to the baseline.</p><p>This results in an identical nfft-window length of 1024 but increases the hoplength from 173 to 450. Effectively the baseline system used windows overlapped by 83.2%, while our variant shrinks this overlap to 56.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model</head><p>The model we have used for our participation in BirdCLEF was based on <ref type="bibr" coords="2,462.33,498.65,14.61,8.74" target="#b9">[10]</ref>. As mentioned above, we use inputs of 50 pixels width for segments that are 5 seconds long. Other than in the paper above, we use more hidden units in each bidirectional LSTM layer. This change was derived during one of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Datasets Before we started training, 5% of the provided training set were put aside in a validation set. We used stratified sampling to ensure that at least one recording for each class was present in the validation split.</p><p>We then went on to generate spectrograms out of these recordings, as described in 2.1. Caches Since the dataset was stored on network storage, we have encountered a bottleneck when loading the samples. To enable for more sequential reads, we first shuffled the generated dataset and then created caches containing 12800 samples and labels each. For this we have made use of pythons pickle module.</p><p>During training, a producer-consumer pattern was applied, with multiple threads loading samples from the caches and augmenting them for training. Every time a cache was loaded the contained sample, and label pairs were shuffled to safeguard against the constellation of samples in a batch repeating each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation</head><p>For augmentation, we have settled on adding noise samples to the training samples. As in <ref type="bibr" coords="3,256.51,555.25,10.52,8.74" target="#b6">[7]</ref> those are byproducts of the preprocessing steps. We have also tested adding some gaussian noise and applying vertical roll. Those augmentation steps did not improve the generalisation of our model, so we have dismissed them. Based on the baseline we did augmentation on the fly with a probability of 0.5.</p><p>Training Duration and Batchsize While experimenting, we could not make out a difference in the performance of our model between training with batch sizes of 64, 128 or 256 samples per batch. The time it took to train however was significantly lower using a higher batch size. For this reason, we have usually used a batch size of 256 samples.</p><p>Time permitting, we trained for at most 100 epochs without applying early stopping. Throughout this paper, we will refer to the time or number of batches it takes until the model was trained on each sample once as one epoch. After each epoch, we saved a snapshot of the trained model. Training one epoch took anywhere from 30 minutes to a couple of hours.</p><p>During training Adam <ref type="bibr" coords="4,248.44,203.04,10.52,8.74" target="#b7">[8]</ref> was used as optimiser with a learning rate of 1e-4.</p><p>We did evaluate the snapshots for each epoch on the provided soundscape validation set. Time permitting, we also calculated an MRR score on our 5% validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To find the settings used for our runs, we have conducted multiple experiments. Out of these, we describe the two most relevant for our submission.</p><p>Hidden Units Since the model was originally used on TIMIT <ref type="bibr" coords="4,415.28,339.44,9.96,8.74" target="#b2">[3]</ref>, a relatively small dataset of studio recordings, its capacity was lacking for the task at hand.</p><p>Our experiments showed that increasing the number of hidden units from 256 to 512 and 768 for each bidirectional LSTM layer lead to a significantly better MRR score. On the soundscape validation set, only a minor increase in performance was seen. While more hidden units lead to a better score, the performance gains showed a diminishing trend between higher numbers. At the same time, the training time increased. For this reason, we did not push past 768 hidden units. Augmentation As <ref type="bibr" coords="4,228.40,620.25,10.51,8.74" target="#b6">[7]</ref> shows, not every form of augmentation is necessarily good for a model. For this reason, we have conducted an experiment to show how augmentation through noise samples, adding of Gaussian noise and applying vertical roll influences the models performance. To do so, we have trained a model using all of these methods and one using none of them as baselines. For each of the augmentation methods, we trained one model where that method was not used, while the other two were. Due to time limitations, we were not able to test all permutations of these augmentation methods. Since the models that were trained either without adding Gaussian noise or without applying vertical roll performed better than the baseline using all augmentation methods, we decided on not using those for our submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>We have submitted four runs, one of which was an ensemble of two other runs. For both the soundscape task and the monophone task, we reported the 100 classes that were predicted with the highest confidence. For MRR the wrong predictions after the correct class do not matter for the score. For the soundscape task, we expected that there would be a sweet spot reporting fewer classes for each segment. However, experiments on the soundscape validation set showed that the score increased with the number of classes reported.</p><p>In the following, we present the settings used to train the models for our submissions. The number of hidden units refers to the hidden units in both bidirectional LSTM layers, while dropout is the setting we used for the first dropout layer. The second dropout layer was set to use half as much dropout as the first. The augmentation probability is the probability determining if any augmentation would be done. The Gaussian noise parameter gives the maximum intensity of Gaussian noise that will be added to the sample, and lastly, we controlled how much downsampling was done, by setting an upper limit on the number of samples per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Run 1</head><p>The model used for Run 1 was trained for 94 epochs on an old dataset which initially had a faulty separation of the training and validation splits, letting it train on audio files that were also used in validation. For the submission, the model was trained for an additional 34 epochs on a new and corrected dataset which did not have that issue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Run 2</head><p>Run 2 was trained for 34 epochs on a dataset, which was downsampled to 1000 samples per class instead of the 1500 used in the experiments and other runs. This run was primarily intended as a test run to evaluate if a new update to our pipeline would work. Due to its performance and the lack of better models, we submitted this run as our second run. Some settings for this run could be proven in our experiments to be beneficial (hidden units, learning rate), the influence of others was not experimentally determined (increased augmentation probability). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Run 3</head><p>This run was an ensemble of both run 1 and run 2. The ensemble was generated by using a python script and averages the values reported in run 1 and 2. We did not use a weighted average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Run 4</head><p>For run 4 the newly generated dataset was used. The selection of the parameters was based on the results of the experiments described earlier in this thesis. Since the results of the first three submissions were lower than expected, we reworked the process used for making predictions. As we used all spectrograms for a given monophone sample or soundscape segment, it is likely that some of them did not contain a usable signal. For these spectrograms, the results would be close to random. Once the predictions for the segment or monophone sample are pooled, these random predictions would inevitably pull down correct predictions.</p><p>As we have used a signal to noise threshold of 0.001 to decide if we train on any given spectrogram, we tested using the same threshold to filter out unsuitable spectrograms during predictions.</p><p>Briefly validating this hypothesis on the soundscape validation set, confirmed that the scores could be improved this way. Unfortunately, we only gained an additional score of 0.02 on the validation set on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results we achieved on both subtasks were underwhelming. Compared to other submissions, our approach consistently performed the worst. For the monophone task, we can see, that the increased capacity of the network and the changes made for run 4 did have a positive effect on the results. On the soundscapes that does not seem to be the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Challenges we faced</head><p>As mentioned before, we participated in BirdCLEF in the context of our bachelors' thesis. It was the first scientific challenge we took part in. Being bachelor students, we were fairly inexperienced in the field of artificial intelligence. In the previous semester, we have attended lectures on the subject, but we had little practical experience but were interested in gathering some experience by doing a bigger project. The BirdCLEF challenge was an ideal candidate for this, as recognising bird calls is an interesting topic with room for improvement. While we were aware at the beginning that it is a challenging task, we overestimated our abilities and ultimately did not achieve the results we strived for. Handling such a large data set brought additional unexpected difficulties. In the following section, we would like to briefly describe the experiences of participating in the challenge and pick out some lessons we have learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Deadline, pressure and code quality</head><p>While we had experienced stressful times and worked towards deadlines both during our studies as well as our careers, we were surprised by the amount of pressure we felt close to the deadline of this challenge. This pressure was partly because we had high expectations for ourselves and partly because at times we hit so many problems that it looked like we would not even be able to submit any runs to the challenge.</p><p>We realised that the closer the deadline came, the more mistakes we began to make. Since the turnaround time for preprocessing a dataset was long, even small mistakes turned out to be costly. To get back on track and get our code quality back to a barely acceptable level, we started to either do code reviews or make use of pair programming. Even though it sometimes seemed to take a lot of time to do so, it was ultimately worth it, since even small mistakes did cost more time as mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Time management</head><p>Initially, we planned to use most of our time to experiment with different approaches. In reality, an unexpectedly large part of our time went into the debugging of numerous steps from the generation of spectrograms to the output of training and validation values through the network.</p><p>We were a bit naive and assumed that we could debug the pipeline by training a model and analysing what did not work as expected. Unfortunately, we had made some mistakes that led to a bad performance of the model, which we could not debug this way.</p><p>An example of such an error was what we dubbed the "reshape bug". Initially, we used a tried and true CNN to verify the pipeline was working. Once satisfied it worked correctly, we replaced the CNN with the LSTM mentioned before. Compared to the CNN, the LSTM expected the input axes to be swapped. Instead of the axes, we mistakenly used numpy.reshape, an operation that rearranges the input to fit the desired shape. The resulting spectrograms did not resemble the initial data at all. With the new input, the network still learned some features but was useless overall. In 3 you can see the original image and the output after the reshape operation.</p><p>Ultimately we used a small set to debug our whole pipeline to find this error. In retrospect, it would have been beneficial to carry out such a test immediately after the pipeline's completion and after introducing the input transformation for the LSTM. Because the training a model and preprocessing the dataset takes a very long time, we lost much time by trying to find the error by training the model on a complete dataset. Even worse, we conducted some experiments while the pipeline was still broken. These experiments were rendered completely unusable, which we only realised after they have been carried out. Repeating the experiments cost time again. Doing a thorough test of the pipeline is particularly important with such large data sets and turnaround times of several days for a complete training run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Machine learning basics</head><p>Validation This is a rather basic lesson that we have learnt: it is important to have meaningful measurements for validation. Since a validation set for the soundscape task was provided, we were a bit lazy in fixing the issues outlined below. In hindsight, we should have prioritised fixing those higher.</p><p>At first, our split between validation and test samples did divide all generated spectrograms without taking into account that spectrograms from the same recording would end up in both splits. Needless to say that our models performed very well on the validation samples.</p><p>We have later fixed that by first splitting the recordings and then generating the spectrograms. However, for the validation split, we did keep all spectrograms, regardless if they contained only noise or indeed a bird. Since no model can predict a bird only from background noise, unless there was a lot of overfitting on a very similar training sample, this resulted in a situation where we would get quite a low score (approx. 22% top 1 accuracy) on the validation samples. As we did not know how many of the generated spectrograms even contained a usable signal, this was not very meaningful.</p><p>The best way to avoid these problems is to use validation methods that closely mirror the measurements used for the submissions. Unfortunately, we did not have enough time to implement such a validation function in keras for use during training. Instead, we have loaded the snapshots of the model and used a script to make predictions on the validation split and subsequently calculate an MRR score.</p><p>To validate the models performance for the soundscape task, we have heavily relied on the provided soundscape validation set and the official script to calculate c-MAP scores. As it turned out, this validation set did was not representative of the soundscape test set. Which lead to a marked difference between the score on this set and the scores achieved in the competition.</p><p>All in all, we have learned to use measurements that are indicative of the performance in the real task and to ensure that the validation set is representative of the test set used in the challenge.</p><p>Experiment When conducting experiments, it is important that they are repeatable and well documented. Again this is nothing new, and we were told that this is important early on.</p><p>We have made sure that we can have a configuration or a script which sets up the experiment for each one. That ensures repeatability up to a point. Namely, we have realised over the course of this challenge, that we have made use of Pythons keyword arguments with default values in various places. This, in turn, led us to omit some "default" parameters in the scripts. Later on, we changed some default values in the method signatures. Hence, the previous configurations were rendered obsolete, and it became hard to be sure we have used the correct settings. For that reason, we had to repeat some experiments.</p><p>While using configuration files or scripts to get reproducible experiments is great when the above pitfall is avoided, we have learned that it is best also to document the reasoning behind the experiment somewhere. Additionally, a consistent naming scheme for artefacts such as snapshots of the trained model, tensorboards, graphs or other analysis on the data can help to keep some order in the experiments. Effectively all time spent on documenting ongoing experiments will save time when writing up the results in a paper or thesis.</p><p>For long-running experiments, it is also advisable to form a robust hypothesis and do some research to validate it, before running an experiment. As such it will ensure, that the precious time it takes to run the experiments will be well spent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Handling a big dataset</head><p>The training set for 2018 contained 113 gigabytes worth of audio recordings. The resulting training sets contained over 1200000 spectrograms, which were saved in the PNG format. Additionally, some spectrograms were saved to be used for augmentation. Converting all recordings into a ready-to-train dataset took upwards of 12 hours.</p><p>Since the turnaround time for a dataset is so big, it is well worth creating a smaller dataset to test the generation process. Since it is not necessary to do any training on that dataset, this can be done by merely choosing recordings from a subset of the 1500 species. Ideally, the quality of the recordings should range from noisy to very clean recordings.</p><p>Such a dataset can be used to quickly see the effect of different settings for the transformations, gain a better understanding of the influence of different signal detection methods or perform sanity checks as described below. Due care needs to be taken to ensure no hyper-parameters are tuned on such a set, as it is not representative of the whole dataset.</p><p>7 Future Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Using longer segments for training:</head><p>One experiment showed that using spectrograms which represented only 0.2 seconds of audio performed significantly worse than using the 0.5 seconds used for our submissions. Additionally, participants in the past seem to be using longer segments (e.g. lengths of 3 seconds were used by <ref type="bibr" coords="11,349.05,526.95,10.30,8.74" target="#b8">[9]</ref>).</p><p>Based on this information, the hypothesis that longer samples improve the performance of the model can be formed and should be experimentally tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Preprocessing</head><p>For this thesis, the preprocessing provided by the baseline system <ref type="bibr" coords="11,425.24,608.30,10.52,8.74" target="#b6">[7]</ref> was used. These steps were originally all used in combination with a CNN and might not be suitable for the use with an RNN. It could be beneficial to evaluate if there are some RNN specific preprocessing steps to help the network better understand the data provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Use of variable input lengths for the BLSTM</head><p>At the moment, the model is limited to a fixed input length. In principle, an LSTM supports variable input lengths. Using variable input lengths could allow for more complex and entirely different preprocessing methods. On the other hand, the whole training sample could be fed to the network, hoping that it will learn to discern noise from signal by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Due to multiple technical issues and the limited time frame, we were not able to show that an LSTM can perform as well as a state of the art CNN in the BirdCLEF challenge.</p><p>While the results show that the network did indeed learn something usable, there certainly is room for improvement. We assume that there is a significant issue either within preprocessing or the used hyper-parameters. Unfortunately, we were not able to determine what the issue is. This is partly due to not being able to run extensive and systematic enough experiments due to time lost to technical issues. For this reason, we cannot conclusively say whether an LSTM based approach is or is not unable to perform as well as a CNN on this task.</p><p>However, the application of an LSTM to the complex task available in the form of BirdCLEF warrants further research, as LSTMs are applied highly successful in similar domains.</p><p>While we did not achieve the results we had hoped for, participating in Bird-CLEF was interesting and a tremendous opportunity to gain practical experience with artificial intelligence. We certainly learned a lot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,137.50,655.03,3.65,5.24;1,144.73,656.80,113.99,7.86"><head>1</head><label></label><figDesc>https://www.xeno-canto.org</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,228.64,397.01,158.08,7.89;3,198.55,115.84,218.25,266.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Architecture used based on<ref type="bibr" coords="3,372.38,397.03,14.34,7.86" target="#b9">[10]</ref> </figDesc><graphic coords="3,198.55,115.84,218.25,266.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,424.35,345.82,7.89;5,134.77,435.33,345.82,7.86;5,134.77,446.29,345.82,7.86;5,134.77,457.25,55.09,7.86;5,134.77,184.58,350.00,225.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance with different augmentation methods. The validation score is reporting lower than we would expect MRR performance to be, because we used top 1 accuracy on all 0.5-second segments for any recording. Including segments that only contain noise.</figDesc><graphic coords="5,134.77,184.58,350.00,225.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,374.32,345.83,7.89;9,134.77,385.30,215.21,7.86;9,206.39,209.38,50.00,128.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Figure showing the "reshape bug", where mistakenly a wrong transformation was done, leading to garbeled inputs for the network.</figDesc><graphic coords="9,206.39,209.38,50.00,128.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,465.68,345.83,94.81"><head>Table 1 .</head><label>1</label><figDesc>MRR scores for different numbers of hidden units on the local 5% validation split.</figDesc><table coords="4,245.20,508.40,124.97,52.10"><row><cell cols="2">Nr. of hidden units max MRR</cell></row><row><cell>256</cell><cell>0.384057</cell></row><row><cell>512</cell><cell>0.437551</cell></row><row><cell>768</cell><cell>0.488797</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,240.56,319.75,134.24,72.50"><head>Table 2 .</head><label>2</label><figDesc>Configuration for run 1</figDesc><table coords="6,244.84,340.55,125.69,51.70"><row><cell>Hidden units</cell><cell>256</cell></row><row><cell>Dropout</cell><cell>0.5</cell></row><row><cell cols="2">Augmentation probability 0.5</cell></row><row><cell>Gaussian noise</cell><cell>0</cell></row><row><cell>Samples per class</cell><cell>1500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,240.56,565.65,134.24,72.50"><head>Table 3 .</head><label>3</label><figDesc>Configuration for run 2</figDesc><table coords="6,244.84,586.45,125.69,51.70"><row><cell>Hidden units</cell><cell>512</cell></row><row><cell>Dropout</cell><cell>0</cell></row><row><cell cols="2">Augmentation probability 0.6</cell></row><row><cell>Gaussian noise</cell><cell>0.05</cell></row><row><cell>Samples per class</cell><cell>1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,240.56,249.47,134.24,72.50"><head>Table 4 .</head><label>4</label><figDesc>Configuration for run 4</figDesc><table coords="7,244.84,270.27,125.69,51.70"><row><cell>Hidden units</cell><cell>768</cell></row><row><cell>Dropout</cell><cell>0.5</cell></row><row><cell cols="2">Augmentation probability 0.5</cell></row><row><cell>Gaussian noise</cell><cell>0</cell></row><row><cell>Samples per class</cell><cell>1500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,116.41,345.83,94.91"><head>Table 5 .</head><label>5</label><figDesc>Results achieved on the monophone task (MRR) and the soundscape task (c-MAP)</figDesc><table coords="8,153.78,148.27,306.40,63.06"><row><cell>Run</cell><cell>MRR only main species</cell><cell>MRR with backgound species</cell><cell>c-mAP</cell><cell>c-mAP Peru</cell><cell>c-mAP Colombia</cell></row><row><cell>1</cell><cell>0.2014</cell><cell>0.1909</cell><cell cols="2">0.0195 0.0079</cell><cell>0.0244</cell></row><row><cell>2</cell><cell>0.2439</cell><cell>0.2285</cell><cell cols="2">0.0319 0.0123</cell><cell>0.037</cell></row><row><cell>3</cell><cell>0.2441</cell><cell>0.2295</cell><cell cols="3">0.0281 0.0126 0.0339</cell></row><row><cell>4</cell><cell>0.2642</cell><cell>0.2466</cell><cell cols="2">0.0291 0.009</cell><cell>0.037</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">Acknowledgements</head><p>We would like to thank all the people that supported us through this journey. Namely, we want to thank <rs type="person">Thilo Stadelmann</rs>, <rs type="person">Martin Braschler</rs> and <rs type="person">Mohammadreza Amirian</rs> from the <rs type="affiliation">ZHAW Datalab</rs> for their great support and supervision of our project. Additionally, we would like to thank <rs type="person">Niclas Simmler</rs> and <rs type="person">Amin Trabi</rs> for discussions and inputs. Also, we want to thank <rs type="person">Stefan Kahl</rs> for answering questions regarding their 2017 submission. Finally, we want to thank the organisers for organising such a great challenge and providing us with all the necessary information for the task.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,602.50,337.63,7.86;12,151.52,613.46,139.05,7.86" xml:id="b0">
	<monogr>
		<ptr target="https://www.britishbirdlovers.co.uk/identifying-birds/types-of-bird-songs-and-calls" />
		<title level="m" coord="12,151.53,602.50,127.52,7.86">Types Of Bird Songs And Calls</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,623.92,337.64,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.81,329.07,7.89;12,151.52,656.80,197.21,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,430.72,623.92,49.87,7.86;12,151.52,634.88,266.07,7.86">How Many Kinds of Birds Are There and Why Does It Matter?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Barrowclough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cracraft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Klicka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Zink</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0166307</idno>
		<ptr target="http://dx.plos.org/10.1371/journal.pone.0166307" />
	</analytic>
	<monogr>
		<title level="j" coord="12,426.76,634.88,53.83,7.86">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">166307</biblScope>
			<date type="published" when="2016-11">Nov 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,151.68,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,360.00,119.67,120.59,7.86;13,151.52,130.63,178.65,7.86">The darpa speech recognition research database: Specifications and status</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Goudie-Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,364.50,130.63,116.09,7.86;13,151.52,141.59,76.83,7.86">Proc. DARPA Workshop on Speech Recognition</title>
		<meeting>DARPA Workshop on Speech Recognition</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,152.55,337.63,7.86;13,151.52,163.48,329.07,7.89;13,151.52,174.47,182.80,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,300.82,152.55,111.20,7.86">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j" coord="13,424.35,152.55,56.23,7.86;13,151.52,163.51,12.29,7.86">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,185.43,337.64,7.86;13,151.52,196.39,329.07,7.86;13,151.52,207.34,329.07,7.86;13,151.52,218.30,47.10,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,223.02,196.39,257.57,7.86;13,151.52,207.34,225.26,7.86">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,395.71,207.34,84.88,7.86;13,151.52,218.30,18.43,7.86">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,229.26,337.63,7.86;13,151.52,240.22,329.07,7.86;13,151.52,251.18,168.81,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,173.26,240.22,257.31,7.86">GIT] Source code of the TUCMI submission to BirdCLEF2017</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<ptr target="https://github.com/kahst/BirdCLEF2017" />
		<imprint>
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,262.14,337.64,7.86;13,151.52,273.10,329.07,7.86;13,151.52,284.06,236.90,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177[cs</idno>
		<idno>arXiv: 1804.07177</idno>
		<ptr target="http://arxiv.org/abs/1804.07177" />
		<title level="m" coord="13,407.96,262.14,72.64,7.86;13,151.52,273.10,211.53,7.86">Recognizing Birds from Sound -The 2018 BirdCLEF Baseline System</title>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,295.02,337.64,7.86;13,151.52,305.98,329.07,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,264.32,295.02,211.98,7.86">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs</idno>
		<idno>arXiv: 1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,316.93,337.63,7.86;13,151.52,327.89,329.07,7.86;13,151.52,338.85,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,349.84,316.93,130.75,7.86;13,151.52,327.89,160.49,7.86">Audio Based Bird Species Identification using Deep Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,332.93,327.89,92.83,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="547" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,349.81,337.97,7.86;13,151.52,360.77,329.07,7.86;13,151.52,371.73,329.07,7.86;13,151.52,382.69,76.79,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,374.03,349.81,106.56,7.86;13,151.52,360.77,259.48,7.86">Capturing Suprasegmental Features of a Voice with RNNs for Improved Speaker Clustering</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Stadelmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Glinski-Haefeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Drr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,360.77,48.07,7.86;13,151.52,371.73,329.07,7.86;13,151.52,382.69,76.79,7.86">Proceedings of the 8th IAPR TC 3 Workshop on Artificial Neural Networks for Pattern Recognition (ANNPR18)</title>
		<meeting>the 8th IAPR TC 3 Workshop on Artificial Neural Networks for Pattern Recognition (ANNPR18)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
