<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.23,115.96,322.89,12.62;1,202.11,133.89,211.14,12.62">Overview of BirdCLEF 2018: monospecies vs. soundscape bird identification</title>
				<funder ref="#_RTYEqe5">
					<orgName type="full">EADM GDR CNRS MADICS</orgName>
				</funder>
				<funder>
					<orgName type="full">Xeno-canto Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Floris&apos;Tic</orgName>
				</funder>
				<funder ref="#_gtYge34">
					<orgName type="full">French CNRS</orgName>
				</funder>
				<funder>
					<orgName type="full">BRILAAM STIC-AmSud</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.43,171.56,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.54,171.56,51.75,8.74"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Chemnitz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.85,171.56,56.56,8.74"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>herve.glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Toulon</orgName>
								<orgName type="institution" key="instit2">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIS</orgName>
								<orgName type="institution" key="instit5">DYNI team</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.96,171.56,68.94,8.74"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.45,171.56,52.47,8.74;1,249.01,183.51,34.87,8.74"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.81,183.51,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM ZENITH team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.23,115.96,322.89,12.62;1,202.11,133.89,211.14,12.62">Overview of BirdCLEF 2018: monospecies vs. soundscape bird identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0DC97F90E270D1FB9CB134FCD17286A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>ecological monitoring 6 Scaled Acoustic Biodiversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The BirdCLEF challenge offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings of their sounds. One of its strengths is that it uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that BirdCLEF is close to the conditions of realworld application, in particular with regard to the number of species in the training set (1500). Two main scenarios are evaluated: (i) the identification of a particular bird species in a recording, and (ii), the recognition of all species vocalising in a long sequence (up to one hour) of raw soundscapes that can contain tens of birds singing more or less simultaneously. This paper reports an overview of the systems developed by the six participating research groups, the methodology of the evaluation of their performance, and an analysis and discussion of the results obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. The general public, especially so-called "birders" as well as professionals such as park rangers, ecological consultants and of course ornithologists are potential users of an automated bird sound identifying system, typically in the context of wider initiatives related to ecological surveillance or biodiversity conservation. The BirdCLEF challenge evaluates the state-of-the-art of audio-based bird identification systems at a very large scale. Before BirdCLEF started in 2014, three previous initiatives on the evaluation of acoustic bird species identification took place, including two from the SABIOD 6 group <ref type="bibr" coords="2,163.34,118.99,10.79,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,174.13,118.99,7.20,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,181.32,118.99,7.20,8.74" target="#b0">1]</ref>. In collaboration with the organizers of these previous challenges, the <ref type="bibr" coords="2,152.55,130.95,68.06,8.74">BirdCLEF 2014</ref><ref type="bibr" coords="2,229.09,130.95,18.15,8.74">BirdCLEF , 2015</ref><ref type="bibr" coords="2,255.73,130.95,19.93,8.74">BirdCLEF , 2016</ref> and 2017 challenges went one step further by (i) significantly increasing the species number by an order of magnitude, (ii) working on real-world social data built from thousands of recordists, and (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of metadata and defining information retrieval oriented metrics. Overall, these tasks were much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (different recording devices, contexts diversity, etc.). The main novelty of the 2017 edition of the challenge with respect to the previous years was the inclusion of soundscape recordings containing time-coded bird species annotations. Usually xeno-canto recordings focus on a single foreground species and result from using mono-directional recording devices. Soundscapes, on the other hand, are generally based on omnidirectional recording devices that monitor a specific environment continuously over a long period. This new kind of recording reflects (possibly crowdsourced) passive acoustic monitoring scenarios that could soon augment the number of collected sound recordings by several orders of magnitude. For the 2018-th edition of the BirdCLEF challenge, we continued evaluating both scenarios as two different tasks: (i) the identification of a particular bird specimen in a recording of it, and (ii), the recognition of all specimens singing in a long sequence (up to one hour) of raw soundscapes that can contain tens of birds singing simultaneously. In this paper, we report the methodology of the conducted evaluation as well as an analysis and a discussion of the results achieved by the six participating groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task1: monospecies (monophone) recordings</head><p>The goal of the task is to identify the species of the most audible bird (i.e. the one that was intended to be recorded) in each of the provided test recordings. Therefore, the evaluated systems have to return a ranked list of possible species for each of the 12,347 test recordings. Each prediction item (i.e. each line of the file to be submitted) has to respect the following format: &lt;MediaId;ClassId;Probability;Rank&gt; Each participating group was allowed to submit up to 4 run files providing the predictions made from 4 different methods. The use of any of the provided metadata complementary to the audio content was authorized. It was also allowed to use any external training data but at the condition that (i) the experiment is entirely re-producible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) participants submit at least one run without external training data so that we can study the contribution of such resources, (iii) the additional resource does not contain any of the test observations. It was in particular strictly forbidden to crawl training data from: www.xeno-canto.org.</p><p>The dataset was the same as the one used for BirdCLEF 2017 <ref type="bibr" coords="3,432.93,178.77,9.96,8.74" target="#b3">[4]</ref>, mostly based on the contributions of the Xeno-Canto network. The training set contains 36,496 recordings covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). It has a massive class imbalance with a minimum of four recordings for Laniocera rufescens and a maximum of 160 recordings for Henicorhina leucophrys. Recordings are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date, the location, textual comments of the authors, multilingual common names and collaborative quality ratings. The test set contains 12,347 recordings of the same type (mono-phone recordings). More details about that data can be found in the overview working note of BirdCLEF 2017 <ref type="bibr" coords="3,319.43,298.32,9.96,8.74" target="#b3">[4]</ref>.</p><p>The used evaluation metric is the Mean Reciprocal Rank (MRR). The MRR is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:</p><formula xml:id="formula_0" coords="3,257.81,390.81,98.05,30.79">M RR = 1 |Q| Q i=1 1 rank i</formula><p>where |Q| is the total number of query occurrences in the test set.</p><p>Mean Average Precision was used as a secondary metric to take into account the background species, considering each audio file of the test set as a query and computed as:</p><formula xml:id="formula_1" coords="3,254.31,484.93,106.74,26.77">mAP = |Q| q=1 AveP (q) Q ,</formula><p>where AveP (q) for a given test file q is computed as</p><formula xml:id="formula_2" coords="3,213.72,532.54,187.91,25.41">AveP (q) = n k=1 (P (k) × rel(k)) number of relevant documents .</formula><p>Here k is the rank in the sequence of returned species, n is the total number of returned species, P (k) is the precision at cut-off k in the list and rel(k) is an indicator function equaling 1 if the item at rank k is a relevant species (i.e. one of the species in the ground truth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task2: soundscape recordings</head><p>The goal of the task was to localize and identify all audible birds within the provided soundscape recordings. Therefore, each soundscape was divided into segments of 5 seconds, and a list of species accomapnied by probability scores had to be returned for each segment. Each prediction item (i.e. each line of the run file) had to respect the following format:</p><p>&lt;MediaId;TC1-TC2;ClassId;Probability&gt; where probability is a real value in [0;1] decreasing with the confidence in the prediction, and where TC1-TC2 is a timecode interval with the format of hh:mm:ss with a length of 5 seconds (e.g.: 00:00:00-00:00:05, then 00:00:05-00:00:10).</p><p>Each participating group was allowed to submit up to 4 run files built from different methods. As for the monophone task, participants were allowed to use the provided metadata and to use external training data at the condition that the experiment is entirely re-producible and not biased.</p><p>The training set provided for this task was the same as that for the monophone task, i.e. 36,496 monophone recordings coming from Xeno-canto and covering 1500 species of Central and South America. Complementary to that data, a validation set of soundscapes with time-coded labels was provided as training data. It contained about 20 minutes of soundscapes representing 240 segments of 5 seconds and with a total of 385 bird species annotations. The test set used for the final blind evaluation contained about 6 hours of soundscapes split into 4382 segments of 5 seconds (to be processed as separate queries). Some of them were stereophonic, offering possibilities of source separation to enhance the recognition. More details about the soundscape data (locations, authors, etc.) can be found in the overview working note of BirdCLEF 2017 <ref type="bibr" coords="4,395.68,430.39,9.96,8.74" target="#b3">[4]</ref>. In a nutshell, 2 hours of soundscapes were recorded in Peru (with the support of Amazon Explorama Lodges within the BRILAAM STIC-AmSud 17-STIC-01 and SABIOD.org project) and 4,5 hours were recorded in Columbia by Paula Caycedo Rosales, ornithologist from the Biodiversa Foundation of Colombia and an active Xenocanto recordist.</p><p>In order to assist participants in the development of their system, a baseline code repository and a validation dataset were shared with the participants. The validation package contained 20 minutes of annotated soundscapes split into 5 recordings taken from last year's test dataset. The baseline repository<ref type="foot" coords="4,440.77,548.93,3.97,6.12" target="#foot_0">7</ref> was developed by Chemnitz University of Technology and offered tools and an example workflow covering all required topics such as spectrogram extraction, deep neural network training, audio classification on field recordings and local validation (more details can be found in <ref type="bibr" coords="4,266.58,598.32,10.30,8.74" target="#b8">[9]</ref>).</p><p>The metric used for the evaluation of the soundscape task was the classification mean Average Precision (cmAP ), considering each class c of the ground truth as a query. This means that for each class c, all predictions with ClassId = c are extracted from the run file and ranked by decreasing probability in order to compute the average precision for that class. Then, the mean across all classes is computed as the main evaluation metric. More formally:</p><formula xml:id="formula_3" coords="5,253.96,175.92,106.25,25.41">cmAP = C c=1 AveP (c) C</formula><p>where C is the number of classes (species) in the ground truth and AveP (c) is the average precision for a given species c computed as:</p><formula xml:id="formula_4" coords="5,236.79,243.63,141.78,26.33">AveP (c) = nc k=1 P (k) × rel(k) n rel (c) .</formula><p>where k is the rank of an item in the list of the predicted segments containing c, n c is the total number of predicted segments containing c, P (k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and n rel (c) is the total number of relevant segments for class c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants and methods</head><p>29 research groups registered for the BirdCLEF 2018 challenge and 6 of them finally submitted a total of 45 runs (23 runs for task1: monophone recordings and 22 runs for task2: soundscape recordings). Details of the methods used and systems evaluated are collected below (by alphabetical order) and further discussed in the working notes of the participants <ref type="bibr" coords="5,342.04,440.25,11.84,8.74" target="#b5">[6,</ref><ref type="bibr" coords="5,353.89,440.25,11.84,8.74" target="#b9">10,</ref><ref type="bibr" coords="5,365.73,440.25,11.84,8.74" target="#b11">12,</ref><ref type="bibr" coords="5,377.58,440.25,7.90,8.74" target="#b7">8,</ref><ref type="bibr" coords="5,385.47,440.25,11.84,8.74" target="#b10">11]</ref>:</p><p>Duke, China-USA, 8 runs <ref type="bibr" coords="5,272.43,464.13,11.46,8.77" target="#b5">[6]</ref>: This participant designed a bi-modal neural network aimed at learning a joint representation space for the audio and the metadata information (latitude, longitude, elevation and time). It relies on a relatively shallow architecture with 6 convolutional layers for the audio and a few full-connected layers aimed at learning features from the meta-data and combining them with the audio features into a single representation space. A softmax is then used for the classification output. Concerning the monophone subtask, DKU SMIIP run3 uses the bi-modal model whereas DKU SMIIP run 2 uses only the audio-based part. DKU SMIIP run 3 is a fusion of both runs. DKU SMIIP run 4 relies on a ResNet model as for comparison with the proposed model. DKU SMIIP run 5 is a combination of all models. Concerning the soundscape subtask, DKU SMIIP run1 uses the bi-modal model, DKU SMIIP run2 uses an ensemble of two bi-modal models (one with data augmentation and one without data augmentation). DKU SMIIP run3 is a fusion and run1 and run2. DKU SMIIP run4 is a fusion of all models including the ResNet. ISEN, France, 4 runs: This participant used the Soundception approach presented in <ref type="bibr" coords="5,176.69,656.12,15.50,8.74" target="#b12">[13]</ref> and which was the best performing system of the previous edition of BirdCLEF. It is based on an Inception-v4 architecture extended with a timefrequency attention mechanism.</p><p>MfN, Germany, 8 runs <ref type="bibr" coords="6,256.42,166.36,16.80,8.77" target="#b9">[10]</ref>: This participant trained an ensemble of convolutional neural networks based on the Inception-V3 architecture applied to mel-scale spectrograms as input. The trained models mainly differ in the preprocessing that was used to extract the spectrograms (with or without high-pass filter, sampling rate value, mono vs. stereo, FFT parameters, frequency scaling parameters, etc. Another particularity of this participant is that he uses intensive data augmentation both in the temporal and frequency domain. About ten different data augmentation techniques were implemented and evaluated separately through cross-validation ablation tests. Among them, the most contributing one is indisputably the addition of background noise or sounds from other files belonging to the same bird species with random intensity, in order to simulate artificially numerous context where a given species can be recorded. Other augmentations seem not to contribute as much taken individually, but one after one, point after point, they lead to significant improvements. Data augmentation most notably included a low-quality degradation based on MP3 encoding-decoding, jitter on duration (up to 0.5 sec), random factor to signal amplitude, random cyclic shift, random time interval dropouts, global and local pitch shift and frequency stretch, as well as color jitter (brightness, contrast, saturation, hue). MfN Run 1 for each subtask included the best single model learned during preliminary evaluations. These two models mainly differ in the pre-processing of audio files and choice of FFT parameters. MfN Run 2 combines both models, MfN Run 3 added a third declination of the model with other FFT parameters, but combined the predictions of the two best snapshots per model (regarding performance on the validation set) for averaging 3x2 predictions per species. MfN Run 4 added 4 more models and earlier snapshots of them, reaching a total combination of 18 predictions per species. No additional metadata was used except for the elimination of species based on the year of introduction in the BirdCLEF challenge.</p><p>OFAI, Austria, 7 runs <ref type="bibr" coords="6,251.63,512.63,16.80,8.77" target="#b11">[12]</ref>: This participant carefully designed a CNN architecture dedicated to birds sounds analysis in the continuity of its previous work described in <ref type="bibr" coords="6,215.01,536.57,10.51,8.74" target="#b4">[5]</ref> (the sparrow model). The main architecture is quite shallow with a first block of 6 convolutional layers aimed at extracting features from mel-spectrograms, a species prediction block aimed at computing local predictions every 9 frames, and a temporal pooling block aimed at combining the local predictions into a single classification for the whole audio excerpt. Several variants of this base architecture were then used to train a total of 17 models (with or without ResNet blocks instead of classical convolutional layers, different temporal pooling settings, with or without background species prediction). Complementary to audio-based models, this participant also studied the use of metadata-based models. In total, 24 MLPs were trained and based on four main variables: date, elevation, localization and time. The different MLPs mainly dif-fer in the used variables (all, only one, all except one, etc.) and various parameter settings.</p><p>TUC MI, Germany, 10 runs <ref type="bibr" coords="7,275.78,165.93,11.46,8.77" target="#b7">[8]</ref>: All runs by this participant were conducted thanks to the baseline BirdCLEF package provided by Chemnitz University <ref type="bibr" coords="7,467.31,177.91,9.96,8.74" target="#b8">[9]</ref>. They ensemble different learning and testing strategies as well as different model architectures. Classical deep learning techniques were used, covering audio-only and metadata assisted predictions. Three different model architectures were employed: First, a shallow, strictly sequential model with only a few layers. Secondly, a custom variation of the WideResNet architecture with multiple tens of layers and thirdly a very slim and shallow model which is suited for inference on low-power devices such as the Raspberry Pi. The inputs for all three models are 256 x 128 pixel mel-scale log-amplitude spectrograms with a frequency range from 500 Hz to 15 kHz. The dataset is pre-processed using a bird activity estimator based on median thresholds similar to previous attempts of this participant <ref type="bibr" coords="7,134.77,309.42,9.96,8.74" target="#b6">[7]</ref>. The most successful run for the monospecies task was an ensemble consisting of multiple trained nets covering different architectures and dataset splits.</p><p>The participant tried to estimate the species list for the soundscape task based on time of the year and location using the eBird database. Despite the success of this approach in last year's attempt, the pre-selection of species did not improve the results compared to a large ensemble. Finally, the participant tried to establish a baseline for real-time deployments of neural networks for long-term biodiversity monitoring using cost-efficient platforms. The participant proposes a promising approach to shrinking model size and reducing computational costs using model distillation. The results of those runs using the slim architecture are only a fraction behind the scores of large ensembles. All additional metadata and code are published online, complementing the baseline BirdCLEF package.</p><p>ZHAW, Switzerland, 8 runs <ref type="bibr" coords="7,283.61,464.81,16.80,8.77" target="#b10">[11]</ref>: In contrast to every other submission, the participants evaluated the use of recurrent neural networks (RNN). Using time-series as inputs for recurrent network topologies seems to be the most intuitive approach for bird sound classification. Yet, this method did not receive much attention in past years. Despite the limitations of time and computational resources, the experiments showed that bidirectional LSTMs are capable of classifying bird species based on two-dimensional inputs. Tuning RNNs to improve the overall performance seems to be challenging, although works from other sound domains showed promising results. The participants noted that not every design decision from other CNN implementations carry their benefit over to a RNN-based approach. Especially dataset augmentation methods like noise samples did not improve the results as expected. The results of the submitted runs suggest that an increased number of hidden LSTM units has significant impact on the overall performance. Additionally, data pre-processing and detection post-filtering impacts the prediction quality. Longer input segments and LSTMs with variable input length should be subject to future research.</p><p>The results achieved by all the evaluated system are displayed on Figure <ref type="figure" coords="8,459.94,142.05,4.98,8.74" target="#fig_1">1</ref> for the monospecies recordings and on Figure <ref type="figure" coords="8,320.42,154.00,4.98,8.74" target="#fig_2">2</ref> for the soundscape recordings. The main conclusion we can draw from that results are the following:</p><p>The overall performance improved significantly over last year for the mono-species recordings but not for the soundscapes: The best evaluated system achieves an impressive MRR score of 0.83 this year whereas the best system evaluated on the same dataset last year <ref type="bibr" coords="8,387.86,213.78,15.50,8.74" target="#b12">[13]</ref> achieved a MRR of 0.71. On the other side, we do not measured any strong progress on the soundscapes. The best system of MfN this year actually reaches a c-mAP of 0.193 whereas the best system of last year on the same test dataset <ref type="bibr" coords="8,425.48,249.64,15.50,8.74" target="#b12">[13]</ref> achieved a c-mAP of 0.182.</p><p>Inception-based architectures perform very well: As previous year, the best performing system of the challenge is based on an Inception architecture, in particular the Inception v3 model used by MfN. In their working note <ref type="bibr" coords="8,134.77,309.42,14.61,8.74" target="#b9">[10]</ref>, the authors report that they also tested (for a few training epochs) more recent or larger architectures that are superior in other image classification tasks (ResNet152, DualPathNet92, InceptionV4, DensNet, InceptionResNetV2, Xception, NasNet). But none of them could meet the performance of the InceptionV3 network with attention branch.</p><p>Intensive data augmentation provides strong improvement: All the runs of MfN (which performed the best within the challenge) made use of intensive data augmentation, both in the temporal and frequency domain (see section 3 for more details). According to the cross-validation experiments of the authors <ref type="bibr" coords="8,134.77,417.01,14.61,8.74" target="#b9">[10]</ref>, such intensive data augmentation allows the MRR score to be increased from 0.65 to 0.74 for a standalone Inception V3 model.</p><p>Shallow and compact architectures can compete with very deep architectures: Even if the best runs of MfN and ISEN are based on a very deep Inception model (Inception v3), it is noteworthy that shallow and compact architectures such as the ones carefully designed by OFAI can reach very competitive results, even with a minimal number of data augmentation techniques. In particular, OFAI Run 1 that is based on an ensemble of shallow networks performs better than the runs of ISEN, based on an Inception v4 architecture.</p><p>Using metadata provides observable improvements: Contrary to all previous editions of LifeCLEF, one participant succeeded this year in improving significantly the predictions of its system by using the metadata associated to each observation (date, elevation, localization and time). More precisely, OFAI Run 2 combining CNNs and metadata-based MLPs achieves a mono-species MRR of 0.75 whereas OFAI Run 1, relying solely on the CNNs, achieves a MRR of 0.72. According to the cross-validation experiments of this participant <ref type="bibr" coords="8,462.32,596.34,14.61,8.74" target="#b11">[12]</ref>, the most contributing information is the localization. The elevation is the second most informative variable but as it is highly correlated to the localization, it does not provide a strong additional improvement in the end. Date and then Time are the less informative but they do contribute to the global improvement of the MRR.</p><p>The brute-force assembling of networks provides significant improvements: as for many machine learning challenges (including previous Bird-CLEF editions), the best runs are achieved by the combination of several deep neural networks (e.g. 18 CNNs for MfN Run 4). The assembling strategy differs from a participant to another. MfN rather tried to assemble as much networks as possible. MfN Run 4 actually combines the predictions of all the networks that were trained by this participant (mainly based on different pre-processing and weights initialization), as well as snapshots of these models recorded earlier  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF bird identification challenge 2018. It confirmed the results of the previous edition that inception-based convolutional neural networks on mel spectrograms provide the best performance. Moreover, the use of large ensembles of such networks and of  intensive data augmentation provides significant additional improvements. The best system of this year achieved an impressive MRR score of 0.83 on the typical Xeno-Canto recordings. It could probabaly even be improved by a few points by combining it with a metadata-based prediction model, as shown by the second best participant to the challenge. This means that the technology is now mature enough for this scenario. Concerning the soundscapes recordings however, we did not observe any significant improvement over the performance of last year. Recognizing many overlapping birds remains a hard problem and none of the efforts made by the participants to tackle it provided observable improvement. In the future, we will continue investigating this scenario, in particular through the introduction of a new dataset of several hundred hours of annotated soundscapes that could be partially used as training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.77,214.64,345.83,8.74;9,134.77,226.59,345.83,8.74;9,134.77,238.55,345.83,8.74;9,134.77,250.50,345.82,8.74;9,134.77,262.46,345.83,8.74;9,134.77,274.41,345.83,8.74;9,134.77,286.37,345.83,8.74;9,134.77,298.32,11.62,8.74"><head></head><label></label><figDesc>during the training phase. The gain of the ensemble over a single model can be observed by comparing MfN Run 4 (M RR = 0.83) to MfN Run 1 (M RR = 0.78). The OFAI team rather tried to select and weight the best performing models according to their cross-validation experiments. Their best performing run (OFAI Run 3) is a weighted combination of 11 CNNs and 8 metadata-based MLPs. It allows reaching a score of M RR = 0.78 whereas the combination of the best single audio and metadata models achieves a score of M RR = 0.69 (OFAI Run 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,141.21,547.43,332.94,7.89;9,150.52,329.44,311.24,203.22"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. BirdCLEF 2018 monophone identification results -Mean Reciprocal Rank.</figDesc><graphic coords="9,150.52,329.44,311.24,203.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,134.77,350.21,345.82,7.89;10,134.77,361.19,39.11,7.86;10,150.52,132.77,311.25,202.67"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. BirdCLEF 2018 soundscape identification results -classification Mean Average Precision.</figDesc><graphic coords="10,150.52,132.77,311.25,202.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,134.77,627.14,345.83,7.89;10,134.77,638.12,132.83,7.86;10,150.52,409.15,311.24,203.22"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. BirdCLEF 2018 soundscape identification results detailed per country -classification Mean Average Precision.</figDesc><graphic coords="10,150.52,409.15,311.24,203.22" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="4,144.73,656.80,186.66,7.86"><p>https://github.com/kahst/BirdCLEF-Baseline</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements The organization of the BirdCLEF task is supported by the <rs type="funder">Xeno-canto Foundation</rs> as well as by the <rs type="funder">French CNRS</rs> project <rs type="projectName">SABIOD</rs>.<rs type="projectName">ORG</rs> and <rs type="funder">EADM GDR CNRS MADICS</rs>, <rs type="funder">BRILAAM STIC-AmSud</rs>, and <rs type="funder">Floris'Tic</rs>. The annotations of some soundscapes were prepared by the regretted wonderful <rs type="person">Lucio Pando</rs> of <rs type="person">Explorama Lodges</rs>, with the support of <rs type="person">Pam Bucur</rs>, <rs type="person">H. Glotin</rs> and <rs type="person">Marie Trone</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gtYge34">
					<orgName type="project" subtype="full">SABIOD</orgName>
				</org>
				<org type="funded-project" xml:id="_RTYEqe5">
					<orgName type="project" subtype="full">ORG</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,380.83,337.64,7.86;11,151.52,391.79,329.07,7.86;11,151.52,402.75,329.07,7.86;11,151.52,413.71,127.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,388.47,380.83,92.13,7.86;11,151.52,391.79,329.07,7.86;11,151.52,402.75,84.33,7.86">The 9th mlsp competition: New methods for acoustic classification of multiple simultaneous bird species in noisy environment</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eftaxias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,258.97,402.75,221.63,7.86;11,151.52,413.71,61.10,7.86">IEEE Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,424.95,337.64,7.86;11,151.52,435.91,329.07,7.86;11,151.52,446.87,329.07,8.11;11,151.52,458.48,122.39,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,441.76,424.95,38.83,7.86;11,151.52,435.91,99.54,7.86">Bioacoustic challenges in icml4b</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.org/ICML4B2013_proceedings.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,290.27,435.91,190.32,7.86;11,151.52,446.87,62.74,7.86">Proc. of 1st workshop on Machine Learning for Bioacoustics</title>
		<meeting>of 1st workshop on Machine Learning for Bioacoustics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,469.07,337.64,7.86;11,151.52,480.03,329.07,7.86;11,151.52,490.99,329.07,7.86;11,151.52,501.95,275.33,8.11" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,277.38,469.07,203.21,7.86;11,151.52,480.03,34.95,7.86">Overview of the 2nd challenge on acoustic bird classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<ptr target="http://sabiod.org/NIPS4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,207.81,480.03,272.78,7.86;11,151.52,490.99,34.46,7.86">Proc. Neural Information Processing Scaled for Bioacoustics. NIPS Int. Conf</title>
		<meeting>Neural Information essing Scaled for Bioacoustics. NIPS Int. Conf<address><addrLine>Halkias X., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,513.19,337.63,7.86;11,151.52,524.15,219.38,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,398.20,513.19,82.39,7.86;11,151.52,524.15,64.20,7.86">Lifeclef bird identification task 2017</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,237.07,524.15,83.66,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,535.40,337.64,7.86;11,151.52,546.36,329.07,7.86;11,151.52,557.31,113.78,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,246.87,535.40,233.73,7.86;11,151.52,546.36,50.55,7.86">Two convolutional neural networks for bird detection in audio signals</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,222.57,546.36,166.42,7.86">Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017 25. 2017</date>
			<biblScope unit="page" from="1764" to="1768" />
		</imprint>
	</monogr>
	<note>th European</note>
</biblStruct>

<biblStruct coords="11,142.96,568.56,337.64,7.86;11,151.52,579.52,329.07,7.86;11,151.52,590.48,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,241.50,568.56,239.09,7.86;11,151.52,579.52,25.76,7.86">Construction and improvements of bird songs&apos; classification system</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Haiwei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.20,579.52,279.39,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,601.72,337.63,7.86;11,151.52,612.68,329.07,7.86;11,151.52,623.64,74.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,168.54,612.68,293.34,7.86">Large-scale bird sound classification using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,623.64,46.21,7.86">CLEF 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.63,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,175.36,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,401.45,634.88,79.14,7.86;11,151.52,645.84,193.12,7.86">A baseline for largescale bird species identification in field recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,364.13,645.84,116.47,7.86;11,151.52,656.80,141.72,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,25.60,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<title level="m" coord="12,409.23,119.67,71.36,7.86;12,151.52,130.63,186.89,7.86">Recognizing birds from sound-the 2018 birdclef baseline system</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,152.55,337.98,7.86;12,151.52,163.51,329.07,7.86;12,151.52,174.47,25.60,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,202.79,152.55,277.80,7.86;12,151.52,163.51,33.97,7.86">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,207.73,163.51,267.89,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,185.43,337.97,7.86;12,151.52,196.39,298.18,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,242.28,185.43,220.44,7.86">Two bachelor students&apos; adventures in machine learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,196.39,269.51,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.98,7.86;12,151.52,218.30,298.18,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,202.19,207.34,260.82,7.86">Bird identification from timestamped, geotagged audio recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,218.30,264.54,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,229.26,337.97,7.86;12,151.52,240.22,329.07,7.86;12,151.52,251.18,329.07,8.12;12,151.52,262.79,61.20,7.47" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,246.74,229.26,233.85,7.86;12,151.52,240.22,187.40,7.86">Audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper_177.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,360.73,240.22,119.86,7.86;12,151.52,251.18,155.86,7.86">Working Notes of CLEF 2017 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
