<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.70,152.48,299.39,12.64;1,154.18,170.12,286.54,12.64">Machines vs. Human Experts: Contribution to the ExpertLifeCLEF 2018 Plant Identification Task</title>
				<funder>
					<orgName type="full">Museum fuer</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,209.22,58.66,9.07"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn.berlin</email>
							<affiliation key="aff0">
								<orgName type="institution">Museum fuer Naturkunde Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.70,152.48,299.39,12.64;1,154.18,170.12,286.54,12.64">Machines vs. Human Experts: Contribution to the ExpertLifeCLEF 2018 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E46B103907FEDA9569A09CC10B0BA96</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant Species Identification</term>
					<term>Biodiversity</term>
					<term>Deep Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Image Classification</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents deep learning techniques for image-based plant identification at very large scale. Deep Convolutional Neural Networks (DCNNs) for image classification are fine-tuned to identify 10,000 species. The proposed approach is evaluated in the ExpertLifeCLEF 2018 campaign. Results are compared with identifications by human experts and the best models of the LifeCLEF 2017 plant identification task. Performance comes close to the most advanced human expertise and surpasses previous state-of-the-art by 6 % on the official test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated plant identification is a promising tool supporting agriculture automation, biodiversity monitoring and education. It can help professionals and amateurs identifying plants in the field or searching large image collections of flora.</p><p>For training automated systems, the data sets of previous LifeCLEF campaigns are provided. They mainly consist of a 'trusted' set of ca. 260,000 images coming from several public databases or institutions dedicated to botany (e.g. Wikimedia, iNaturalist, Flickr, etc.) and a much larger 'noisy' data set of over 1.4 million images collected by searching the web.</p><p>In 2017, models and model ensembles based on DCNNs achieved impressive identification results with accuracies near 90 % on the PlantCLEF 2017 test set. This raises the question of how far such automated systems are from human expertise. For this to find out, a subset of the test set is given this year also to plant experts for identification. Results are compared with those of automated systems. Similar man vs. machine experiments were conducted previously in this domain and show significant performance increase over the last few years thanks to recent advances in deep learning <ref type="bibr" coords="1,124.63,629.53,10.54,9.07" target="#b3">[4]</ref>, <ref type="bibr" coords="1,138.69,629.53,10.54,9.07" target="#b4">[5]</ref>. They now come close to the most advanced human expertise. More information on data collection and task can be found in the ExpertLifeCLEF 2018 lab overview <ref type="bibr" coords="1,164.26,653.65,11.64,9.07" target="#b0">[1]</ref> and the task summary <ref type="bibr" coords="1,268.36,653.65,10.54,9.07" target="#b1">[2]</ref>.</p><p>This paper is building on previous solutions to the LifeCLEF plant identification task. More details regarding data preparation and augmentation methods can be found in the corresponding working note of 2017 <ref type="bibr" coords="2,287.66,173.91,10.64,9.07" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation Details and Model Training</head><p>New models are trained via PyTorch <ref type="bibr" coords="2,275.82,243.08,12.10,9.07" target="#b5">[6]</ref> instead of MXNet <ref type="bibr" coords="2,366.70,243.08,11.64,9.07">[7]</ref> or Digits/Caffe <ref type="bibr" coords="2,444.50,243.08,11.17,9.07" target="#b6">[8]</ref>, <ref type="bibr" coords="2,459.39,243.08,11.17,9.07" target="#b7">[9]</ref> used in 2017. The first model is based on a 152 layer Residual Network architecture (ResNet-152) <ref type="bibr" coords="2,181.56,266.84,15.26,9.07" target="#b8">[10]</ref>. The second model is based on a 92 layers deep Dual Path Network (DPN-92) <ref type="bibr" coords="2,167.86,279.08,15.28,9.07" target="#b9">[11]</ref>. Both models were pre-trained on ImageNet <ref type="bibr" coords="2,371.38,279.08,15.28,9.07" target="#b10">[12]</ref>. ResNet-152 on the ILSVRC <ref type="bibr" coords="2,164.26,290.96,16.70,9.07" target="#b11">[13]</ref> version of the data set containing 1.2 million images of 1000 object categories and DPN-92 on a larger version covering 5k object classes, a subset of ImageNet10K <ref type="bibr" coords="2,186.24,315.10,15.26,9.07" target="#b12">[14]</ref>. The models are publically available and can be imported in PyTorch via torchvision or pretrainedmodels [15] python packages.</p><p>A major difference of PyTorch compared to MXNet is the data loading and augmentation pipeline. Besides random horizontal flip, random rotation and applying color jitter to saturation and brightness for models in 2017, a random resized crop is applied for augmentation in PyTorch. This technique was successfully used for ImageNet training by <ref type="bibr" coords="2,173.62,387.13,15.28,9.07" target="#b13">[16]</ref>. It crops various patches at random position of the image with an evenly distributed size between 8 and 100 percent of the image area and additionally changes aspect ratio between 3/4 and 4/3. Furthermore color augmentation is extended by also applying jitter to contrast (factor: 0.3) and hue (factor 0.01) of the image.</p><p>For training, images from both, the trusted and noisy data sets are used. A fixed learning rate policy is applied starting with a base learning rate of 0.01. It is decreased by a factor of 0.1 whenever performance on the validation set stopped improving. This is done twice within the training process. Stochastic gradient descent with Nesterov momentum of 0.9 and weight decay of 1e-4 is used for optimization. Although not really necessary for training in PyTorch, all images were resized such that the shorter side becomes 256 pixel while preserving the original aspect ratio. Another difference to 2017 is the number of crops extracted from the test images. The 5 crops from middle and each corner of the image are doubled by also adding their mirrored version (horizontal flip). To identify the test set, for each plant observation predictions are hierarchically averaged over:  all 10 patches cropped and mirrored from the image  all images belonging to the same observation  all models within the ensemble (for run 1 &amp; 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Identification Figure <ref type="figure" coords="3,154.11,614.74,5.04,9.07" target="#fig_0">1</ref> compares scores of all submissions to the ExpertLifeCLEF 2018 task. The above described models ranked second place among all participating teams. In Figure <ref type="figure" coords="3,124.63,638.89,5.04,9.07" target="#fig_1">2</ref> performance is compared with those of human experts. Only 5 out of 9 experts achieve better identification results than the submitted automated systems even if only a single model is involved. More results and evaluation details can be accessed via the ExpertLifeCLEF 2018 webpage <ref type="bibr" coords="3,255.25,674.89,16.84,9.07" target="#b14">[17]</ref> and the crowdAI leaderboard <ref type="bibr" coords="3,393.70,674.89,15.28,9.07">[18]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Both single models of the second and third run give better results than the ensemble of 12 models of the first run from the previous year. Although using the same data set for training, performance is significantly improved with PyTorch and its extended data augmentation methods.  Nevertheless adding the less performing older models to the ensemble of new models, significantly improves results on the whole test set in the fourth run. On the contrary, results on the expert subset are not improved for this run. The expert set seems to be much more challenging. Identification accuracies are consistently lower for all runs compared to the whole test set. Despite the difficulties, all submitted 'machines' could keep up with almost half of the human experts in this image-based plant identification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,129.67,384.26,336.15,8.10;4,179.76,395.06,236.03,8.10;4,143.15,172.90,308.94,200.95"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official scores (top-1 accuracy) of the ExpertLifeCLEF 2018 plant identification task.The above described methods and submitted runs belong to MfN.</figDesc><graphic coords="4,143.15,172.90,308.94,200.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,129.31,664.46,336.80,8.10;4,199.56,675.26,196.07,8.10;4,143.15,452.90,308.34,201.50"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of top-1 accuracy between "machines" and the human experts. The above described methods and submitted runs belong to MfN.</figDesc><graphic coords="4,143.15,452.90,308.34,201.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.63,176.79,346.18,408.86"><head>Table 1 .</head><label>1</label><figDesc>performance is evaluated on the whole ExpertLifeCLEF 2018 test set and the expert subset. Results are summarized in table 1 for both sets. To better compare results and to find out if and how much progress was made on identification performance since last year, the best performing ensemble of PlantCLEF 2017 was left unchanged and used in the first run to predict the 2018 test sets. The 12 models in the ensemble consist of different network types (GoogLeNet, ResNet, ResNeXT), are trained on different data sets and use various data augmentation techniques (see<ref type="bibr" coords="3,206.68,272.96,11.72,9.07" target="#b2">[3]</ref> for details). For the second run a single model was trained via PyTorch by fine-tuning a pre-trained ResNet-152 network. Training was done with a batch size of 144 and took about one week for 84 epochs using 3 GPUs (Nvidia Geforce 1080 Ti). The model provides the most competitive run to identify images of the expert test set. Official scores on the ExpertLifeCLEF 2018 test sets.</figDesc><table coords="3,124.63,212.82,296.51,372.83"><row><cell>Run 1. Run 2. Run</cell><cell># Models</cell><cell>Top-1 Accuracy [%]</cell><cell>Top-1 Accuracy [%]</cell></row><row><cell></cell><cell></cell><cell>Whole test set</cell><cell>Expert test set</cell></row><row><cell>1</cell><cell>12</cell><cell>82.6</cell><cell>76.0</cell></row><row><cell>2</cell><cell>1</cell><cell>84.8</cell><cell>78.7</cell></row><row><cell>3</cell><cell>1</cell><cell>84.7</cell><cell>77.3</cell></row><row><cell>4</cell><cell>14</cell><cell>87.5</cell><cell>77.3</cell></row></table><note coords="3,124.63,356.89,345.76,9.07;3,124.63,369.13,345.31,9.07;3,124.63,381.01,345.94,9.07;3,124.63,392.89,345.94,9.07;3,124.63,405.15,346.18,9.07;3,124.63,417.03,183.36,9.07;3,124.63,441.15,345.77,9.07;3,124.63,453.03,345.84,9.07;3,124.63,464.94,250.39,9.07"><p><p><p><p><p>Run 3. For the third run a Dual Path Network pre-trained on ImageNet-5k was used for training. With a batch size of 102 training took about two weeks for 141 epochs (also using 3 GPUs). Although performing better on ImageNet-1k compared to Res-Net-152 (top-1 accuracy of 79.4 % for DPN-92 vs. 78.4 % for ResNet-152 [15]) performance regarding plant identification is slightly lower on the whole test set and much lower on the expert subset (see table</p>1</p>).</p>Run 4.</p>An ensemble of all 14 models from the previous runs (PlantClef 2017 models, ResNet-152 &amp; DPN-92) was created for the fourth run by averaging all predictions. This is the best performing run regarding the complete test set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,124.63,212.82,345.94,69.21"><head></head><label></label><figDesc>Table 2 compares performance of different network architectures from 2017 and 2018 models. All models use the exact same data sets for training and validation. The relatively low accuracy scores can be explained by the fairly high number of non-plant images in the validation set because both trusted and noisy data sets were used for training and validation. Also validation was carried out per image (one center patch per image) without augmentation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,124.63,308.99,345.93,197.05"><head>Table 2 .</head><label>2</label><figDesc>Performance of different network architectures.Even with the same network type (ResNet-152) results are better with PyTorch. This shows once again the big influence of data augmentation methods. Another reason for better performance in PyTorch may be the pre-processing of images for training via MXNet. Image quality is slightly reduced when creating the MXNet Image RecordIO data set for faster reading, possibly resulting in a loss of some relevant information.</figDesc><table coords="5,153.82,328.43,283.18,78.92"><row><cell>Year</cell><cell>Framework</cell><cell>Network architecture</cell><cell>Top-1 Accuracy [%]</cell></row><row><cell>2017</cell><cell>Digits/Caffe</cell><cell>GoogLeNet</cell><cell>52.7</cell></row><row><cell>2017</cell><cell>MXNet</cell><cell>ResNet-152</cell><cell>52.5</cell></row><row><cell>2017</cell><cell>MXNet</cell><cell>ResNeXt-101-64x4d</cell><cell>53.0</cell></row><row><cell>2018</cell><cell>PyTorch</cell><cell>ResNet-152</cell><cell>59.4</cell></row><row><cell>2018</cell><cell>PyTorch</cell><cell>DualPathNet92_5k</cell><cell>59.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Goëau</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Pierre Bonnet</rs> and <rs type="person">Concetto Spampinato</rs> for organizing this task. I also want to thank the <rs type="funder">Museum fuer</rs> <rs type="institution">Naturkunde Berlin</rs> and the <rs type="institution">Bundesministerium fuer Wirtschaft und Energie</rs> for supporting my research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,132.60,176.80,336.98,8.10;6,141.58,187.99,328.73,8.10;6,141.58,199.15,272.82,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,167.83,187.99,302.48,8.10;6,141.58,199.15,156.29,8.10">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,316.39,199.15,77.66,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,209.95,337.59,8.10;6,141.58,221.11,283.60,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,268.56,209.95,201.63,8.10;6,141.58,221.11,174.11,8.10">Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,329.68,221.11,75.46,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,231.91,337.97,8.10;6,141.58,243.09,216.82,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,209.21,231.91,261.36,8.10;6,141.58,243.09,44.54,8.10">Image-based Plant Species Identification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,204.59,243.09,153.81,8.10">Working Notes of CLEF 2017 Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,253.89,337.61,8.10;6,141.58,265.05,254.06,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,292.95,253.89,100.62,8.10">Plant Identification: Man vs</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,399.38,253.89,70.83,8.10;6,141.58,265.05,95.62,8.10">Machine, Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1647" to="1665" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,275.76,337.97,8.19;6,141.58,287.01,328.99,8.10;6,141.58,298.19,329.35,8.10;6,141.58,308.99,246.11,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,381.83,275.85,88.74,8.10;6,141.58,287.01,169.98,8.10">Plant Identification: Experts vs. Machines in the Era of Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,238.10,298.19,232.83,8.10;6,141.58,308.99,67.87,8.10">Multimedia Tools and Applications for Environmental &amp; Biodiversity Informatics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Karatzas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Karppinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,320.15,337.25,8.10;6,141.58,330.95,30.82,8.10;6,129.31,342.11,340.90,8.10;6,141.58,352.94,328.92,8.10;6,141.58,364.10,131.83,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,314.16,320.15,137.59,8.10;6,218.15,342.11,252.06,8.10;6,141.58,352.94,117.24,8.10">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,141.58,330.95,30.82,8.10;6,129.31,342.11,3.29,8.10;6,279.87,352.94,190.63,8.10;6,141.58,364.10,107.85,8.10">Neural Information Processing Systems, Workshop on Machine Learning Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2017. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>NIPS-W 7</note>
</biblStruct>

<biblStruct coords="6,132.60,374.90,296.09,8.10" xml:id="b6">
	<monogr>
		<ptr target="https://developer.nvidia.com/digits" />
		<title level="m" coord="6,141.58,374.90,60.60,8.10">Digits Homepage</title>
		<imprint>
			<date type="published" when="2018-05-29">2018/05/29</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.60,386.06,337.74,8.10;6,141.58,397.22,83.69,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,216.73,386.06,235.25,8.10">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,408.04,338.53,8.10;6,141.58,419.20,45.87,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,280.74,408.04,172.86,8.10">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,141.58,419.20,20.79,8.10">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,430.00,330.72,8.10" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,266.03,430.00,71.15,8.10">Dual Path Networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01629</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,131.89,441.16,338.32,8.10;6,141.58,451.96,328.41,8.10;6,141.58,463.15,93.34,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,359.05,441.16,111.16,8.10;6,141.58,451.96,78.01,8.10">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,237.96,451.96,228.22,8.10">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,473.95,338.54,8.10;6,141.58,485.11,83.63,8.10" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,254.13,473.95,197.99,8.10">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,496.27,337.88,8.10;6,141.58,507.07,235.74,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,291.70,496.27,178.07,8.10;6,141.58,507.07,71.25,8.10">What Does Classifying More Than 10,000 Image Categories Tell Us?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,227.19,507.07,91.58,8.10">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,529.05,316.50,8.10" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="6,228.27,529.05,118.61,8.10">Going Deeper with Convolutions</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,131.89,540.21,338.65,8.10;6,141.58,551.01,40.84,8.10" xml:id="b14">
	<monogr>
		<ptr target="http://www.imageclef.org/node/231,lastaccessed2018/05/29" />
		<title level="m" coord="6,141.58,540.21,129.06,8.10">ExpertLifeCLEF 2018 Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
