<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.45,115.96,280.45,12.62;1,189.42,133.89,236.52,12.62">A Baseline for Large-Scale Bird Species Identification in Field Recordings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.55,171.56,51.75,8.74"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.86,171.56,100.52,8.74"><forename type="first">Thomas</forename><surname>Wilhelm-Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,310.93,171.56,60.09,8.74"><forename type="first">Holger</forename><surname>Klinck</surname></persName>
							<email>holger.klinck@cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.56,171.56,69.63,8.74"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Cornell Lab of Ornithology</orgName>
								<orgName type="laboratory">Bioacoustics Research Program</orgName>
								<address>
									<addrLine>159 Sapsucker Woods Road</addrLine>
									<postCode>14850</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.23,183.51,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.45,115.96,280.45,12.62;1,189.42,133.89,236.52,12.62">A Baseline for Large-Scale Bird Species Identification in Field Recordings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08287359E3EC98DA9EE122CBBF12CE6E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bioacoustics</term>
					<term>Bird Sounds</term>
					<term>Deep Learning</term>
					<term>BirdCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF bird identifcation task poses a difficult challenge in the domain of acoustic event classification. Deep learning techniques have greatly impacted the field of bird sound recognition in recent years. We discuss our attempt of large-scale bird species identification using the 2018 BirdCLEF baseline system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>Large-scale bird sound identification in audio recordings is the foundation of long-term species diversity monitoring. Aiding this labor intensive task with automated systems that can recognize multiple hundreds of species has been the focus in recent years. As part of the 2018 LifeCLEF workshop <ref type="bibr" coords="1,402.08,463.60,9.96,8.74" target="#b0">[1]</ref>, the BirdCLEF bird identification challenges <ref type="bibr" coords="1,261.73,475.55,10.52,8.74" target="#b1">[2]</ref> provide large datasets containing almost 50.000 recordings to assess the performance of various systems attempting to push the boundaries of automated bird sound recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In 2016, Sprengel et al. <ref type="bibr" coords="1,244.99,548.52,10.52,8.74" target="#b2">[3]</ref> demonstrated the superior performance of convolutional neural networks (CNN) for the classification of bird sounds. Following that approach, we were able to improve the performance on a larger dataset containing 1500 different species with our 2017 BirdCLEF participation <ref type="bibr" coords="1,443.74,584.39,9.96,8.74" target="#b3">[4]</ref>. This year, we present an implementation of a streamlined workflow built on the most fundamental principles of visual classification using CNN. We published the code repository as baseline system complementing the 2018 BirdCLEF challenge <ref type="bibr" coords="1,467.31,620.25,9.96,8.74" target="#b4">[5]</ref>. The following workflow design, training scheme and submission results are entirely based on that system, establishing a good overall baseline for future comparisons and improvements.</p><p>The key stages of our workflow include dataset pre-processing, spectrogram extraction, CNN training and evaluation. We adopted our last year's attempt and focused mainly on basic deep learning techniques, keeping the code base as simple and comprehensible as possible, while maintaining a good overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Handling</head><p>Using convolutional neural networks for the classification of acoustic events proved to be very effective despite the fact that these techniques are tailor made for visual recognition. Representing audio recordings as spectrograms overcomes this gap between the two domains of audio and image. We decided to use MELscale log-amplitude spectrograms which have been effectively used in similar approaches (e.g. <ref type="bibr" coords="2,210.94,287.76,10.30,8.74" target="#b5">[6]</ref>). A more detailed description of the extraction and preprocessing process can be found in <ref type="bibr" coords="2,288.91,299.71,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Our baseline training process supports multiple shallow and deep model architectures, extensive dataset augmentation, learning rate scheduling, model pretraining and result pooling. We implemented two basic CNN concepts: Fullyconvolutional architectures with simple layer sequences and ResNet variations with shortcut connections. We also provide eBird<ref type="foot" coords="2,351.58,395.37,3.97,6.12" target="#foot_0">4</ref> checklist metadata for both soundscape locations in Peru and Columbia along with the basline repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Distillation</head><p>Most CNN implementations are computationally expensive and rely on powerhungry hardware. Future applications of automated bird sound recognition will include field recorders capable of not only of recording, but also analyzing audio data in real-time. In those cases, battery life becomes an issue. In recent years, (semi-) mobile hardware -mostly used for IoT-applications -has been designed to aid this task. However, those hardware platforms are not yet suited for deep learning inference using complex models.</p><p>In 2015, Hinton et. al <ref type="bibr" coords="2,243.65,553.96,10.52,8.74" target="#b6">[7]</ref> presented an approach to distill knowledge in neural networks. We followed that scheme of model distillation and implemented a basic variant of teacher-student learning. Our baseline system allows to replace binary training targets with log-probability predictions of either single models or entire ensembles. We designed a simple shallow model that can predict species probabilities of one-second audio chunks in less than one second running on a Raspberry Pi 3+. The resulting scores are slightly lower than those of large single models, but still above the initial capabilities of the tiny CNN model.</p><p>The prediction performance of this approach is promising and model distillation may have significant impact on the field of mobile real-time species diversity assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We tried to cover different basic training and prediction schemes with our run submissions, including single baseline models, large and diverse model ensembles, metadata assisted attempts with species pre-selection and knowledge distillation training of tiny models. Table <ref type="table" coords="3,272.01,229.32,4.98,8.74" target="#tab_0">1</ref> provides an overview of selected results from our submissions. The results show that our baseline attempt yields competitive results considering the complex evaluation task. Most results did match our expectations for the audio-only classification of field recordings. The key takeaways of the analysis of the submission results are:</p><p>-Diverse model ensembles covering different net architectures and dataset splits outperform single neural nets by a significant margin. This comes as no surprise in the domain of metric-centered competitions, but might not be applicable to real-world scenarios due to increased computational costs.</p><p>-Pre-selecting species did not improve the overall performance as expected.</p><p>In some cases, selecting species based on time of the year and location helps to reduce training time. Using metadata as post-filter to eliminate false detections or as input during model training might lead to better results.</p><p>-Model distillation is a powerful tool to increase the classification performance of tiny neural networks. The results show comparable performance in the soundscape domain despite much smaller model architectures, when compared to model ensembles.</p><p>We published our entire code repository<ref type="foot" coords="4,324.92,117.42,3.97,6.12" target="#foot_1">5</ref> and encourage future participants and interested research groups to build upon our results and improve the performance for the analysis of complex soundscapes -the most crucial aspect of species diversity monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>Assessing high-quality field recordings for the presence of bird species using convolutional neural networks is an effective application of deep learning techniques to the domain of acoustic event detection. Considering our own scores and those of other participants, current machine learning algorithms yield very strong results for this task. However, the 2018 BirdCLEF evaluation showed that the transfer of knowledge extracted from monophonic community recordings to the domain of long-term soundscape recordings is still very difficult. Hardly any improvements over last year's result have been accomplished. Future research should specifically focus on this task. Additionally, power-hungry hardware and computationally expensive algorithms are not well-suited for real-world applications such as mobile reorders. Improving techniques to shrink the size of neural networks while maintaining the overall performance will greatly help the field of long-term species diversity assessment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,270.02,345.83,139.05"><head>Table 1 .</head><label>1</label><figDesc>Selected submission results (run IDs in brackets, not all runs listed for clarity). Large ensembles including different net architectures and dataset splits perform best. Pre-selecting species for specific locations does not improve the results. Model distillation helps to reduce computational costs and can maintain results that are comparable with the performance of large single nets.</figDesc><table coords="3,139.65,334.66,325.77,74.41"><row><cell></cell><cell cols="2">Monophone Task</cell><cell cols="2">Soundscape Task</cell></row><row><cell>Run</cell><cell>MRR</cell><cell>MRR</cell><cell>c-mAP</cell><cell>c-mAP</cell></row><row><cell>Description</cell><cell>Foreground</cell><cell>Background</cell><cell>Peru</cell><cell>Columbia</cell></row><row><cell>Best Single</cell><cell>0.487 (1)</cell><cell>0.448 (1)</cell><cell>-</cell><cell>-</cell></row><row><cell>Best Ensemble</cell><cell>0.644 (5)</cell><cell>0.588 (5)</cell><cell>0.086 (6)</cell><cell>0.117 (6)</cell></row><row><cell>Pre-Selection</cell><cell>-</cell><cell>-</cell><cell>0.081 (7)</cell><cell>0.052 (8)</cell></row><row><cell>Raspberry Pi</cell><cell>0.425 (4)</cell><cell>0.385 (4)</cell><cell>0.077 (9)</cell><cell>0.083 (9)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,656.80,91.97,7.86"><p>www.ebird.org/explore</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="4,144.73,656.80,186.66,7.86"><p>https://github.com/kahst/BirdCLEF-Baseline</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,409.59,342.24,7.86;4,146.91,420.55,333.68,7.86;4,146.91,431.50,333.68,7.86;4,146.91,442.46,49.66,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,193.97,420.55,286.62,7.86;4,146.91,431.50,224.58,7.86">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,393.58,431.50,87.01,7.86;4,146.91,442.46,18.43,7.86">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,453.42,342.25,7.86;4,146.91,464.38,333.68,7.86;4,146.91,475.34,73.78,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,432.62,453.42,47.97,7.86;4,146.91,464.38,252.97,7.86">Overview of BirdCLEF 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,420.94,464.38,59.65,7.86;4,146.91,475.34,21.05,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,486.30,342.24,7.86;4,146.91,497.26,319.55,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,345.79,486.30,134.80,7.86;4,146.91,497.26,149.16,7.86">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Martin Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,317.29,497.26,96.44,7.86">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,508.22,342.24,7.86;4,146.91,519.18,333.68,7.86;4,146.91,530.13,136.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,162.99,519.18,299.63,7.86">Large-Scale Bird Sound Classification using Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,146.91,530.13,83.66,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,541.09,342.24,7.86;4,146.91,552.05,333.68,7.86;4,146.91,563.01,28.16,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<title level="m" coord="4,407.54,541.09,73.06,7.86;4,146.91,552.05,198.63,7.86">Recognizing Birds from Sound -The 2018 BirdCLEF Baseline System</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,138.35,573.97,342.24,7.86;4,146.91,584.93,333.68,7.86;4,146.91,595.89,100.47,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,235.04,573.97,245.56,7.86;4,146.91,584.93,25.62,7.86">Two convolutional neural networks for bird detection in audio signals</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,196.10,584.93,171.04,7.86">Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1764" to="1768" />
		</imprint>
	</monogr>
	<note>25th European</note>
</biblStruct>

<biblStruct coords="4,138.35,606.85,342.24,7.86;4,146.91,617.81,161.61,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,292.44,606.85,183.83,7.86">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
