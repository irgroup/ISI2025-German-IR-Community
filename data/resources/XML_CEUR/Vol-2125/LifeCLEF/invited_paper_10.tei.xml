<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.79,115.96,307.77,12.62;1,144.68,133.89,325.99,12.62;1,260.52,151.82,94.31,12.62">Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.25,189.64,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.36,189.64,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.23,189.64,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.79,115.96,307.77,12.62;1,144.68,133.89,325.99,12.62;1,260.52,151.82,94.31,12.62">Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F8D2CD1C7E446746BD7BBF73FC9F77BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>ExpertCLEF</term>
					<term>plant</term>
					<term>expert</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flower</term>
					<term>fruit</term>
					<term>bark</term>
					<term>stem</term>
					<term>branch</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>species identification</term>
					<term>citizen-science</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated identification of plants and animals has improved considerably in the last few years, in particular thanks to the recent advances in deep learning. The next big question is how far such automated systems are from the human expertise. Indeed, even the best experts are sometimes confused and/or disagree between each others when validating visual or audio observations of living organism. A picture actually contains only a partial information that is usually not sufficient to determine the right species with certainty. Quantifying this uncertainty and comparing it to the performance of automated systems is of high interest for both computer scientists and expert naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was designed to allow this comparison between human experts and automated systems. In total, 19 deep-learning systems implemented by 4 different research teams were evaluated with regard to 9 expert botanists of the French flora. The main outcome of this work is that the performance of state-of-the-art deep learning models is now close to the most advanced human expertise. This paper presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated identification of plants and animals has improved considerably in the last few years. In the scope of LifeCLEF 2017 <ref type="bibr" coords="1,351.82,608.30,10.52,8.74" target="#b5">[6]</ref> in particular, we measured impressive identification performance achieved thanks to recent deep learning models (e.g. up to 90 % classification accuracy over 10K species). This raises the question of how far automated systems are from the human expertise and of whether there is a upper bound that can not be exceeded. A picture actually contains only a partial information about the observed plant and it is often not sufficient to determine the right species with certainty. For instance, a decisive organ such as the flower or the fruit, might not be visible at the time a plant was observed. Or some of the discriminant patterns might be very hard or unlikely to be observed in a picture such as the presence of pills or latex, or the morphology of the root. As a consequence, even the best experts can be confused and/or disagree between each others when attempting to identify a plant from a set of pictures. Similar issues arise for most living organisms including fishes, birds, insects, etc. Quantifying this intrinsic data uncertainty and comparing it to the performance of the best automated systems is of high interest for both computer scientists and expert naturalists. This was the goal of the ExpertCLEF challenge, organized as part of the LifeCLEF 2018 campaign <ref type="bibr" coords="2,356.54,250.50,9.96,8.74" target="#b4">[5]</ref>.</p><p>In the following subsections, we synthesize the resources and assessments of the challenge, summarize the approaches and systems employed by the participating research groups, and provide an analysis of the main outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>To evaluate the above mentioned scenario at a large scale and in realistic conditions, we built and shared several different datasets coming from different sources. As training data, we provided all the previous datasets used during the previous PlantCLEF challenge <ref type="bibr" coords="2,271.44,381.15,9.96,8.74" target="#b2">[3]</ref>. The test set was built with the best experts in the plant domains, in western Europe. For that test set we created sets of observations that were identified in the field by other experts (in order to have a near-perfect golden standard). These pictures were immersed in a much larger test set that had to be processed by the participating systems.</p><p>Trusted and Noisy data in Training Set expertclef2018 : a trusted subtraining set based on the online collaborative Encyclopedia Of Life (EoL). A list of 10K species were selected as the most populated species in EoL data after a curation pipeline (taxonomic alignment, duplicates removal, herbaria sheets removal, etc.). The training set contains 256,287 pictures in total but has a strong class imbalance with a minimum of 1 picture for Achillea filipendulina and a maximum of 1245 pictures for Taraxacum laeticolor. A noisy sub-training set built through Web crawlers (Google and Bing image search engines) and containing about 1.2 million images. This training set is also imbalanced with a minimum of 4 pictures for Plectranthus sanguineus and a maximum of 1732 pictures for Fagus grandifolia.</p><p>The main objective of providing these 2 sub-datasets was to offer to the participants the opportunity to evaluate to what extent machine learning techniques can learn from noisy data compared to trusted data. Pictures of EoL are themselves coming from different sources, including institutional databases as well as public data sources such as Wikimedia, iNaturalist, Flickr or various websites dedicated to botany. This aggregated data is continuously revised and rated by the EoL community so that the quality of the species labels is globally very good. On the other side, the noisy web dataset contains more images but with several types and levels of noise: some images are labeled with the wrong species name (but sometimes with the correct genus or family), some are portraits of a botanist specialist of the targeted species, some are labeled with the correct species name but are drawings or herbarium sheets, etc.</p><p>Pl@ntNet test set: the test data to be analyzed within the challenge is a large sample of the query images submitted by the users of the mobile application Pl@ntNet (iPhone<ref type="foot" coords="3,235.54,225.02,3.97,6.12" target="#foot_0">4</ref> &amp; Androd<ref type="foot" coords="3,288.50,225.02,3.97,6.12" target="#foot_1">5</ref> ). It contains a large number of wild plant species mostly coming from the Western Europe Flora and the North American Flora, but also plant species used all around the world as cultivated or ornamental plants including some endangered species. This test set was obtained after a curation pipeline (collaborative species identification evaluation, author reputation, visual quality evaluation, etc.). This test set was extended with expert observations, according to the following procedure. First, 125 plants were photographed between May and June 2017, in a botanical garden called the "Parc floral de Paris", and in a natural area located in the north of Montpellier city (southern part of France, close to the Mediterranean sea). The photos have been done with two smartphone models, an iPhone 5 and a Samsung S5 G930F, by a botanist and an amateur under his supervision. The selection of the species has been motivated by several criteria including (i) their membership to a difficult plant group (i.e. a group known as being the source of many confusions), (ii) the availability of well developed specimens with well visible organs on the spot and (iii), the diversity of the selected set of species in terms of taxonomy and morphology. About fifteen pictures of each specimen were acquired in order to cover all the informative parts of the plant. However, all pictures were not included in the final test set in order to deliberately hide a part of the information and increase the difficulty of the identification. Therefore, a random selection of only 1 to 5 pictures was operated for each specimen. In the end, a subset of 75 plants illustrated by a total of 216 images related to 33 families and 58 genera was selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Based on the previously described testbed, we conducted a system-oriented evaluation involving different research groups who downloaded the data and ran their system. Each participating group was allowed to submit up to 5 run files built from different methods (a run file is a formatted text file containing the species predictions for all test items). Semi-supervised, interactive or crowdsourced approaches were allowed but had to be clearly signaled within the submission system. But none of the participants employed such methods. The main evaluation metric was the top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>28 participants were registered to the ExpertCLEF challenge 2018. Among this large raw audience, 4 research groups finally succeeded in submitting run files. Details of the used methods and evaluated systems are synthesized below and further developed in the working notes of the participants (CMP <ref type="bibr" coords="4,422.46,177.91,14.61,8.74" target="#b9">[10]</ref>, MfN <ref type="bibr" coords="4,467.31,177.91,9.96,8.74" target="#b7">[8]</ref>, Sabanci Gebze <ref type="bibr" coords="4,194.90,189.87,13.31,8.74" target="#b0">[1]</ref>, TUC <ref type="bibr" coords="4,242.89,189.87,9.96,8.74" target="#b3">[4]</ref>. The following paragraphs give a few more details about the methods and the overall strategy employed by each participant.</p><p>CMP, Dept. of Cybernetics, Czech Technical University in Prague, Czech Republic, 5 runs, <ref type="bibr" coords="4,262.60,237.66,16.47,8.77" target="#b9">[10]</ref>: used an ensemble of a dozen Convolutional Neural Networks (CNNs) based on 2 state-of-the-art architectures (Inception-ResNet-v2 and Inception-v4). The CNNs were initialized with weights pre-trained on ImageNet, then fine-tuned with different hyper-parameters and with the use of data augmentation (random horizontal flip, color distortions and random crops for some models). Each single test image is also augmented with 14 transformations (central/corner crops, horizontal flips, none) to combine and improve the predictions. Still at test time, the predictions are computed using the Exponential Moving Average feature of TensorFlow, i.e. by averaging the predictions of the set of models trained during the last iterations of the training phase (with an exponential decay). This popular procedure is inspired from Polyak averaging method <ref type="bibr" coords="4,188.78,369.19,10.52,8.74" target="#b8">[9]</ref> and is known to sometimes produce significantly better results than using the last trained model solely. As a last step in their system, assuming that there is a strong unbalanced distribution of the classes between the test and the training sets, the outputs of the CNNs are adjusted according to an estimation of the class prior probabilities in the test set based on an Expectation Maximization algorithm. The best score of 88.4% top-1 accuracy during the challenge was obtained by this team with the largest ensemble (CMP Run 3). With half less combined models, the CMP Run 4 reached a close top-1 accuracy and even obtained a slightly better accuracy on the smaller test subset identified by human experts. It can be explained by the strategy during the training of using the trusted and noisy sets: a comparison between CMP Run 1 and 4 clearly illustrates that refining further a model with only the trusted training set after learning it on the whole noisy training set is not relevant. CMP Run 3 which combines all the models seems to have its performances degraded by the inclusion of the models refined on the trusted training set when we compare it with CMP Run 4 on the test subset identified by human experts.</p><p>MfN, Museum fuer Naturkunde Berlin, Leibniz Institute for Evolution and Biodiversity Science, Germany, 4 runs, <ref type="bibr" coords="4,399.41,584.36,11.15,8.77" target="#b7">[8]</ref>: followed quite similar approaches used last year during the PlantCLEF2017 challenge <ref type="bibr" coords="4,444.83,596.34,9.96,8.74" target="#b6">[7]</ref>. This participant used an ensemble of fine-tuned CNNs pretrained on ImageNet, based on 4 architectures (GoogLeNet, ResNet-152, ResNeXT, DualPathNet92), each trained with bagging techniques. Data augmentation was used systematically for each training, in particular random cropping, horizontal flipping, variations of saturation, lightness and rotation. For the three last transformations, the inten-sity of the transformation is correlated to the diminution of the learning rate during training to let the CNNs see patches progressively closer to the original image at the end of the training. Test images followed similar transformations for combining and boosting the accuracy of the predictions. MfN Run 1 used basically the best and winning approach during PlantCLEF2017 by averaging the prediction of 11 models based on 3 architectures (GoogLeNet, ResNet-152, ResNeXT). However, surprisingly, the runs MfN Run 2 and 3, which are based on only one architecture (respectively ResNet152 and DualPathNet92), performed both better than the Run 1 combining several architectures and models. The combination of all the approaches in MfN Run 4 seems even to be penalized by the winning approach during PlantCLEF2017.</p><p>SabanciU-GTU, Sabanci University, Turkey, 5 runs, <ref type="bibr" coords="5,399.17,262.43,11.15,8.77" target="#b0">[1]</ref>: fine-tuned and combined two recent successful CNN architectures: DenseNet (Densely connected convolutional Networks), and SeNet (Squeeze-and-excitation Networks), more precisely a SeNet-ResNet-50. Indeed, SeNet introduces building blocks that can be integrated to any modern CNN such as ResNet-50 and that are designed for improving channel interdependencies by adding parameters to each channel of a convolutional block so that the network can adaptively adjust the weighting of each feature map. For its part, a DenseNet is composed of dense blocks where each unit inside is connected to every unit before it. DenseNet has a counterintuitive property where fewer parameters than a traditional CNN are required while lessening the vanishing-gradient problem. For the challenge, Sabanci-GTU fine-tuned three pre-trained SeNet-ResNet-50 models and one DenseNet. The two first SeNet-ResNet-50 model were trained only on the trusted dataset, while the third one and the DenseNet were fine-tuned on all the available training datasets. Saliency detection, flip, and several rotation angles were used as data augmentation. SabanciU-GTU run 1, 3, 4 and 5 are various weighted combinations of the outputs of the four fine-tuned models. The best result was obtained by the run 5 by weighting the outputs of the CNNs according to the "quality" and "organ" tags provided in the xml metadata files. Run 3 used also the organ tag with manually fixed weights for giving more weight to pictures with "sexual" organs (flower, fruit) or the entire view. Run 2 applied a Error-Correcting Output Codes approach (ECOC) expressing the 10k classes problem through a n-bits (n = 200 here) error-correcting output code. Each bit is related to a binary classifier splitting arbitrarily and randomly into two sets the 10k species. A binary classifier was a 2-hidden layer shallow networks (500 hidden nodes at each layer) taking as input the features from the last layer of the first SeNet-ResNet-50 trained model. Unfortunately, this approach performed the worst during the challenge.</p><p>TUC MI, Technische Universitt Chemnitz, Germany, 5 runs, <ref type="bibr" coords="5,446.11,609.13,11.15,8.77" target="#b3">[4]</ref>: this team based their system on three architectures (ResNet-50, Inception-v3 and DenseNet-201) fine-tuned on the noisy or trusted dataset with various data augmentations (horizontal and vertical flip, zooming, rotating, shearing and shift-ing). DenseNet-201 models was fine-tuned with adjusted class weights over multiple iterations to attempt to balance the classes. The best results was obtained by Run 1 and Run 5 which are ensemble classifiers. Run 1 is based on one ResNet, one Inception-v3 and three DenseNet-201, all fine-tuned with the noisy training dataset, and weighted according to their validation accuracy. Run 5 performed slightly better on the whole test set by using only 3 fine-tuned models instead of 5 in Run 1, (2 ResNet-50 and 1 DenseNet-201) and without a specific weighting rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and "Experts vs. Machines" evaluation</head><p>Figure <ref type="figure" coords="6,165.46,258.13,4.98,8.74">1</ref> and Table <ref type="table" coords="6,218.32,258.13,4.98,8.74" target="#tab_0">1</ref> provides the performance achieved by the 19 collected runs.</p><p>Considering the automated approaches solely, without comparisons with the experts, we can confirm and remind quickly the same conclusions noticed during the last PlantCLEF 2017 challenge:</p><p>the measured performances are very high despite the difficulty of the task, the best results were obtained mostly by systems that were learned on both the trusted and the noisy datasets, all teams used and fine-tuned popular Convolutional Neural Networks confirming definitively the supremacy of this kind of approach over previous methods, the best results were obtained by ensemble classifiers of ConvNets with many data augmentations.</p><p>Figure <ref type="figure" coords="6,182.11,428.97,4.98,8.74">2</ref> reports the comparison of the Top-1 accuracy between the automated systems and the human experts. The main outcomes we derived from the results of the evaluation are the following ones:</p><p>A difficult task, even for experts: as a first noticeable outcome, none of the botanist correctly identified all observations. The top-1 accuracy of the experts is in the range 0.613 -0.96. with a median value of 0.8. This illustrates the difficulty of the task, especially when reminding that the experts were authorized to use any external resource to complete the task, Flora books in particular. It shows that a large part of the observations in the test set do not contain enough information to be identified with confidence when using classical identification keys. Only the four experts with an exceptional field expertise were able to correctly identify more than 80% of the observations. Deep learning algorithms were defeated by the best experts but the margin of progression is becoming tighter and tighter. The top-1 accuracy of the evaluated systems is in the range 0.32 -0.84 with a median value of 0.64. This is globally lower than the experts but it is noticeable that the best systems were able to perform better than 5 of the highly skilled participating experts. Moreover, regarding a previous Man vs. Machine evaluation in <ref type="bibr" coords="6,406.58,656.12,9.96,8.74" target="#b1">[2]</ref>, we can notice  that some participants succeeded to improve their system in one short year on the same dataset (the best top-1 accuracy was 0.733 in the previous experiment, 0.84 during this ExpertCLEF 2018 challenge). We can assume that there is still a room for improvement and that the machines would probably be able to compete with the 3 best human experts next year when the challenge will be re-open on the crowdai platform.</p><p>Identification failures (machines): looking in details the results, we can notice that some of the best automated systems can perform as well as experts for about 86% of the observations This is the case for the best evaluated system CMP Run 4 where 65 of the 75 test observations ranked the right species at a lower or equal rank than the best expert. Among the 10 remaining observations, 5 were correctly identified in the top-2 predictions, 2 in the top-3 and only 3 observations were completely failed (see Table <ref type="table" coords="8,346.41,536.57,3.87,8.74" target="#tab_1">2</ref>). The causes of the identification failures differs from an observation to another one. For one observation (2792091) it is probably due to a mismatch between the training data and the test sample. Actually, the training samples of the correct species usually contain visible open yellow flowers whereas only beige buds are visible in the test sample. In the second missed observation (2791146), it is more likely that the failure is due to the intrinsic difficulty of the associated genus Lathyrus within which many species are visually very similar (but most of the proposals in machine runs are nevertheless under the Lathyrus genus). The same for the last missed observation (2791317) related to the genus Galium with an additional difficulty related to fact the observation contains only one entire view. Identification failures (experts): on the other hand, it is important to notice that some automated systems can perform better in some cases than the experts. If we compare again the best automated system CMP Run 4 and the best expert, we can notice that three observations have been better identified by the automated approach (see Table <ref type="table" coords="9,293.57,459.28,3.87,8.74" target="#tab_2">3</ref>). For one observation (2792706) the best system gave the correct species at rank 1 while it was at rank 2 for the best expert. For the two observations 2790900 and 2791110, the best automated system gave the correct species at rank 3 while there were no species propositions at all from the best human expert. The two observations are actually cultivated plants, probably varieties visually different from the "original" species, and relatively far from the core expertise of the human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF 2018 expert identification challenge following the seven previous LifeCLEF plant identification challenges conducted within CLEF evaluation forum. The task was performed again on the biggest plant images dataset ever published in the literature, but focused on an expert vs. machine evaluation. The main goal behind that was to answer the question of whether automated plant identification systems still have a margin of progression or if already perform as well as experts for identifying plants in images. We showed that identifying plants from images solely is a difficult task, even for some of the highly skilled specialists who accepted to participate to the experiment. This confirms that pictures of plants only contain partial information and that it is often not sufficient to determine the right species with certainty. Regarding the performance of the automated approaches, we shows that there is still a margin of progression but that it is becoming tighter and tighter. The best system was able to correctly classify 84% of the test samples including some belonging to very difficult taxonomic groups. This performance is still far from the best expert who correctly identified 96.7% of the test samples. However, a strength of the automated systems is that they can return quickly an exhaustive list of all the possible species whereas this is a very difficult task for humans. We believe that this already makes them highly powerful tools for modern botany. Furthermore, the performance of automated systems will continue to improve in the following years thanks to the quick progress of deep learning technologies. They have the potential to become essential tools for teachers and students, but they should not replace an in-depth understanding of botany. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,634.56,345.83,7.89;7,134.77,645.54,71.93,7.86;7,141.88,405.86,328.54,213.93"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Scores achieved by all systems evaluated within the expert identification task of LifeCLEF 2018</figDesc><graphic coords="7,141.88,405.86,328.54,213.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,141.88,115.84,328.54,214.36"><head></head><label></label><figDesc></figDesc><graphic coords="8,141.88,115.84,328.54,214.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,164.37,127.20,286.62,246.18"><head>Table 1 :</head><label>1</label><figDesc>Results of the LifeCLEF 2018 Expert Identification Task</figDesc><table coords="7,199.83,146.39,212.67,226.99"><row><cell>Run</cell><cell cols="2">Top1 (expert) Top1 (whole)</cell></row><row><cell>CMP Run 4</cell><cell>0.840</cell><cell>0.867</cell></row><row><cell>CMP Run 3</cell><cell>0.827</cell><cell>0.884</cell></row><row><cell>MfN Run 2</cell><cell>0.787</cell><cell>0.848</cell></row><row><cell>MfN Run 4</cell><cell>0.773</cell><cell>0.875</cell></row><row><cell>CMP Run 2</cell><cell>0.773</cell><cell>0.856</cell></row><row><cell>MfN Run 3</cell><cell>0.773</cell><cell>0.847</cell></row><row><cell>CMP Run 5</cell><cell>0.773</cell><cell>0.832</cell></row><row><cell>CMP Run 1</cell><cell>0.760</cell><cell>0.868</cell></row><row><cell>MfN Run 1</cell><cell>0.760</cell><cell>0.826</cell></row><row><cell>TUC MI Run 5</cell><cell>0.640</cell><cell>0.770</cell></row><row><cell>TUC MI Run 1</cell><cell>0.640</cell><cell>0.755</cell></row><row><cell>TUC MI Run 2</cell><cell>0.640</cell><cell>0.755</cell></row><row><cell>SabanciU-GTU Run 5</cell><cell>0.613</cell><cell>0.744</cell></row><row><cell>SabanciU-GTU Run 3</cell><cell>0.613</cell><cell>0.743</cell></row><row><cell>TUC MI Run 3</cell><cell>0.613</cell><cell>0.718</cell></row><row><cell>SabanciU-GTU Run 1</cell><cell>0.600</cell><cell>0.741</cell></row><row><cell>SabanciU-GTU Run 4</cell><cell>0.587</cell><cell>0.721</cell></row><row><cell>TUC MI Run 4</cell><cell>0.587</cell><cell>0.698</cell></row><row><cell>SabanciU-GTU Run 2</cell><cell>0.320</cell><cell>0.418</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,147.99,345.83,234.30"><head>Table 2 :</head><label>2</label><figDesc>Examples of observations well identified by experts but missed by the automated identification system.</figDesc><table coords="9,160.83,170.63,175.61,211.66"><row><cell>Obs. Id</cell><cell>Photos</cell></row><row><cell>2792091</cell><cell></cell></row><row><cell>2791146</cell><cell></cell></row><row><cell>2791317</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,343.45,345.82,166.37"><head>Table 3 :</head><label>3</label><figDesc>Examples of observations well identified by the automated system but missed by expert.</figDesc><table coords="10,160.83,366.10,175.61,143.73"><row><cell>Obs. Id</cell><cell>Photos</cell></row><row><cell>2790900</cell><cell></cell></row><row><cell>2791110</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,646.48,268.32,7.47"><p>https://itunes.apple.com/fr/app/plantnet/id600547573?mt=8</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,657.44,273.03,7.47"><p>https://play.google.com/store/apps/details?id=org.plantnet</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,590.22,337.64,7.86;10,151.52,601.18,329.07,7.86;10,151.52,612.14,224.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,191.56,601.18,196.86,7.86">Plant identification with deep learning ensembles</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yankoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ganiyusufoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yildirir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,409.97,601.18,70.63,7.86;10,151.52,612.14,191.00,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,621.65,337.64,10.13;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,235.64,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,295.32,634.88,128.48,7.86">Plant Identification: Experts vs</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Malécot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jauzein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Melet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-76445-0_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-76445-0_8" />
	</analytic>
	<monogr>
		<title level="m" coord="10,431.49,634.88,49.10,7.86;10,151.52,645.84,102.07,7.86">Machines in the Era of Deep Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="131" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.63,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,207.81,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,283.03,119.67,197.56,7.86;11,151.52,130.63,212.56,7.86">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,386.44,130.63,94.15,7.86;11,151.52,141.59,136.96,7.86">CLEF 2017-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,152.55,337.64,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,131.09,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,332.49,152.55,148.11,7.86;11,151.52,163.51,139.49,7.86">Large-scale plant classification using deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,311.91,163.51,168.68,7.86;11,151.52,174.47,97.44,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,185.43,337.64,7.86;11,151.52,196.39,329.07,7.86;11,151.52,207.34,329.07,7.86;11,151.52,218.30,329.07,7.86;11,151.52,229.26,329.07,7.86;11,151.52,240.22,329.07,7.86;11,151.52,251.18,43.00,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,166.80,196.39,313.80,7.86;11,151.52,207.34,173.70,7.86">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.77,218.30,54.82,7.86;11,151.52,229.26,329.07,7.86;11,151.52,240.22,172.94,7.86">CLEF: Cross-Language Evaluation Forum for European Languages. Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Avigon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,262.14,337.63,7.86;11,151.52,273.10,329.07,7.86;11,151.52,284.06,329.07,7.86;11,151.52,295.02,329.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,348.80,273.10,131.79,7.86;11,151.52,284.06,152.70,7.86">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,325.61,284.06,154.98,7.86;11,151.52,295.02,208.98,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,305.98,337.63,7.86;11,151.52,316.93,260.66,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,203.54,305.98,277.05,7.86;11,151.52,316.93,47.81,7.86">Image-based plant species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,220.74,316.93,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,327.89,337.63,7.86;11,151.52,338.85,329.07,7.86;11,151.52,349.81,58.51,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,204.95,327.89,275.64,7.86;11,151.52,338.85,71.01,7.86">Machines vs. experts: Working note on the expertlifeclef 2018 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.62,338.85,236.97,7.86;11,151.52,349.81,24.87,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,360.77,337.63,7.86;11,151.52,371.73,286.35,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,271.61,360.77,208.98,7.86;11,151.52,371.73,11.14,7.86">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,169.44,371.73,177.81,7.86">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,382.69,337.98,7.86;11,151.52,393.65,329.07,7.86;11,151.52,404.61,104.97,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,276.58,382.69,204.01,7.86;11,151.52,393.65,108.99,7.86">Plant recognition by inception networks with testtime class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,283.90,393.65,196.69,7.86;11,151.52,404.61,71.33,7.86">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
