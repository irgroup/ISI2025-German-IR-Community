<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.93,115.96,293.50,12.62;1,172.66,133.89,270.03,12.62">Plant Recognition by Inception Networks with Test-time Class Prior Estimation</title>
				<funder ref="#_yxgnYk7">
					<orgName type="full">UWB</orgName>
				</funder>
				<funder>
					<orgName type="full">Electrolux Student Support Programme</orgName>
				</funder>
				<funder ref="#_UeJzXN4">
					<orgName type="full">Czech Science Foundation</orgName>
				</funder>
				<funder ref="#_xuNPkqV">
					<orgName type="full">CTU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,287.64,196.85,52.92,8.74"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<email>picekl@kky.zcu.cz</email>
						</author>
						<author>
							<persName coords="1,457.72,196.85,14.56,8.74;1,248.38,208.80,26.90,8.74"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Perception</orgName>
								<orgName type="department" key="dep2">Dept. of Cybernetics</orgName>
								<orgName type="department" key="dep3">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University in</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<settlement>Pilsen</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.93,115.96,293.50,12.62;1,172.66,133.89,270.03,12.62">Plant Recognition by Inception Networks with Test-time Class Prior Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9EA156D1D20F4130C0527551ED842514</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant Recognition</term>
					<term>Plant Identification</term>
					<term>Computer Vision</term>
					<term>Convolutional Neural Networks</term>
					<term>Machine Learning</term>
					<term>Class Prior Estimation</term>
					<term>Fine-grained</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper describes an automatic system for recognition of 10,000 plant species from one or more images. The system finished 1st in the ExpertLifeCLEF 2018 plant identification challenge with 88.4% accuracy and performed better than 5 of the 9 participating plant identification experts. The system is based on the Inception-ResNet-v2 and Inception-v4 Convolutional Neural Network (CNN) architectures. Performance improvements were achieved by: adjusting the CNN predictions according to the estimated change of the class prior probabilities, replacing network parameters with their running averages, and test-time data augmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ExpertLifeCLEF <ref type="bibr" coords="1,233.37,522.44,10.52,8.74" target="#b2">[3]</ref> plant identification challenge is organized in connection with the LifeCLEF 2018 workshop <ref type="bibr" coords="1,313.10,534.39,10.52,8.74" target="#b3">[4]</ref> at the Conference and Labs of the Evaluation Forum. The goal of the challenge is assess the quality of automatic, machine-learned recognition systems and to compare their accuracy with human experts in plant sciences. For practical reasons, the experts are evaluated on a small subset of the test data.</p><p>The data provided for the challenge cover 10 000 species of plants -herbs, trees and ferns -and consist from:</p><p>-PlantCLEF 2017 EOL: 256K images from the Encyclopedia of Life<ref type="foot" coords="1,436.00,623.75,3.97,6.12" target="#foot_0">3</ref> provided in the 2017 challenge <ref type="bibr" coords="1,246.63,637.28,10.52,8.74" target="#b1">[2]</ref> as the "trusted" training set.</p><p>-PlantCLEF 2017 web: 1.4M images automatically retrieved by web search engines, provided in the 2017 challenge <ref type="bibr" coords="2,325.30,130.95,10.52,8.74" target="#b1">[2]</ref> as the "noisy" training set. -PlantCLEF 2017 test set: 25K test images from the 2017 challenge <ref type="bibr" coords="2,446.54,145.71,9.96,8.74" target="#b1">[2]</ref>, now available with ground truth label annotations.</p><p>-PlantCLEF 2016 subset: 64K images from the PlantCLEF 2016 <ref type="bibr" coords="2,427.80,172.43,10.52,8.74" target="#b0">[1]</ref>   The class frequencies in the training data follow a long-tailed distribution. It is reasonable to expect that the training data, whose significant majority was downloaded from the web, have different class prior probabilities than the test set. In section 2.4 we consider the problem of different class prior probability distributions and implement an existing method <ref type="bibr" coords="2,348.74,593.54,10.96,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,359.70,593.54,7.31,8.74" target="#b5">6]</ref> to improve the CNN predictions by estimating the test-time priors.</p><p>Section 3 describes the 5 submissions we made. Results of the challenge are presented in Section 4. One of the submitted plant recognition methods achieved the best accuracy among automated systems, and thus placed 1st in the challenge and it outperformed 5 of 9 human experts. The proposed method is based on two architectures -Inception Resnet v2 and Inception v4 <ref type="bibr" coords="3,192.47,309.87,10.52,8.74" target="#b6">[7]</ref> -and their ensembles described in Section 3. The TensorFlow-Slim API was used to adjust and fine-tune the networks from the publicly available ImageNet-pretrained checkpoints <ref type="foot" coords="3,299.27,332.20,3.97,6.12" target="#foot_1">4</ref> . All networks in our experiments shared the optimizer settings enumerated in Table <ref type="table" coords="3,328.30,345.73,3.87,8.74" target="#tab_0">1</ref>. Batch size, input resolution and random crop area range were set differently for each network listed in Table <ref type="table" coords="3,469.40,357.69,3.87,8.74" target="#tab_1">2</ref>. The following image pre-processing was used for training:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks</head><p>• Random crop, with aspect ratio range (0.75, 1.33) and with different area ranges listed in Table <ref type="table" coords="3,248.42,398.89,3.87,8.74" target="#tab_1">2</ref>, • Random left-right flip,</p><p>• Brightness and Saturation distortion.</p><p>At test-time, 14 predictions per image are generated by using 7 crops and their mirrored versions:</p><p>• 1x Full image, • 1x Central crop covering 80% of the original image, • 1x Central crop covering 60% of the original image, • 4x corner crops covering 60% of the original image. Another set of networks, denoted as #1 clean ,..,#6 clean , was fine-tuned from models #1,..,#6 without using the noisy PlantCLEF 2017 web set. For this finetuning, we also added most of the PlantCLEF 2017 test set, keeping only 1 000 observations (1 403 images) as a min-val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Running Averages</head><p>Preliminary experiments, using the 2017 test set for validation, showed a significant improvement in accuracy when using running averages of the trained variables instead of the values from the last training step. Namely we used an exponential decay with decay rate of 0.999.</p><p>In this task where majority of the training data is noisy, we interpret this as keeping a stable version of the variables, since mini-batches with noisy samples may produce large gradients pointing outside of the local optima. Another possible interpretation is that the learning rate was still too high. Unfortunately, we did not have the computational time to experiment with different learning rate schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Class Prior Estimation</head><p>In many computer vision tasks, the class prior probabilities are assumed to be the same for the training data and test data. In ExpertLifeCLEF, however, it is reasonable to assume that class priors change: The largest part of the training set comes from the web, where the class frequencies may not correspond with the test-time priors (dependening on the species incidence, the interest of users, etc.). The problem of adjusting CNN outputs to the change in class prior probabilities was discussed in <ref type="bibr" coords="4,246.34,502.19,9.96,8.74" target="#b5">[6]</ref>, where it was proposed to recompute the posterior probabilities (predictions) p(c k |x i ) by Equation <ref type="formula" coords="4,344.90,514.14,3.87,8.74" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="4,147.41,544.60,333.18,51.00">p e (c k |x i ) = p(c k |x i ) p e (c k )p(x i ) p(c k )p e (x i ) = p(c k |x i ) p e (c k ) p(c k ) K j=1 p(c j |x i ) p e (c j ) p(c j ) ∝ p(c k |x i ) p e (c k ) p(c k ) ,<label>(1)</label></formula><p>The subscript e denotes probabilities on the evaluation/test set. The posterior probabilities p(c k |x i ) are estimated by the Convolutional Neural Network outputs, since it was trained with the cross-entropy loss. For class priors p(c k ) we have an empirical observation -the class frequency in the training set. The evaluation/test set priors p e (c k ) are, however, unknown.</p><p>We follow the proposition from <ref type="bibr" coords="5,291.63,118.99,10.52,8.74" target="#b5">[6]</ref> to use an existing EM algorithm <ref type="bibr" coords="5,454.25,118.99,10.52,8.74" target="#b4">[5]</ref> for estimation of test set priors by maximization of the likelihood of the test observations. The E and M step are described by Equation <ref type="formula" coords="5,369.28,142.90,3.87,8.74" target="#formula_2">2</ref>, where the super-scripts (s) or (s + 1) denote the step of the EM algorithm.</p><formula xml:id="formula_1" coords="5,235.49,180.66,146.03,81.19">p (s) e (c k |x i ) = p(c k |x i ) p (s) e (c k ) p(c k ) K j=1 p(c j |x i ) p (s) e (c j ) p(c j ) , p<label>(s+1)</label></formula><p>e</p><formula xml:id="formula_2" coords="5,263.12,222.11,217.47,50.91">(c k ) = 1 N N i=1 p (s) e (c k |x i ),<label>(2)</label></formula><p>In our submissions, we estimated the class prior probabilities for the whole test set. However, one may also consider estimating different class priors for different locations, based on the GPS-coordinates of the observations. Moreover, as discussed in <ref type="bibr" coords="5,201.89,321.19,9.96,8.74" target="#b5">[6]</ref>, one may use this procedure even in the cases where the new test samples come sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>In the challenge, each team was allowed to submit up to 5 different run-files with predictions. We used this opportunity to evaluate the following 5 submissions: CMP Run 1 is an ensemble of 6 CNNs: #1 clean ,..,#6 clean described in Section 2.2. This submission used the automatic test set class-prior estimation from the CNN outputs, discussed in Section 2.4. CMP Run 2 was predicted by the ensemble from Run 1 without class prior estimation on the test data. CMP Run 3 is an ensemble of 12 CNNs: #1,..,#6 described in Section 2.1 and #1 clean ,..,#6 clean described in Section 2.2. This submission used the automatic test set class-prior estimation. CMP Run 4 is an ensemble of 6 CNNs: #1,..,#6 described in Section 2.1. This submission used the automatic test set class-prior estimation. CMP Run 5 is a single Inception-v4 model, denoted as CNN #4 clean , using the automatic test set class-prior estimation.</p><p>In all runs, the predictions (optionally improved by the class prior estimation) for all crops of the test image are averaged to compute the final image prediction. Moreover, for observations with several images (connected by the ObservationID values in the provided data), the final classification decision is taken based on the average of all corresponding image predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The official results of the challenge are displayed in Figure <ref type="figure" coords="6,382.38,413.02,3.87,8.74" target="#fig_2">2</ref>. Our system achieved the best results among automatic methods: 88.4% accuracy on the full test set. The best scoring submission was the largest ensemble -CMP Run 3 -using all 12 models. Results of all CMP submissions are listed in Table <ref type="table" coords="6,407.82,448.88,3.87,8.74" target="#tab_2">3</ref>.</p><p>When evaluated against human experts in plant sciences, the system (both CMP Run 3 and CMP Run 4) outperformed 5 of 9 tested human experts. That means that in the task of plant recognition from images, machine learning systems reached human expert performance -achieving better accuracy than the median of human experts. The detailed results are displayed in Figure <ref type="figure" coords="6,444.63,508.66,3.87,8.74" target="#fig_3">3</ref>.</p><p>Interestingly, while fine-tuning on "clean" data slightly improved the recognition accuracy on the full test set, it significantly decreased the accuracy on the test subset for human experts. Similarly, test-time prior estimation on the full test set noticeably improved the accuracy, but had an opposite effect on the subset. We assume that the test subset selected for human experts was too  small to provide a representative, identically distributed, sample of the full test set. Therefore the results on the test subset for human experts may be biased towards a small number of species contained in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The proposed machine-learning system for recognition of 10 000 plant species achieved an excellent accuracy of 88.4% in the ExpertLifeCLEF 2018 challenge, scoring 1st among automated systems. The ensemble of Convolutional Neural Networks benefited from the following improvements:</p><p>1. Adjusting the CNN predictions according to the estimated change of the class prior probabilities. 2. Replacing network parameters by their running averages with exponential decay. 3. Test-time data augmentation.</p><p>The experiment with human experts shows that machine learning reached the expert knowledge in plant recognition: our system scored better than an average (median) human expert in plant recognition, achieving better recognition rate than 5 of the 9 evaluated human experts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,441.01,172.43,39.58,8.74;2,151.70,184.38,328.89,8.74;2,151.70,196.34,328.89,8.74;2,151.70,208.29,108.65,8.74;2,140.99,223.02,339.60,8.77;2,151.70,235.01,328.89,8.74"><head></head><label></label><figDesc>challenge training-and test sets, covering only 717 of the 10k species. The remaining classes from the 2016 challenge do not exactly taxonomically match the 2017/2018 list of species. -ExpertLifeCLEF 2018 test set: 6 892 unlabeled images used for evaluation of the submitted methods. Examples from the set are displayed in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,172.05,443.74,271.26,7.89;2,275.08,353.80,65.20,65.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ExpertLifeCLEF 2018 test set -randomly selected samples.</figDesc><graphic coords="2,275.08,353.80,65.20,65.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,179.22,355.57,256.92,7.89;6,134.77,115.83,342.38,224.97"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results of runs submitted by the challenge participants.</figDesc><graphic coords="6,134.77,115.83,342.38,224.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,191.18,356.35,232.99,7.89;7,134.77,115.83,342.37,225.75"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results of the "Experts vs Machines" experiment.</figDesc><graphic coords="7,134.77,115.83,342.37,225.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,139.07,175.17,337.22,94.42"><head>Table 1 .</head><label>1</label><figDesc>Optimizer hyper-parameters, common to all networks in the experiments:</figDesc><table coords="3,221.76,195.57,168.77,74.02"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Optimizer</cell><cell>rmsprop</cell></row><row><cell>RMSProp momentum</cell><cell>0.9</cell></row><row><cell>RMSProp decay</cell><cell>0.9</cell></row><row><cell>Initial learning rate</cell><cell>0.01</cell></row><row><cell>Learning rate decay type</cell><cell>Exponential</cell></row><row><cell>Learning rate decay factor</cell><cell>0.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,150.91,540.41,310.46,94.41"><head>Table 2 .</head><label>2</label><figDesc>Networks and hyper-parameters used in the experiments:</figDesc><table coords="3,150.91,560.81,310.46,74.02"><row><cell># Net architecture</cell><cell cols="3">Batch size Input Resolution Random crop area</cell></row><row><cell>1 Inception-ResNet v2</cell><cell>32</cell><cell>299 × 299</cell><cell>5% -100%</cell></row><row><cell>2 Inception-ResNet v2</cell><cell>16</cell><cell>498 × 498</cell><cell>25% -100%</cell></row><row><cell>3 Inception-ResNet v2</cell><cell>16</cell><cell>498 × 498</cell><cell>5% -100%</cell></row><row><cell>4 Inception v4</cell><cell>32</cell><cell>299 × 299</cell><cell>5% -100%</cell></row><row><cell>5 Inception v4</cell><cell>32</cell><cell>598 × 598</cell><cell>5% -100%</cell></row><row><cell>6 Inception v4</cell><cell>32</cell><cell>299 × 299</cell><cell>50% -100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,599.84,345.83,61.54"><head>Table 3 .</head><label>3</label><figDesc>Results of CMP submissions on the full test set and its subset for human experts.</figDesc><table coords="6,154.00,631.19,304.29,30.18"><row><cell>CMP Run</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Accuracy (full test set)</cell><cell>86.8%</cell><cell>85.6%</cell><cell>88.4%</cell><cell>86.7%</cell><cell>83.2%</cell></row><row><cell>Accuracy (smaller test set)</cell><cell>76.0%</cell><cell>77.3%</cell><cell>82.7%</cell><cell>84.0%</cell><cell>77.3%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,657.44,84.73,7.47"><p>http://www.eol.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,144.73,646.48,296.56,7.47;3,144.73,657.44,47.07,7.47"><p>https://github.com/tensorflow/models/tree/master/research/slim# Pretrained</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>MS was supported by the <rs type="funder">CTU</rs> student grant <rs type="grantNumber">SGS17/185/OHK3/3T/13</rs> and by the <rs type="funder">Electrolux Student Support Programme</rs>. JM was supported by The <rs type="funder">Czech Science Foundation</rs> Project <rs type="grantNumber">GACR P103/12/G084 P103/12/G084</rs>. LP was supported by the <rs type="funder">UWB</rs> project No. <rs type="grantNumber">SGS-2016-039</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xuNPkqV">
					<idno type="grant-number">SGS17/185/OHK3/3T/13</idno>
				</org>
				<org type="funding" xml:id="_UeJzXN4">
					<idno type="grant-number">GACR P103/12/G084 P103/12/G084</idno>
				</org>
				<org type="funding" xml:id="_yxgnYk7">
					<idno type="grant-number">SGS-2016-039</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,234.24,342.24,7.86;8,146.91,245.20,147.90,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,275.24,234.24,201.26,7.86">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,160.99,245.20,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,256.16,342.24,7.86;8,146.91,267.12,333.68,7.86;8,146.91,278.08,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,280.41,256.16,200.18,7.86;8,146.91,267.12,183.56,7.86">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,360.34,267.12,120.25,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,289.04,342.24,7.86;8,146.91,300.00,333.68,7.86;8,146.91,310.96,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,276.56,289.04,204.03,7.86;8,146.91,300.00,206.32,7.86">Overview of expertlifeclef 2018: how far automated identification systems are from the best experts ?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,372.40,300.00,108.19,7.86">CLEF working notes 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,321.92,342.25,7.86;8,146.91,332.87,333.68,7.86;8,146.91,343.83,333.68,7.86;8,146.91,354.79,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,192.66,332.87,287.93,7.86;8,146.91,343.83,199.92,7.86">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.43,343.83,111.16,7.86">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,365.75,342.25,7.86;8,146.91,376.68,333.68,7.89;8,146.91,387.67,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,323.25,365.75,157.34,7.86;8,146.91,376.71,187.58,7.86">Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,342.81,376.71,81.66,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,398.63,342.24,7.86;8,146.91,409.59,133.43,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08235</idno>
		<title level="m" coord="8,229.72,398.63,221.54,7.86">Improving cnn classifiers by estimating test-time priors</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,138.35,420.55,342.24,7.86;8,146.91,431.48,333.68,8.14;8,146.91,443.11,84.73,7.47" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,297.31,420.55,183.28,7.86;8,146.91,431.50,135.23,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>CoRR abs/1602.07261</idno>
		<ptr target="http://arxiv.org/abs/1602.07261" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
