<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.29,116.98,274.78,12.60;1,204.79,134.92,205.79,12.60">Bird Identification from Timestamped, Geotagged Audio Recordings</title>
				<funder ref="#_jf4g5xN">
					<orgName type="full">Vienna Science and Technology Fund (WWTF)</orgName>
				</funder>
				<funder>
					<orgName type="full">&apos;Tic, SABIOD and EADM MaDICS</orgName>
				</funder>
				<funder ref="#_TSKAXXd">
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,280.34,172.50,54.68,8.80"><forename type="first">Jan</forename><surname>Schl√ºter</surname></persName>
							<email>jan.schlueter@ofai.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Austrian Research Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.29,116.98,274.78,12.60;1,204.79,134.92,205.79,12.60">Bird Identification from Timestamped, Geotagged Audio Recordings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA7A09CD9CFFFBE05E6142DE2BBBEB40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bioacoustics</term>
					<term>Bird vocalizations</term>
					<term>BirdCLEF 2018</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Spectrograms</term>
					<term>Metadata</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale biodiversity monitoring would profit from automated solutions supporting or complementing human experts and citizen scientists. To foster their development, the yearly BirdCLEF scientific challenge compares approaches for identifying bird species in recorded vocalizations. The solution described in this work is based on an ensemble of Convolutional Neural Networks (CNNs) processing a mel spectrogram combined with Multi-Layer Perceptrons (MLPs) processing the recording date, time and geographic location. In BirdCLEF 2018, it achieved a mean average precision of 0.705 in detecting 1,500 South American bird species (0.785 for the foreground species), the second best entry to the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Biodiversity monitoring is important to assess the impact of human actions on other species, and inform decisions on short-term projects and long-term policies. However, manually observing, classifying and counting animals is limited in scale and scope by the supply of experts, the costs of human labor and basic human needs. Automated or semi-automated approaches for recording and analyzing observations would allow monitoring more locations, over longer time spans and in forbidding terrain.</p><p>For many animals, acoustic monitoring is an interesting option -it allows to detect and classify individuals even when they are difficult to see, which is precisely the reason for some of their vocalizations. Acoustic recording devices paired with automatic classifiers could support both experts in the field in identifying species, as well as enable passive acoustic monitoring.</p><p>The CLEF (Conference and Labs of the Evaluation Forum) initiative fosters the development of automatic classifiers for the case of bird vocalizations through the yearly BirdCLEF challenge <ref type="bibr" coords="1,278.83,633.14,14.86,8.80" target="#b13">[13,</ref><ref type="bibr" coords="1,293.69,633.14,7.43,8.80" target="#b8">8]</ref>. In 2018, it provided 36,496 recordings of 1,500 South American bird species to develop classifiers on, and challenged participants in two tasks: <ref type="bibr" coords="1,251.69,657.05,12.73,8.80" target="#b0">(1)</ref> Recognizing species in 12,347 recordings mostly captured with monodirectional microphones aimed at individual birds, and (2) recognizing species at 5-second intervals in 51 long-term recordings taken with multidirectional recording devices.</p><p>In the following, I describe my submission to BirdCLEF 2018, which reached the second place among six participating teams. Section 2 details the layout and training of a Convolutional Neural Network (CNN) processing the audio signals, Section 3 describes a Multilayer Perceptron (MLP) predicting the species from the metadata of each recording, and Section 4 explains how the predictions of multiple audio and metadata networks are combined into a final result. Section 5 compares results on an internal validation set and the official test sets. Finally, Section 6 discusses ideas that did not turn out successful, and Section 7 concludes with a summary and an outlook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Predictions from Audio</head><p>The foundation of my submission is an ensemble of Convolutional Neural Networks (CNNs) that process a mel spectrogram to produce occurrence probabilities for each of the 1500 possible bird species. In the following, I describe how the audio signal is preprocessed to be consumed by the network, what network architectures were chosen, how the networks were trained, and how they are applied at test time. Both for the architecture and for training I used multiple variants which were later combined in an ensemble (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spectrogram Extraction</head><p>Adopted from <ref type="bibr" coords="2,198.83,428.20,14.87,8.80" target="#b22">[21,</ref><ref type="bibr" coords="2,213.70,428.20,7.43,8.80" target="#b9">9]</ref>, I first resample the audio signal to <ref type="bibr" coords="2,382.43,428.20,9.08,8.80" target="#b23">22</ref>.05 kHz sample rate, compute an STFT with a Hann window of 1024 samples and hop size of 315 samples (obtaining 70 frames per second), retain only the magnitudes, and apply a mel filterbank of 80 triangular filters from 27.5 Hz to 10 kHz. These settings were chosen as a starting point since they had proved to work well for bird detection, and were not varied much during the experiments: reducing the upper bound to 8 kHz diminished results, increasing it to 11 kHz did not have a significant effect, a window size of 2048 samples (for better low-frequency resolution) did not help. Note that the magnitudes are linear; they will be compressed in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>The network architecture is a CNN based on my submission to a bird detection challenge (submission sparrow in <ref type="bibr" coords="2,277.40,585.32,10.30,8.80" target="#b9">[9]</ref>), but modified to handle classification rather than detection, and varied in different ways both in search of a better solution and to obtain multiple variants for an ensemble.</p><p>Figure <ref type="figure" coords="2,180.82,621.19,4.98,8.80" target="#fig_0">1</ref> gives an overview of the architecture variants. The overarching idea is to have a fully-convolutional network -i.e., a network with only convolutional and pooling layers, no fixed-size fully-connected layers -process an input spectrogram into a sequence of local predictions for each bird species, followed by a global pooling strategy that produces a single prediction per species for the full recording. This type of architecture can be directly trained and tested on recordings of arbitrary length and does not require a hand-chosen averaging or majority voting procedure to produce recording-wise predictions at test time.</p><p>In the following, I will describe a common preprocessing frontend as well as the local prediction and global prediction stages in detail.</p><p>Frontend: The first step is a learned nonlinear magnitude transformation:</p><formula xml:id="formula_0" coords="3,213.58,311.39,267.01,10.81">y = x œÉ(a) , where œÉ(a) = 1/ (1 + exp(-a))<label>(1)</label></formula><p>It is applied elementwise to each time-frequency bin x, enhancing low magnitudes similar to the more common application of a logarithmic function. a is initialized to zero (so y = ‚àö x) and learned as part of the network, shared over all mel bands, typically reaching values between -1.2 and -1.7 (yielding y = x 0.23 to y = x 0.15 ). This is followed by batch normalization <ref type="bibr" coords="3,323.14,379.23,15.50,8.80" target="#b12">[12]</ref> with means and standard deviations shared over all time steps, but separated per mel band, and without learned scale and shift (Œ≥ and Œ≤ in <ref type="bibr" coords="3,248.47,403.14,14.76,8.80" target="#b12">[12]</ref>). At test time, this is comparable to standardizing each mel band to zero mean and unit standard deviation over the training set (as in <ref type="bibr" coords="3,163.04,427.05,14.76,8.80" target="#b22">[21]</ref>), but at training time, it adds seemingly helpful noise through computing the statistics for the current minibatch only, and it dynamically adapts the standardization to any changes in the learned magnitude transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local predictions:</head><p>The frontend is followed by one of two variants for computing a sequence of local predictions from the input spectrogram.</p><p>A) The first variant is based on submission sparrow in <ref type="bibr" coords="3,390.30,501.64,9.96,8.80" target="#b9">[9]</ref>, but enlarged and adapted to perform classification rather than detection. It starts with two convolutional layers of 64 3√ó3 units each, followed by non-overlapping max-pooling of 3√ó3, and two more convolutional layers of 128 3√ó3 units each. All convolutions are unpadded ("valid"). At this point, the temporal resolution has been reduced to a third, and there are 21 frequency bands remaining. I apply a convolutional layer of 128 3√ó17 units, leaving 5 frequency bands, followed by 3√ó5 max-pooling, squashing all frequency bands. This combination of convolution and pooling is meant to fuse information over all frequency bands while introducing some pitch invariance, and has proven helpful in <ref type="bibr" coords="3,299.37,609.23,14.87,8.80" target="#b21">[20,</ref><ref type="bibr" coords="3,314.24,609.23,7.43,8.80" target="#b9">9]</ref>. This is followed by a convolutional layer of 1024 9√ó1 units, fusing information over 103 original spectrogram frames (1.5 s), finally followed by 1024 1√ó1 and 1500 1√ó1 units for classification (the last three layers resemble what would be fully-connected layers in a fixed-input-size CNN, but here they apply to an arbitrarily long input sequence).</p><p>All layers except the last one are followed by batch normalization (the usual variant with statistics shared over all spatial locations, but separated per channel, and with learned scale and shift) and leaky rectification as max(x/100, x). The last three layers are preceded by 50% dropout <ref type="bibr" coords="4,371.50,155.80,14.61,8.80" target="#b11">[11]</ref>. The very last layer neither has batch normalization nor a nonlinearity. While its 1500 features will lead to predictions for the 1500 bird species, the nonlinearity will be applied after global pooling, for reasons discussed below.</p><p>B) As a variation on this architecture, I replaced the first four convolutional layers with two residual blocks with pre-activation following <ref type="bibr" coords="4,400.41,215.57,15.50,8.80" target="#b10">[10,</ref><ref type="bibr" coords="4,419.30,215.57,28.43,8.80">Fig. 1b</ref>]. Each residual block consists of four 3√ó3 convolutional layers, each of which is preceded by batch normalization and linear rectification as max(x, 0) (except for the initial convolution following the preprocessing frontend, where pre-activation would be redundant). Convolution inputs are padded with a leading and trailing row and column of zeros so the output size matches the input size. Skip connections add the input of each pair of two convolutions to its output. If needed, the skip connection includes a 1 √ó 1 convolutional layer to adjust the number of channels. The first residual block has 64 channels throughout and is followed by 3√ó3 max-pooling, the second residual block has 128 channels and is followed by batch normalization, linear rectification and an unpadded convolutional layer of 128 units leaving 5 frequency bands (compared to the first CNN variant, this requires 3√ó22 units instead of 3√ó17 units since the convolutions are padded). 3√ó5 max-pooling and remaining layers are adopted from the first CNN variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global predictions:</head><p>The previous stage produces a sequence of vectors of size 1500, corresponding to the 1500 possible bird species. Specifically, since the network includes two pooling operations over 3 frames each, it produces a local prediction every 9 frames,<ref type="foot" coords="4,247.84,434.73,3.97,6.16" target="#foot_0">1</ref> for the part of the input spectrogram exceeding the length of the network's receptive field (103 frames for the first, 119 for the second variant) minus padding (none for the first, 32 frames for the second variant).</p><p>A) Treating this as a multiple-instance learning (MIL) problem [6,1] -for each recording and species, the global occurrence label should be positive if at least one of the local occurrence labels is positive -, the natural choice is to form a global prediction by taking the maximum over time for each species. However, this leads to very sparse gradients. Instead, I use log-mean-exp pooling <ref type="bibr" coords="4,134.77,531.93,15.50,8.80" target="#b20">[19,</ref><ref type="bibr" coords="4,153.58,531.93,25.15,8.80">Eq. 6]</ref>:</p><formula xml:id="formula_1" coords="4,226.11,543.28,254.48,30.20">lme(y; a) = 1 a log 1 T T -1 t=0 exp(a ‚Ä¢ y t )<label>(2)</label></formula><p>Here, y denotes the time series of local predictions (y 0 , y 1 , . . . , y T -1 ) for a single species, and a is a hyperparameter controlling the behaviour of the function: for a ‚Üí ‚àû, it approximates the maximum, for a ‚Üí 0, it approximates the mean. I set a = 1 (a = 0.2 or a = 5 work just as well, I did not explore it further).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B)</head><p>As an alternative to a fixed pooling strategy that prefers larger inputs, I explored pooling with temporal attention. I let the network produce a weight Œ± t for each time step, and compute a weighted sum:</p><formula xml:id="formula_2" coords="5,247.85,161.88,232.74,30.20">att(y, Œ±) = T -1 t=0 Œ± t y t = Œ± T y (3)</formula><p>The Œ± t are shared over all species, and Œ± is produced with multi-head attention: I extend the last convolutional layer of the local prediction stage by K units, such that it produces 1500 + K time series. I split off the K additional time series C and apply a softmax over time to each one:</p><formula xml:id="formula_3" coords="5,239.71,253.88,240.88,30.32">B k,t = exp(C k,t )/ T -1 i=0 exp(C k,i )<label>(4)</label></formula><p>Finally, I average the K soft-maxed time series B to obtain Œ±:</p><formula xml:id="formula_4" coords="5,271.05,310.13,209.55,30.55">Œ± t = 1 K K-1 k=0 B k,t<label>(5)</label></formula><p>By construction, Œ± sums to 1.0 regardless of the sequence length. Using multiple heads (I chose K = 16 or K = 64) makes it easier for the network to produce attention series with multiple local maxima. Note that there is no additional supervision needed to learn what to attend to; the global loss will be backpropagated through (3), ( <ref type="formula" coords="5,227.21,396.39,3.87,8.80" target="#formula_4">5</ref>), ( <ref type="formula" coords="5,246.37,396.39,4.24,8.80" target="#formula_3">4</ref>) to reach the last layer of the local prediction stage.</p><p>For both log-mean-exp pooling and attentional pooling, we now end up with a single vector of 1500 values. Assuming that we want to predict the occurrence of each species independently, the natural choice would be to apply the logistic sigmoid function œÉ(x) = 1/(1 + exp(-x)) to squash each value into the range (0, 1) and treat it as a binary prediction. However, since the training examples strongly focus on capturing a single prominent species per recording, it turns out to work slightly better to treat this as a categorical classification problem, so I apply the softmax function to produce a distribution over 1500 classes instead.</p><p>Note that when planning to train with categorical cross-entropy loss, it is crucial to apply the softmax after the global temporal pooling operation, otherwise the competition between classes is introduced at the wrong level. Considering the case of global temporal max pooling, having the softmax last ensures the loss gradient reaches the frame of maximal activation for each species. Having the softmax first (i.e., on each local prediction) would propagate the gradient to the frame where the correct species is most dominant over the others, and then push down competing species for that frame only. Similar considerations apply to log-mean-exp pooling and attentional pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>To deal with the fact that training recordings are usually a few minutes long, but only sparsely populated with vocalizations of the target bird (without any information on where in a recording the target bird is audible), several authors employed hand-designed methods to find potential bird calls and trained on these passages only <ref type="bibr" coords="6,221.00,143.84,15.84,8.80" target="#b18">[17,</ref><ref type="bibr" coords="6,236.84,143.84,11.88,8.80" target="#b24">23,</ref><ref type="bibr" coords="6,248.72,143.84,11.88,8.80" target="#b14">14,</ref><ref type="bibr" coords="6,260.60,143.84,7.92,8.80" target="#b5">5]</ref>. Here I take a different route: I randomly choose extracts long enough to hopefully contain at least one vocalization of every annotated bird for a recording, and directly train on those, relying on the global pooling operation to distribute the gradient to the relevant local predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization:</head><p>The network is trained on mini-batches of 16 excerpts of up to 30 seconds to minimize categorical cross-entropy between network output and targets (see below for three variants of forming targets). 10% of the recordings are reserved as a validation set, selected to contain at least one recording per species (the same split as <ref type="bibr" coords="6,252.64,255.42,15.50,8.80" target="#b14">[14]</ref> through personal communication). Weights are updated with ADAM <ref type="bibr" coords="6,233.56,267.37,14.61,8.80" target="#b15">[15]</ref>, with an initial learning rate of 0.001 dropped to a tenth whenever the validation loss did not improve for 10 consecutive miniepochs of 1000 updates each. Training is stopped at the third drop. The learning rate for the magnitude compression parameter a (Equation <ref type="formula" coords="6,398.49,303.24,4.43,8.80" target="#formula_0">1</ref>) is kept 10 times as high throughout training, otherwise it learns too slowly.</p><p>Loss and targets: Optimization minimizes the cross-entropy loss between predictions y and targets t:</p><formula xml:id="formula_5" coords="6,253.25,373.30,227.34,30.20">ce(y, t) = - S-1 s=0 t s log(y s )<label>(6)</label></formula><p>I employ three different variants for defining the target vector t for a recording.</p><p>A) The most obvious choice for a categorical classification task is to set t s to 1 for the annotated foreground species for the recording, and to 0 otherwise. Denoting the foreground species as S f , we have:</p><formula xml:id="formula_6" coords="6,264.51,469.42,216.09,23.14">t s = 1 if s = S f 0 otherwise<label>(7)</label></formula><p>B) To factor in the annotated background species, I set t s to 2 for the foreground species, to 1 for any of the background species, and to 0 otherwise. Denoting the set of background species as S b , we have:</p><formula xml:id="formula_7" coords="6,264.09,545.98,216.50,40.47">t s = Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ 2 if s = S f 1 if s ‚àà S b 0 otherwise (8)</formula><p>Obviously, the network can never reach this target. However, as categorical crossentropy (Equation <ref type="formula" coords="6,219.87,609.23,4.43,8.80" target="#formula_5">6</ref>) is linear in the targets, this is equivalent to adding the cross-entropy losses against each background species and the foreground species, weighting the latter twice. Furthermore, since ADAM is invariant to the scale of the gradient, it is unimportant if we set the foreground target to 2 and background to 1, or the foreground to 1 and background to 0.5, for example. C) As a third variant, I train a network as a Born-Again Network (BAN), following <ref type="bibr" coords="7,177.83,131.89,9.96,8.80" target="#b7">[7]</ref>. Using a trained model, I compute predictions ≈∑ for the excerpt encountered during training, and set the target as follows:</p><formula xml:id="formula_8" coords="7,254.22,164.37,226.37,23.14">t s = ≈∑t + 1 if s = S f ≈∑t otherwise<label>(9)</label></formula><p>Again, note that this is equivalent to adding the loss against the target of variant A to the loss against ≈∑ (or averaging the two losses, if training with ADAM).</p><p>Even if the model computing ≈∑ has the same architecture as the new model to be trained, this provides a helpful regularization, leading to faster convergence and possibly improved generalization (or an additional model for ensembling).</p><p>Of course the same modification is possible for variant B (omitted for brevity).</p><p>Length bucketing: While aiming to train on 30-second excerpts (assuming that even for long recordings, a random 30-second excerpt will often contain vocalizations of all annotated birds), about 60% of the recordings are actually shorter than 30 seconds. For those, we can just take the full recording. To avoid wasted computation, I ensure to collect mini-batches of similarly-sized recordings: All recordings between 3 s and 30 s length are ordered by length and split into 8 same-sized buckets; recordings shorter than 3 s and longer than 30 s are sorted into 2 additional buckets. When preparing a mini-batch, a random bucket is chosen with probability proportional to its size, and 16 recordings within are selected uniformly at random. Recordings shorter than 3 s are looped to reach 3 s, recordings longer than 30 s are randomly cropped to 30 s, other recordings are randomly cropped to match the length of the shortest among the 16. This produces mini-batches of same-sized excerpts without excessive cropping or padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline:</head><p>Before training, all recordings are transformed to monophonic signals at <ref type="bibr" coords="7,146.58,468.59,9.08,8.80" target="#b23">22</ref>.05 kHz sample rate and written to fast persistent storage. During training, multiple background threads read excerpts of these signals and apply an STFT to prepare mini-batches of magnitude spectrograms. The mel filterbank is applied on GPU as part of the network. Mel spectrograms are not precomputed before training to enable easy experimentation with different mel spectrogram settings (and learned mel filterbanks). STFTs are not precomputed because that would increase I/O costs (for the settings in Section 2.1, a minute-long magnitude spectrogram in 32-bit floatingpoint precision takes 8.2 MiB, while a same-length 22.05 kHz signal in 16-bit integer precision takes 2.5 MiB), cancelling out savings in computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Testing</head><p>Since the network can process input of arbitrary size into a vector of 1500 class probabilities, we can directly apply it to full test recordings for the first subtask of the challenge (monodirectional recordings), and to 5-second excerpts for the second subtask (long-term multidirectional recordings at 5-second intervals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Predictions from Metadata</head><p>Not all 1500 bird species of the challenge are likely to occur across the whole South American continent. Similarly, some migrating birds are not likely to occur in specific seasons, and some species are more likely to be heard during dawn than at noon. To learn about and use these dependencies, I train additional networks on the metadata supplied with the training recordings, which is also available for the test recordings: the date, time, geocoordinates, and elevation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Encoding</head><p>As a first step, I cleaned up and unified the different formats for the time (e.g., "14:20", "11am", "morning", "PM") and elevation (e.g., "1400", "300-400", "1.270m", "to 1100 m") data, both of which seem to be drawn from freeform text fields. Date and geocoordinates were already unified, but not all fields are filled for each recording.</p><p>To process the metadata with a neural network, I encode it as a fixed-size vector. The numeric values for longitude, latitude and elevation are used directly. The date is converted to the day of the year, and the time to the minute of the day. To avoid a discontinuity from December 31, to January, 1 and from 23:59 to 0:00, I employ a K-dimensional circular encoding of these values using phaseshifted sinusoid functions:</p><formula xml:id="formula_9" coords="8,245.08,373.84,231.08,17.39">dk = ‚àö 2 sin(2œÄd/D + œÄk/K), (<label>10</label></formula><formula xml:id="formula_10" coords="8,476.16,382.43,4.43,8.80">)</formula><p>where d is the value to encode (e.g., the minute of the day), D is the number of possible values (e.g., 1440), and d is the resulting K-dimensional encoding of d. I experimented with two settings: K = 12 and K = 3. Missing metadata fields are represented as zeros, and 5 additional binary values indicate which fields are valid. In total, this results in a 14-dimensional vector for K = 3. As a final step, all values are standardized to zero mean and unit variance using statistics over the training set (except for the circular-encoded values, which are approximately standardized due to the ‚àö 2 factor in Equation 10, assuming a uniform distribution of days of the year and minutes of the day).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>The network architecture for this part is very simple: The 14-dimensional input vector is processed by two fully-connected hidden layers of 256 and 512 leaky rectified units each, and finally classified by a fully-connected output layer of 1500 softmax units. Dropout or batch normalization were not helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Optimization uses the same settings as for the audio network, except that monitoring for early stopping uses the mean average precision (MAP, see Section 5) on the validation set rather than the cross-entropy loss, and mini-epochs of 5000 instead of 1000 updates.</p><p>Training targets are chosen according to strategy A or B of Section 2.3, with strategy B turning out to be more useful for ensembling.</p><p>To obtain additional variations for ensembling, I varied training in two ways: A) I added Gaussian noise to the metadata values (before circular encoding, if applicable), either with small standard deviations (3 days, 3 minutes, 5 meters, 0.3 degrees), medium standard deviations (7 days, 10 minutes, 20 meters, 1 degree) or large standard deviations (14 days, 30 minutes, 100 meters, 3 degrees). B) I trained networks on a subset of metadata fields, either leaving out one of the fields or all but one field.</p><p>As an additional regularization, I experimented with randomly dropping single fields (handling them like missing fields, i.e., replacing with zeros and flipping the validity indicator variable), but this only worsened results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensembling of Predictions</head><p>Both to combine audio-based and metadata-based predictions and to benefit from multiple variations among the audio-based and metadata-based models, it is crucial to be able to merge predictions of multiple models.</p><p>In its simplest form, predictions by multiple models for the same recording can be merged by averaging them. There are different stages at which this could happen; empirically, it worked best to average predictions after global temporal pooling, but before applying the softmax output nonlinarity.</p><p>Averaging is sufficient for few similar models, but not optimal for larger sets of diverse models, or for combining audio-based and metadata-based predictions: in these cases, it pays off to weight the predictions of each model differently. To automate the choice of weights, I employ hyperopt <ref type="bibr" coords="9,384.76,420.95,9.96,8.80" target="#b1">[2]</ref>, a general-purpose hyperparameter optimization tool. Given a set of models, I use it to search a weight in [0, 1) for each model that optimizes the ensemble's mean average precision (MAP) on the validation set (specifically, the average of the MAPs for the foreground and background species; see the next section for details on the evaluation). When the set of models becomes large, hyperopt can also be used to choose which models to include in the ensemble at all. To support this, I extend the search space by a binary choice for each weight, allowing hyperopt to more easily set it to zero. While this strategy greatly helps in finding a good ensemble, of course it bears the risk of over-optimizing towards the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Challenge submissions are evaluated in terms of mean average precision (MAP), both when using only the annotated foreground species as ground truth and when including the background species as additional correct classes. Given class probabilities y and a set of annotated species S for a recording, the average precision is computed as ap(y, S) = 1 |S| s‚ààS p(y, S, rank(y, s)), <ref type="bibr" coords="9,462.88,648.61,17.71,8.80" target="#b11">(11)</ref> where rank(y, s) is the number of values in y less than or equal to y s :</p><formula xml:id="formula_11" coords="10,252.08,141.71,228.51,19.97">rank(y, s) = i [y i ‚â§ y s ] ,<label>(12)</label></formula><p>and p(y, S, k) is the precision at k:</p><formula xml:id="formula_12" coords="10,233.21,188.36,242.95,26.86">p(y, S, k) = 1 k s‚ààS [rank(y, s) ‚â§ k] . (<label>13</label></formula><formula xml:id="formula_13" coords="10,476.16,195.10,4.43,8.80">)</formula><p>The mean average precision is the mean of the average precisions of all recordings. Note that when |S| = 1 (e.g., when only evaluating against foreground species), the average precision is equal to the reciprocal rank of the correct species, as easy to see when inserting ( <ref type="formula" coords="10,256.58,260.09,8.86,8.80" target="#formula_12">13</ref>) into <ref type="bibr" coords="10,293.39,260.09,16.38,8.80" target="#b11">(11)</ref>.</p><p>As an additional evaluation measure, I use the classification accuracy with respect to the foreground species, and the top-5 classification accuracy (which treats a prediction as correctly classified when the foreground species is among the 5 largest class probabilities for a recording).</p><p>Table <ref type="table" coords="10,177.24,319.87,4.98,8.80">1</ref> lists validation results for all candidate model variants partaking in the ensemble search. In addition, it shows the ensembling weights, validation and official test results for three ensembles formed from the candidate models. We can draw several interesting insights from these results:</p><p>-All else kept equal, the residual network outperforms the plain CNN (variants A and B in Section 2.2, "Local predictions"). It still pays off to combine them in an ensemble. -All else kept equal, log-mean-exp pooling tends to outperform temporal attention (variants A and B in Section 2.2, "Global predictions"). Again, it still pays off to combine them in an ensemble. -All else kept equal, including background species in the targets tends to decrease the foreground MAP against foreground species (and the classification accuracy), while increasing the MAP against background species (variants A and B in Section 2.3, "Loss and targets"). -All else kept equal, retraining a model as a Born-Again Network (BAN) tends to improve its performance (variant C in Section 2.3, "Loss and targets"). -The metadata alone suffices to classify 21% of validation recordings correctly.</p><p>-Including all metadata fields gives better performance than omitting some.</p><p>-When training on a single metadata field, it seems location is the most informative cue. -There is no systematic difference between 12-dimensional and 3-dimensional date/time encodings (but there is only a single pair of results with all else kept equal). -Adding Gaussian noise to metadata diminishes results. This might indicate that the models learn to exploit unwanted correlations between training and validation data, such as recordings done on the same day at the same location, rather than learning robust occurrence patterns. However, when ensembling with audio models, blurred (noisy) metadata is superior to unmodified data.</p><p>Table <ref type="table" coords="11,163.69,117.28,4.13,8.02">1</ref>. Results for several model variants on the validation set (columns "MAP-fg" to "Top-5"), both for audio (upper part) and metadata networks (lower part). The last three columns show model selections for three ensembles, with corresponding weights. The last four rows display ensemble results on the validation set and official test set.</p><p>The official submissions are based on the three ensembles shown in the table. Specifically, I created four submissions for the monodirectional task: For the multidirectional task, I submitted predictions using Ensembles A, B and C applied to 5-second excerpts (Run 3 was not included since there is no information to use for filtering). They achieved c-MAP<ref type="foot" coords="12,378.55,396.45,3.97,6.16" target="#foot_1">2</ref> scores of 0.113, 0.146 and 0.097, respectively. Note that for both tasks, ensemble A outperformed ensemble C, while the latter was superior on the internal validation set. This might indicate that the validation set results are not a reliable indicator for choosing models after all. <ref type="foot" coords="12,473.94,444.27,3.97,6.16" target="#foot_2">3</ref></p><formula xml:id="formula_14" coords="12,134.77,151.44,93.70,8.80">Run 1: Ensemble A,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Dead Ends</head><p>During experimentation, I tried several ideas that did not turn out to be helpful in the end, and were not discussed above. I will describe a few in the following.  <ref type="bibr" coords="13,337.11,227.94,11.15,8.80" target="#b4">[4,</ref><ref type="bibr" coords="13,348.25,227.94,11.15,8.80" target="#b16">16]</ref>. Some even consider this to be a design dimension on the same level as network depth and width, under the term multiplicity <ref type="bibr" coords="13,245.47,251.85,15.50,8.80" target="#b25">[24]</ref> or cardinality <ref type="bibr" coords="13,325.83,251.85,14.61,8.80" target="#b27">[26]</ref>.</p><p>An easy way to implement such an architecture is to use grouped convolutions -a variation of convolutional layers that divides the input channels into non-overlapping groups to be processed separately. Multiple such layers with the same number of groups result in separated deep computation paths.</p><p>Even with only two groups, this can promote the development of different feature sets (e.g., edge and color features in AlexNet <ref type="bibr" coords="13,387.95,323.79,15.50,8.80" target="#b16">[16,</ref><ref type="bibr" coords="13,407.20,323.79,27.96,8.80">Fig. 3]</ref>). I experimented with different numbers of groups, keeping either the total number of feature maps or total number of parameters constant, but a single group always performed best. I also tried combining this with grouped dropout, i.e., randomly dropping complete groups at training time, to promote independence between computation paths, but to no avail. Per-Channel Energy Normalization (PCEN): As an alternative to the basic learned magnitude transformation in Equation <ref type="formula" coords="13,375.26,407.69,3.87,8.80" target="#formula_0">1</ref>, I tried PCEN as proposed in <ref type="bibr" coords="13,191.05,419.64,14.61,8.80" target="#b26">[25]</ref>. It also includes a learned root compression, but with separate exponents per mel band, and in combination with an automatic gain control meant to improve robustness to different recording conditions. However, it worked significantly worse on the internal validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Combining an ensemble of audio networks that individually achieve a Mean Average Precision (MAP) score of up to 0.701 with an ensemble of metadata networks that achieve up to 0.277, it is possible to reach a MAP score of 0.809 (for detecting the foreground species in an internal validation set split off from the BirdCLEF 2018 training data). Compared to an audio-only ensemble, including the metadata improved scores by 3 to 5 percent points and helped securing the second place in the official challenge results.</p><p>Results could be improved in a number of ways. For one, I hardly varied the spectrogram settings -especially the sampling rate of 22.05 kHz (and upper frequency bound of 10 kHz for the mel filterbank) may be a suboptimal choice. While this was enough to win a bird detection challenge <ref type="bibr" coords="13,392.16,633.14,9.96,8.80" target="#b9">[9]</ref>, it discards a lot of information from the original 44.1 kHz or 48 kHz signals. Secondly, seeing the improvement from going from a plain CNN to a residual network, there probably still is a lot to gain from larger networks or more modern design strategies such as dual-path networks <ref type="bibr" coords="14,232.96,131.89,9.96,8.80" target="#b2">[3]</ref>. Thirdly, for convenience, I used a single validation set both for early stopping during training and for model selection and weighting for ensembling. This bears a strong risk of overfitting to the validation set, and means a subset of recordings will never be used for training. Furthermore, I used the same validation set as <ref type="bibr" coords="14,289.85,179.71,15.50,8.80" target="#b14">[14]</ref> (to be able to compare results during development), which was designed to contain at least one recording per species and otherwise picked randomly. This does not match the distribution of the test set and disregards the fact that some recordists upload several recordings from the same day and location, which may partly explain the high performance of the metadata-only networks (especially when not corrupting metadata with slight noise). Using cross-validation with more careful splits as <ref type="bibr" coords="14,422.99,251.44,15.50,8.80" target="#b19">[18,</ref><ref type="bibr" coords="14,443.21,251.44,37.38,8.80">Sec. 2.2]</ref> would lead to more informed model selection and more diverse model variants for ensembling (trained on different sets of recordings). Finally, it is surprising that data augmentation did not help in the variants I tried; a further exploration of augmentation methods may improve generalization.</p><p>When judging the relevance of results for practical applications, it is important to distinguish the two subtasks. The MAP score used for the first subtask is purely rank-based (Equation <ref type="formula" coords="14,272.19,335.12,8.30,8.80">11</ref>): it rewards models giving the correct species higher scores than incorrect ones, but does not require a clear decision on which species occur in a recording. Furthermore, as the test set only contains files that include at least one of the possible 1500 bird species, models will never need to decide whether a file contains a bird (or a known bird) at all. Finally, the score takes an average over all test recordings, so species with more recordings will be given higher importance than rare species. Thus, the first subtask only captures the setting of an expert in the field interested in classification suggestions for a particular vocalization, not a passive acoustic monitoring scenario. The c-MAP score for the second subtask weights all species equally, and rewards models giving segments that contain a particular species higher scores than those that do not contain that species. This is closer to the requirements for passive monitoring, and still has a lot of room for improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,207.04,188.32,201.27,8.02"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Outline of the audio network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,134.77,151.44,345.83,211.82"><head></head><label></label><figDesc>an ensemble of only audio-processing networks Run 2: Ensemble B, an ensemble of audio and metadata networks Run 3: Ensemble B, but with an additional filter. The metadata of each training and test recording includes the year the respective file was added to BirdCLEF (2014, 2015 or 2017). Since past editions used a smaller set of classes, this information can be used to exclude particular species for 2014 and 2015, by forcibly setting their probabilities to zero. While this is only reliable for the foreground species, it noticeably improves results: Compared to Run 2, MAP-fg raises from 0.752 to 0.785, and MAP-fg from 0.692 to 0.705. The same technique was used by Runs 2-4 of the leading team in BirdCLEF 2018 (personal communication). Run 4: Ensemble C, the best combination of a single audio and metadata network, but combined into a joint model and carefully fine-tuned (with initial learning rate 0.00001, halved whenever the validation error did not improve for 10 consecutive mini-epochs of 1000 updates, and trained until the third learning rate drop). Fine-tuning slightly improved validation results from a MAP-fg of 0.764 to 0.768 and a MAP-bg of 0.693 to 0.698. Test set results reported in Table 1 are for the fine-tuned model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,134.77,534.46,345.83,68.57"><head>mixup:</head><label></label><figDesc>Several authors augmented training examples by mixing them with noise, based on a hand-designed approach for segmenting recordings into passages containing birds and passages only containing background noise<ref type="bibr" coords="12,151.70,570.33,15.50,8.80" target="#b24">[23,</ref><ref type="bibr" coords="12,167.20,570.33,11.62,8.80" target="#b14">14,</ref><ref type="bibr" coords="12,178.82,570.33,7.75,8.80" target="#b5">5]</ref>. When such a separation stage is not available, an alternative is to mix training examples regardless of bird vocalizations and update the labels accordingly. An interesting proposal in this regard is mixup<ref type="bibr" coords="12,422.24,594.24,14.61,8.80" target="#b28">[27]</ref>: Produce weighted combinations of training examples paired with weighted combinations of their target vectors. While this seems to be especially well-suited to audio recordings, in my experiments, it slowed down convergence and lead to worse results. Several authors proposed to design a network architecture to have multiple separate computation paths processing the same input that are merged in the end</figDesc><table coords="13,134.77,167.96,345.83,44.87"><row><cell>Data augmentation: While data augmentation is often reported to be helpful</cell></row><row><cell>for audio classification (using transformations such as time stretching, pitch</cell></row><row><cell>shifting or equalization), in my experiments, it only diminished results.</cell></row><row><cell>Grouped convolution/dropout:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,635.82,335.87,7.92;4,144.73,646.78,335.86,7.92;4,144.73,657.74,122.80,7.92"><p>It would be possible to obtain a prediction for every single frame by introducing dilated convolutions and pooling as described in<ref type="bibr" coords="4,331.18,646.78,13.52,7.92" target="#b23">[22]</ref>, but this increases computational costs without a strong benefit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="12,144.73,613.91,335.86,7.92;12,144.73,624.86,270.22,7.92"><p>c-MAP computes the average precision per species over all segments, rather than per segment over all species, and then takes the mean over species.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="12,144.73,635.82,335.86,7.92;12,144.73,646.78,335.86,7.92;12,144.73,657.74,256.20,7.92"><p>On a related note, beware of trying to interpret the ensemble weights -they are not indicative of which models are optimal; there are wildly different model selections and weights that perform about the same on the validation set.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>I would like to thank <rs type="person">Herv√© Glotin</rs>, <rs type="person">Herv√© Go√´au</rs>, <rs type="person">Willem-Pier Vellinga</rs>, and <rs type="person">Alexis Joly</rs> for organizing this challenge, supported by <rs type="person">Xeno-Canto</rs>, <rs type="person">Floris</rs><rs type="funder">'Tic, SABIOD and EADM MaDICS</rs>. This research is supported by the <rs type="funder">Vienna Science and Technology Fund (WWTF)</rs> under grants <rs type="grantNumber">NXT17-004</rs> and <rs type="grantNumber">MA14-018</rs>. I also gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of two Tesla K40 GPUs and a Titan Xp GPU used for this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jf4g5xN">
					<idno type="grant-number">NXT17-004</idno>
				</org>
				<org type="funding" xml:id="_TSKAXXd">
					<idno type="grant-number">MA14-018</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://github.com/f0k/birdclef2018</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,635.82,337.63,7.92;14,151.52,646.68,329.07,8.02;14,151.52,657.74,230.32,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,202.12,635.82,278.47,7.92;14,151.52,646.78,20.09,7.92">Multiple instance classification: Review, taxonomy and comparative study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2013.06.003</idno>
		<ptr target="http://refbase.cvc.uab.es/files/Amo2013.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="14,178.82,646.78,84.36,7.92">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,120.62,337.63,7.92;15,151.52,131.58,329.07,7.92;15,151.52,142.54,329.07,7.92;15,151.52,153.49,329.07,7.92;15,151.52,164.45,158.81,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,287.24,120.62,193.35,7.92;15,151.52,131.58,261.40,7.92">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<ptr target="https://github.com/hyperopt/hyperopt" />
	</analytic>
	<monogr>
		<title level="m" coord="15,432.52,131.58,48.07,7.92;15,151.52,142.54,329.07,7.92;15,151.52,153.49,109.89,7.92">Proceedings of the 30th International Conference on Machine Learning (ICML). Proceedings of Machine Learning Research</title>
		<meeting>the 30th International Conference on Machine Learning (ICML). Machine Learning Research<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun 2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,174.51,337.63,7.92;15,151.52,185.47,296.28,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,380.54,174.51,80.44,7.92">Dual path networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,151.52,185.47,215.42,7.92">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,451.88,185.47,28.71,7.92;15,151.52,196.43,314.43,7.92" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7033-dual-path-networks" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,206.48,337.64,7.92;15,151.52,217.44,329.07,7.92;15,151.52,228.40,329.07,7.92;15,151.52,239.36,197.88,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,300.68,206.48,179.91,7.92;15,151.52,217.44,49.79,7.92">Multi-column deep neural networks for image classification</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">M</forename><surname>Dan Cire≈üan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://www.idsia.ch/~ciresan/data/cvpr2012.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,223.78,217.44,256.81,7.92;15,151.52,228.40,117.52,7.92">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun 2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,249.42,337.64,7.92;15,151.52,260.38,329.07,7.92;15,151.52,271.34,243.56,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,351.63,249.42,128.97,7.92;15,151.52,260.38,168.96,7.92">A multi-modal deep neural network approach to bird-song identification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper_179" />
	</analytic>
	<monogr>
		<title level="m" coord="15,343.08,260.38,98.02,7.92">Working Notes of CLEF</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">Sep 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,281.39,337.63,7.92;15,151.52,292.25,329.07,8.02;15,151.52,303.31,329.07,7.92;15,151.52,314.27,120.41,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,259.39,281.39,216.87,7.92">A review of multi-instance learning assumptions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1017/S026988890999035X</idno>
		<ptr target="http://www.cs.waikato.ac.nz/~ml/publications/2010/FouldsAndFrankMIreview.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="15,151.52,292.35,133.07,7.92">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2010-03">Mar 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,324.33,337.64,7.92;15,151.52,335.29,329.07,7.92;15,151.52,346.25,329.07,7.92;15,151.52,357.20,135.05,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,368.58,324.33,107.77,7.92">Born again neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.04770" />
	</analytic>
	<monogr>
		<title level="m" coord="15,164.77,335.29,315.82,7.92;15,151.52,346.25,168.66,7.92">Proceedings of the 35th International Conference on Machine Learning (ICML). Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning (ICML). Machine Learning Research<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,367.26,337.63,7.92;15,151.52,378.22,329.07,7.92;15,151.52,389.18,152.08,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,432.74,367.26,47.85,7.92;15,151.52,378.22,249.61,7.92">Overview of BirdCLEF 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,421.07,378.22,59.52,7.92;15,151.52,389.18,32.31,7.92">Working Notes of CLEF</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,399.24,337.63,7.92;15,151.52,410.20,329.07,7.92;15,151.52,421.15,329.07,7.92;15,151.52,432.11,70.96,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,247.15,399.24,233.44,7.92;15,151.52,410.20,50.68,7.92">Two convolutional neural networks for bird detection in audio signals</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schl√ºter</surname></persName>
		</author>
		<ptr target="http://ofai.at/~jan.schlueter/pubs/2017_eusipco.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,222.94,410.20,257.66,7.92;15,151.52,421.15,46.31,7.92">Proceedings of the 25th European Signal Processing Conference (EUSIPCO)</title>
		<meeting>the 25th European Signal Processing Conference (EUSIPCO)<address><addrLine>Kos Island, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,442.17,337.98,7.92;15,151.52,453.13,329.07,7.92;15,151.52,464.09,329.07,7.92;15,151.52,475.05,329.07,7.92;15,151.52,486.01,62.00,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,312.69,442.17,167.90,7.92;15,151.52,453.13,21.39,7.92">Identity mappings in deep residual networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
	</analytic>
	<monogr>
		<title level="m" coord="15,197.69,453.13,282.90,7.92;15,151.52,464.09,31.05,7.92">Proceedings of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV)<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,496.06,337.98,7.92;15,151.52,507.02,329.07,7.92;15,151.52,517.88,276.13,8.02" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,151.52,507.02,300.08,7.92">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arXiv e-prints abs/1207.0580</idno>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012-07">Jul 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,528.04,337.98,7.92;15,151.52,539.00,329.07,7.92;15,151.52,549.96,329.07,7.92;15,151.52,560.91,329.07,7.92;15,151.52,571.87,65.79,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,239.35,528.04,241.25,7.92;15,151.52,539.00,129.23,7.92">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="15,302.56,539.00,178.03,7.92;15,151.52,549.96,324.81,7.92">Proceedings of the 32nd International Conference on Machine Learning (ICML). Proceedings of Machine Learning Research</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML). Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,581.93,337.98,7.92;15,151.52,592.89,329.07,7.92;15,151.52,603.85,329.07,7.92;15,151.52,614.81,141.58,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,225.28,592.89,255.31,7.92;15,151.52,603.85,249.28,7.92">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,422.09,603.85,58.50,7.92;15,151.52,614.81,21.81,7.92">Proceedings of CLEF</title>
		<meeting>CLEF<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,624.86,337.98,7.92;15,151.52,635.82,329.07,7.92;15,151.52,646.78,329.07,7.92;15,151.52,657.74,59.66,7.92" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,168.54,635.82,293.34,7.92">Large-scale bird sound classification using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper_143.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,151.52,646.78,92.46,7.92">Working Notes of CLEF</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">Sep 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,120.62,337.98,7.92;16,151.52,131.58,329.07,7.92;16,151.52,142.54,248.52,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,243.03,120.62,180.51,7.92">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m" coord="16,445.36,120.62,35.23,7.92;16,151.52,131.58,306.94,7.92">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,152.59,337.98,7.92;16,151.52,163.55,329.07,7.92;16,151.52,174.51,329.07,7.92;16,151.52,185.47,76.37,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,340.05,152.59,140.54,7.92;16,151.52,163.55,125.70,7.92">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,258.17,174.51,222.42,7.92">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,231.77,185.47,248.82,7.92;16,151.52,196.43,281.19,7.92" xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,206.48,337.98,7.92;16,151.52,217.44,329.07,7.92;16,151.52,228.40,329.07,7.92;16,151.52,239.36,274.04,7.92" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,208.16,206.48,272.44,7.92;16,151.52,217.44,104.15,7.92">Bird song classification in field recordings: Winning solution for NIPS4B 2013 competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<ptr target="http://www.animalsoundarchive.org/RefSys/Nips4b2013NotesAndSourceCode/WorkingNotes_Mario.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,277.62,217.44,202.98,7.92;16,151.52,228.40,48.18,7.92">NIPS Workshop on Neural Information Scaled for Bioacoustics</title>
		<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,249.42,337.98,7.92;16,151.52,260.38,329.07,7.92;16,151.52,271.34,193.96,7.92" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,205.58,249.42,275.01,7.92;16,151.52,260.38,115.61,7.92">Improved automatic bird identification through decision tree based feature selection and bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1391/160-CR.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,288.30,260.38,95.04,7.92">Working Notes of CLEF</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,281.39,337.97,7.92;16,151.52,292.35,329.07,7.92;16,151.52,303.31,329.07,7.92;16,151.52,314.27,328.82,7.92;16,151.52,325.23,174.96,7.92" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,276.38,281.39,204.21,7.92;16,151.52,292.35,77.61,7.92">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2015/html/Pinheiro_From_Image-Level_to_2015_CVPR_paper.html" />
	</analytic>
	<monogr>
		<title level="m" coord="16,251.79,292.35,228.79,7.92;16,151.52,303.31,162.25,7.92">Proceedings of the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun 2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,335.29,337.97,7.92;16,151.52,346.25,329.07,7.92;16,151.52,357.20,329.07,7.92;16,151.52,368.16,125.37,7.92" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,205.82,335.29,270.41,7.92">Learning to pinpoint singing voice from weakly labeled examples</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schl√ºter</surname></persName>
		</author>
		<ptr target="http://ofai.at/~jan.schlueter/pubs/2016_ismir.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,165.72,346.25,314.87,7.92;16,151.52,357.20,80.13,7.92">Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)</title>
		<meeting>the 17th International Society for Music Information Retrieval Conference (ISMIR)<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">Aug 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,378.22,337.98,7.92;16,151.52,389.18,329.07,7.92;16,151.52,400.14,329.07,7.92;16,151.52,411.10,204.33,7.92" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,246.69,378.22,233.90,7.92;16,151.52,389.18,123.64,7.92">Exploring data augmentation for improved singing voice detection with neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schl√ºter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<ptr target="http://ofai.at/~jan.schlueter/pubs/2015_ismir.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,296.92,389.18,183.67,7.92;16,151.52,400.14,212.85,7.92">Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)</title>
		<meeting>the 16th International Society for Music Information Retrieval Conference (ISMIR)<address><addrLine>M√°laga, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,421.15,337.97,7.92;16,151.52,432.11,329.07,7.92;16,151.52,443.07,328.16,7.92" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,233.18,421.15,247.41,7.92;16,151.52,432.11,85.47,7.92">Dense prediction on sequences with time-dilated convolutions for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.09288" />
	</analytic>
	<monogr>
		<title level="m" coord="16,257.01,432.11,223.59,7.92;16,151.52,443.07,68.09,7.92">NIPS Workshop on End-to-end Learning for Speech and Audio Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">Nov 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,453.13,337.97,7.92;16,151.52,464.09,329.07,7.92;16,151.52,475.05,220.98,7.92" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,349.98,453.13,130.61,7.92;16,151.52,464.09,145.82,7.92">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1609/16090547.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,316.87,464.09,92.84,7.92">Working Notes of CLEF</title>
		<meeting><address><addrLine>√âvora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,485.10,337.98,7.92;16,151.52,495.96,329.07,8.02;16,151.52,507.02,121.17,7.92" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="16,296.51,485.10,184.09,7.92;16,151.52,496.06,119.01,7.92">Residual networks are exponential ensembles of relatively shallow networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>arXiv e-prints 1605.06431v1</idno>
		<ptr target="https://arxiv.org/abs/1605.06431v1" />
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,517.08,337.98,7.92;16,151.52,528.04,329.08,7.92;16,151.52,539.00,329.07,7.92;16,151.52,549.96,329.07,7.92;16,151.52,560.91,131.41,7.92" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,406.05,517.08,74.54,7.92;16,151.52,528.04,161.75,7.92">Trainable frontend for robust and far-field keyword spotting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Getreuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2017.7953242</idno>
		<ptr target="http://arxiv.org/abs/1607.05666" />
	</analytic>
	<monogr>
		<title level="m" coord="16,333.92,528.04,146.68,7.92;16,151.52,539.00,307.74,7.92">Proceedings of the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2017-03">Mar 2017</date>
			<biblScope unit="page" from="5670" to="5674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,570.97,337.97,7.92;16,151.52,581.93,329.07,7.92;16,151.52,592.89,329.07,7.92;16,151.52,603.85,304.51,7.92;16,151.52,614.81,217.91,7.92" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="16,368.86,570.97,111.73,7.92;16,151.52,581.93,160.39,7.92">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html" />
	</analytic>
	<monogr>
		<title level="m" coord="16,341.68,581.93,138.92,7.92;16,151.52,592.89,275.61,7.92">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,624.86,337.97,7.92;16,151.52,635.82,329.07,7.92;16,151.52,646.78,329.07,7.92;16,151.52,657.74,44.03,7.92" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="16,377.20,624.86,103.39,7.92;16,151.52,635.82,68.18,7.92">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.09412" />
	</analytic>
	<monogr>
		<title level="m" coord="16,239.91,635.82,240.69,7.92;16,151.52,646.78,95.09,7.92">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
