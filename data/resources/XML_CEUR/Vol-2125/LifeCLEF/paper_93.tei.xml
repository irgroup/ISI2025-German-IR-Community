<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.54,115.96,306.28,12.62;1,146.48,133.89,322.40,12.62">Species Prediction based on Environmental Variables using Machine Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.53,172.40,65.62,8.74"><forename type="first">Stefan</forename><surname>Taubert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.70,172.40,75.03,8.74"><forename type="first">Max</forename><surname>Mauermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.28,172.40,51.75,8.74"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.59,172.40,69.63,8.74"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.23,184.36,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.54,115.96,306.28,12.62;1,146.48,133.89,322.40,12.62">Species Prediction based on Environmental Variables using Machine Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E29FC58850341E48011DD0F7FB035960</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Species Prediction</term>
					<term>Machine Learning</term>
					<term>Environmental Variables</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our working notes we discuss different machine learning approaches for the prediction of species for different geolocations based on environmental variables. We used convolutional neural networks for image classification as well as data extraction with tree boosting as models. Furthermore, we analyzed different approaches for data extraction, as well as data augmentation and group forming to improve our models performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Description of the dataset and task</head><p>The training dataset provided for the 2018 GeoLifeCLEF task <ref type="bibr" coords="1,421.05,472.36,10.52,8.74" target="#b0">[1]</ref> as part of the LifeCLEF workshop <ref type="bibr" coords="1,243.13,484.31,10.52,8.74" target="#b1">[2]</ref> includes around 260.000 occurrences of plants from different species. Each occurrence is associated with a 33-channel TIFF image. Each channel represents a map of values for a different Environmental Variable (EV), centered around the location of the occurrence. Additionally, a .csv-file containing single values for each EV at the center of each occurrence is given. Since each occurrence is associated with a species, the goal of the GeoLifeCLEF task was to train a model, that is able to predict the species based on these EV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Analysis and Pre-processing</head><p>In this section we will introduce the different analysis methods we applied to assess the dataset. This provides an understanding of how the dataset was organized and structured. We also present various methods of pre-processing we utilized to extract the data, that was used in our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extracting single values from images</head><p>Since the dataset was given in two versions -images and a .csv-file containing single values -we planned to develop approaches for both types. After some analysis of the .csv-file, we noticed, a large amount of cells without values. These cells render their respective rows unusable. To fill out the missing values, we first tried to calculate them by averaging over the 4 centered pixels of the respective images, since the single values are supposed to be the EV values at the exact location of the occurrence. We also did this calculation for the images, for which values existed in the .csv-file, but realized that it was not possible to reproduce these values. Because of this discrepancy, we decided very early to not use the given single values from the .csv-file, but rather to create our own .csv-file by extracting the single values from the images like mentioned above. We also used another variation of this extracting algorithm, were we averaged the single values over the whole image instead of only using the 4 centered pixels. Both variations were used in different submissions we made. We did not transfer the values of each EV into their respective ranges, but rather kept them at the range from 0 to 255, like in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Most common values for each class</head><p>After this pre-processing step, we encountered a lot of raw numbers associated with different classes. They were not intuitively understandable, since the number of classes was also very high. For this reason we tried to create a fingerprint for each class. We did this by counting the single values for each class and creating a diagram, which shows how often each value occurs in every class. This way we gained some insight which values are discriminative for a specific class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finding the most discriminative channels</head><p>The relatively high number of 33 different channels indicated that not all channels are equally useful. We decided it would be worth to cut some of them, because they are not sufficiently discriminative. To achieve this, we searched for combinations of channels based on the extracted single values, that were sufficient to fully separate the training set as good as possible. We came across multiple combinations, one of which was used later when training a multi-model CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Creating Heatmaps for all classes</head><p>Since the TIFF images of the dataset are no photographic images, it was hard to grasp the features that identify a specific class. In order to get a better understanding of what characterizes different classes, we created an heatmap for each class, which contains the average values of all images for that classes. Furthermore, all values were normalized to the range between 0 and 1. However, the heatmaps for the most common classes looked very similar, which further proved our assumption that the given data is not distinctive enough to predict classes properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Splitting a validation set</head><p>We used 10% of the training set, to create a separate validation set. We used this split to estimate, how well our models and different approaches performed and to generate intermediate checkpoints for some of our models. We have ensured, that the validation set contained no species which were not included in the remaining train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>In this section, we will comment on our submissions made during the challenge. For each submission, we will describe the approach we used and we will discuss why some approaches produce better results then others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Guessing</head><p>The most basic model we used was based on random guessing. We extracted all 3336 possible species and assigned 100 of them randomly to each occurrence of the test set so that every occurrence has a set of 100 different species. The score of this submission (ST 2) was 0.0016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probability Model</head><p>For the probability model, we first counted the occurrences of each species in the training set. Then we assigned the species with descending probabilities to each test occurrence (so every occurrence had the same prediction). That means that the most common species was at rank one, the second most common species at rank two and so on. The score of this submission (ST 7) was 0.0134.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vector Model</head><p>The vector model calculated the differences between the occurrences of the training set and the test set and predicted each test occurrence with the species of the training occurrence with the smallest difference. Therefore we took all 33 features and shaped a 33-dimensional vector V out of each occurrence. We calculated the difference D as followed:</p><formula xml:id="formula_0" coords="4,216.22,516.39,182.91,67.94"># » V n = Vector of n-th training occurrence # » V m = Vector of m-th test occurrence # » V nm = # » V n - # » V m D nm = || # » V nm || 2</formula><p>Then we sorted all D nm values in descending order and looked which species S nm were assigned. As the species occurred multiple times for each m we always kept the first occurrence in S nm . The result was a sorted list of 3336 species for each occurrence m. At the end we took the first 100 species of the lists and ranked them ascending for the final submission. The score of this submission (ST 5) was 0.0271.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Convolutional Neural Networks</head><p>When using the TIFF-images directly for training, we decided to use convolutional neural networks (CNN). In this section we will discuss the different approaches we tried during the challenge. We used the Keras framework <ref type="bibr" coords="5,457.52,162.12,10.52,8.74" target="#b4">[5]</ref> to implement all models that were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-like-models</head><p>The first approach was a simple feed forward CNN, similar to the popular VGG model <ref type="bibr" coords="5,260.51,215.20,9.96,8.74" target="#b6">[7]</ref>, which won the ImageNet contest in 2014. We choose this relatively simple structure to better understand how the model works with the given data.</p><p>The exact architecture we used consisted of five convolutional layers followed by a global average pooling layer and the classification layer. We implemented max pooling after the first two and every second convolutional layer, which divides the spatial dimensions of the image by half. Additionally, we doubled the number of filters in the convolutional layers after each pooling layer .</p><p>During training we also recognized, that our model was suffering from severe overfitting. We decided to use regularization in form of L2-normalization in the convolutional layers and dropout after each global pooling layer.</p><p>We also used two approaches for this architecture. One model that is trained with all 33 channels of the image and one ensemble, which consists of 6 individually trained models, each one using one of 6 channels we assumed to be the most discriminative ones (1, 5, 13, 14, 21 and 22). These channels were found by using the method we described earlier (2.3).</p><p>To further improve our models, we tested classic affine transformations as data augmentation on the images that were used for training. First, we only used random flipping (vertical or horizontal) and random rotations. We later added random crops to the range of possible augmentations, to see if they would lead to further improvements. When doing the crops we selected a random area inside the image. Since the crops are random the all have different sizes, so we resized them to the original 64x64 pixels.</p><p>DenseNet We also included the more complex DenseNet <ref type="bibr" coords="5,396.58,507.39,9.96,8.74" target="#b5">[6]</ref>, despite the low scores of our CNN approach. We decided to not use any pre-trained weights. We assumed that this would not be helpful, since the images of our dataset did not share any features with the photographic images from ImageNet. We submitted a single DenseNet which was trained without any of the augmentations that were mentioned above. Additionally, we trained the DenseNet as a single model, not using the ensemble approach we used with the VGG-like models.</p><p>Training the CNN All of our CNN were trained with checkpoint learning (or early stopping). This means that we used the validation set to calculate a validation loss after each epoch of training. Every time the validation loss decreased, we saved the models weights. This way, we only used the best possible weights, that occurred during the training, to create our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of the CNN</head><p>The following table shows the results of our submissions that were accomplished with the CNN. The table shows the results for the single model and the multi-model approach for the VGG-like network. Rows indicate which data augmentations were used. We noticed that the result had very high fluctuations and that it was not clearly visible which augmentation methods did provide an improvement, when looking at the test set scores. For the single model, only flips and rotations seemed to provide an improvement and the ensemble seemed to work best without any augmentations. The crops did not lead to any improvement and the DenseNet is about as good as the VGG-like single model.</p><p>However, these scores are the result of the official test set evaluation. When validating on our separate validation set, we generally found all augmentations to be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">XGBoost</head><p>One other approach besides neuronal networks was the usage of Extreme Gradient Boosting (XGBoost <ref type="bibr" coords="6,248.35,437.33,20.40,8.74">[3][4]</ref>. With XGBoost it is possible to predict a target variable based on features of the training data. This section describes our two strategies for building a model with XGBoost. The train data was created from the images and the occurrences .csv-file. We took all 33 channel values, as well as the parameters latitude and longitude for each occurrence in the datasets and saved those information in a new .csv-file. Like with our CNN we used the seperated validation set (2.5) and early stopping to avoid overfitting. The model parameters were adjusted every run and we used early stopping rounds to stop the training when the model overfitted. XGBoost has one parameter max depth which defined how deep the boosted trees were build. That means that the higher the depth was defined the more complex the resulting model was. We tried different depths to get a understanding how complex our model has to be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Model</head><p>The first strategy was to build one single model for the prediction. In this case the target variable mentioned earlier was a list of species probabilities for each dataset occurrence. The evaluation method we choose during the training was the multi-class classification error rate (merror-metric in XGBoost). We submitted the variations at Table <ref type="table" coords="6,351.58,656.12,3.87,8.74">2</ref>.</p><p>Table <ref type="table" coords="7,163.62,115.91,4.13,7.89">2</ref>. The results show that the value 10 for max depth was to big and the model was to complex whereas value 2 was that one with the best score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Model</head><p>The second strategy was to predict each species separately. Therefore we created 3336 different models. In this case the target variable mentioned earlier was a list of probabilities for the occurrence of the current selected single species (one out of 3336) for a dataset occurrence. Each model was evaluated during the training with the log-loss metric because the resulting probabilities were decimal numbers. We tried out different parameter settings, which can be seen at Table <ref type="table" coords="7,256.31,288.70,3.87,8.74" target="#tab_2">3</ref>. Groups Model To get better scores, we created an advanced model which built several groups with species that are similar. The idea was to group species, because we argued that it is likely that more than one species occur at one location. In order to find similar species, we looked at the occurrences for each species in the train set and calculated a representative value per species. One way of doing so was to look at each feature column and estimate which value occurred most. The second method was to calculate the mean value for each feature column. This way, we calculated a simple fingerprint for each species.</p><p>In the next step we calculate the difference between all species with the same method like in the Vector Model. The result was a 3336x3336 matrix in which the distance between each species was held. We had to round the feature values to 2 digits and we ignored species which occurred less then a count of o.</p><p>After that, we defined the parameter t for threshold which indicates how great the distance between two species could be to consider them in the same group. After this, we created a network in which all species relations were contained. The following figure illustrates some of the problems that arise with that approach.</p><p>Therefore, we defined a additional parameter min edges which defines how much neighbours a species needs to be in a group. As result we got less groups but instead ones with more similar species.  At both runs we used all 64x64 pixels. For the test submission we first predicted the groups for each test occurrence and then we mapped them back to the species. Therefor we took all species of the specific groups and sorted them descending with regard of their probability in the train set.</p><p>Table <ref type="table" coords="8,164.21,383.96,4.13,7.89">4</ref>. The score of the run in which the mean was calculated was slightly better than the other run but not as good as the run without any groups. At variation 1 we have got 2148 groups in which were 266 species in groups with an size greater than one. We then trained each group with 2148 models. At variation 2 we got 3301 groups and 39 species in groups with a size greater one.</p><p>We realized that it was not an improvement to build groups, because the test scores were not as good as the scores without groups. The reason could be that the machine learning algorithm detected by itself which species are similar and it therefore was more accurate then building the groups manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Source Code</head><p>We published our source code at GitHub<ref type="foot" coords="8,315.76,600.01,3.97,6.12" target="#foot_0">3</ref> . The code is written in Python and uses different third party libraries. The project page provides instructions on how to execute the code. We used an i7 INTEL CPU and a NVIDIA P6000 for model training.</p><p>As the submissions of other participants and our own results show, predicting plant species based on environmental variables can be a challenging task. Furthermore, it is especially challenging when it comes to predicting only a single species, since there are many species that may occur under the same environmental conditions. However, we evaluated various machine learning techniques to get an idea of which work best for this problem. Our approaches were divided into two main groups, one using the 33-channel images and CNN, the other one using values, that were extracted from these images. For the CNN part, we mainly used a traditional feed forward network in two variations: One using all 33 channels of the images, the second was an ensemble of 6 individually trained models, each using one of 6 selected channels. After using the unprocessed versions of the images, we also added different affine transformations as data augmentation. On the test score, augmentations did not lead to an improvement, but when evaluating them on our local validation set they had proven to be useful. In general, at their best, our CNN were merely better then a probability based approach, which implies that more optimization would be needed to make this technique work properly.</p><p>Our single value approach used boosted trees with the XGBoost framework. The first parameter we tried to optimize with this approach was finding the right depth for our trees. We started with a very high depth, but we found, that less depth and therefore less complex models yield better results. After that, we also changed the way in which we extract the single values from the images. We were able to show that averaging only over the 4 centered pixels leads to better results than averaging over the whole image. Finally, we also tried out forming groups of similar species and predicting them instead of single species. We defined predicting a group as the prediction of all species within this group ordered along their frequency in the train set. This data processing step did not lead to an improvement of our score.</p><p>In general we have to state, that our local validation score constantly differed from the challenges test score, which led us to the conclusion that the species of the tasks dataset are not completely distinguishable using the given EV.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,538.86,345.83,7.89;2,134.77,549.82,345.83,7.89;2,134.77,560.81,345.83,7.86;2,134.77,571.77,311.40,7.86;2,171.79,413.97,138.33,108.87"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparing test-and training set: The value occurrences for the channels chbio 5 and chbio 13 of the test set (top) and training set (bottom) show that test and training set have different value occurrences. This could be one of the reasons, for the great difference between our test-and validation scores we describe later.</figDesc><graphic coords="2,171.79,413.97,138.33,108.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,530.80,345.82,7.89;3,134.77,541.78,88.34,7.86;3,148.98,436.97,103.74,77.81"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Heatmaps for the 3 most common classes in the training set, for channel 12 (annual precipitation)</figDesc><graphic coords="3,148.98,436.97,103.74,77.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,162.52,345.83,7.89;8,134.77,173.48,345.83,7.89;8,134.77,184.44,178.71,7.89;8,183.15,102.19,249.07,59.20"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Each point represents a species and the connections are distances between the species which are within t. The main problem is that species a has a difference of 3t to d in the worst case and is not really similar.</figDesc><graphic coords="8,183.15,102.19,249.07,59.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,154.83,282.78,305.69,7.89;8,183.15,204.34,249.06,77.10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Now the species in the groups are more similar at a min edges of 3.</figDesc><graphic coords="8,183.15,204.34,249.06,77.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,134.77,456.06,345.83,7.89;9,134.77,467.04,208.88,7.86;9,134.77,233.79,345.83,207.50"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. As in the result graph is shown the overall scores were really low. This may indicate, that predicting this dataset is a hard task.</figDesc><graphic coords="9,134.77,233.79,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,185.16,345.83,72.90"><head>Table 1 .</head><label>1</label><figDesc>The final results for the used architectures. For each architecture the run number is given as well.</figDesc><table coords="6,176.78,216.92,261.80,41.14"><row><cell cols="2">augmentations VGG-like single VGG-like multi</cell><cell>DenseNet</cell></row><row><cell>none</cell><cell cols="2">0.0096 / ST 11 0.0153 / ST 1 0.0099 / ST 19</cell></row><row><cell>flip, rotate</cell><cell>0.0153 / ST 3 0.0144 / ST 14</cell><cell>-</cell></row><row><cell cols="2">flip, rotate, crop 0.0103 / ST 15 0.0096 / ST 18</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,318.63,345.83,72.90"><head>Table 3 .</head><label>3</label><figDesc>Multi-model evaluation with different parameter settings. Scores increased after calculating the pixel values solely from the four pixels of the center of the images.</figDesc><table coords="7,168.86,350.39,277.63,41.14"><row><cell cols="6">Variation Run max depth Pixel Count Test-Score Validation-Score</cell></row><row><cell>1</cell><cell>ST 6</cell><cell>3</cell><cell>64x64</cell><cell>0.0338</cell><cell>0.0942</cell></row><row><cell>2</cell><cell>ST 13</cell><cell>2</cell><cell>64x64</cell><cell>0.0352</cell><cell>0.0963</cell></row><row><cell>3</cell><cell>ST 16</cell><cell>2</cell><cell>2x2</cell><cell>0.0358</cell><cell>0.0884</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="8,144.73,656.80,203.64,7.86"><p>https://github.com/stefantaubert/lifeclef-geo-2018</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,294.02,342.25,7.86;10,146.91,304.98,253.25,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,285.65,294.02,194.95,7.86;10,146.91,304.98,95.11,7.86">Overview of GeoLifeCLEF 2018: location-based species recommendation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,263.78,304.98,103.52,7.86">CLEF working notes 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,315.94,342.24,7.86;10,146.91,326.90,333.68,7.86;10,146.91,337.86,333.68,7.86;10,146.91,348.81,49.66,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,193.97,326.90,286.62,7.86;10,146.91,337.86,224.58,7.86">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.58,337.86,87.01,7.86;10,146.91,348.81,18.43,7.86">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,359.77,342.24,7.86;10,146.91,370.73,333.68,7.86;10,146.91,381.69,83.70,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,247.98,359.77,163.70,7.86">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tianqi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.52,359.77,48.07,7.86;10,146.91,370.73,333.68,7.86;10,146.91,381.69,25.89,7.86">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,392.65,342.24,7.86;10,146.91,403.61,153.79,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,216.49,392.65,245.43,7.86">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,403.61,76.10,7.86">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,414.57,135.48,7.86" xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,219.49,414.57,25.67,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,425.53,342.24,7.86;10,146.91,436.49,333.67,7.86;10,146.91,447.44,208.67,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,404.80,425.53,75.79,7.86;10,146.91,436.49,90.25,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,257.97,436.49,222.62,7.86;10,146.91,447.44,95.27,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,458.40,342.24,7.86;10,146.91,469.36,213.09,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,264.92,458.40,215.67,7.86;10,146.91,469.36,43.20,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556.(201</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
