<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.98,152.48,300.59,12.64;1,202.44,170.12,189.96,12.64">Audio-based Bird Species Identification with Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,209.22,58.66,9.07"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn.berlin</email>
							<affiliation key="aff0">
								<orgName type="institution">Museum fuer Naturkunde Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.98,152.48,300.59,12.64;1,202.44,170.12,189.96,12.64">Audio-based Bird Species Identification with Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44C6C9409999019C4827BCD7CE708799</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Species Identification</term>
					<term>Biodiversity</term>
					<term>Deep Learning</term>
					<term>Deep Convolutional Neural Networks</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents deep learning techniques for audio-based bird identification at very large scale. Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 1500 species. Various data augmentation techniques are applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the BirdCLEF 2018 campaign and provides the best system in all subtasks. It surpasses previous state-of-the-art by 15.8 % identifying foreground species and 20.2 % considering also background species achieving a mean reciprocal rank (MRR) of 82.7 % and 74.0 % on the official BirdCLEF Subtask1 test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A system for audio-based bird identification has proven to be particularly useful for biodiversity monitoring and education. It can help professionals working with bioacoustic sounds by processing large audio collections in a fast and automated way. Or help amateurs to identify birds in self-made recordings. The LifeCLEF bird identification task challenges participants to identify different bird species in a large collection of audio recordings provided by Xeno-Canto <ref type="bibr" coords="1,308.67,520.76,10.55,9.07" target="#b0">[1]</ref>. The number of training files as well as the number of species to identify constantly grew over the last few years. Starting with 501 species in 2014 the number was doubled in 2015. Since 2016 there are 1500 different species to identify in the test sets. An overview and further details about the BirdCLEF tasks are given in <ref type="bibr" coords="1,245.22,568.66,10.61,9.07" target="#b1">[2]</ref>. They are among others part of the LifeCLEF 2018 evaluation campaign <ref type="bibr" coords="1,209.90,580.54,10.62,9.07" target="#b2">[3]</ref>. Distinguishing between so many different birds is a challenging fine-grained classification problem. Until 2016, the best system used template matching of small sound elements combined with a large number of acoustic features and a random forest ensemble learning method for classification <ref type="bibr" coords="1,293.93,628.81,10.54,9.07" target="#b3">[4]</ref>, <ref type="bibr" coords="1,307.98,628.81,10.54,9.07" target="#b4">[5]</ref>. With the rise of deep learning, since 2016, the best systems are based on convolutional neural networks <ref type="bibr" coords="1,406.30,640.69,10.54,9.07" target="#b5">[6]</ref>, <ref type="bibr" coords="1,420.36,640.69,10.54,9.07" target="#b6">[7]</ref>. The approach described in this paper also uses neural networks and deep learning. It is building on the work of previous solutions to the task and combines proven techniques with new methods, especially regarding data preparation and augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation Details and Model Training Data Preparation</head><p>Audio files provided in the training and test sets are not only of different duration and quality, they also have different formats regarding sample rate, bit depth and number of channels. In a first step, two data sets are formed with homogeneous file properties.</p><p>For the first set all files are resampled to 44.1 kHz followed by normalization to a maximum signal amplitude of -3 dB. For a second set all files are first high pass filtered at a frequency of 2 kHz (Q = 0.707), resampled to 22050 Hz, mixed to mono and finally also normalized to -3 dB. The training set is augmented by additionally extracting 381 audio files using the time coded annotation of species in the metadata of the newly provided soundscape validation set. All files belonging to the training set are further processed to create additional data sets with different content described more detailed below:</p><formula xml:id="formula_0" coords="2,124.63,347.03,59.16,45.92"> BirdsOnly  NoiseOnly  AtmosOnly  LowQuality</formula><p>Via segmentation, the audio content of each training file is separated in signal and noise parts. Segmentation is done in frequency domain applying image processing methods like median clipping <ref type="bibr" coords="2,249.62,427.83,11.63,9.07" target="#b7">[8]</ref> and further morphological operations on the spectrogram image to extract individual sound events. The method was already successfully used in previous bird identification tasks amongst others to extract segments for feature engineering via template matching <ref type="bibr" coords="2,298.88,463.86,10.98,9.07" target="#b3">[4,</ref><ref type="bibr" coords="2,309.85,463.86,7.32,9.07" target="#b4">5,</ref><ref type="bibr" coords="2,317.17,463.86,7.32,9.07" target="#b8">9]</ref>. The position of audio segments in time can be used to divide an audio file into signal parts, representing bird call or song elements, and noise parts, representing background sounds or noise. One data set is formed by concatenating all frames of a file belonging to bird sounds (BirdsOnly) and another one by concatenating all frames belonging to background sounds (NoiseOnly). Since the noise files are created by concatenating small parts, sometimes only a few milliseconds in duration, taken from various positions of the original source file, they can have rapid changes in amplitude and frequency. To get a more realistic set of background atmosphere a third data set (AtmosOnly) is formed using and concatenating only parts consisting of longer sequences of successive frames related to noise with an overall duration greater than one second. This set is significantly smaller compared to the other sets since most files don't have parts without birds singing for a longer period of time. Training is done with a batch size of ca. 100 -200 samples using either 2 or 3 GPUs (Nvidia Geforce 1080 and 1080 Ti). Categorical cross entropy is used as loss function considering only foreground species as ground truth targets. Stochastic gradient descent is used as optimizer with Nesterov momentum of 0.9, weight decay of 1e-4 and an initial learning rate of 0.01. Learning rate is decreased during training by ca. 10 -1 in 3 to 4 steps until 0.0001 whenever performance on the validation set stopped improving.</p><p>Validation and post-processing for submission. For the validation and test set audio chunks are extracted successively from each file and channel (if not mono) with an overlap of 10 % for validation files during training and 80 % for files in the test set. Predictions are averaged for each file (Subtask1: monophone recordings) and time interval (Subtask2: soundscape recordings) by taking the mean over all chunks and channels. To emphasize chunks with higher prediction values, suggesting a more confident identification, all predictions are squared before taking the mean. A similar technique called mean exponential pooling is employed in the baseline system provided by <ref type="bibr" coords="3,161.70,653.65,16.71,9.07" target="#b11">[12]</ref> this year. If species composition is known in advance, e.g. based on knowledge about recording habitat or time, removing predictions of unlikely species can further improve identifi-cation performance. To exploit this fact, predictions of species, not part of data sets in previous years, are removed for corresponding files of the validation and test sets.</p><p>Other metadata besides year of recording or data set membership is not utilized and none of the provided metadata is used for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>Various data augmentation techniques are employed to prevent overfitting and improve model accuracy. The following augmentation methods are applied in time domain regarding audio chunks:</p><p> use files from different data sets (Original, BirdsOnly, LowQuality)  extract chunks from random position in file (wrap around if end of file is reached and continue reading from beginning)  apply jitter to duration (ca. ± 0.5 s)  add 2 audio chunks from random files of the NoiseOnly set  add 2 audio chunks from random files of the AtmosOnly set  add 2 audio chunks from random files belonging to the same bird species  apply random factor to signal amplitude of all chunks before summation  apply random cyclic shift  skip random number of samples (time interval dropout)  reconstruct audio signal by summing up individual sound elements Some of the above mentioned techniques were used before in previous BirdCLEF tasks for example adding background noise or sounds from other files belonging to the same bird class with random intensity <ref type="bibr" coords="4,295.11,450.15,11.85,9.07" target="#b5">[6,</ref><ref type="bibr" coords="4,306.97,450.15,11.85,9.07" target="#b12">13,</ref><ref type="bibr" coords="4,318.82,450.15,11.85,9.07" target="#b13">14]</ref>. Applying cyclic shift to the sample array by a random amount was also employed by <ref type="bibr" coords="4,346.86,462.42,11.68,9.07" target="#b5">[6]</ref> and has the same effect as cutting the audio chunk in two pieces at random position and switching their order <ref type="bibr" coords="4,124.63,486.18,15.28,9.07" target="#b13">[14]</ref>. However, some techniques from the above list were not used previously: Time interval dropout. With a chance of ca. 30 % a random number of samples corresponding to a time interval between zero and the duration of an entire chunk are skipped at random time when reading from the audio file.</p><p>Reconstruction via sound elements with dropout. With a chance of ca. 30 % the audio chunk is (re)constructed using the information extracted in the segmentation preprocessing step. Single sound elements are cut from the audio file, multiplied by a short fade in and out envelope, band pass filtered and summed up to recreate the original audio signal. With this procedure it is possible to vary position and length of each sound event individually by adding a small offset to its starting time or by applying a small amount of time stretching to it. Furthermore sound element dropout can be implemented by choosing some elements to be skipped and not added to the audio signal sum.</p><p>All above described operations are performed in time domain. The final audio chunk is than transformed to frequency domain by applying a short-time Fourier transform. Different FFT parameters are used depending on the sample rate and whether frequencies are chosen to be linear or mel scaled. Normalization and logarithm is applied yielding a dynamic range of approximately 100 dB. Low and high frequencies are removed to get a spectrogram representing a frequency range of about 150 to 10500 Hz. Furthermore the spectrogram is resized to fit the input dimension of the DCNN used for training (e.g. 299x299 pixels for InceptionV3 <ref type="bibr" coords="5,347.98,234.06,14.88,9.07" target="#b14">[15]</ref>). Since all networks were pre-trained with RGB images the grayscale image is duplicated to all three color channels. Further augmentation is applied in frequency domain to the spectrogram image:</p><p> pitch shift &amp; frequency stretch by removing additional high and low frequency bands (random number of the first 10 and last 6 rows of the image are cut)  piecewise time stretch by resizing random number of columns at random position  piecewise frequency stretch by resizing random number of rows at random position  use different interpolation filters for resizing  apply color jitter (brightness, contrast, saturation, hue) Pitch or frequency shifting and stretching was previously applied in similar ways by e.g. <ref type="bibr" coords="5,141.19,383.17,11.67,9.07" target="#b6">[7]</ref> and <ref type="bibr" coords="5,172.53,383.17,15.29,9.07" target="#b15">[16]</ref>.</p><p>Piecewise time stretch. Besides manipulating the duration or speed of an entire audio chunk, time stretching is also applied piecewise with a 50 % chance to change speed at multiple times within a chunk. This is accomplished by dividing the spectrogram image in several vertical pieces, each having a width randomly chosen between 10 and 100 pixel. Then, pieces are resized individually by a factor randomly chosen between 0.9 and 1.1 along the horizontal (time) axis.</p><p>Piecewise frequency stretch. The same procedure is applied in an analogous way to the frequency axis with a 40 % chance and a stretch factor between 0.95 and 1.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random choice of interpolation filter.</head><p>For resizing, the high-quality Lanczos filter of the Python Imaging Library is used by default. However in 15 % of the cases a different resampling filter is chosen from the library with equal chance: Nearest, Box, Bilinear, Hamming and Bicubic.</p><p>As a last augmentation step color jitter is applied to the final spectrogram image. This is a very common method during training for image classification (e.g. ImageNet). Brightness, contrast and saturation of the spectrogram image are slightly varied (factor 0.3) as well as hue (factor 0.01). Color jitter was also used by <ref type="bibr" coords="5,386.23,623.41,10.75,9.07" target="#b6">[7]</ref>.</p><p>Table <ref type="table" coords="5,149.47,647.17,5.04,9.07" target="#tab_1">1</ref> demonstrates the effect of augmentation on model performance. All models in the table are trained using the same parameter settings (if not mentioned otherwise) with a learning rate of 0.01 until performance on the validation set stopped improving (ca. 200 epochs). The first two models in the table show a MRR gain of almost 10 % for using no augmentation at all compared to applying all above described techniques. For the other models augmentation methods are separately turned off to show their individual influence. The greatest impact on identification performance is gained by adding background noise from the NoiseOnly and AtmosOnly sets. The least influence is achieved by randomly choosing chunks from the LowQuality set leading even to a better MRR score if this method is turned off. Table <ref type="table" coords="6,150.90,463.86,5.04,9.07" target="#tab_2">2</ref> shows the impact of using different post-processing methods to get a filebased (or for Subtask2 a file and time interval based) prediction for each species. The default method uses 5 seconds chunks with an overlap of 10 %. Predictions are summarized for each file by simply averaging all chunk predictions. Better identification results are achieved by squaring chunk predictions before taking the mean, using more chunks with larger overlap and removing predictions of species known to be not present in a recording. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>For the first run different models were used for Subtask1 and Subtask2 whereas for runs 2, 3 &amp; 4 the same models or ensemble of models were used for both subtasks. The main properties of models for the first 3 runs are listed in table <ref type="table" coords="7,402.36,200.94,3.78,9.07" target="#tab_3">3</ref>. Results on the official BirdCLEF test sets are summarized in table <ref type="table" coords="7,332.83,212.82,3.78,9.07" target="#tab_4">4</ref>. Run 1 A single model was trained for each subtask (M1 for Subtask1, M2 for Subtask2) by fine-tuning pre-trained InceptionV3 networks with mel spectrograms of 5 seconds audio chunks as inputs. Models M1 and M2 mainly differ in the pre-processing of audio files and choice of FFT parameters (see table <ref type="table" coords="7,348.34,587.02,3.60,9.07" target="#tab_3">3</ref>). For Subtask1 pre-filtered, mono audio files were used with a sample rate of 22050 Hz. For Subtask2 audio files were not filtered or mixed down to mono.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>For the second run an ensemble of the two models from the first run was used for both subtasks by averaging their predictions. Furthermore predictions of new species were removed for recordings belonging to older BirdCLEF data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3</head><p>For this run a third model (M3) was added to the ensemble of models from the previous runs. M3 used 6 seconds audio chunks, a larger number of mel bands and slightly different FFT parameters (see table <ref type="table" coords="8,268.73,185.82,3.60,9.07" target="#tab_3">3</ref>). It was trained on a different training/validation split than M1 and M2. Furthermore predictions of the two best snapshots per model (regarding performance on the validation set) were chosen for averaging instead of just one, resulting in 6 predictions per species and file for Subtask1 (or file and time interval for Subtask2). For this and the next run, unlikely species were also removed for older recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4</head><p>Again all models and snapshots from the previous runs were reused plus two additional snapshots per model. Furthermore snapshots of 4 models created in an earlier phase were added to the ensemble. Those early models were trained using spectrogram images with linear instead of mel frequency scaling and did not have the later developed piecewise time and frequency stretch augmentation. One of the models was trained by fine-tuning a SENet154 network <ref type="bibr" coords="8,309.79,347.89,16.68,9.07" target="#b16">[17]</ref> with input dimension of 224x224 pixel. Although performing worse on the validation set (MRR: 72.7 -75.8 %) compared to the models used in the first three runs, adding those early models to the ensemble still helped to raise identification performance in both subtasks. The above described approaches provided best systems for both, identifying birds in monophone and soundscape recordings. More results and evaluation details can be accessed via the BirdCLEF 2018 webpage <ref type="bibr" coords="8,303.08,593.86,16.67,9.07" target="#b17">[18]</ref> and the crowdAI leaderboards <ref type="bibr" coords="8,451.29,593.86,15.26,9.07" target="#b18">[19]</ref>, <ref type="bibr" coords="8,124.63,605.74,15.26,9.07" target="#b19">[20]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In Augmentation prevents overfitting and strongly improves accuracy of models or model ensembles. A few techniques were adapted from previous work and could be successfully complemented with some new approaches. In an extra experiment it was tried to show their individual influence by turning them off one by one. Another maybe even better approach would have been to start from the model without augmentation and turning them on individually. For example the impact of duration jitter cannot be really verified if piecewise time stretching is also applied. Some augmentation methods did not work as well as expected like e.g. deliberately degrading the audio quality. Nevertheless it still might be a beneficial training strategy if the model is deployed for bird recognition on mobile or embedded devices where recordings need to be highly compressed because of energy and storage constrains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,125.71,357.17,343.78,8.19;9,155.26,368.06,284.67,8.10;9,146.15,148.90,303.55,198.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official scores of the BirdCLEF 2018 "Subtask1: monophone recordings" bird identification task. The above described methods and submitted runs belong to MfN.</figDesc><graphic coords="9,146.15,148.90,303.55,198.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,124.99,625.13,345.22,8.19;9,155.26,636.02,284.67,8.10;9,143.15,413.90,308.94,200.95"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Official scores of the BirdCLEF 2018 "Subtask2: soundscape recordings" bird identification task. The above described methods and submitted runs belong to MfN.</figDesc><graphic coords="9,143.15,413.90,308.94,200.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,161.38,233.71,272.60,212.54"><head>Table 1 .</head><label>1</label><figDesc>Influence of augmentation methods on identification performance.</figDesc><table coords="6,170.38,253.53,253.33,192.72"><row><cell>Settings</cell><cell>MRR [%]</cell></row><row><cell>Without augmentation</cell><cell>65.538</cell></row><row><cell>Complete augmentation</cell><cell>74.466</cell></row><row><cell>Without adding NoiseOnly and AtmosOnly chunks</cell><cell>67.893</cell></row><row><cell>Without adding AtmosOnly chunks</cell><cell>72.696</cell></row><row><cell>Without adding NoiseOnly chunks</cell><cell>73.186</cell></row><row><cell>Without piecewise time and frequency stretch</cell><cell>73.681</cell></row><row><cell>Without time interval dropout</cell><cell>73.825</cell></row><row><cell>Without using BirdOnly chunks</cell><cell>73.848</cell></row><row><cell>Without reconstruct via sound elements</cell><cell>73.853</cell></row><row><cell>Without color jitter</cell><cell>73.984</cell></row><row><cell>Without adding same class chunks</cell><cell>74.098</cell></row><row><cell>Without duration jitter</cell><cell>74.105</cell></row><row><cell>Without using LowQuality chunks</cell><cell>74.533</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,153.46,560.01,286.47,83.98"><head>Table 2 .</head><label>2</label><figDesc>Influence of post-processing methods on identification performance.</figDesc><table coords="6,153.46,579.47,28.67,8.10"><row><cell>Settings</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,148.42,248.85,292.44,254.67"><head>Table 3 .</head><label>3</label><figDesc>Main properties of models used for submitted runs 1-3.</figDesc><table coords="7,148.42,268.65,292.44,234.87"><row><cell>Model ID</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell></row><row><cell>Included in run</cell><cell>1, 2, 3, 4</cell><cell>1, 2, 3, 4</cell><cell>3, 4</cell></row><row><cell>High pass filtered</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Sample rate [Hz]</cell><cell>22050</cell><cell>44100</cell><cell>22050</cell></row><row><cell>Number of channels</cell><cell>mono</cell><cell>multi</cell><cell>mono</cell></row><row><cell>Chunk duration [s]</cell><cell>5</cell><cell>5</cell><cell>6</cell></row><row><cell>Duration jitter [s]</cell><cell>0.45</cell><cell>0.45</cell><cell>0.5</cell></row><row><cell>FFT size [samples]</cell><cell>1024</cell><cell>4096</cell><cell>1536</cell></row><row><cell>FFT hop size [samples]</cell><cell>256</cell><cell>512</cell><cell>384</cell></row><row><cell>Lowest frequency [Hz]</cell><cell>160</cell><cell>160</cell><cell>160</cell></row><row><cell>Highest frequency [Hz]</cell><cell>10300</cell><cell>11000</cell><cell>1100</cell></row><row><cell>Frequency scaling</cell><cell>Mel</cell><cell>Mel</cell><cell>Mel</cell></row><row><cell>Number of mel bands</cell><cell>256</cell><cell>256</cell><cell>310</cell></row><row><cell>Network architecture</cell><cell cols="3">InceptionV3 InceptionV3 InceptionV3</cell></row><row><cell>Image input size [pixel]</cell><cell>299x299</cell><cell>299x299</cell><cell>299x299</cell></row><row><cell>Validation fold</cell><cell>7</cell><cell>7</cell><cell>6</cell></row><row><cell>Validation MRR [%]</cell><cell>77.63</cell><cell>77.58</cell><cell>76.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,117.07,419.92,359.90,146.62"><head>Table 4 .</head><label>4</label><figDesc>Official scores on the BirdCLEF 2018 test sets.</figDesc><table coords="8,117.07,439.72,359.90,126.82"><row><cell cols="3">Run #Models #Snap-</cell><cell>MRR Subtask1 [%]</cell><cell>MRR Subtask1 [%]</cell><cell>c-mAP Subtask2 [%]</cell></row><row><cell></cell><cell></cell><cell>shots</cell><cell>(main species only)</cell><cell>(+ background species)</cell><cell>(soundscapes)</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>78.0 (M1)</cell><cell>69.6 (M1)</cell><cell>14.3 (M2)</cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>81.5</cell><cell>72.8</cell><cell>18.1</cell></row><row><cell>3</cell><cell>3</cell><cell>6</cell><cell>82.3</cell><cell>73.7</cell><cell>18.7</cell></row><row><cell>4</cell><cell>7</cell><cell>18</cell><cell>82.7</cell><cell>74.0</cell><cell>19.3</cell></row><row><cell></cell><cell cols="5">Figure 2 and 3 compare scores of all submissions to the BirdCLEF 2018 subtasks.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,124.63,176.79,346.01,285.31"><head></head><label></label><figDesc>the following paragraph some insights are summarized about what worked and what did not work so well during the experiments and processing of the task. Although low frequencies are discarded, pre-process the audio files by applying a soft high pass filter led to slightly better identification results. Also mel spectrograms performed better compared to spectrograms with linear frequency scaling. The best network for the task seemed to be the InceptionV3 architecture. Other networks were also tested for a few training epochs (ResNet152, DualPathNet92, Incep-tionV4, DensNet, InceptionResNetV2, Xception, NasNet). But even the more recent or larger architectures that are superior in other image classification tasks could not meet the performance of the InceptionV3 network with attention branch. Of course that doesn't mean that other networks are not potentially able to give better results with the proper tuning. For model training extracting audio chunks at random position on the fly worked better than using pre-sliced, overlapping chunks. Squaring predictions of chunks before taking the mean was beneficial when summarizing the results per file or time interval. It proved to be a good decision to create training and validation splits with respect to bird individuals recorded over several files. For Subtask1 the MRR scores were in most cases even better on the test set than on the validation set. If species composition is known in advance based on knowledge about habitat or recording season and time, removing unlikely species leads to less confusion and significantly helps classification. It would be interesting to see to what extent identification performance can be even further improved if metadata was additionally used and incorporated into the training process.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Goëau</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Hervé Glotin</rs> and <rs type="person">Willem-Pier Vellinga</rs> for organizing this task. I especially want to thank <rs type="person">Elias Sprengel</rs> for sharing his knowledge and the fruitful cooperation during the bird detection challenge [21]. I also want to thank the <rs type="institution">Museum fuer Naturkunde Berlin</rs> and the <rs type="institution">Bundesministerium fuer Wirtschaft und Energie</rs> for supporting my research.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,132.60,176.80,296.21,8.10" xml:id="b0">
	<monogr>
		<ptr target="https://www.xeno-canto.org/,lastaccessed2018/05/29" />
		<title level="m" coord="11,141.58,176.80,83.00,8.10">Xeno-Canto Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,187.99,337.90,8.10;11,141.58,199.15,329.01,8.10;11,141.58,209.95,17.81,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,422.90,187.99,47.59,8.10;11,141.58,199.15,232.28,8.10">Overview of BirdCLEF 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,393.00,199.15,77.59,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,221.11,336.98,8.10;11,141.58,231.91,328.73,8.10;11,141.58,243.09,272.82,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,167.83,231.91,302.48,8.10;11,141.58,243.09,156.29,8.10">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,316.39,243.09,77.66,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,253.89,337.97,8.10;11,141.58,265.05,255.55,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,212.52,253.89,258.05,8.10;11,141.58,265.05,29.52,8.10">Towards Automatic Large-Scale Identification of Birds in Audio Recordings</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,186.57,265.05,128.46,8.10">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9283</biblScope>
			<biblScope unit="page" from="364" to="375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,275.85,337.15,8.10;11,141.58,287.01,276.45,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,212.44,275.85,257.31,8.10;11,141.58,287.01,108.61,8.10">Improved Automatic Bird Identification through Decision Tree based Feature Selection and Bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.73,287.01,149.30,8.10">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,298.19,337.61,8.10;11,141.58,308.99,251.40,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,337.38,298.19,132.83,8.10;11,141.58,308.99,125.99,8.10">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,285.62,308.99,87.36,8.10">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,320.15,337.75,8.10;11,141.58,330.95,276.65,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,241.21,320.15,229.14,8.10;11,141.58,330.95,149.03,8.10">Audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,309.06,330.95,88.82,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,342.11,337.37,8.10;11,141.58,352.85,328.79,8.18;11,141.58,364.01,328.56,8.19;11,141.58,374.90,146.83,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,213.88,342.11,256.09,8.10;11,141.58,352.85,97.22,8.18">Bird Song Classification in Field Recordings: Winning Solution for NIPS4B 2013 Competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<ptr target="http://sabiod.org/NIPS4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,256.65,352.85,213.71,8.18;11,141.58,364.01,265.65,8.19">Proceedings of &apos;Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data -NIP4B&apos;, joint to NIPS Conf</title>
		<meeting>&apos;Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data -NIP4B&apos;, joint to NIPS Conf</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.60,386.06,337.14,8.10;11,141.58,397.22,262.73,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,209.56,386.06,260.18,8.10;11,141.58,397.22,135.26,8.10">Improving Bird Identification using Multiresolution Template Matching and Feature Selection during Training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,294.79,397.22,89.17,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,408.04,338.13,8.10;11,141.58,419.20,30.82,8.10;11,124.63,430.00,10.89,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,314.33,408.04,137.59,8.10">Automatic differentiation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.58,419.20,30.82,8.10;11,124.63,430.00,7.26,8.10">NIPS-W 11</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,141.58,430.00,328.98,8.10;11,141.58,441.16,140.80,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,304.34,430.00,73.90,8.10">librosa/librosa: 0.6.0</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balke</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1174893</idno>
		<ptr target="http://doi.org/10.5281/zenodo.1174893" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Version 0.6.0</note>
</biblStruct>

<biblStruct coords="11,131.89,451.96,338.59,8.10;11,141.58,463.15,255.90,8.10" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,329.56,451.96,140.91,8.10;11,141.58,463.15,118.55,8.10">Recognizing Birds from Sound -The 2018 BirdCLEF Baseline System</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,131.89,473.95,338.15,8.10;11,141.58,485.11,263.25,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,329.20,473.95,140.84,8.10;11,141.58,485.11,134.94,8.10">Large-Scale Bird Sound Classification using Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,295.30,485.11,89.17,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,496.27,338.47,8.10;11,141.58,507.07,207.00,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,286.84,496.27,183.52,8.10;11,141.58,507.07,80.05,8.10">A Multi-modal Deep Neural Network approach to Bird-song Identication</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,239.06,507.07,89.17,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,518.25,339.04,8.10;11,141.58,529.05,168.16,8.10" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="11,295.51,518.25,175.42,8.10;11,141.58,529.05,43.13,8.10">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,131.89,540.21,337.78,8.10;11,141.58,551.01,191.18,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,296.91,540.21,172.76,8.10;11,141.58,551.01,63.16,8.10">Recognizing Bird Species in Audio Files Using Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.24,551.01,89.17,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,562.17,338.44,8.10;11,141.58,572.99,65.65,8.10" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="11,276.79,562.17,126.46,8.10">Squeeze-and-Excitation Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,131.89,584.15,336.53,8.10" xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Birdclef</surname></persName>
		</author>
		<ptr target="http://www.imageclef.org/node/230,lastaccessed2018/05/29" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,595.31,338.32,8.10;11,141.58,606.11,202.21,8.10" xml:id="b18">
	<monogr>
		<ptr target="https://www.crowdai.org/challenges/lifeclef-2018-bird-monophone/leaderboards" />
		<title level="m" coord="11,141.58,595.31,137.62,8.10">crowdAI Subtask1 leaderboard page</title>
		<imprint>
			<date type="published" when="2018-05-29">2018/05/29</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,617.27,338.32,8.10;11,141.58,628.10,201.14,8.10" xml:id="b19">
	<monogr>
		<ptr target="https://www.crowdai.org/challenges/lifeclef-2018-bird-soundscape/leaderboards" />
		<title level="m" coord="11,141.58,617.27,137.62,8.10">crowdAI Subtask2 leaderboard page</title>
		<imprint>
			<date type="published" when="2018-05-29">2018/05/29</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.89,639.26,338.33,8.10;11,141.58,650.06,157.86,8.10" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="11,328.90,639.26,141.33,8.10;11,141.58,650.06,32.79,8.10">Bird detection in audio: a survey and a challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03417</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
