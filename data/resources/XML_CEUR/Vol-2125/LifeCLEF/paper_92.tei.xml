<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.51,115.96,306.35,12.62;1,196.10,133.89,223.15,12.62">Large-Scale Plant Classification using Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.91,171.56,52.22,8.74"><forename type="first">Josef</forename><surname>Haupt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.68,171.56,51.76,8.74"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.99,171.56,69.63,8.74"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.55,171.56,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.51,115.96,306.35,12.62;1,196.10,133.89,223.15,12.62">Large-Scale Plant Classification using Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B7CEF60DAB51265AEEC8144A8772C29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Plant Classification</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have significantly improved plant species classification in recent years. The goal of the 2018 ExpertLife-CLEF challenge was to compare the performance of human experts to machines trained on the PlantCLEF 2017 dataset containing 10.000 classes. We used the Inception, ResNet and DenseNet architectures to solve this complex task. In our experiments, complex neural net layouts yield strong results, comparable to human performance. We further push the overall accuracy through iterative adjustment of class weights. An ensemble consisting of a ResNet50 and two DenseNet201 with fine-tuned class weights reached a top1-accuracy of 77% on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ExpertLifeCLEF 2018 challenge <ref type="bibr" coords="1,297.41,471.16,10.52,8.74" target="#b0">[1]</ref> is the continuation of last year's Plant-CLEF 2017 task and part of LifeCLEF 2018 <ref type="bibr" coords="1,330.45,483.11,9.96,8.74" target="#b1">[2]</ref>. The main goal was to compare the performance of human experts and machines. In this paper we are going to describe our approach, the model architectures we used and our training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The dataset provided by CLEF is split into two parts. One part consists only of trusted images from the Encyclopedia Of Life (EoL), which implies that the object shown is a plant and is also labelled correctly. The second dataset has been built using Bing and Google search engines with no further validation in place <ref type="bibr" coords="1,159.33,608.30,9.96,8.74" target="#b2">[3]</ref>. This means, the number of samples per class is much higher, but images might not be labeled correctly or do not contain any plants at all. This year's training set is exactly the same as for the PlantCLEF 2017 task. The clean dataset holds 256.288 samples and the noisy set 1.432.162. These samples are for 10.000 different plants. Both datasets show massive class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Preparation</head><p>No changes have been made regarding the clean set. Some images from the noisy set had to be removed since they were either not compatible with image processing tools or/and had corrupt EXIF data, which led to complications. This affected 4.398 total images which is a very small part of the whole dataset and therefore neglectable. No further filtering has been done to either of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We tested and used three network architectures, namely Inception v3 <ref type="bibr" coords="2,425.15,290.84,11.76,8.74" target="#b3">[4]</ref>, ResNet50 <ref type="bibr" coords="2,480.83,290.84,14.17,8.74" target="#b4">[5]</ref> and DenseNet201 <ref type="bibr" coords="2,204.85,302.79,13.99,8.74" target="#b5">[6]</ref>. We performed most of our experiments using the DenseNet model which seemed to perform much better than the Inception v3 architecture and slightly better than the ResNet50 model. Additionally, DenseNet is a relatively recent architecture which was not used in the previous years (see e.g. <ref type="bibr" coords="2,134.77,350.61,10.30,8.74" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning</head><p>The mentioned three CNNs have been tested with various learning rates, batch sizes and optimizers to find the best hyperparameter setup. All experiments described have been done using the clean dataset.</p><p>Batch Size We used a NVIDIA P6000 graphics card for training. Since it was not possible to utilize all of the 24GB of VRAM for the training of a single model, the GPU memory was split into parts of 8GB. This way, we were able to train multiple models at the same time and finish more experiments. The restriction of VRAM per model led to a maximum batch size of 64 for the Inception v3 which outperformed models trained with a lower batch size. The ResNet50 had similar results, but with a maximum batch size of 32. The DenseNet201 used even more of the GPU memory and could only be used with a batch size of 16.</p><p>Optimizer Inception, ResNet and DenseNet were each tested with Stochastic Gradient Descent (SGD) and Adam. A static learning rate of 0.01 was used for the SGD. All architectures performed much better with the SGD than with then the adaptive optimizer. Learning Rate Since the SGD optimization was much more successful than Adam, we tried to increase the performance with further fine-tuning. We tested all networks with three different settings for the SGD:</p><p>a static learning rate of 0.01 a static learning rate of 0.001 and 0.9 Nesterov momentum a decaying learning rate schedule with 0.9 Nesterov momentum</p><p>The decaying learning rate schedule calculates the learning rate after every epoch using the following equation:</p><formula xml:id="formula_0" coords="3,234.91,400.27,245.68,12.98">lr = initial * drop rate 1+epoch drop epoch (1)</formula><p>The objective was to divide the learning rate by half after every tenth of the planned maximum epochs. Which means drop epoch calculates from max epochs/10. The experiments showed that the decay schedule was best for the Inception v3 and the ResNet50. The DenseNet201 did not benefit from this schedule and performed better with a static learning rate and Nesterov momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation</head><p>We used a variety of augmentation operations to further increase the dataset diversity. Selected experiments showed that the validation error can be greatly  reduced by the augmentations. Finally, we decided to implement following augmentation methods: Horizontal and vertical flip, zooming, rotating, shearing and shifting.</p><p>The range of every augmentation method was determined by empirical testing. Augmentation artifacts, for example as result of rotation, were filled in with a reflection of the original image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We trained the ResNet and Inception models according to best practices. However, we tried to fine-tune the DenseNet201 with adjusted class weights over multiple iterations and we will describe this specific process in more detail. For all experiments, ten percent of the trusted image set were used for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine-Tuning Class Weights</head><p>In an attempt to balance the classes, the initial weights were generated from the initial distribution of the samples. Therefore, the weight of each class was initially calculated using the average samples per class divided by the number of samples of that particular class. This redistribution of class weights already improved the accuracy by 9% over our local test set. Over the course of six iterations, the class weights were further modified. With each iteration a small test set was predicted and the error rates per class were calculated. The error rate is the percentage of samples which were classified correctly.</p><p>We modified each class weight using the following equation:</p><formula xml:id="formula_1" coords="4,224.77,566.77,255.82,8.74">weight = weight * (1.85 -error rate)<label>(2)</label></formula><p>We reduced weights for classes with an accuracy of 85% or higher by a small percentage; classes with a lower accuracy got higher weights in the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Process</head><p>Each model was trained with 10.000 samples per epoch, for up to 1.000 epochs.</p><p>Even with a very powerful GPU, the training of one model took three to five days. After the training, new class weights were generated using the error rates of the model on the test set and another model was trained with the modified class weights. We repeated this process six times until we submitted our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This year's ExpertLifeCLEF challenge was themed as human experts vs machines. The results show that some of the experts did have a remarkably high accuracy, which was rather surprising -but not all experts did beat the machine performance. Our system was able to beat one expert and scores were within a small margin with other experts. Most of our models were trained on the noisy data set, because the last year's PlantCLEF results showed that the highest scores were achieved when training mostly on the noisy data <ref type="bibr" coords="5,247.33,286.51,9.96,8.74" target="#b2">[3]</ref>. We submitted five runs, but run 1 and 2 are only marginally different. Therefore, run 2 will not be discussed here. Fig. <ref type="figure" coords="5,153.45,568.95,3.87,8.74">3</ref>: Official ExpertLifeCLEF scores. Machine learning systems outperform the majority of the human experts. However, some of the human expert are still leading with scores well above 90% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TUC MI run1</head><p>The first submission was based on a weighted voting between five models which were all trained on the noisy dataset. These five models consisted of one ResNet50, an Inception v3 and three DenseNet201. The weight of the vote was calculated using the validation accuracy of each model. We trained each of the DenseNets with a different class weight configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TUC MI run3</head><p>This submission consisted of the predictions of one DenseNet201. This model had the best single net validation accuracy of all our trained models. It was only trained on the noisy set and is the last iteration of the fine-tuning process described in 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TUC MI run4</head><p>Another single DenseNet201 was used for the fourth run, taken from the last iteration of the class weight fine-tuning. Unlike run3, this model was mainly trained on the clean data set.</p><p>TUC MI run5 Weighted voting of three models -two DenseNet201 and a single ResNet50 -resulted in our best scores. The two DenseNets were taken from run3 and run4. The ResNet50 model was trained on the noisy set, but with no further weight configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In our experiments, the DenseNet outperformed Inception v3 and the ResNet50 architectures. The adjustment of the class weights used during training did improve the performance of the DenseNet gradually. A single fine-tuned DenseNet already scored a Top1-accuracy of 71.8% which should increase further with more training time. Considering the strong results of our experiments and the submissions of other participants, we can conclude that machine plant classification is within reach of human-like performance. Some experts are able to identify plants based on images with very high accuracy. Most likely, further improvements of deep learning techniques will close this gap in the next few years.</p><p>Our system was implemented with Keras <ref type="bibr" coords="6,324.77,644.16,10.52,8.74" target="#b7">[8]</ref> and the source code is publicly available at: https://github.com/Josef-Haupt/ExpertCLEF2018.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,229.63,345.83,8.74;3,134.77,241.58,43.75,8.74;3,134.77,115.84,113.77,85.33"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: This figure shows the results of some of our experiments regarding the optimizer.</figDesc><graphic coords="3,134.77,115.84,113.77,85.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,211.43,223.84,192.51,8.74;4,139.16,115.83,79.54,79.54"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Examples of the used augmentations.</figDesc><graphic coords="4,139.16,115.83,79.54,79.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,331.63,345.83,225.80"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,331.63,345.83,225.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,191.99,482.19,231.38,60.45"><head>Table 1 :</head><label>1</label><figDesc>The final settings for the used architectures.</figDesc><table coords="3,211.31,501.50,189.66,41.14"><row><cell cols="4">architecture batch size optimizer learning rate</cell></row><row><cell>Inception V3</cell><cell>64</cell><cell>SGD</cell><cell>decay</cell></row><row><cell>ResNet50</cell><cell>32</cell><cell>SGD</cell><cell>decay</cell></row><row><cell>DenseNet201</cell><cell>16</cell><cell cols="2">SGD 0.001-0.0001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,358.33,345.82,82.96"><head>Table 2 :</head><label>2</label><figDesc>The models used in our submission and their respective scores on the official two testset splits.</figDesc><table coords="6,136.44,389.20,342.48,52.10"><row><cell>Submission</cell><cell>Models</cell><cell cols="2">Top1-experts Top1-testset</cell></row><row><cell cols="2">TUC MI run1 1 ResNet50, 1 Inception v3, 3 DenseNet201</cell><cell>64%</cell><cell>75.5%</cell></row><row><cell>TUC MI run3</cell><cell>1 DenseNet201</cell><cell>61.3%</cell><cell>71.8%</cell></row><row><cell>TUC MI run4</cell><cell>1 DenseNet201</cell><cell>58.7%</cell><cell>69.8%</cell></row><row><cell>TUC MI run5</cell><cell>1 ResNet50, 2 DenseNet201</cell><cell>64%</cell><cell>77%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,142.59,342.24,7.86;7,146.91,153.55,333.67,7.86;7,146.91,164.51,47.11,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,279.32,142.59,201.27,7.86;7,146.91,153.55,226.68,7.86">Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,395.86,153.55,84.73,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,175.46,342.25,7.86;7,146.91,186.42,333.68,7.86;7,146.91,197.38,333.68,7.86;7,146.91,208.34,47.11,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,193.97,186.42,286.62,7.86;7,146.91,197.38,224.58,7.86">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vallinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,393.58,197.38,87.01,7.86;7,146.91,208.34,18.43,7.86">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,219.30,342.24,7.86;7,146.91,230.26,269.64,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,296.38,219.30,184.22,7.86;7,146.91,230.26,169.29,7.86">Plant identification based on noisy web data: the amazing performance of deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,324.11,230.26,38.68,7.86">LifeCLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,241.22,342.24,7.86;7,146.91,252.18,333.68,7.86;7,146.91,263.14,274.26,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,402.92,241.22,77.67,7.86;7,146.91,252.18,162.79,7.86">Rethinking the Incpetion Archtecture of Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,331.74,252.18,148.85,7.86;7,146.91,263.14,182.52,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,274.09,342.25,7.86;7,146.91,285.05,333.67,7.86;7,146.91,296.01,75.26,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,290.34,274.09,186.07,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,160.45,285.05,316.21,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,306.97,342.24,7.86;7,146.91,317.93,333.68,7.86;7,146.91,328.89,201.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,402.45,306.97,78.14,7.86;7,146.91,317.93,93.97,7.86">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,261.01,317.93,219.58,7.86;7,146.91,328.89,93.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,339.85,342.25,7.86;7,146.91,350.81,204.82,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,197.81,339.85,282.78,7.86;7,146.91,350.81,49.40,7.86">Image-based Plant Species Identification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,217.92,350.81,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,361.77,175.21,7.86" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
