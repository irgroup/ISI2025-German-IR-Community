<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.36,115.96,328.63,12.62;1,157.69,133.89,299.98,12.62;1,256.57,151.82,102.22,12.62">Detection of Multidrug-Resistant Tuberculosis Using Convolutional Neural Networks and Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,229.86,189.49,70.10,8.74"><forename type="first">Martha</forename><surname>Tatusch</surname></persName>
							<email>tatusch@cs.uni-duesseldorf.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.65,189.49,62.85,8.74"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>conrad@cs.uni-duesseldorf.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.36,115.96,328.63,12.62;1,157.69,133.89,299.98,12.62;1,256.57,151.82,102.22,12.62">Detection of Multidrug-Resistant Tuberculosis Using Convolutional Neural Networks and Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04F0043958EF4C0471B05C91FD232D65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Networks</term>
					<term>Image Processing</term>
					<term>Classification</term>
					<term>Tuberculosis</term>
					<term>Multidrug-resistance</term>
					<term>ImageCLEF 2018</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2018, tuberculosis was one of the top 10 causes of death worldwide. Especially patients that develop a multidrug-resistance are endangered and need special medical treatment. Within the ImageCLEF 2018 challenge the automatic distinction between drug-sensitive and multidrug-resistant tuberculosis was investigated by only using the CT scan, age and gender of a patient. In this paper, we present different approaches using convolutional neural networks, decision trees and the combination of both classifiers. We show that our models achieve competitive results regarding the other participants of the challenge and show an improvement with respect to our last year's results. All of them are represented in the ranks between 4th and 26th of 39. Our best method regarding the AUC measure reached a score of 0.5810. In regard of accuracy the best approach got a result of 0.572.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis is a disease that is caused by an infection with the mycobacterium tuberculosis. Although the bacterium was found about 135 years ago, it still is one of the top ten causes of death worldwide according to the World Health Organization (WHO) <ref type="foot" coords="1,227.62,516.60,3.97,6.12" target="#foot_0">1</ref> . Due to medical progress the disease can be cured. Some patients, however, can develop a resistance to several drugs. This circumstance complicates the medical treatment and must therefore be recognised as soon as possible. Since the distinction between drug-sensitive (DS) and multidrugresistant (MDR) tuberculosis is difficult and necessitates several expensive tests, it would be helpful to find an automated solution that only requires the CT scans that are usually done anyway.</p><p>This year, the ImageCLEF 2018 tuberculosis Task 1 <ref type="bibr" coords="1,374.09,601.86,10.52,8.74" target="#b8">[9]</ref> took up this challenge once again. As last year's results as a whole were not satisfying for us yet, we wanted to participate in the competition one more time. The goal was to elaborate an automatic model that can predict a score for the probability whether a patient suffers from MDR. In contrast to the last year's challenge, additionally to the images the age and gender of the patients were given. Also the number of images was increased. Nevertheless, the amount of training data is still relatively small and therefore another challenging factor. In this paper we introduce our approaches for the task which include the usage of convolutional neural networks, decision trees and the combination of both classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the ImageCLEF 2017 tuberculosis challenge the same task was set for the first time <ref type="bibr" coords="2,171.76,268.00,12.25,8.74" target="#b7">[8]</ref>. Only the provided dataset varied slightly. There were around 10% fewer training images and neither the age nor the gender of the patients was given. In 2017, the best approach regarding the AUC score was a graph-based model that is described in <ref type="bibr" coords="2,250.16,303.87,9.96,8.74" target="#b6">[7]</ref>. The authors divided the lung into 36 regions that were represented by nodes. These nodes could then be connected by edges using different methods. The goal was to describe the lungs by a feature vector extracted from the underlying graph. These features could then be used to train a support vector machine. This model achieved ranks 1 to 3 with AUC scores between 0.5825 and 0.5624. The second best team used a combination of convolutional and recurrent neural networks and achieved results between 0.562 and 0.5501 on the ranks 4 -6. The model was published in <ref type="bibr" coords="2,236.36,399.51,14.61,8.74" target="#b14">[15]</ref>. The UIIP team was ranked 7th and thus represented the third best team. In <ref type="bibr" coords="2,235.88,411.47,15.50,8.74" target="#b12">[13]</ref> the authors introduced their own segmentation algorithm and their method based on feature extraction by considering supervoxels. This team used external sources to segment the lungs.</p><p>Regarding the accuracy, however, our team aimed the first rank with 0.5681 using convolutional neural networks with a flat network architecture <ref type="bibr" coords="2,445.76,459.29,9.96,8.74" target="#b1">[2]</ref>. The score was around 4% better than a model that only classifies into the most represented class. This classifier would reach an accuracy of 0.528.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Since the manual distinction between DS and MDR tuberculosis using only CT Scans is not possible until now, it is very difficult to determine relevant features of the images. For this reason, the usage of neural networks, which do not require feature extraction, is reasonable. On the one hand a convolutional neural network can be used, which only considers the images, on the other hand an architecture can be developed, which examines the images as well as the additional features age and gender. The combination of the images and the textual features cannot only be done by a CNN with multiple inputs, but also by creating a feature vector of the CNN's result, the age and the gender, and using it as the input of another classifier. In this work, a decision tree has been chosen as the second classifier. Before discussing the different approaches, the preprocessing of the images, which has a great impact on the results, will be explained, since it is the same for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The CT scans are three-dimensional grayscale images whose intensity values are specified by Hounsfield Units (HU) <ref type="bibr" coords="3,311.08,422.25,10.52,8.74" target="#b2">[3]</ref> -a uniform measure for CT scans. Although the Hounsfield Units are fixed, they leave a little scope for the actual values, which can vary due to different computer tomograph settings <ref type="bibr" coords="3,438.41,446.16,9.96,8.74" target="#b5">[6]</ref>. These circumstances can explain the high diversity of the given scans. In Figure <ref type="figure" coords="3,456.93,458.11,4.98,8.74" target="#fig_0">1</ref> two lungs of the given CT scans are shown in 1a) and 2a). In 1a) the Hounsfield Units are in the range of <ref type="bibr" coords="3,248.63,482.02,51.84,8.74">[-1024, 2017</ref>]. In 2a) the values vary between -1582 and 1941. Due to the different size of the HU ranges, the intensity of the scans varies a lot. This is also reflected in the representation of the images in Figure <ref type="figure" coords="3,472.84,505.93,3.87,8.74" target="#fig_0">1</ref>. Because of this, a preprocessing of the images is essential. As the preprocessing method developed in <ref type="bibr" coords="3,229.95,529.85,10.52,8.74" target="#b1">[2]</ref> led to an improvement of the results, it was used this year, too. The procedure is explained in <ref type="bibr" coords="3,312.29,541.80,10.52,8.74" target="#b1">[2]</ref> and can be summarized in 5 steps:</p><p>1. Set the smallest Hounsfield Unit value of the images to the second smallest value -1. The first step is necessary, as the smallest value represents the background of the scan and causes a large gap between the smallest and the second smallest value of it. The fourth step is done to decrease the noise of the image. As illustrated in Figure <ref type="figure" coords="4,178.93,154.86,3.87,8.74" target="#fig_0">1</ref>, the diversity of the segmented and normalized scans is very high. According to <ref type="bibr" coords="4,196.82,166.81,10.52,8.74" target="#b3">[4]</ref> the relevant features in an image are mainly represented in the properties of the nodules. These regions contain relatively high Hounsfield Units. Since the noise mostly occurs in the darker parts of the scan and the relevant regions are represented by high values, it is reasonable to reduce the noise which occurs beneath a certain threshold. In Figure <ref type="figure" coords="4,388.00,214.64,3.87,8.74" target="#fig_0">1</ref>, the lower bound of image 1a) has been increased from -1024 to -888 in image 1b). Thus, the value range has been decreased from 3041 values to 2905. Regarding the image 2a), the lower bound went from -1582 to -860 in 2b), so that the value range only contained 2801 values instead of 3523. The difference of the lower bounds of the two considered images has therefore been decreased from 558 to 28. Highlighting bright areas in the resulting scans by increasing the contrast is also helpful, since relevant regions get greater emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Neural Networks</head><p>The first approach of this work is a classification of the images using a convolutional neural network, that takes the three dimensional scans as input. This was done by using the Keras API <ref type="bibr" coords="4,263.90,374.67,10.52,8.74" target="#b4">[5]</ref> with Tensorflow <ref type="bibr" coords="4,349.72,374.67,10.52,8.74" target="#b0">[1]</ref> backend. Five quite similar architectures have been submitted. The architecture of two networks using the Spatial Pyramid Pooling Layer (SPP) <ref type="bibr" coords="4,303.22,398.59,15.50,8.74" target="#b10">[11]</ref> is shown in Table <ref type="table" coords="4,401.29,398.59,3.87,8.74" target="#tab_0">1</ref>.</p><p>Since the usual SPP was made for 2D images and the used CNNs were three dimensional, a few modifications had to be made. These were already carried out and described last year in <ref type="bibr" coords="4,277.69,434.71,10.52,8.74" target="#b1">[2]</ref> and could therefore be reused. The CNN named Conv48 creates 4 feature maps in the first convolutional layer. It has the binary cross entropy as its loss function and the stochastic gradient descent as optimizer. Conv68, however, determines 6 filters in the first block and uses the  categorical cross entropy and the adam optimizer. Both networks can handle an unfixed input size because of the SPP. Nevertheless, the nets have been trained and tested with images of different as well as uniform sizes.</p><p>In Table <ref type="table" coords="5,189.62,358.89,4.98,8.74" target="#tab_1">2</ref> the architecture of the second type of CNNs is shown. Instead of the SPP, the flatten layer is used. Besides, one additional block of a maxpooling and convolution layer is inserted. The difference between the structure of Flatten and Flatten3 is in the second and third block, where Flatten performs the convolution only over the X-and Y-axis of the images while Flatten3 considers all axes. Both networks use binary cross entropy and stochastic gradient descent. Another submitted architecture is called FlattenX. It is very similar to Flatten. Instead of the first maxpooling layer, however, it has a convolutional layer with 8 filters and a filter size of <ref type="bibr" coords="6,258.45,130.95,11.62,8.74" target="#b4">(5,</ref><ref type="bibr" coords="6,271.73,130.95,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="6,281.14,130.95,7.75,8.74" target="#b2">3)</ref>. The intention of this modification was to avoid a possible loss of information in the first layer. The last submitted CNN is the MultiInputCNN, which takes an image and a two dimensional vector as inputs. As can be seen in Figure <ref type="figure" coords="6,371.00,166.81,3.87,8.74" target="#fig_1">2</ref>, the first part of the net is identical to the Flatten CNN. After the flatten layer, however, a dense layer with 200 units is used. Its result is merged with the second input which contains the age and gender of the considered patient. This data is then processed by a dense layer with the size of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decision Trees</head><p>As already mentioned in the introduction of Section 3, all given information can be combined by using multiple classifiers. To refine the results of the CNN, a decision tree was trained using the CNN's binarized output, the age and the gender. The decision tree was chosen because the considered features and decisions are easy to track. This is very helpful to understand the impact of CNN's results. Depending on the selection of the network the classifier learned whether to consider the CNN's result or not. In Figure <ref type="figure" coords="6,308.62,355.36,4.98,8.74" target="#fig_3">3</ref> the structures of the two best decision trees are illustrated. On the right, the result of using the Flatten net is displayed. After checking age and gender the CNN's output is considered, as well. In contrast to this, the results of the Flatten3 net have no influence on the decision tree's assignment at all. The structure is shown on the left of Figure <ref type="figure" coords="6,472.84,403.18,3.87,8.74" target="#fig_3">3</ref>. It is noticeable that the tree in a) exactly corresponds to the first part of the one in b). The models have been retrieved by using the DecisionTreeClassifier from Scikit-Learn <ref type="bibr" coords="6,213.60,439.04,14.61,8.74" target="#b13">[14]</ref>. The parameters of both trees are the same: the minimum fraction per leaf was set to 10%, the minimum impurity decrease was 0.01. All architectures used the Gini Impurity as a measure for the information gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The training set that is provided by the ImageCLEF 2018 <ref type="bibr" coords="7,405.06,183.95,15.50,8.74" target="#b11">[12]</ref> contains 259 training images. 134 of them belong to patients with DS tuberculosis, the other 125 scans represent MDR. The test set is composed of 236 images, 99 of which have DS and 137 MDR. The ratio of the types is therefore very different in the two data sets. The CT scans consist of 50 to 400 slices with 512 × 512 pixels.</p><p>The images have been resized by using three-dimensional bounding boxes around the lungs. That means, the side lengths of the two-dimensional slices have been cropped to the outermost points of the whole lung and empty slices have been removed from the scan. When using a fixed input size, all images have then been resized to a X-and Y-length of 250 pixels and a total number of 100 slices. the length of a side (x, y or z) was too small, it was enlarged by adding equally black borders on both sides of the axis. If the length was too big, interpolation was used to downsize the side with the biggest variance regarding the desired dimension, so that the proportions of the side lengths remained the same. Afterwards the sides that became too small were enlarged as described.</p><p>All networks have been trained with the complete training set and for at least 30 epochs. In the combination of CNN and decision tree, the network first had been trained with 180 and the tree with 40 randomly selected CT scans which had an even type distribution. Afterwards the network was post-trained with all given training images.</p><p>In Table <ref type="table" coords="7,188.74,435.00,4.98,8.74" target="#tab_2">3</ref> the preliminary results of the challenge are displayed as published on the website<ref type="foot" coords="7,198.75,445.38,3.97,6.12" target="#foot_2">2</ref> . For a better understanding a few files of our team have been renamed. As our results are among the top 20 regarding the AUC score, only an excerpt of the top 25 of 39 runs was selected. The combination of the Flatten3 CNN and the decision tree achieved the best AUC as well as Accuracy score regarding the complete set of our submitted runs. This method was ranked 6th with respect to the AUC measure and 4th in regard of the Accuracy. The run is named "MDR Flatten3 DTree.txt". "MDR Flatten DTree.txt" represents the predictions of the combined approach with the Flatten CNN. It is noticeable that both runs retrieve exactly the same results. That is, because the predictions are the same, as well. This phenomenon can be explained by the fact that the network classified all scans of patients of gender 1 and the age between 43 and 55 as not resistant. The additional branch of the tree (in regard to the architecture without CNN) has therefore never been reached using the test set. Just behind our best runs we have the results of Conv68 which was trained with images of a fixed size. The networks Conv48, Flatten and Flatten3 reached with similar results the AUC ranks 11, 12 and 13. Regarding the accuracy, the Flatten architecture achieved the second best rank for our team. Unexpectedly, the MultiInputCNN received the worst results. The achieved accuracy of 0.5593 is the same for the ranks 9 to 12. The FlattenX network reached the second worst AUC results for our team. This confirms the thesis from <ref type="bibr" coords="8,413.52,484.88,10.52,8.74" target="#b1">[2]</ref> that smaller network architectures are better suited for the data set than deep ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that our approaches achieve competitive results. Despite the small amount of data, the use of convolutional neural networks can be reasonable if the architecture is not too deep. All our runs reached AUC ranks between 6 and 20 and accuracy ranks between 4 and 26 of 39. The best classifier turned out to be the decision tree, which only takes into account the age and gender of the patient. Its AUC and Accuracy results are only about 4% worse than the best scores of the challenge. Nevertheless, it has to be said that the accuracy results are worse than those of a classifier that only categorizes into the class with the most representatives. This one would reach a score of 58.05%.</p><p>The quality of the classifiers can certainly be increased by improving the preprocessing of the images with the help of medical expertise. Furthermore, the optimization of the provided masks could lead to better results, because these do not consider relevant regions of the lungs and contain parts of bones in some cases. Also, it would be interesting to perform the classification with other known medical data besides the age and gender of the patients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,305.99,345.83,8.74;3,134.77,317.95,91.74,8.74;3,178.19,207.45,102.36,62.11"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration of two example scans before (left) and after (right) the denoising step from [2].</figDesc><graphic coords="3,178.19,207.45,102.36,62.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,138.97,584.80,194.11,8.74;3,138.97,596.62,341.62,8.74;3,151.70,608.57,110.78,8.74;3,138.97,620.39,341.62,8.74;3,151.70,632.35,328.89,8.74;3,151.70,644.30,220.48,8.74;3,138.97,656.12,328.35,8.74"><head>2 .</head><label>2</label><figDesc>Normalize all values to the range of [0, 1]. 3. Segment the scans using the provided masks that were computed by the method presented in<ref type="bibr" coords="3,244.22,608.57,14.61,8.74" target="#b9">[10]</ref>. 4. Denoise the image by creating an intensity histogram with 256 even distributed bins. Set all values v with v ≤ u to u, where u is the upper border of the bin with the highest number of occurrences. 5. Increase the contrast by normalizing the range from [u, 1] to [0, 1] again.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,191.18,657.51,232.99,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture of the CNN with multiple inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,177.32,652.53,260.72,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Illustration of the two best decision trees' structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,194.57,514.05,227.76,146.92"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the CNNs using the SPP.</figDesc><table coords="4,194.57,536.35,227.76,124.63"><row><cell>Layer</cell><cell cols="3">Number of Filters Filter Size Stride</cell></row><row><cell>MaxPooling 0</cell><cell>n.a.</cell><cell cols="2">(4, 4, 1) (4, 4, 1)</cell></row><row><cell>Convolution 1</cell><cell>4 or 6</cell><cell cols="2">(3, 3, 3) (1, 1, 1)</cell></row><row><cell>MaxPooling 1</cell><cell>n.a.</cell><cell cols="2">(2, 2, 2) (1, 1, 1)</cell></row><row><cell>Convolution 2</cell><cell>8</cell><cell cols="2">(3, 3, 3) (1, 1, 1)</cell></row><row><cell>MaxPooling 2</cell><cell>n.a.</cell><cell cols="2">(2, 2, 2) (1, 1, 1)</cell></row><row><cell>Dropout 0.25</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>Spatial Pyramid</cell><cell>[1,2,4,8]</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>Dense</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,178.96,127.36,260.18,175.62"><head>Table 2 :</head><label>2</label><figDesc>Architecture of the CNNs using the flatten layer.</figDesc><table coords="5,178.96,149.66,260.18,153.32"><row><cell>Layer</cell><cell>Number of Filters</cell><cell>Filter Size</cell><cell>Stride</cell></row><row><cell>MaxPooling 0</cell><cell>n.a.</cell><cell>(4, 4, 1)</cell><cell>(4, 4, 1)</cell></row><row><cell>Convolution 1</cell><cell>16</cell><cell>(3, 3, 3)</cell><cell>(1, 1, 1)</cell></row><row><cell>MaxPooling 1</cell><cell>n.a.</cell><cell>(2, 2, 2)</cell><cell>(1, 1, 1)</cell></row><row><cell>Convolution 2</cell><cell>8</cell><cell cols="2">(3, 3, 3) or (3, 3, 1) (1, 1, 1)</cell></row><row><cell>MaxPooling 2</cell><cell>n.a.</cell><cell>(2, 2, 2)</cell><cell>(1, 1, 1)</cell></row><row><cell>Convolution 3</cell><cell>8</cell><cell cols="2">(3, 3, 3) or (3, 3, 1) (1, 1, 1)</cell></row><row><cell>MaxPooling 3</cell><cell>n.a.</cell><cell>(2, 2, 2)</cell><cell>(1, 1, 1)</cell></row><row><cell>Dropout 0.25</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>Flatten</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>Dense</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,127.36,345.83,314.98"><head>Table 3 :</head><label>3</label><figDesc>The top 25 of 39 results of the MDR detection tuberculosis task of the ImageCLEF 2018 challenge ranked by the AUC (R1) and accuracy score (R2).</figDesc><table coords="8,138.16,161.82,336.34,280.52"><row><cell>Group Name</cell><cell>Run</cell><cell cols="3">AUC R1 Accuracy R2</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-06-Mohan-SL-F3-Personal.txt</cell><cell>0.6178 1</cell><cell>0.5593</cell><cell>8</cell></row><row><cell cols="2">San Diego VA HCS/UCSD MDSTest1a.csv</cell><cell>0.6114 2</cell><cell>0.6144</cell><cell>1</cell></row><row><cell>VISTA@UEvora</cell><cell cols="2">MDR-Run-08-Mohan-voteLdaSmoF7-Personal.txt 0.6065 3</cell><cell>0.5424</cell><cell>17</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-09-Sk-SL-F10-Personal.txt</cell><cell>0.5921 4</cell><cell>0.5763</cell><cell>3</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-10-Mix-voteLdaSl-F7-Personal.txt</cell><cell>0.5824 5</cell><cell>0.5593</cell><cell>9</cell></row><row><cell>HHU-DBS</cell><cell>MDR Flatten3 DTree.txt</cell><cell>0.5810 6</cell><cell>0.5720</cell><cell>4</cell></row><row><cell>HHU-DBS</cell><cell>MDR Flatten DTree.txt</cell><cell>0.5810 7</cell><cell>0.5720</cell><cell>5</cell></row><row><cell>HHU-DBS</cell><cell>MDR Conv68adam fl.txt</cell><cell>0.5768 8</cell><cell>0.5593</cell><cell>10</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-07-Sk-LDA-F7-Personal.txt</cell><cell>0.5730 9</cell><cell>0.5424</cell><cell>18</cell></row><row><cell>UniversityAlicante</cell><cell>MDRBaseline0.csv</cell><cell>0.5669 10</cell><cell>0.4873</cell><cell>32</cell></row><row><cell>HHU-DBS</cell><cell>MDR Conv48sgd.txt</cell><cell>0.5640 11</cell><cell>0.5466</cell><cell>16</cell></row><row><cell>HHU-DBS</cell><cell>MDR Flatten.txt</cell><cell>0.5637 12</cell><cell>0.5678</cell><cell>7</cell></row><row><cell>HHU-DBS</cell><cell>MDR Flatten3.txt</cell><cell>0.5575 13</cell><cell>0.5593</cell><cell>11</cell></row><row><cell>UIIP BioMed</cell><cell cols="2">MDR run TBdescs2 zparts3 thrprob50 rf150.csv 0.5558 14</cell><cell>0.4576</cell><cell>36</cell></row><row><cell>UniversityAlicante</cell><cell>testSVM SMOTE.csv</cell><cell>0.5509 15</cell><cell>0.5339</cell><cell>20</cell></row><row><cell>UniversityAlicante</cell><cell>testOpticalFlowwFrequencyNormalized.csv</cell><cell>0.5473 16</cell><cell>0.5127</cell><cell>24</cell></row><row><cell>HHU-DBS</cell><cell>MDR Conv48sgd fl.txt</cell><cell>0.5424 17</cell><cell>0.5508</cell><cell>15</cell></row><row><cell>HHU-DBS</cell><cell>MDR Conv68 DTree.txt</cell><cell>0.5346 18</cell><cell>0.5085</cell><cell>26</cell></row><row><cell>HHU-DBS</cell><cell>MDR FlattenX.txt</cell><cell>0.5322 19</cell><cell>0.5127</cell><cell>25</cell></row><row><cell>HHU-DBS</cell><cell>MDR MultiInputCNN.txt</cell><cell>0.5274 20</cell><cell>0.5551</cell><cell>13</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-01-sk-LDA.txt</cell><cell>0.5260 21</cell><cell>0.5042</cell><cell>28</cell></row><row><cell>MedGIFT</cell><cell>MDR Riesz std correlation TST.csv</cell><cell>0.5237 22</cell><cell>0.5593</cell><cell>12</cell></row><row><cell>MedGIFT</cell><cell>MDR HOG std euclidean TST.csv</cell><cell>0.5205 23</cell><cell>0.5932</cell><cell>2</cell></row><row><cell>VISTA@UEvora</cell><cell>MDR-Run-05-Mohan-RF-F3I650.txt</cell><cell>0.5116 24</cell><cell>0.4958</cell><cell>30</cell></row><row><cell>MedGIFT</cell><cell>MDR AllFeats std correlation TST.csv</cell><cell>0.5095 25</cell><cell>0.4873</cell><cell>33</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.80,300.02,7.86"><p>http://www.who.int/en/news-room/fact-sheets/detail/tuberculosis date:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="1,447.83,656.80,32.77,7.86"><p>28.06.18   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="7,144.73,656.80,179.59,7.86;7,424.78,656.80,55.81,7.86"><p>http://www.imageclef.org/2018/tuberculosis date: 28.06.18</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,232.06,337.64,7.86;9,151.52,243.02,329.07,7.86;9,151.52,253.98,329.07,7.86;9,151.52,264.94,329.07,7.86;9,151.52,275.90,329.07,7.86;9,151.52,286.86,329.07,7.86;9,151.52,297.82,329.07,7.86;9,151.52,308.78,115.85,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,167.61,297.82,280.76,7.86">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,319.49,337.64,7.86;9,151.52,330.45,329.08,7.86;9,151.52,341.41,138.56,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,356.55,319.49,124.04,7.86;9,151.52,330.45,264.43,7.86">Convolutional neural networks for multidrug-resistant and drug-sensitive tuberculosis distinction</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Singhof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,437.46,330.45,43.13,7.86;9,151.52,341.41,58.30,7.86">CLEF2017 Working Notes</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,352.13,337.63,7.86;9,151.52,363.06,329.07,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,199.19,352.13,281.39,7.86;9,151.52,363.09,62.52,7.86">A quantitative theory of the hounsfield unit and its application to dual energy scanning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,221.06,363.09,173.32,7.86">Journal of Computer Assisted Tomography</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="493" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,373.80,337.64,7.86;9,151.52,384.76,329.07,7.86;9,151.52,395.72,329.07,7.86;9,151.52,406.65,261.36,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,176.55,384.76,304.04,7.86;9,151.52,395.72,329.07,7.86;9,151.52,406.68,83.63,7.86">Radiological findings of extensively drug-resistant pulmonary tuberculosis in non-aids adults: Comparisons with findings of multidrug-resistant and drugsensitive tuberculosis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C A</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,242.12,406.68,116.52,7.86">Korean Journal of Radiology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,417.40,203.96,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="9,226.66,417.40,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,428.11,337.63,7.86;9,151.52,439.07,329.07,7.86;9,151.52,450.01,121.94,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,331.28,428.11,149.31,7.86;9,151.52,439.07,177.59,7.86">Scanner and kVp dependence of measured CT numbers in the ACR CT phantom</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Cropp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Seslija</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.93,439.07,143.66,7.86;9,151.52,450.03,29.92,7.86">Journal of Applied Clinical Medical Physics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="338" to="349" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,460.75,337.64,7.86;9,151.52,471.71,329.07,7.86;9,151.52,482.67,263.10,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,343.09,460.75,137.50,7.86;9,151.52,471.71,329.07,7.86;9,151.52,482.67,37.13,7.86">Textured graph-model of the lungs for tuberculosis type classification and drug resistance prediction: Participation in imageclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="9,229.86,482.67,104.50,7.86">CLEF2017 Working Notes</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,493.38,337.63,7.86;9,151.52,504.34,329.07,7.86;9,151.52,515.30,198.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,433.23,493.38,47.36,7.86;9,151.52,504.34,325.30,7.86">Overview of ImageCLEFtuberculosis 2017 -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,515.30,104.50,7.86">CLEF2017 Working Notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,526.02,337.63,7.86;9,151.52,536.98,329.07,7.86;9,151.52,547.94,314.90,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,377.29,526.02,103.30,7.86;9,151.52,536.98,329.07,7.86;9,151.52,547.94,109.22,7.86">Overview of ImageCLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="9,281.67,547.94,104.50,7.86">CLEF2018 Working Notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,558.65,337.97,7.86;9,151.52,569.61,329.07,7.86;9,151.52,580.57,329.07,7.86;9,151.52,591.53,72.95,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,447.44,558.65,33.15,7.86;9,151.52,569.61,247.01,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,421.36,569.61,59.23,7.86;9,151.52,580.57,278.23,7.86">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,602.25,337.98,7.86;9,151.52,613.20,329.07,7.86;9,151.52,624.16,328.99,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,295.90,602.25,184.69,7.86;9,151.52,613.20,123.74,7.86">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.46,624.16,126.04,7.86">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8691</biblScope>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,634.88,337.98,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,329.07,7.86;10,151.52,119.67,329.07,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,210.96,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,200.93,119.67,275.68,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.33,130.63,284.13,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<title level="s" coord="10,456.15,130.63,24.45,7.86;10,151.52,141.59,141.41,7.86">LNCS Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,152.55,337.98,7.86;10,151.52,163.51,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,262.48,152.55,218.11,7.86;10,151.52,163.51,127.56,7.86">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,298.47,163.51,103.19,7.86">CLEF2017 Working Notes</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,174.47,337.98,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,329.07,7.86;10,151.52,207.32,324.47,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,394.26,196.39,86.33,7.86;10,151.52,207.34,72.43,7.86">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,231.41,207.34,155.12,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,218.30,337.98,7.86;10,151.52,229.26,317.15,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,329.02,218.30,151.57,7.86;10,151.52,229.26,111.05,7.86">Imageclef 2017: Imageclef tuberculosis task -the sgeast submission</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="10,283.92,229.26,104.51,7.86">CLEF2017 Working Notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
