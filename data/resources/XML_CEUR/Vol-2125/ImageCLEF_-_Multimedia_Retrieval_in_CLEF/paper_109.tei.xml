<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.84,115.96,217.67,12.62;1,212.49,133.89,190.37,12.62;1,205.67,151.82,204.02,12.62">Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.29,189.49,69.79,8.74"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.11,189.49,66.57,8.74"><forename type="first">Tung</forename><surname>Dinh-Duy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.11,189.49,81.09,8.74"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.65,201.45,73.57,8.74"><forename type="first">Viet-Khoa</forename><surname>Vo-Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.56,201.45,67.64,8.74"><forename type="first">Quoc-An</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.90,201.45,80.80,8.74"><forename type="first">Vinh-Tiep</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM University of Information Technology</orgName>
								<orgName type="institution" key="instit3">VNU-HCM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.84,115.96,217.67,12.62;1,212.49,133.89,190.37,12.62;1,205.67,151.82,204.02,12.62">Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6BA93EC10E174DCD65F5F85001DD420</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog Retrieval</term>
					<term>Visual Concept</term>
					<term>Visual Captioning</term>
					<term>Text-based Query Expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lifelog data provide potential insight analysis and understanding about people in their daily activities. However, it is still a challenging problem to index lifelog data efficiently and to provide a userfriendly interface that supports users to retrieve moments of interest. This motivates our proposed system to retrieve lifelog moment based on visual concept fusion and text-based query expansion. We first extract visual concepts, including entities, actions, and places from images. Besides NeuralTalk, we also proposed a novel method using conceptencoded feature augmentation to generate text descriptions to exploit further semantics of images. Our proposed lifelog retrieval system allows a user to search for lifelog moment with four different types of queries on place, time, entity, and extra biometric data. Furthermore, the key feature of our proposed system is to automatically suggest concepts related to input query concepts to efficiently assist a user to expand a query. Experimental results on Lifelog moment retrieval dataset of ImageCLEF 2018 demonstrate the potential usage of our method and system to retrieve lifelog moments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing number of wearable cameras and smart devices, people can easily capture their daily emotional and memorial events Besides visual data, such as images or video clips, lifelog data <ref type="bibr" coords="1,314.16,596.34,15.50,8.74" target="#b9">[10]</ref> can be of different categories, such as audio clips, GPS or biometric data (heart rate, galvanic skin response, calorie burn, etc).</p><p>As a new promising trend for research and applications <ref type="bibr" coords="1,384.68,632.21,9.96,8.74" target="#b5">[6]</ref>, lifelogging analysis <ref type="bibr" coords="1,134.77,644.16,15.50,8.74" target="#b9">[10]</ref> to retrieve moments of interest from lifelog data helps people to revive memories <ref type="bibr" coords="1,179.12,656.12,14.61,8.74" target="#b17">[18]</ref>, verify events, find entities, or analyse people's social traits <ref type="bibr" coords="1,453.54,656.12,10.98,8.74" target="#b6">[7]</ref>.</p><p>There are two main problems for lifelogging analysis and moment retrieval: (i) to analyze and efficiently index lifelog data, and (ii) to enhance usability and ease-of-use for users to input queries and retrieve moments of interest. To solve the first problem, we propose to extract concepts (entities and places) <ref type="bibr" coords="2,465.09,154.86,15.50,8.74" target="#b23">[24]</ref> and generate text descriptions from images. Currently, we gather lifelog images into visual shots but we still process each image independently. For the second problem, we develop a lifelog retrieval system that supports four types of queries (place, time, entity, and metadata) <ref type="bibr" coords="2,281.95,202.68,18.56,8.74" target="#b23">[24]</ref> and automatically suggests hints to users the related concepts from an input query.</p><p>Comparing to our initial system <ref type="bibr" coords="2,291.24,226.83,14.61,8.74" target="#b23">[24]</ref>, our current system for lifelog data processing and retrieval has two main improvements to further exploit the semantic from text descriptions for images. First, we propose a new method for text description generation based on the spatial attention model by Kelvin Xu et. al <ref type="bibr" coords="2,465.09,262.70,15.50,8.74" target="#b24">[25]</ref> and the semantic attention model by Quanzeng You et. al <ref type="bibr" coords="2,392.58,274.65,14.87,8.74" target="#b25">[26]</ref>. Second, we develop the related concept recommendation module into our retrieval system for query expansion. From an initial concept, our system can automatically suggests potential related concepts for users to expand the query with the expectation to cover a wider range of retrieved results. Using our proposed method and system, we achieve the score of 0.479 for Lifelog moment retrieval (LMRT) with Lifelog data <ref type="bibr" coords="2,151.84,346.38,12.81,8.74" target="#b5">[6]</ref> in ImageCLEF 2018 <ref type="bibr" coords="2,253.63,346.38,17.71,8.74" target="#b14">[15]</ref> , ranked second in the challenge of LMRT.</p><p>In Section 2, we briefly review recent achievements in Lifelog, place retrieval, and visual instance search. Then we propose our method to offline process lifelogging data in Section 3 and go deeply on image captioning in Section 4. Our system to assist users to find a moment of interest based on an arbitrary query is presented in Section 5. The conclusion and open questions for future work are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The comparison on evaluating the effectiveness of information access and retrieval systems operating over personal lifelog data has been considered for a long time. In 2012, the tasks in NTCIR-12 which are the first ones focus on known-item search and activity understanding applied over lifelog data <ref type="bibr" coords="2,446.97,512.42,9.96,8.74" target="#b7">[8]</ref>. The lifelog data is collected form 3 different volunteers wearing a camera to record visual daily life data for a month. In addition, they also provide a visual concept information using Caffe CNN-based visual concept detector. Because of the large lifelog data, many different analytic approaches and applications have been discussed in the workshop. The area of interest is widen to other aspect than the origin information retrieval purpose <ref type="bibr" coords="2,290.25,584.15,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,302.43,584.15,7.01,8.74" target="#b8">9]</ref>. While some team focus on improve the friendly UX/UI for an end-user, the others considered the crucial in privacy and data security. In addition, the way for preservation and maintenance of lifelogs is also discussed in the workshop.</p><p>Moreover, they keep on enrich the lifelog data by adding more information in semantic locations like a cafe, restaurant and physical activity such as walking, cycling and running in ImageCLEFlifelog 2017 <ref type="bibr" coords="2,339.09,656.12,9.96,8.74" target="#b4">[5]</ref>. The tasks on this lifelog data are (1) retrieval task which is the evaluation on the correctness of returned image followed several specific queries and (2) summarization task that summarize all the images according to specific requirment.</p><p>Location plays an important role in Lifelog Moment Retrieval Task (LMRT) that increases the accuracy of the whole system. In the task, the information about a place is very important so it is better to retrieve as much as possible diverse images. Duc-Tien et al. have introduced the method to deal with this problem using the dataset collected on Flickr <ref type="bibr" coords="3,332.77,203.63,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,344.95,203.63,7.01,8.74" target="#b2">3]</ref>. The precision of the method has improved up to 75%.</p><p>Sivic has introduced first Bag-of-visual-word (BoW) model which is on of the most state-of-the-art approaches for video retrieval <ref type="bibr" coords="3,361.45,240.45,14.61,8.74" target="#b22">[23]</ref>. The model follows the key assumption, which is the two similar images sharing the significant number of local path matched against each other. In order to boost the performance of Instance Search system, many techniques, such as RootSIFT feature <ref type="bibr" coords="3,442.28,276.32,9.96,8.74" target="#b1">[2]</ref>, large vocabulary <ref type="bibr" coords="3,184.96,288.27,14.61,8.74" target="#b19">[20]</ref>, soft assignment <ref type="bibr" coords="3,274.74,288.27,14.61,8.74" target="#b20">[21]</ref>, multiple detectors and feature combination at late fusion, query-adaptive asymmetrical dissimilarities <ref type="bibr" coords="3,393.82,300.23,14.61,8.74" target="#b27">[28]</ref>, are applied. In this paper, we focus on baseline model of BOVW for easy of implementation and better performance.</p><p>Word Representations in Vector Space is a problem that learns high-quality distributed vector representations, capturing a large number of precise syntactic and semantic word presentation <ref type="bibr" coords="3,295.26,360.96,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="3,312.42,360.96,11.62,8.74" target="#b15">16]</ref>. The state of the art is Word2Vec which is based on Skip-gram model introduced by Mikolov et. al. They improve the Skip-gram in time consuming by simplifying the 'hierarchical softmax' with 'negative sampling' and learning regular word representation.</p><p>Generating an natural language description for one specific image is challenging problem in Computer Vision. Although a lot of work concentrates on labeling images with fixed set on visual categories, their drawback are relying on hard-coded visual concepts and sentence templates that reducing complex visual scene. To overcome this problem, Karpathy and Fei-Fei introduced NeuralTalk, state-of-the-art model, to generate the caption of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We inherit our framework in the Lifelog Search Challenge <ref type="bibr" coords="3,382.34,559.52,15.50,8.74" target="#b23">[24]</ref> to solve the LMRT challenge. However, the main difference between the two methods is that we further take advantage of the semantic that can be provided by text descriptions generated from an image. To exploit different aspects as well as styles of semantics in text description, we use NeuralTalk and our Concept Augmentation with Attention method to generate different descriptions corresponding to one image with the expectation to get more insight information about that image.</p><p>Figure <ref type="figure" coords="3,182.39,644.16,4.98,8.74" target="#fig_0">1</ref> presents the overview of our proposed method to offline process lifelogging data. Our method has four main components:  Comparing to our proposed method to offline process lifelogging data <ref type="bibr" coords="4,462.32,425.40,14.61,8.74" target="#b23">[24]</ref>, our enhanced method further exploit the text descriptions of images. We use NeuralTalk to generate the baseline text descriptions, and we propose our new method with Concept Augmentation and Attention for better description generation. In this section, we briefly review the three components that we reuse from our framework <ref type="bibr" coords="4,226.34,485.17,14.61,8.74" target="#b23">[24]</ref>, and we reserve Section 4 to present our new method for image captioning.</p><p>Visual Clustering: Instead of processing all images, we first group similar contiguous images into visual shots <ref type="bibr" coords="4,286.91,522.43,16.21,8.74" target="#b23">[24]</ref>. By this way, we can get better context of a scene in a visual shot. As images are captured with 45 second interval, the location and pose of an entity may change in two consecutive images. Therefore, we use FlowNet <ref type="bibr" coords="4,209.82,558.30,15.50,8.74" target="#b13">[14]</ref> to estimate the optical flow vectors at all corresponding pixels in two continuous frames, then determine if the two frames are similar.</p><p>To link shots corresponding to the same scene, we propose a solution to use our Bag-of-Visual-Word (BoVW) framework for visual retrieval <ref type="bibr" coords="4,429.17,595.55,15.43,8.74" target="#b18">[19]</ref>. For an image x in a visual shot S i , we retrieve similar images in other shots. The distance between the two visual shots S i and S j is determined by the mean distance between their images. distance(S i , S j ) = mean{distance(x, y), f or x ∈ S i and y ∈ S j }</p><p>To represent the similarity relationship between visual shots, we create an undirected graph with nodes as visual shots and edges. There is an undirected edge to link two nodes S i and S j if their distance is less than a threshold max distance .</p><p>Each connected component represents a cluster of similar visual shots, and is expected to represent the same or similar scene in real life.</p><p>Concept Extraction: We focus on three types of concepts that can be extracted from an image. We use MIT Place API <ref type="bibr" coords="5,360.11,190.72,15.50,8.74" target="#b26">[27]</ref> to determine the scene category and scene attributes of an image. To extract entities, we use Faster RCNN <ref type="bibr" coords="5,168.69,214.64,14.60,8.74" target="#b21">[22]</ref>. For possible action detection, we extract the main verb in each description generated from an image. Besides NeuralTalk, we propose our new method for image captioning, presented in Section4.</p><p>Augmented Data Processing: We feed the information about bodymetrics of the volunteers provided by the challenge to retrieve a better result. In this LMRT, for each bodymetric information, we cluster them into a range of value. Considering heart-rate for an example, we divide the range from 1 to 150 into fifteen 10-unit wide periods. Then the images is clustered into group based on those period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image Captioning with Concept Augmentation and Attention</head><p>Our proposed model is based on the spatial attention model in work of Kelvin Xu et. al <ref type="bibr" coords="5,175.21,386.30,15.50,8.74" target="#b24">[25]</ref> and the semantic attention model in work of Quanzeng You et. al <ref type="bibr" coords="5,142.20,398.26,14.87,8.74" target="#b25">[26]</ref>. Our model consists of two main modules: feature extraction and caption generation.</p><p>In the feature extraction module, with an input image I, we use a deep convolutional neural network to produce a N × N × D feature map. We then use an object detection model to extract the labels of the objects in the image. The object detection model produces the probability that an object appears in the image or not. We choose k labels with the highest scores to avoid noise in the image. These labels are represented as one-hot vector and then multiplied with an embedding matrix E to produce L-dimension embedded vectors. Next, both the feature map and the embedded vectors are passed into the caption generation module.</p><p>In the caption generation module, the feature map and the embedded vectors are first processed through two different attention models. The first attention model uses a weight value α i produced by the combination of information from previous hidden state and each feature from the feature map to show how much "attention" is on the feature vector a i in the region i of the feature map. The image context vector at the current timestep t is then produced from the feature map and the weight value.</p><formula xml:id="formula_1" coords="5,236.11,619.87,244.48,9.65">α ti = f sof tmax (f attend1 (a i , h t-1 ))<label>(2)</label></formula><formula xml:id="formula_2" coords="5,269.57,637.94,211.03,30.32">z ta = N ×N i=1 α ti • a i<label>(3)</label></formula><p>The function f sof tmax helps the model to generate weights α i for each region sumed up to 1 so that the context vector would be an expected context at timestep t.</p><p>We use the similar method in the second attention model for the embedded vectors of the labels. Each vector b j in the k embedded vectors is multiplied with a weight value β j and summed up to produce the label context vector.</p><formula xml:id="formula_3" coords="6,236.98,202.68,239.37,9.65">β ti = f sof tmax (f attend2 (b i , h t-1 )) (<label>4</label></formula><formula xml:id="formula_4" coords="6,476.35,202.68,4.24,8.74">)</formula><formula xml:id="formula_5" coords="6,274.44,227.69,206.15,30.32">z tb = k j=1 β j • b j (5)</formula><p>Finally, the image context vector z a and the label context vector z b are fed into an LSTM <ref type="bibr" coords="6,193.27,280.71,21.79,8.74" target="#b12">[13]</ref> to generate one word at each time step. The two context vectors are combined with the embedded vector of the word in the previous time step by a linear function which is also a fully connected layer in the model to get a context vector z. The LSTM takes in the vector z and produce a hidden state h t at each time step t. The hidden state h t is then passed through a fully connected layer to predict the next word in the caption. The predicted word is fed back into the attention models to produce a new set of weight values α, β and calculate a new context vector z for the next time step. Our entire model is showed in Figure <ref type="figure" coords="6,212.19,376.35,4.98,8.74" target="#fig_2">2</ref> .  Unlike the work of Kelvin Xu et. al <ref type="bibr" coords="6,297.54,632.21,15.50,8.74" target="#b24">[25]</ref> which only uses the attention on image features and the work Quanzeng You et. al <ref type="bibr" coords="6,327.00,644.16,15.50,8.74" target="#b25">[26]</ref> which only looks at the image once and then uses attention on attributes, our model combines information from both features and labels. At each time step, the model will pay attention on certain region of image and on some certain tags to generate image caption.</p><p>Our model is trained on MS COCO dataset. In our implementation, we use the ResNet101 <ref type="bibr" coords="7,196.23,155.04,19.05,8.74" target="#b11">[12]</ref> model as our convolutional neural network and the Mask R-CNN <ref type="bibr" coords="7,161.59,166.99,21.46,8.74" target="#b10">[11]</ref> as our object detector. The feature map is extracted from the last convolutional layer with size 14x14x1024. For the labels, we choose k=15 highestscore labels. Each label is embedded into a 512-dimension vector. The size of the final context vector is 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Strategy</head><p>In this Section, we present our system overview that can assist users to retrieve a memory in the past corresponding to a given query described in natural language text. Currently, we do not aim to make our system smart enough to automatically analyze and fully understand the query, but the system can help a user to stepby-step retrieve then filter with multiple criteria to get the appropriate results.</p><p>Our system provides multiple strategies to query for past memory in lifelogging data, but it is the user who actively decides which sequence of strategies to search for a specific memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Query</head><p>Query 1: Find the moments when I was preparing salad. To be considered relevant, the moments must show the lifelogger preparing a salad, in a kitchen or in an office environment. Eating salad is not considered relevant. Preparing other types of food is not considered relevant.</p><p>First of all, we consider on locations of the scenario. In this scenario, we focus on kitchen, office. Then we focus on the main context of the scenario. We mainly mining the main keyword "preparing salad". Through Word2Vec <ref type="bibr" coords="7,418.66,488.39,15.50,8.74" target="#b16">[17]</ref> model, we explore the similar and relevant words such as vegetable, fruit to extend the retrieval results. Finally, we get the candidate results and we manually choose the best results to submit. To create the variance of the result, we sort result based on time and choose the results at the different time. Figure <ref type="figure" coords="7,429.14,536.21,4.98,8.74">3</ref> illustrates some image results of the query. Query 2: Find the moments when I was with friends in Costa coffee. To be considered relevant, the moment must show at least a person together with the lifelogger in any Costa Coffee shop. Moments that show the user alone are not considered relevant.</p><p>In the query, the information of time is not mentioned so we could not apply a day periods to retrieve an image. In addition, the number of images have a context in the coffee shop is numerous and the volunteer drink coffee in different shops, the user has to decide which shop is Costa. Moreover, because the volunteer visit a coffee shop in different day, for each day, we select only one image that meets the information in the query. Figure <ref type="figure" coords="8,354.88,519.80,4.98,8.74">4</ref> show the retrieved images corresponding to the query. Query 3: Find the moments when I was having dinner at home. Moments in which the user was having dinner at home are relevant. Dinner in any other location is not relevant. Dinner usually occurs in the evening time.</p><p>For this scenario, we consider time period and location. We mainly explore the context that is dinner indoor in the evening. Besides, we expand the results through keywords which are similar and relevant to dinner such as eat, drink, feed. With the retrieval list, we manually select the best results as a submission. Because the dinner is a daily activity, it almost exists every day so we select images at the different time and different days to create a variance of the submission. Figure <ref type="figure" coords="8,219.51,656.12,4.98,8.74" target="#fig_4">5</ref> illustrates retrieval images corresponding to the query. Query 4: Find the moments when I was giving a presentation to a large group of people. To be considered relevant, the moments must show more than 15 people in the audience. Such moments may be giving a public lecture or a lecture in the university.</p><p>The system could recognize that the scene of this query description is in the lecture hall where lot of students attend. The system suggests the "lecture' as a keyword for this scenario. Furthermore, "chair" and "table" are the object could be considered. From these keywords, the system return a list of images represent presentation. The user has to count the number of student appear in the image to decide which one is satisfied the query. The result images are shown in Figure <ref type="figure" coords="9,134.77,448.90,3.87,8.74" target="#fig_5">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Result</head><p>Table <ref type="table" coords="10,162.35,140.68,4.98,8.74" target="#tab_1">1</ref> illustrates our result at LMRT challenge <ref type="bibr" coords="10,350.99,140.68,9.96,8.74" target="#b5">[6]</ref>. Our system is current the assistant retrieval system which helps user can retrieve the image by the textual query. Our long term goal aims to the automatically retrieval system with the prior textual query given by user. The current result demonstrates the potential use of our system for moment retrieval in lifelog data. Our strategy is searching by keywords attention-based on an image description. The location and time period are mainly considered to filter the result corresponding to the context of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our proposal system assists user can retrieve lifelog moments based on the different types of queries (i.g. place, time, entity, extra biometric data). Leverage our system in Lifelog Search Challenge workshop to this challenge, we further explore the image caption to gain a better result. Besides, we proposed a novel method for generating image caption, it makes the description for every image can be diverse. For novice users, they could not know how to search with their existent keywords. To deal with this problem, we proposed a Keywords Recommendation by Word2Vec. We build-up a keywords dictionary which helps users can select the useful keywords for searching based on their existent keywords.</p><p>However, there are some weakness in our system. The retrieved results are not diverse. Diversity results play an important role in retrieval systems, especially in the Lifelog Moment Retrieval system. It helps to achieve a comprehensive and complete view on the query. Diversification of search results allows for better and faster search, gaining knowledge about different perspectives and viewpoints on retrieved information sources.</p><p>In following works, we will push new filters to remove noise and non-relative retrieved results. Additionally, we use new way to visualize images, specifically, we will cluster images into many group based on their features. It helps users can have a good visualization and easier to select images. Furthermore, our longterm goal is build-up a system which automatically searches with a raw textual query, is learning on strategies of users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,177.62,241.06,260.11,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our Proposed Method in Lifelog Moment Retrieval Task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,233.44,596.43,148.48,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our image captioning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,247.71,286.22,119.94,7.89;8,176.70,194.94,129.89,69.16"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. The results of query 1</figDesc><graphic coords="8,176.70,194.94,129.89,69.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,247.71,286.22,119.94,7.89;9,176.70,194.94,129.89,69.16"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The results of query 3</figDesc><graphic coords="9,176.70,194.94,129.89,69.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,247.71,655.03,119.94,7.89;9,176.70,563.75,129.89,69.16"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The results of query 4</figDesc><graphic coords="9,176.70,563.75,129.89,69.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,177.27,208.74,260.81,83.85"><head>Table 1 .</head><label>1</label><figDesc>Result on ImageCLEF 2018 Lifelog -LMRT challenge.</figDesc><table coords="10,250.57,229.54,114.21,63.06"><row><cell>Rank</cell><cell>Team</cell><cell>Score</cell></row><row><cell>1</cell><cell>AIlabGTi</cell><cell>0.545</cell></row><row><cell>2</cell><cell>HCMUS</cell><cell>0.479</cell></row><row><cell>3</cell><cell>Regim Lab</cell><cell>0.424</cell></row><row><cell>4</cell><cell>NLP-lab</cell><cell>0.395</cell></row><row><cell cols="3">5 CAMPUS-UPB 0.216</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,145.39,337.64,7.86;11,151.52,156.35,140.03,7.86" xml:id="b0">
	<monogr>
		<title level="m" coord="11,151.53,145.39,324.99,7.86">LTA &apos;16: Proceedings of the First Workshop on Lifelogging Tools and Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,167.87,337.64,7.86;11,151.52,178.83,329.07,7.86;11,151.52,189.79,329.07,7.86;11,151.52,200.75,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,287.19,167.87,193.41,7.86;11,151.52,178.83,59.84,7.86">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,231.83,178.83,248.76,7.86;11,151.52,189.79,116.44,7.86;11,340.37,189.79,40.83,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
	<note>CVPR &apos;12</note>
</biblStruct>

<biblStruct coords="11,142.96,212.27,337.63,7.86;11,151.52,223.23,329.07,7.86;11,151.52,234.19,325.03,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,457.24,212.27,23.35,7.86;11,151.52,223.23,260.59,7.86">A hybrid approach for retrieving diverse social images of landmarks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G B D</forename><surname>Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.86,223.23,44.73,7.86;11,151.52,234.19,236.53,7.86">2015 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,245.71,337.64,7.86;11,151.52,256.67,329.07,7.86;11,151.52,267.60,329.07,7.89;11,151.52,278.58,22.02,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,455.25,245.71,25.34,7.86;11,151.52,256.67,329.07,7.86;11,151.52,267.62,25.50,7.86">Multimodal retrieval with diversification and relevance feedback for tourist attraction images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G B D</forename><surname>Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,183.70,267.62,202.44,7.86">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017-08">Aug 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,290.10,337.64,7.86;11,151.52,301.06,329.07,7.86;11,151.52,312.02,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,151.52,301.06,282.34,7.86">Overview of imagecleflifelog 2017: Lifelog retrieval and summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,455.90,301.06,24.69,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,323.54,337.64,7.86;11,151.52,334.50,329.07,7.86;11,151.52,345.46,329.07,7.86;11,151.52,356.42,292.63,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,442.93,323.54,37.66,7.86;11,151.52,334.50,329.07,7.86;11,151.52,345.46,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,200.54,345.46,108.44,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="11,318.26,345.46,162.34,7.86;11,151.52,356.42,14.99,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,367.94,337.63,7.86;11,151.52,378.90,329.07,7.86;11,151.52,389.86,329.07,7.86;11,151.52,400.82,283.31,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,290.56,367.94,190.03,7.86;11,151.52,378.90,166.47,7.86">Social relation trait discovery from visual lifelog data with facial multi-attribute framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,337.90,378.90,142.69,7.86;11,151.52,389.86,256.38,7.86">Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the 7th International Conference on Pattern Recognition Applications and Methods<address><addrLine>ICPRAM; Funchal, Madeira -Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-01-16">2018. January 16-18, 2018. 2018</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,412.34,337.64,7.86;11,151.52,423.29,329.07,7.86;11,151.52,434.25,329.07,7.86;11,151.52,445.21,329.07,7.86;11,151.52,456.17,329.07,7.86;11,151.52,467.13,220.42,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<idno>number SFI/12/RC/</idno>
		<title level="m" coord="11,406.02,412.34,74.57,7.86;11,151.52,423.29,57.71,7.86;11,245.37,423.29,235.21,7.86;11,151.52,434.25,153.21,7.86">the authors acknowledge the financial support of Science Foundation Ireland (SFI) under grant</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Overview of ntcir-12 lifelog task. 2289 and the input of the DCU ethics committee and the risk &amp; compliance officer. We acknowledge financial support by the European Science Foundation via its Research Network Programme ?Evaluating Information Access Systems?</note>
</biblStruct>

<biblStruct coords="11,142.96,478.65,337.63,7.86;11,151.52,489.61,329.07,7.86;11,151.52,500.57,329.07,7.86;11,151.52,511.53,140.03,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,166.70,489.61,275.13,7.86">Lta 2017: The second workshop on lifelogging tools and applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<idno>MM &apos;17</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,463.04,489.61,17.56,7.86;11,151.52,500.57,220.63,7.86">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1967" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,523.05,337.98,7.86;11,151.52,533.98,163.59,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,326.37,523.05,119.11,7.86">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,452.82,523.05,27.77,7.86;11,151.52,534.01,68.06,7.86">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014-06">Jun 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,545.53,337.97,7.86;11,151.52,556.48,235.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,338.33,545.53,44.36,7.86">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,403.96,545.53,76.63,7.86;11,151.52,556.48,189.01,7.86">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,568.00,337.98,7.86;11,151.52,578.96,329.07,7.86;11,151.52,589.92,47.74,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,299.05,568.00,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.84,578.96,314.75,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,601.42,337.98,7.89;11,151.52,612.40,89.33,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,284.94,601.44,98.82,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,392.01,601.44,65.58,7.86">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,623.92,337.98,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,157.56,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,430.80,623.92,49.79,7.86;11,151.52,634.88,219.17,7.86">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,390.82,634.88,89.77,7.86;11,151.52,645.84,240.42,7.86">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.97,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,329.07,7.86;12,151.52,174.47,329.07,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,46.58,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,197.95,152.55,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,163.51,329.07,7.86;12,151.52,174.47,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="12,151.52,185.43,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,206.51,337.98,7.86;12,151.52,217.45,231.48,7.89" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,341.33,206.51,139.26,7.86;12,151.52,217.47,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,227.60,337.98,7.86;12,151.52,238.56,329.07,7.86;12,151.52,249.52,329.07,7.86;12,151.52,260.47,204.75,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,395.73,227.60,84.86,7.86;12,151.52,238.56,224.48,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,402.67,249.52,77.92,7.86;12,151.52,260.47,128.10,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,270.60,337.97,7.86;12,151.52,281.56,329.07,7.86;12,151.52,292.52,329.07,7.86;12,151.52,303.48,184.31,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,351.85,270.60,128.74,7.86;12,151.52,281.56,234.42,7.86">Nowandthen: A social networkbased photo recommendation tool supporting reminiscence</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fjeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,406.67,281.56,73.92,7.86;12,151.52,292.52,272.11,7.86;12,151.52,303.48,37.62,7.86">Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia</title>
		<meeting>the 15th International Conference on Mobile and Ubiquitous Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
	<note>MUM &apos;16</note>
</biblStruct>

<biblStruct coords="12,142.62,313.60,337.97,7.86;12,151.52,324.54,329.07,7.89;12,151.52,335.52,25.60,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,380.27,313.60,100.32,7.86;12,151.52,324.56,234.39,7.86">A combination of spatial pyramid and inverted index for large-scale image retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,392.87,324.56,38.26,7.86">IJMDEM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,345.65,337.97,7.86;12,151.52,356.61,329.07,7.86;12,151.52,367.57,212.82,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,376.47,345.65,104.12,7.86;12,151.52,356.61,156.11,7.86">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,330.64,356.61,149.95,7.86;12,151.52,367.57,184.15,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,377.69,337.98,7.86;12,151.52,388.65,314.11,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,349.11,377.69,131.48,7.86;12,151.52,388.65,226.00,7.86">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,410.34,388.65,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,398.78,337.98,7.86;12,151.52,409.74,329.07,7.86;12,151.52,420.70,123.97,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,313.02,398.78,167.57,7.86;12,151.52,409.74,158.93,7.86">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,331.43,409.74,149.16,7.86;12,151.52,420.70,95.31,7.86">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,430.82,337.98,7.86;12,151.52,441.78,329.07,7.86;12,151.52,452.74,329.07,7.86;12,151.52,463.70,47.61,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,247.70,430.82,232.90,7.86;12,151.52,441.78,33.27,7.86">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,203.85,441.78,276.74,7.86;12,151.52,452.74,25.39,7.86;12,269.33,452.74,36.56,7.86">Proceedings of the Ninth IEEE International Conference on Computer Vision</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
	<note>ICCV &apos;03</note>
</biblStruct>

<biblStruct coords="12,142.62,473.83,337.98,7.86;12,151.52,484.78,329.07,7.86;12,151.52,495.74,327.19,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,376.17,473.83,104.42,7.86;12,151.52,484.78,111.37,7.86">Lifelogging retrieval based on semantic concepts fusion</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dinh-Duy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,284.02,484.78,196.57,7.86;12,151.52,495.74,97.33,7.86;12,300.67,495.74,31.35,7.86">Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge</title>
		<meeting>the 2018 ACM Workshop on The Lifelog Search Challenge<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
	<note>LSC &apos;18</note>
</biblStruct>

<biblStruct coords="12,142.62,505.87,337.98,7.86;12,151.52,516.83,329.07,7.86;12,151.52,527.79,329.07,7.86;12,151.52,538.75,329.07,7.86;12,151.52,549.71,216.07,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,201.52,516.83,279.07,7.86;12,151.52,527.79,35.25,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,306.49,527.79,174.10,7.86;12,151.52,538.75,114.37,7.86">Proceedings of the 32nd International Conference on Machine Learning</title>
		<title level="s" coord="12,273.15,538.75,171.00,7.86">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07-09">07-09 Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,559.83,337.98,7.86;12,151.52,570.79,329.07,7.86;12,151.52,581.75,84.60,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,348.82,559.83,131.77,7.86;12,151.52,570.79,35.25,7.86">Image captioning with semantic attention</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,207.25,570.79,273.34,7.86;12,151.52,581.75,33.79,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,591.88,337.98,7.86;12,151.52,602.84,329.07,7.86;12,151.52,613.79,329.07,7.86;12,151.52,624.75,132.04,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,377.78,591.88,102.81,7.86;12,151.52,602.84,156.78,7.86">Learning deep features for scene recognition using places database</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,331.92,613.79,148.67,7.86;12,151.52,624.75,63.44,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,634.88,337.97,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,299.60,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,275.01,634.88,205.58,7.86;12,151.52,645.84,77.08,7.86">Query-adaptive asymmetrical dissimilarities for visual object retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.42,645.84,233.17,7.86;12,151.52,656.80,16.79,7.86">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">December 1-8, 2013. 2013</date>
			<biblScope unit="page" from="1705" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
