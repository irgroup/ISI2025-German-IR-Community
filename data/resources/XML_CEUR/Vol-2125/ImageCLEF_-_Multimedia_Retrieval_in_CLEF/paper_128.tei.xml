<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,115.96,345.82,12.62;1,215.11,133.89,185.14,12.62">Tuberculosis detection using optical flow and the activity description vector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.61,171.73,68.73,8.74"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>Carretera San Vicente del Raspeig s/n 03690 San Vicente del Raspeig</addrLine>
									<settlement>-Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.79,171.73,89.90,8.74"><forename type="first">Andrés</forename><surname>Fuster-Guilló</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>Carretera San Vicente del Raspeig s/n 03690 San Vicente del Raspeig</addrLine>
									<settlement>-Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.30,171.73,55.63,8.74"><forename type="first">Juan</forename><surname>Ramón</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>Carretera San Vicente del Raspeig s/n 03690 San Vicente del Raspeig</addrLine>
									<settlement>-Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,437.33,171.73,23.41,8.74;1,240.10,183.69,56.35,8.74"><forename type="first">Jorge</forename><surname>Azorín-López</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>Carretera San Vicente del Raspeig s/n 03690 San Vicente del Raspeig</addrLine>
									<settlement>-Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.84,183.69,51.43,8.74"><forename type="first">Irene</forename><surname>Llopis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>Carretera San Vicente del Raspeig s/n 03690 San Vicente del Raspeig</addrLine>
									<settlement>-Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,115.96,345.82,12.62;1,215.11,133.89,185.14,12.62">Tuberculosis detection using optical flow and the activity description vector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">701AC68DE63243EB590975D7F2EEBB1F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tuberculosis</term>
					<term>Optical Flow</term>
					<term>Activity Description</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early detection of tuberculosis can save many lives as it remains one of the leading causes of death, half a century after its discovery. The analysis of chest CT scanned images can be a quick and economic mechanism for detecting not only the type of tuberculosis, but also differentiating whether or not the disease is multi-drug resistant. These are two of the objectives of the ImageClef Tuberculosis task of 2018, and are the ones studied by the group of the University of Alicante in this edition. We have carried out two work approaches, one based exclusively on the use of Deep Learning techniques on a sequence of 2D images extracted from a 3D tomography and on a second approach using Optical Flow to convert the 3D tomography into a motion representation in order to calculate the ADV (a previous descriptor provided by the group). This descriptor is able to synthesize the information of a sequence into one image. This article presents the experiments carried out and the results obtained within the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageClefTuberculosis is one of the tasks of ImageClef 2018 <ref type="bibr" coords="1,407.28,525.40,14.61,8.74" target="#b10">[11]</ref>. The Image-CLEFtuberculosis task 2018 <ref type="bibr" coords="1,261.29,537.35,10.52,8.74" target="#b4">[5]</ref> includes three independent subtasks. In this first participation our initial objective was to compare two models, Deep learning and Optical Flow to check their results in task 1. Finally we made a delivery about task 2 using the second model that had given us better results in the experimentation of the task 1.</p><p>This document is structured as follows: in sections 2 we present the architectures of the models used: Deep Learning and Optical Flow. In section 3 we show the experimetntation done with both models. Section 4 presents the official results of the experiments and Section 5 summarizes the document and offers a series of proposals for future work.</p><p>2 Our approaches to the solution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning</head><p>Deep neural networks have managed to solve problems or increase efficiency in problems related to image processing <ref type="bibr" coords="2,304.04,303.83,9.96,8.74" target="#b6">[7]</ref>. On the one hand, the convolutional layers manage to extract discriminative characteristics from the images so that they can be evaluated by subsequent layers <ref type="bibr" coords="2,329.62,327.74,14.61,8.74" target="#b11">[12]</ref>. On the other hand, recurrent neural networks have also evolved in their approach and are mainly used in sequence analysis <ref type="bibr" coords="2,213.53,351.66,14.61,8.74" target="#b14">[15]</ref>.</p><p>To address the issue of the first task (resistant form of tuberculosis), 3D chest images of CT scan are used. In a first stage, these 3D images are transformed into a sequence of 2D images each one that represent the entry of the neural networks. Different approaches will be proposed to address the problem of tuberculosis detection:</p><p>1. Convolutional neural network (CNN) with data augmentation: The main idea is to use the advantages of convolutional layers for a single multi-channel image. In this case, each channel would be a 2D gray image (see Figure <ref type="figure" coords="2,466.85,632.11,3.87,8.74" target="#fig_0">1</ref>). treatment is to combine it in networks with multiple inputs per tomography.</p><p>Figure <ref type="figure" coords="3,183.14,388.81,4.98,8.74">2</ref> shows a basic scheme about this approach. 3. Pretrained network and classification: As first approximation to extract features from an image VGG16 deep convolutional neural network <ref type="bibr" coords="3,432.23,412.90,15.50,8.74" target="#b15">[16]</ref> is used with the ImageNet <ref type="bibr" coords="3,235.56,424.85,15.50,8.74" target="#b11">[12]</ref> weights learned (4096 features per image). The main idea is concatenate the features of each input image belonging to the same tomography to get the final features vector to classify in a classical way. 4. Pretrained network and classification as a sequence using recurrent neural network. In this case, the extraction of features is similar to the one described in the previous paragraph and each feature vector would be considered as a component of a sequence to be treated by a well knowns recurrent neural network called Long-Shot Term Memory (LSTM) <ref type="bibr" coords="3,370.96,508.72,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optical Flow plus ADV</head><p>In this sections we propose a combined method based on optical flow and a characterization method called ADV, to deal with the classification of chest CT scan images affected by different types of tuberculosis. The key point of this method is the interpretation of the set of cross-sectional chest images provided by CT scan, not as a volume but as a sequence of video images. We can extract movement descriptors capable of classifying tuberculosis affections by analyzing deformations or movements produced in these video sequences. The concept of optical flow refers to the estimation of displacements of intensity patterns. This concept has been extensively used in computer vision in different application domains: robot or vehicle navigation, car driving, video surveillance or facial expression <ref type="bibr" coords="4,275.00,130.95,9.96,8.74" target="#b5">[6]</ref>. In biomedical context optical flow has been used to analyze organ deformations <ref type="bibr" coords="4,299.68,142.90,11.15,8.74" target="#b8">[9,</ref><ref type="bibr" coords="4,310.83,142.90,11.15,8.74" target="#b16">17]</ref>. We can find different methods in the literature to obtain the optical flow <ref type="bibr" coords="4,315.82,154.86,9.96,8.74" target="#b2">[3]</ref>. One of the most used method to estimate motion at each pixel is Lucas Kanade <ref type="bibr" coords="4,349.67,166.81,14.61,8.74" target="#b12">[13]</ref>. In this work we will use Lucas Kanade method to extract optical flow comparing sequences of consecutive images. Nevertheless, we need not only to estimate motion but describe this motion.</p><p>In order to describe motion there are several methods used in different computer vision context like human behavior recognition <ref type="bibr" coords="4,373.12,226.59,9.96,8.74" target="#b7">[8]</ref>. A successful method to describe human behavior based on trajectory analysis is presented in <ref type="bibr" coords="4,447.18,238.55,9.96,8.74" target="#b0">[1]</ref>. The paper proposes a description vector called (ADV Activity Description Vector) tested in several contexts <ref type="bibr" coords="4,245.52,262.46,9.96,8.74" target="#b1">[2]</ref>. In summary, the ADV vector describes the activity in image sequence by counting for each region of the image the movements produced in four directions of the 2D space. A detailed description of the method can be found in <ref type="bibr" coords="4,208.11,298.32,9.96,8.74" target="#b0">[1]</ref>. In this paper we propose the use of ADV to describe motion in the optical flow obtained from sequences of cross-sectional chest images provided by CT scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper CLEF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OF-MIADV. Optical Flow plus MIADV</head><p>In this sections we propose a combined method based on optical flow and a characterization method called MIADV, to deal with the classification of chest CT scan images affected by different types of tuberculosis. The key point of this method is the interpretation of the set of cross-sectional chest images provided by CT scan, not as a volume but as a sequence of video images. We can extract movement descriptors capable of classifying tuberculosis affections by analysing deformations or movements produced in these video sequences.</p><p>The concept of optical flow refers to the estimation of displacements of intensity patterns. This concept has been extensively used in computer vision in different application domains: robot or vehicle navigation, car driving, video surveillance or facial expression <ref type="bibr" coords="4,377.01,168.60,65.97,8.29" target="#b5">(Fortun et al. 2015)</ref>. In biomedical context optical flow has been used to analyse organ deformations <ref type="bibr" coords="4,397.84,179.51,53.98,8.29" target="#b8">(Hata et al. 2000</ref>) <ref type="bibr" coords="4,136.21,190.35,59.58,8.29" target="#b16">(Xavier et al. 2012</ref>). We can find different methods in the literature to obtain the optical flow <ref type="bibr" coords="4,136.21,201.26,57.33,8.29" target="#b2">(Chao et al. 2014</ref>). One of the most used method to estimate motion at each pixel is Lucas Kanade <ref type="bibr" coords="4,163.07,212.16,75.80,8.29" target="#b12">(Patel &amp; Saurahb 2013)</ref>. In this work we will use Lucas Kanade method to extract optical flow comparing sequences of consecutive images. Nevertheless, we need not only to estimate motion but describe this motion. The figure summarizes the successive stages of the process for extracting the activity descriptors (optical flow+ADV) that will be the input of a classifier. In the first stage a transformation over the cross-sectional chest images provided by the CT scan is performed in order to transform image formats into video sequences adapted to calculate optical flow. The second stage implements the Lucas Kanade method to obtain optical Flow. The third stage calculates the activity description vector ADV (3x3x5) accumulating within each 3x3 region of the image, the displacements of the optical flow in four directions of a 2D space (right, left, up, down). The fifth component of the ADV calculates the frequencies in direction changes. In the fourth stage a normalization of the ADV vector in performed. Finally, the last stage uses the ADV vector normalized as the input for a generic classifier in order to evaluate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary experiments using Deep Learning</head><p>In order to validate the results the wide 10-fold cross validation (10-CV) technique are used and 7 images of 2D are extracted from the original 3D tomography. For the experiments Keras v2.1.6 <ref type="bibr" coords="5,308.19,227.19,10.52,8.74" target="#b3">[4]</ref> and scikit-learn v0.19.1 <ref type="bibr" coords="5,429.30,227.19,15.50,8.74" target="#b13">[14]</ref> Python software are used, in order to build deep neural networks and apply classifiers, respectively.</p><p>Table <ref type="table" coords="5,177.76,263.06,4.98,8.74" target="#tab_1">1</ref> shows the first approach using CNN. The results are close to 50% which means that the network has not learned the difference between the two classes.</p><p>The second approach consists of combination of CNN with RNN. In this case, 2 layers are used and the filters are: 32 (3x3), 64 (3x3). The accuracy is 0.50 and individual proofs are [0.54 0.58 0.42 0.50 0.48 0.52 0.52 0.44 0.48 0.52]. The results are also unsatisfactory and we will try a new approach. The third try using a pretrained network (VGG16) with ImageNet weights configuration. In this case, VGG16 is used to extract the weights of the penultimate layer descriptors of image. These features extracted from the latest layers of the neural network are called neural codes. The number of final characteristics is 28672 corresponding to 7 images per times 4096 neural codes per image. Table <ref type="table" coords="5,194.76,524.61,4.98,8.74" target="#tab_2">2</ref> summarize the experiments using classifiers belonging different families of algorithms attending to neural codes directly or normalizing with L2 function.</p><p>Last approach using deep learning architectures consists of get neural codes as previous try and classify the sequence of 7 images with a recurrent neural network (LSTM). Again, the accuracy is 0.49 and detailed fold results are [0.62 0.54 0.42 0.46 0.28 0.48 0.48 0.48 0.56 0.56].</p><p>In general, the results per folder (10-CV) are very different probably due to the nature of neuronal networks with random initialization of neurons, the optimizers that have to adjust thousands of parameters that finally find local minimums and also due to the small amount of images available to train a neuronal network where small differences between the training and test sets allow generating sets easier to classify in some cases than in others. On the other hand, no preprocessing has been applied to 2D images which could also influence the high variations in results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary experiments using Optical Flow plus ADV</head><p>For this experiments, the wide 10-fold cross validation (10-CV) technique have been used again. All images of the original 3D tomography are used to calculate the optical flow for each patient. For the experiments Matlab R2013b has been used to calculate the optical flow, the ADV and the classifiers. Table <ref type="table" coords="6,177.11,543.88,4.24,8.74">3</ref>.2 shows the performance results of the proposed method.</p><p>Classifier OF size ADV Accuracy MDR Accuracy DS Accuracy SVM 64x64 3x3 0,5097 0,312 0,6567 3-knn 64x64 3x3 0,5135 0,52 0,4627 Table <ref type="table" coords="6,215.51,621.06,4.13,7.89">3</ref>. Classification results using Optical flow plus ADV</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Frequency Matrix with Deep Learning</head><p>A modification of the Optical Flow experiment was to use the frequency matrices generated as input to a neural network. In figure <ref type="figure" coords="7,189.87,561.73,4.98,8.74" target="#fig_0">1</ref>  As can be see in the table <ref type="table" coords="8,262.82,211.40,4.98,8.74" target="#tab_4">4</ref> the model of Optical Flow SVM obtains the best results, for the sake of using only selected images. The results were significantly better using the 3-nn but very far from the rest of the participants 5. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,138.97,560.13,341.62,8.74;1,151.70,572.09,328.89,8.74;1,151.70,584.04,113.41,8.74;1,138.97,596.17,341.62,8.74;1,151.70,608.12,328.89,8.74;1,151.70,620.08,265.56,8.74;1,138.97,632.21,341.61,8.74;1,151.70,644.16,328.89,8.74;1,151.70,656.12,215.44,8.74"><head>1 .</head><label>1</label><figDesc>Subtask 1: MDR detection The goal of this subtask is to assess the probability of a TB patient having resistant form of tuberculosis based on the analysis of chest CT scan. 2. Subtask 2: TBT classification The goal of this subtask is to automatically categorize each TB case into one of the following five types: (1) Infiltrative, (2) Focal, (3) Tuberculoma, (4) Miliary, (5) Fibro-cavernous. 3. Subtask 3: Severity scoring This subtask is aimed at assessing TB severity score based on chest CT image. The Severity score is a cumulative score of severity of TB case assigned by a medical doctor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,140.81,540.03,333.75,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Basic scheme of a deep convolutional neural network for classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,216.48,516.59,182.40,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Optical flow plus ADV process stages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,254.99,527.45,105.37,7.89;7,143.41,184.03,328.64,328.64"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Frequency Matrix.</figDesc><graphic coords="7,143.41,184.03,328.64,328.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,138.97,644.16,341.62,20.69"><head></head><label></label><figDesc>2. Convolutional layers combined with a recurrent neural network: The natural way to combine the advantages of convolutional layers and sequential</figDesc><table coords="3,134.77,137.39,345.82,213.28"><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell>Convolutional modules</cell><cell>Convolutional</cell><cell>layers</cell><cell>Convolutional</cell><cell>layers</cell><cell>...</cell><cell>Convolutional</cell><cell>layers</cell></row><row><cell>LSTM</cell><cell cols="2">Internal state</cell><cell cols="2">Internal state</cell><cell>...</cell><cell cols="2">Internal state</cell><cell>Dense layer</cell><cell>Output</cell></row><row><cell></cell><cell cols="2">h 1</cell><cell cols="2">h 2</cell><cell>...</cell><cell cols="2">h n</cell></row><row><cell cols="9">Fig. 2. Basic scheme of a combination of a convolutional layer with a Long-Short Term</cell></row><row><cell cols="3">Memory network for classification task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,370.75,367.05,58.79"><head>Table 1 .</head><label>1</label><figDesc>Classification results using CNN.</figDesc><table coords="5,134.77,370.75,367.05,45.77"><row><cell cols="2">Conv. Detail</cell><cell>Accuracy</cell><cell>10-CV</cell></row><row><cell cols="2">layers filters x kernel</cell><cell>mean</cell><cell>results</cell></row><row><cell>4</cell><cell>64x7, 64x3, 64x3, 64x3</cell><cell>0.52</cell><cell>[0.46 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52]</cell></row><row><cell>2</cell><cell>64x7, 64x3</cell><cell>0.51</cell><cell>[0.46 0.54 0.54 0.50 0.52 0.52 0.52 0.52 0.48 0.52]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,120.94,357.06,250.09"><head>Table 2 .</head><label>2</label><figDesc>Classification results applied to features extraction with VGG16 pretrained network.</figDesc><table coords="6,134.77,120.94,357.06,226.11"><row><cell cols="2">Neural Classifier</cell><cell cols="2">Accuracy 10-CV</cell></row><row><cell cols="2">Codes algorithms</cell><cell cols="2">mean results</cell></row><row><cell></cell><cell>Nearest Neighbors</cell><cell>0.48</cell><cell>[0.54 0.46 0.65 0.35 0.36 0.6 0.32 0.56 0.44 0.52]</cell></row><row><cell></cell><cell>Linear SVM</cell><cell>0.48</cell><cell>[0.46 0.46 0.69 0.54 0.4 0.6 0.24 0.52 0.44 0.48]</cell></row><row><cell></cell><cell>RBF SVM</cell><cell>0.53</cell><cell>[0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52]</cell></row><row><cell></cell><cell>Decision Tree</cell><cell>0.54</cell><cell>[0.62 0.54 0.58 0.5 0.44 0.44 0.48 0.72 0.56 0.52]</cell></row><row><cell>original</cell><cell>Random Forest</cell><cell>0.41</cell><cell>[0.27 0.54 0.5 0.38 0.4 0.56 0.28 0.4 0.4 0.4 ]</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.49</cell><cell>[0.46 0.65 0.58 0.42 0.28 0.52 0.52 0.64 0.36 0.48]</cell></row><row><cell></cell><cell>Naive Bayes</cell><cell>0.47</cell><cell>[0.54 0.58 0.5 0.27 0.48 0.52 0.36 0.48 0.48 0.44]</cell></row><row><cell></cell><cell>Logistic Regression</cell><cell>0.48</cell><cell>[0.5 0.46 0.69 0.54 0.44 0.52 0.28 0.48 0.4 0.44]</cell></row><row><cell></cell><cell>XGBoost</cell><cell>0.52</cell><cell>[0.46 0.65 0.65 0.46 0.32 0.6 0.4 0.6 0.6 0.44]</cell></row><row><cell></cell><cell>Nearest Neighbors</cell><cell>0.50</cell><cell>[0.62 0.5 0.69 0.42 0.4 0.48 0.32 0.64 0.4 0.48]</cell></row><row><cell></cell><cell>Linear SVM</cell><cell>0.53</cell><cell>[0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52]</cell></row><row><cell></cell><cell>RBF SVM</cell><cell>0.45</cell><cell>[0.42 0.54 0.54 0.46 0.24 0.44 0.28 0.6 0.44 0.56]</cell></row><row><cell></cell><cell>Decision Tree</cell><cell>0.52</cell><cell>[0.58 0.5 0.5 0.58 0.56 0.4 0.52 0.6 0.36 0.56]</cell></row><row><cell>L2</cell><cell>Random Forest</cell><cell>0.52</cell><cell>[0.42 0.73 0.42 0.42 0.6 0.48 0.48 0.52 0.52 0.6 ]</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.47</cell><cell>[0.54 0.58 0.54 0.31 0.4 0.56 0.24 0.52 0.44 0.56]</cell></row><row><cell></cell><cell>Naive Bayes</cell><cell>0.47</cell><cell>[0.54 0.58 0.5 0.27 0.48 0.52 0.36 0.48 0.48 0.44]</cell></row><row><cell></cell><cell>Logistic Regression</cell><cell>0.47</cell><cell>[0.42 0.62 0.58 0.46 0.2 0.48 0.28 0.6 0.56 0.52]</cell></row><row><cell></cell><cell>XGBoost</cell><cell>0.50</cell><cell>[0.65 0.54 0.62 0.42 0.32 0.56 0.4 0.6 0.44 0.48]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,561.73,345.83,103.13"><head></head><label></label><figDesc>you can see an example of Frequency Matrix..</figDesc><table coords="7,134.77,593.45,345.83,71.40"><row><cell>3. Run 3 Frequency Normalized. In this model we apply Deep Learning tech-</cell></row><row><cell>niques on the normalized frequency matrix obtained through the Optical</cell></row><row><cell>Flow.</cell></row><row><cell>4. Run 4 In this model we apply Deep Learning techniques (Decision Tree) on</cell></row><row><cell>a subset of 2D images of the tomography.</cell></row><row><cell>5. Run 5 In this model we apply Deep Learning techniques (Decision Tree) on</cell></row><row><cell>a subset of 2D images of the tomography.</cell></row><row><cell>4 Results</cell></row><row><cell>1. Run 1: MDR Baseline. The Baseline is a probabilistic model in which the</cell></row><row><cell>image was not analyzed and only the data of sex and age have been taken</cell></row><row><cell>into account.</cell></row><row><cell>2. Run 2: ADV 3x3, SVM, 1000 SMOTE upsampling</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,254.49,345.82,211.13"><head>Table 4 .</head><label>4</label><figDesc>Results of University of Alicante vs better results at SubTask 1 Due to we had little time available for second task, we only present the two models of Optical Flow, SVM and 3nn.Run 1: ADV 3x3, SVM, 1000 SMOTE upsampling Run 2: ADV 3x3, 3-nn, 1000 SMOTE upsampling</figDesc><table coords="8,136.56,275.29,310.81,118.25"><row><cell>Run</cell><cell cols="2">AUC Rank AUC ACC Rank ACC</cell></row><row><cell>VISTA@UEvora</cell><cell>0.6178 1</cell><cell>0.5593 8</cell></row><row><cell>San Diego VA HCS/UCSD</cell><cell>0.6114 2</cell><cell>0.6144 1</cell></row><row><cell>MDRBaseline0</cell><cell>0.5669 10</cell><cell>0.4873 32</cell></row><row><cell>testSVMSMOTE</cell><cell>0.5509 15</cell><cell>0.5339 20</cell></row><row><cell cols="2">testOpticalFlowwFrequencyNormalized 0.5473 16</cell><cell>0.5127 24</cell></row><row><cell>DecisionTree25v2</cell><cell>0.5049 26</cell><cell>0.5000 29</cell></row><row><cell>testOFFullVersion2</cell><cell>0.4971 29</cell><cell>0.4958 31</cell></row><row><cell>testOpticalFlowFull</cell><cell>0.4845 32</cell><cell>0.5169 23</cell></row><row><cell>testFrequency</cell><cell>0.4781 34</cell><cell>0.4788 34</cell></row><row><cell>testflowI</cell><cell>0.4740 35</cell><cell>0.4492 39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,134.77,511.93,345.83,152.92"><head>Table 5 .</head><label>5</label><figDesc>Results of University of Alicante vs better results at SubTask 2Early detection of tuberculosis is a major social challenge, given the devastating effects of the disease. On the other hand, it represents a scientific challenge of the highest level. As the organizers claim, "you have to work to get methods that allow a correct detection of the disease that kills thousands and thousands of people". In this paper we have proposed two different approaches to face the problem. The first one is based on the use of Deep Learning techniques on a sequence of 2D images extracted from a 3D tomography. The second approach uses Optical Flow to convert the 3D tomography into a motion representation in order to calculate the ADV (a previous descriptor provided the group). This descriptor is able to synthesize the information of a sequence into one image. The experiments carried out in these two approaches allow us to confirm the interest of these lines of research and encourage us to seek improvements in the proposed methodologies.</figDesc><table coords="8,134.77,532.73,212.41,97.36"><row><cell>Run</cell><cell cols="2">AUC Rank AUC ACC Rank ACC</cell></row><row><cell cols="2">UIIP BioMed 0.2312 1</cell><cell>0.4227 1</cell></row><row><cell cols="2">T23nnFinal 0.0204 32</cell><cell>0.2587 31</cell></row><row><cell cols="2">T2SVMFinal -0.0920 38</cell><cell>0.1167 38</cell></row><row><cell cols="3">5 Conclusions and future work</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,294.62,337.63,7.86;9,151.52,305.58,329.07,7.86;9,151.52,316.53,329.07,7.86;9,151.52,327.49,183.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,465.49,294.62,15.10,7.86;9,151.52,305.58,324.82,7.86">Human behaviour recognition based on trajectory analysis using neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Azorin-Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saval-Calvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuster-Guillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2013.6706724</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2013.6706724" />
	</analytic>
	<monogr>
		<title level="m" coord="9,165.58,316.53,283.79,7.86">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,338.51,337.64,7.86;9,151.52,349.47,329.07,7.86;9,151.52,360.43,329.07,7.86;9,151.52,371.39,329.07,7.86;9,151.52,382.35,183.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,276.90,349.47,203.69,7.86;9,151.52,360.43,179.28,7.86">Group activity description and recognition based on trajectory analysis and neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Azorin-Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saval-Calvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuster-Guillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Signes-Pont</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727387</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2016.7727387" />
	</analytic>
	<monogr>
		<title level="m" coord="9,378.92,360.43,101.67,7.86;9,151.52,371.39,154.78,7.86">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,393.37,337.64,7.86;9,151.52,404.33,329.07,7.86;9,151.52,415.26,328.20,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,291.89,393.37,188.71,7.86;9,151.52,404.33,92.42,7.86">A survey of optical flow techniques for robotics navigation applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Napolitano</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10846-013-9923-6</idno>
		<ptr target="https://doi.org/10.1007/s10846-013-9923-6" />
	</analytic>
	<monogr>
		<title level="j" coord="9,251.45,404.33,229.15,7.86;9,151.52,415.28,50.48,7.86">Journal of Intelligent and Robotic Systems: Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="361" to="372" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,426.30,216.43,8.11" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="9,226.66,426.30,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,437.32,337.64,7.86;9,151.52,448.28,329.07,7.86;9,151.52,459.24,329.07,7.86;9,151.52,470.20,329.07,7.86;9,151.52,481.16,34.31,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,378.22,437.32,102.37,7.86;9,151.52,448.28,329.07,7.86;9,151.52,459.24,108.19,7.86">Overview of ImageCLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,279.94,459.24,103.81,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="9,390.71,459.24,89.89,7.86;9,151.52,470.20,82.70,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,492.18,337.63,7.86;9,151.52,503.11,329.07,7.89;9,151.52,514.09,170.81,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,327.18,492.18,153.41,7.86;9,151.52,503.13,58.42,7.86">Optical flow modeling and computation: A survey</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fortun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2015.02.008</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2015.02.008" />
	</analytic>
	<monogr>
		<title level="j" coord="9,218.56,503.13,182.87,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,525.11,337.64,8.12;9,151.52,536.72,122.39,7.47" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m" coord="9,317.26,525.11,56.88,7.86">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,547.09,337.64,7.86;9,151.52,558.02,329.07,7.89;9,151.52,569.01,329.07,8.12;9,151.52,580.61,119.18,7.47" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,337.98,547.09,142.62,7.86;9,151.52,558.05,165.84,7.86">Automated human behavior analysis from surveillance videos: a survey</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gowsikhaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abirami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baskaran</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-012-9341-3</idno>
		<ptr target="https://doi.org/10.1007/s10462-012-9341-3" />
	</analytic>
	<monogr>
		<title level="j" coord="9,327.31,558.05,122.22,7.86">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="747" to="765" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,590.98,337.64,7.86;9,151.52,601.94,329.07,7.86;9,151.52,612.90,329.07,7.86;9,151.52,623.84,329.08,7.89;9,151.52,634.82,23.04,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,202.88,601.94,277.71,7.86;9,151.52,612.90,200.87,7.86">Three-dimensional optical flow method for measurement of volumetric brain deformation from intraoperative MR images</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nabavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M L</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
		<idno type="DOI">10.1097/00004728-200007000-00004</idno>
		<ptr target="https://doi.org/10.1097/00004728-200007000-00004" />
	</analytic>
	<monogr>
		<title level="j" coord="9,359.77,612.90,120.82,7.86;9,151.52,623.86,50.45,7.86">Journal of Computer Assisted Tomography</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="538" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,645.84,337.98,7.86;9,151.52,656.77,92.85,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,288.76,645.84,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,398.69,645.84,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.98,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,329.07,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,95.77,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,197.95,152.55,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,163.51,329.07,7.86;10,151.52,174.47,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="10,197.87,185.43,169.61,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
			<biblScope unit="volume">11018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,86.01,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,328.09,207.34,152.50,7.86;10,151.52,218.30,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.64,218.30,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,240.22,337.98,7.86;10,151.52,251.15,161.00,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,250.32,240.22,225.55,7.86">Optical flow measurement using Lucas Kanade method</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Saurahb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,251.18,78.20,7.86">Int J Comput Appl</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,262.14,337.98,7.86;10,151.52,273.10,329.07,7.86;10,151.52,284.06,329.07,7.86;10,151.52,294.99,325.87,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,394.26,284.06,86.33,7.86;10,151.52,295.02,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,232.82,295.02,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,305.98,337.97,7.86;10,151.52,316.93,329.07,7.86;10,151.52,327.87,103.88,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,322.03,305.98,158.56,7.86;10,151.52,316.93,72.99,7.86">Learning sequential structure in simple recurrent networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,231.11,316.93,249.48,7.86;10,151.52,327.89,66.83,7.86">Parallel distributed processing: Experiments in the microstructure of cognition</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,338.85,337.97,7.86;10,151.52,349.81,231.27,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,278.92,338.85,201.67,7.86;10,151.52,349.81,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.62,360.77,337.98,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.69,329.07,7.86;10,151.52,393.62,325.28,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,432.02,360.77,48.57,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.69,113.27,7.86">An adapted optical flow algorithm for robust quantification of cardiac wall motion from standard cine-MR examinations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brunotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Legrand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITB.2012.2204893</idno>
		<ptr target="https://doi.org/10.1109/TITB.2012.2204893" />
	</analytic>
	<monogr>
		<title level="j" coord="10,273.73,382.69,206.86,7.86;10,151.52,393.65,49.03,7.86">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="859" to="868" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
