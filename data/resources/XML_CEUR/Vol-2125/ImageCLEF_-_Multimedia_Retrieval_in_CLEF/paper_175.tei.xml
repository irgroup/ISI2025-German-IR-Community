<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,76.14,75.59,442.76,12.58;1,251.62,94.13,92.09,12.58">Analysing TB Severity Levels With An Enhanced Deep Residual Learning -Depth-Resnet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,201.14,143.30,56.34,9.02"><forename type="first">Xiaohong</forename><surname>Gao</surname></persName>
							<email>x.gao@mdx.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Middlesex University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.73,143.30,83.29,9.02"><forename type="first">Carl</forename><surname>James-Reynolds</surname></persName>
							<email>c.james-reynolds@mdx.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Middlesex University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.06,143.30,39.13,9.02"><forename type="first">Ed</forename><surname>Currie</surname></persName>
							<email>e.currie@mdx.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Middlesex University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,76.14,75.59,442.76,12.58;1,251.62,94.13,92.09,12.58">Analysing TB Severity Levels With An Enhanced Deep Residual Learning -Depth-Resnet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1747F9C3F3AEB83E3B010F8C2B7B45A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep residual learning</term>
					<term>classification</term>
					<term>severity of Tuberculosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work responds to the Competition of Tuberculosis Task organised by imageCLEF 2018. While Task #3 appears to be challenging, the experience was very enjoyable. If time had been permitted, it was certain that more accurate results could have been achieved. The authors submitted 2 runs. Based on the given training datasets with severity levels of 1 to 5, an enhanced deep residual learning architecture, depth-ResNet, is developed and applied to train the datasets to classify 5 categories. The datasets are pre-processed with each volume being segmented into twenty-128Ã—128Ã—depth blocks with ~64 pixel overlaps. While each block has been predicted with a severity level, assembling all constituent block scores together to give an overall label for the concerned volume tends to be more challenging. Since the probability of high severity is not provided from the training datasets, which bears little resemblance to the classification probability, the submission of probability for the first run was manually assigned as 0.9, 0.7, 0.5, 0.3, and 0.1 to severity levels of 1 to 5 respectively. After the deadline was extended, the model was re-trained with frame numbers increased from 1 to 8, which takes much longer to train. In addition, a new measure was introduced to calculate the overall probability of high severity based on the block scores. As a result, with regard to classification accuracy, the 2 nd submitted run achieved place 14 over a total of 36 submissions, a significant improvement from position of 35 from the first run.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Deep Residual Learning -Depth-Resnet</head><p>Since a convolutional neural network (CNN) architecture can be constructed by stacking multiple layers of convolution and subsampling in an alternating fashion, a CNN network can be enhanced into going deeper by piling a large number of layers. However, the increased depth appears to have little contribution to the accuracy of a trained model. This is due to the well-known vanishing gradient obstacle, i.e. as the gradient is backpropagated to earlier layers, repeated multiplication may compose the gradient infinitely small. As a result, as the network becomes deeper, its performance gets saturated or even starts degrading rapidly.</p><p>Recently, deep residual networks (ResNet) <ref type="bibr" coords="1,255.59,573.58,8.05,9.02" target="#b0">[1]</ref><ref type="bibr" coords="1,263.64,573.58,4.03,9.02" target="#b1">[2]</ref><ref type="bibr" coords="1,267.67,573.58,8.05,9.02" target="#b2">[3]</ref> introduce the notion of 'identity shortcut connection' that bypasses one or more layers. A key advantage of residual units is that their skip connections allow direct signal propagation from the first to the last layer of the network, especially during backpropagation. This is due to the fact that gradients are propagated directly from the loss layer to any previous layer while skipping intermediate weight layers that have potential to trigger vanishing or deterioration of the gradient signal.</p><p>In this work, an enhanced ResNet, i.e. depth-Resnet is applied for analysis of the level of severity of tuberculosis from CT lung images, which is built on ResNet-50 model and illustrated in Figure <ref type="figure" coords="1,454.14,664.90,3.76,9.02" target="#fig_0">1</ref>. In Figure <ref type="figure" coords="2,115.03,474.04,5.01,9.02" target="#fig_0">1</ref> built on the inception concept, the depth (ğ‘§) convolution block operates on the dimensionality reduced input, ğ‘¥ ğ‘™,ğ‘§ with a bank of 3D filters, ğ‘Š ğ‘™,ğ‘§ . Biases ğ‘ âˆˆ â„ ğ¶ are also applied with initial values of 0 as formulated in Eq. <ref type="bibr" coords="2,144.77,501.58,10.61,9.02" target="#b0">(1)</ref>.</p><formula xml:id="formula_0" coords="2,147.86,526.18,371.72,11.39">ğ‘¥ ğ‘™,ğ‘§ = ğ‘Š ğ‘™,ğ‘§ ğ‘¥ ğ‘™,1 + ğ‘<label>(1)</label></formula><p>Hence the residual unit â„± is expressed in Eq. <ref type="bibr" coords="2,254.28,553.36,10.64,9.02" target="#b1">(2)</ref>.</p><formula xml:id="formula_1" coords="2,147.86,582.58,371.72,11.39">â„± = ğ‘“ (ğ‘Š ğ‘™,3 (ğ‘† ğ‘™ ğ‘“(ğ‘¥ ğ‘™,ğ‘§ ) + ğ‘“(ğ‘Š ğ‘™,2 ğ‘“(ğ‘Š ğ‘™,1 ğ‘¥ ğ‘™,1 )))<label>(2)</label></formula><p>where ğ‘† ğ‘™ is affine scaling along depth direction with a bias between 0 and 0.01. This scaling is adaptive to facilitate generalisation performance and will be learnt during the training of the network. The convolution at each convolution layer along depth (ğ‘§) direction (ğ‘¥ ğ‘™,ğ‘§ ) takes place between 3 neighbouring slices or feature maps, i.e. front, current, and back, with a randomly chosen stride (between 1 and 7 in this study). This feature then is added to the block with a scaling factor as a component of the residual unit. The pooling involves two stages.</p><p>The avg-pool occurs for 2D spatial global average pooling whereas max-pool is conducted along z direction performing global max pooling upon those feature maps.</p><p>On the other hand, to integrate block scores into a volumetric label for each dataset, a support vector machine (SVM) is applied. The system is implemented in Matlab with MatConvNet [4] toolbox by following standard ConvNet training procedures <ref type="bibr" coords="2,192.91,745.86,10.85,9.02" target="#b3">[5,</ref><ref type="bibr" coords="2,207.06,745.86,7.28,9.02" target="#b4">6]</ref>. Upon training, 8 slices is chosen from each block with randomly selected stride between 1 and 7 from 5 categories with a batch of 128 (=16 blocks). At testing time, each dataset undertakes the same pre-processing procedure to generate 128ï‚´128ï‚´depth blocks. Then the trained depth-Resnet model (Figure <ref type="figure" coords="3,162.30,88.44,4.17,9.02" target="#fig_0">1</ref>) takes each block as a whole, selects 8 slices at equal depth space and propagates these slices through the trained model to produce a single prediction for this block with severity scores labeled between 1 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets</head><p>Data are collected from the competition organised by ImageCLEF2018 on Tuberculosis severity scoring task (task#3) <ref type="bibr" coords="3,107.45,195.74,10.81,9.02" target="#b5">[7,</ref><ref type="bibr" coords="3,120.91,195.74,7.49,9.02" target="#b6">8,</ref><ref type="bibr" coords="3,130.99,195.74,7.23,9.02" target="#b7">9]</ref>, with 170 for training and 109 for testing. The training data include chest CT scans of TB from 170 patients with the corresponding severity scores (1 to 5) and the severity levels designated as "high" and "low", which contains 90 low severity (with scores 4 and 5) and 80 high severity (with scores 1, 2 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image Pre-processing</head><p>The collected data are pre-processed to remove background and to segment into smaller blocks, which is because that most abnormalities occur in small regions and spread over only a few slices. Figure <ref type="figure" coords="3,462.73,301.76,5.01,9.02" target="#fig_1">2</ref> demonstrates the process to remove background by the application of masks that are dilated in advance. As illustrated in Figure <ref type="figure" coords="3,100.79,328.22,5.01,9.02" target="#fig_1">2</ref> (b), some masks <ref type="bibr" coords="3,176.90,328.22,16.66,9.02" target="#b8">[10]</ref> over-remove lung information. Hence all the masks are dilated (Figure <ref type="figure" coords="3,483.53,328.22,16.02,9.02" target="#fig_1">2(d)</ref>) by a diameter of 30 pixels found empirically to ensure the balance between over-and under-removing of background (Figure <ref type="figure" coords="3,156.88,354.62,3.66,9.02" target="#fig_1">2</ref>(e)). Figure <ref type="figure" coords="3,213.19,354.62,3.76,9.02" target="#fig_1">2</ref>(f) depicts the final image of removing background, which has a size of 460 Ã— 340 Ã— ğ‘§ (z is the depth and varies between 50 to 400). Then upon the segmented volume of 460 Ã— 340 Ã— ğ‘§, 24 blocks of size of 128 Ã— 128 Ã— ğ‘§ are created with overlapping of ~64 pixels as illustrated in Figure <ref type="figure" coords="4,269.51,751.38,3.76,9.02" target="#fig_2">3</ref>. Since some corner blocks comprise large amount of background information, i.e. pixel value being 0, these frames, in particular at front and back of a volume along ğ‘§ direction, are removed when its background region occupies more than one third of the total space. Hence the depth (ğ‘§) of each block varies between 11 and 250 for all datasets after segmentation. As a result, many 3D volume datasets have less than 24 blocks after preprocessing. Each block has been resized to 256ï‚´256ï‚´ğ‘§ from 128ï‚´128ï‚´ğ‘§ to save training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Since each volume of 3D dataset contains around 24 blocks with individual severity scores, the final score for this dataset has to be integrated. In principle, the five levels of severity can be treated as 2 classes labeled as 'high' (with scores 1, 2 and 3) and 'low' (with scores 4 and 5). Hence, three measures can then be formulated to convey the inter-relationships between blocks scored 1 to 3, 4 to 5 and 1 to 5 are then calculated using Eqs. ( <ref type="formula" coords="5,512.74,477.76,3.54,9.02">3</ref>), (4) and ( <ref type="formula" coords="5,107.91,490.96,3.89,9.02" target="#formula_2">5</ref>) respectively where scores of 1 to 5 are assigned initial probabilities of high severity linearly by 0.9, 0.7, 0.5, 0. </p><p>Hence the probability of a whole volume dataset can then be decided by these measures, which is in turn utilized to score the overall severity of a volume. For example, in this study, if a dataset has ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ â„ğ‘–ğ‘”â„ &gt; 0.7 and ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ ğ‘™ğ‘œğ‘¤ &lt; 0.20 and ğ‘ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜1 &gt; 0 , then this dataset is classified as severity 1. In Table <ref type="table" coords="5,495.56,635.08,3.76,9.02" target="#tab_1">1</ref>, two approaches are applied. One is based on the overall probability (Level-1) as formulated in Eq. ( <ref type="formula" coords="5,473.85,648.28,3.55,9.02" target="#formula_2">5</ref>), which is simple and straightforward. Level 1 approach has been applied to the imageCLEF Tuberculosis 2018 competition <ref type="bibr" coords="5,122.79,674.68,10.89,9.02" target="#b6">[8,</ref><ref type="bibr" coords="5,136.75,674.68,7.23,9.02" target="#b7">9]</ref>, which ranks number 14 (out of 36 submissions) in terms of accuracy (AUC=0.6534) by the authors of this paper when using different set of test data (n=109) with unknown severity levels. These results are based on three runs using training datasets where 100 data are randomly selected from 170 training sets and remaining 70 as test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Prediction of probability of high severity level appears to be a challenging task since this information has to be determined from the severity scores of 1 to 5. Due to the limited computer power (with only 1 GPU), each run takes 4 days to train (100 datasets) and 2 days to test, the results from Level-2 approach were only obtained after the deadline. However, the experience gained from this competition was very enjoyable with many lessons learnt in relation to designing deep residual learning network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,437.59,451.07,8.10;2,72.00,449.53,137.89,8.10;2,178.23,72.00,238.83,348.90"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The depth-ResNet architecture applied in this paper, where Ã—N at each conv level refers to the block (e.g. conv5_x) repeats N (e.g. 3) times consecutively.</figDesc><graphic coords="2,178.23,72.00,238.83,348.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,693.79,451.19,8.10;4,72.00,705.69,451.12,8.10;4,72.00,717.63,330.12,8.10;4,231.85,556.00,131.65,96.70"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The process of removing background using dilated mask. (a) an original slice of image; (b) original mask and the masked image (c) between (a) and (b); (d) dilated mask and (e) the result between (a) and (d). (f) final segmented masked image after removing background. The arrow points to the diseased region to be concerned.</figDesc><graphic coords="4,231.85,556.00,131.65,96.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.00,297.77,386.60,8.10;5,185.95,78.80,223.00,195.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation of processed volume into 24 overlapping blocks with each one being 128ï‚´128ï‚´depth.</figDesc><graphic coords="5,185.95,78.80,223.00,195.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,108.02,504.16,407.72,85.70"><head></head><label></label><figDesc>3, and 0.1 respectively.</figDesc><table coords="5,108.02,528.88,407.72,60.97"><row><cell cols="3">ğ‘ğ‘Ÿğ‘œğ‘_â„ğ‘–ğ‘”â„ =</cell><cell>0.9Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜1 +0.7Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜2 +0.5Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜3 ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜1 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜2 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 3</cell><cell>(3)</cell></row><row><cell cols="2">ğ‘ğ‘Ÿğ‘œğ‘_ğ‘™ğ‘œğ‘¤ =</cell><cell cols="2">0.3Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜4 +0.1Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜5 ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜4 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜5</cell><cell>(4)</cell></row><row><cell>ğ‘ğ‘Ÿğ‘œğ‘_ğ‘ğ‘™ğ‘™ =</cell><cell cols="3">0.9Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜1 +0.7Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜2 +0.5Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜3 +0.3Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜4 +0.1Ã—ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜5 ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜1 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜2 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜ 3 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜4 +ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘œğ‘ğ‘˜5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,91.64,89.91,423.60,52.46"><head>Table 1 .</head><label>1</label><figDesc>The accuracy performance from both Level-1 and Level-2 calculations.</figDesc><table coords="6,91.64,110.49,423.60,31.88"><row><cell>Severity</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Average</cell></row><row><cell>Level-1</cell><cell>0.80 Â± 0.00</cell><cell>0.60 Â± 0.05</cell><cell>0.75 Â± 0.00</cell><cell>0.88 Â± 0.02</cell><cell>0.82 Â± 0.12</cell><cell>75.88 Â± 3.80%</cell></row><row><cell>Level-2</cell><cell>0.86 Â± 0.08</cell><cell>0.70 Â± 0.01</cell><cell>0.77 Â± 0.02</cell><cell>0.90 Â± 0.00</cell><cell>0.84 Â± 0.04</cell><cell>85.29 Â± 3.00%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,93.22,314.75,428.16,8.10;6,107.72,325.13,82.77,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,220.45,314.75,165.77,8.10">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,392.74,314.75,128.64,8.10;6,107.72,325.13,54.28,8.10">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,335.45,430.10,8.10;6,107.72,345.83,173.03,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,225.95,335.45,174.42,8.10">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.47,335.45,115.85,8.10;6,107.72,345.83,113.73,8.10">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,356.15,430.05,8.10;6,107.72,366.53,266.75,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,260.08,356.15,235.41,8.10">Temporal Residual Networks for Dynamic Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,503.83,356.15,19.44,8.10;6,107.72,366.53,238.21,8.10">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,387.23,293.43,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,218.94,387.23,52.65,8.10">Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,277.76,387.23,23.34,8.10">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,397.55,430.12,8.10;6,107.72,407.93,241.55,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,248.71,397.55,274.62,8.10;6,107.72,407.93,148.46,8.10">Imagenet classification with deep convolutional neural networks, Advances in neural information processing systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,281.02,407.93,39.72,8.10">NIPS 2012</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,418.18,430.11,8.19;6,107.72,428.65,260.69,8.10" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Eds</surname></persName>
		</author>
		<title level="m" coord="6,290.27,418.18,233.05,8.19;6,107.72,428.65,164.78,8.10">CLEF 2018 Working Notes, Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.22,438.97,430.15,8.10;6,107.72,449.35,415.51,8.10;6,107.72,459.67,243.77,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,318.22,438.97,205.14,8.10;6,107.72,449.35,298.04,8.10">Overview of ImageCLEFtuberculosis 2018 -Detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,412.96,449.35,76.87,8.10;6,134.78,459.67,52.16,8.10">CLEF working notes</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14. 2018</date>
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="6,93.22,470.05,429.90,8.10;6,107.72,480.37,415.51,8.10;6,107.72,490.75,415.44,8.10;6,107.72,501.07,415.59,8.10;6,107.72,511.45,254.99,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,153.36,490.75,269.88,8.10">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,432.58,490.75,90.58,8.10;6,107.72,501.07,415.59,8.10;6,107.72,511.45,91.34,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.34,523.57,421.91,8.10;6,107.72,533.95,409.24,8.10;6,107.72,544.27,112.23,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,320.66,523.57,198.59,8.10;6,107.72,533.95,51.67,8.10">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>JimÃ©nez-Del-Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,263.51,533.95,253.45,8.10;6,107.72,544.27,83.76,8.10">Proceedings of the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<meeting>the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
