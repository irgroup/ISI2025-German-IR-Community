<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,193.64,133.89,228.07,12.62;1,217.22,151.82,180.91,12.62">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.40,189.69,103.51,8.74"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
							<email>duc-tien.dang-nguyen@dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.45,189.69,46.86,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
							<email>luca.piras@diee.unica.it</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Pluribus One</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.86,189.69,67.69,8.74"><forename type="first">Michael</forename><surname>Riegler</surname></persName>
							<email>michael@simula.no</email>
							<affiliation key="aff2">
								<orgName type="institution">Simula Metropolitan Center for Digital Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.11,189.69,51.61,8.74"><forename type="first">Liting</forename><surname>Zhou</surname></persName>
							<email>zhou.liting2@mail.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.65,201.64,55.54,8.74"><forename type="first">Mathias</forename><surname>Lux</surname></persName>
							<email>mlux@itec.aau.at</email>
							<affiliation key="aff3">
								<orgName type="department">ITEC</orgName>
								<orgName type="institution">Klagenfurt University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.13,201.64,62.11,8.74"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<email>cathal.gurrin@dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,193.64,133.89,228.07,12.62;1,217.22,151.82,180.91,12.62">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE439038DBC3E4C3309FAE1DB4E095D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Benchmarking in Multimedia and Retrieval related research fields has a long tradition and important position within the community. Benchmarks such as the MediaEval Multimedia Benchmark or CLEF are well established and also served by the community. One major goal of these competitions beside of comparing different methods and approaches is also to create or promote new interesting research directions within multimedia. For example the Medico task at MediaEval with the goal of medical related multimedia analysis. Although lifelogging creates a lot of attention in the community which is shown by several workshops and special session hosted about the topic. Despite of that there exist also some lifelogging related benchmarks. For example the previous edition of the lifelogging task at ImageCLEF. The last years ImageCLEFlifelog task was well received but had some barriers that made it difficult for some researchers to participate (data size, multi modal features, etc.) The ImageCLEFlifelog 2018 tries to overcome these problems and make the task accessible for an even broader audience (e.g., pre-extracted features are provided). Furthermore, the task is divided into two subtasks (challenges). The two challenges are lifelog moment retrieval (LMRT) and the Activities of Daily Living understanding (ADLT). All in all seven teams participated with a total number of 41 runs which was an significant increase compared to the previous year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelogging is a research field that gets more and more attention in the last years. This is not just due to the interesting challenges that this direction offers (huge amount of data, complex patterns, multi-modal learning, etc.) but also because of the availability of devices. A great amount of people used devices such as smart watches and other type of sensors. These sensors in combination with smartphones that are an almost natural companion for a person nowadays enable powerful and more insightful lifelogging.</p><p>The data collected using these different devices is called lifelogs. A lifelog is a digital record of a persons daily routines. Such a lifelog can look different for different people depending on their habits and devices they use. Some people might record the whole day with videos others rely more on sensors. Nevertheless of the composition of such a lifelog it is clear that the collected data reaches huge dimensions for each specific user. This calls for research with focus on systems that are able to analyze these huge amounts of data in a meaningful way. Such analysis can be manging fold and span from simple re-finding events task to summarization or information retrieval.</p><p>For example people that log their daily life want to recall certain things such as persons they saw during the day, products they found interesting in a shopping window while they were strolling trough the streets. Lifelogs can not only be used for the users need but hold also potential for other applications such as recommender systems. For example one could get recommendations based on items they focused on in a shopping window. Examples for events that a lifelogger might want to retrieve from their log can be seen in Figure <ref type="figure" coords="2,395.78,339.09,3.88,8.74" target="#fig_0">1</ref>. The ImageCLEF2018lifeLog task is the second edition of it. The first lifeLog task was performed in 2017 <ref type="bibr" coords="2,260.31,608.30,10.52,8.74" target="#b1">[2]</ref> and was inspired by the image annotation and retrieval tasks that were part of ImageCLEF for more than a decade (since 2003). With the lifeLogging task at ImageCLEF the focus lies on multi-modal analysis of large data collections. This is following the general evolution of Im-ageCLEF (the focus changed of pure image retrieval to a more multi-modal approach including concept localization and natural language description of images <ref type="bibr" coords="3,156.40,130.95,16.13,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,172.53,130.95,12.10,8.74" target="#b12">13,</ref><ref type="bibr" coords="3,184.63,130.95,12.10,8.74" target="#b14">15,</ref><ref type="bibr" coords="3,196.73,130.95,12.10,8.74" target="#b13">14]</ref>. In the last three editions <ref type="bibr" coords="3,327.29,130.95,10.89,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,338.18,130.95,7.26,8.74" target="#b5">6,</ref><ref type="bibr" coords="3,345.43,130.95,7.26,8.74" target="#b6">7]</ref>).</p><p>This paper provides an overview of the second edition of the ImageCLE-Flifelog task which is again part of the overall benchmark campaign organized every year by ImageCLEF <ref type="bibr" coords="3,250.53,167.14,10.52,8.74" target="#b7">[8]</ref> under the CLEF initiative<ref type="foot" coords="3,376.22,165.57,3.97,6.12" target="#foot_0">1</ref> . The overview paper is organized as following: In section 2 we provide a detailed task description. This includes rules, data and resources. In the following section 3 submissions and results are presented and discussed. In he final section 4 the paper is concluded and final remarks and future work are discussed.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>The main goal of the task is to make use of lifelogging data and explore the possibilities that come with it. As discussed in the introduction there are several interesting and useful applications that can emerge from this data. To limit the scope for the 2018 version of the task two sub-tasks are proposed. This makes it easier for the participants to focus on a specific outcome and participants were also allowed to submit only for one of the subtasks.</p><p>The two subtasks focus on two different topics. The first one Lifelog moment retrieval (LMRT) asks the participants to retrieve a specific moment in the daily life of a logger. Specific queries are provided that should be answered. The second subtask, Daily Living understanding (ADLT), targets understanding of daily living over a period of time and for specific concepts. In the following the two subtasks are described in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lifelog moment retrieval (LMRT)</head><p>The participants have to retrieve a number of specific moments in a lifeloggers life. We define moments as semantic events, or activities that happened throughout the day. For example, they should return the relevant moments for the query "Find the moment(s) when I was shopping for wine in the supermarket." Particular attention should be paid to the diversification of the selected moments with respect to the target scenario. The ground truth for this subtask was created using manual annotation. Daily Living understanding (ADLT) Given a period of time, e.g., "From 13 August to 16 August" or "Every Saturday", the participants should analyse the lifelog data and provide a summarisation based on the selected concepts (provided by the task organizers) of Activities of Daily Living (ADL) and the environmental settings / contexts in which these activities take place. Some examples of ADL concepts: "Commuting (to work or another common venue)", "Traveling (to a destination other than work, home or another common social event)", "Preparing meals (include making tea or coffee)", "Eating/drinking", and contexts: "In an office environment", "In a home", "In an open space". Appendix A provides the full ontology of the concepts and contexts. The summarisation should be described as the number of times and the spending time the queried event happened. For example:</p><p>-ADL: "Eating/drinking: 6 times, 90 minutes", "Traveling: 1 time, 60 minutes" -Context: "In an office environment: 500 minutes", "In a church: 30 minutes"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The task will be split into two related subtasks using a completely new multimodal dataset which consists of 50 days of data from a lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, etc.), music listening history. The dataset is built based on the data available for the NTCIR-13 -Lifelog 2 task. Table <ref type="table" coords="4,475.61,510.87,4.98,8.74" target="#tab_0">1</ref> summarises the data collection.</p><p>Format of the metadata. The metadata is stored in an .xml file, which is a simple aggregation of all users data. It is structured as follows:</p><p>The root node of the data is the USERS tag. Each user element contains all the data of that user (u1 or u2). Each user has a tag USER that contains the user ID as an attribute, example: [user id=u1]. For this year, only user u1 is considered. Inside the USER element, is his/her data:</p><p>Following that there is a tag DAYS, this tag contains the lifelogging information of that user organised per day, each day is included in a tag DAY that has the data (a tag DATA), the relative path to the directory that contains the images captured in that particular day (the tag IMAGES-DIRECTORY), then the minutes of of that day under a root tag called MINUTES.</p><p>At the start of each day there is a set of daily metatdata for that user. This data is of three forms; BIOMETRICS, ACTIVITIES &amp; PERSONAL LOGS. The biometrics contains WEIGHT, FAT MASS, HEART RATE, SYSTOLIC blood pressure &amp; DIASTOLIC blood pressure, which were readings taken after waking up each day. The activities contains summary activities: STEPS taken that day, DISTANCE walked in meters that day &amp; ELEVATION climbed in meters that day. The personal logs contain HEALTH LOGS, including the TIME of reading, GLU Glucose levels in the blood, BP Blood Pressure, HR Heart Rate, MOOD manually logged every morning and sometimes a COMMENT, as well as DRINK LOGS and FOOD LOGS which were manually logged throughout the data.</p><p>Following that, the days data is organised into minutes. The MINUTES element, contains exactly 1440 child elements (called MINUTE), each child has an ID (example: [minute id=0], [minute id=1], [minute id=2] etc.), and it represent one minute in the day ordered from 0 = 12:00 AM, to 1439 = 23:59PM.</p><p>Each minute contains: 0 or 1 location information (LOCATION tag), 0 or one activity information (ACTIVITY tag), biometrics, 0 or more captured images (IMAGES tag with IMAGE child element (each element has has a relative path to the image and a unique image ID), and 0 or 1 MUSIC tag giving details of the music listened to at that point in time.</p><p>The location information is captured by Moves app (https://www.movesapp.com/), and they represent to semantic locations (Home, Work, DCU Computing building, GYM, Name of a Store, etc), or to landmark locations registered by Moves. This tag can contain information in several languages. For locations that are not (HOME) or (WORK), the GPS locations are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Performance Measures</head><p>Metrics LMRT. For assessing performance, classic metrics are deployed. These metrics are:</p><p>-Cluster Recall at X (CR@X) -a metric that assesses how many different clusters from the ground truth are represented among the top X results; -Precision at X (P@X) -measures the number of relevant photos among the top X results; -F1-measure at X (F1@X) -the harmonic mean of the previous two.</p><p>Various cut off points are to be considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics is the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p><p>Participants are allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time is allowed per topic. In particular, the organizers would like to emphasize methods that allow interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) are encouraged.</p><p>Metrics ADLT. The final score is computed as the percentage of similarity between the ground-truth and the submitted values, measured as average of the number of times and minutes differences, as follows:</p><formula xml:id="formula_0" coords="6,149.71,155.06,254.34,14.60">ADL score = 1 2 max(0, 1 - |n-ngt| ngt ) + max(0, 1 - |m-mgt| mgt )</formula><p>where n, n gt are the submitted and ground-truth values for how many times the events occurred, respectively, and m, m gt are the submitted and ground-truth values for how long (in minutes) the events happened, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Ground Truth Format</head><p>Ground truth is provided in two individual txt files: one file for the cluster ground truth and one file for the relevant image ground truth.</p><p>In the cluster ground-truth file each line corresponds to a cluster where the first value is the topic id, followed by cluster id number, followed by the cluster user tag separated by comma. Lines are separated by an end-of-line character (carriage return). An example is presented below:</p><formula xml:id="formula_1" coords="6,140.99,341.03,147.97,72.42">-1, 1, Badger &amp; Dodo Cafe -1, 2, Costa coffee - -2, 1, Airport Restaurant -2, 2, Arnotts Department Store -</formula><p>In the relevant ground-truth file the first value on each line is the topic id, followed by a unique photo id, and then followed by the cluster id number (that corresponds to the values in the cluster ground-truth file) separated by comma. Each line corresponds to the ground truth of one image and lines are separated by an end-of-line character (carriage return). An example is presented below: </p><formula xml:id="formula_2" coords="6,140.99,503.32,18.46,8.77">-1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participating Groups and Runs Submitted</head><p>This year the number of participants was considerably higher with respect to 2017: we received in total 41 runs: 29 (21 official, 8 additional) for LMRT and 12 (8 official, 4 additional) for ADLT, from 7 teams from Brunei, Taiwan, Vietnam, Greece-Spain, Tunisia, Romania, and a multi-nation team from Ireland, Italy, Austria, and Norway. The received approaches range from fully automatic to fully manual, from using a single information source provided by the task to using all information as well as integrating additional resources, from traditional learning methods (e.g. SVMs) to deep learning and ad-hoc rules. Submitted runs and their results are summarized in Tables <ref type="table" coords="7,324.66,258.92,4.98,8.74" target="#tab_2">2</ref> and<ref type="table" coords="7,352.34,258.92,3.87,8.74">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results for ADLT and LMRT Tasks</head><p>In this section we provide a short description of all submitted approaches followed by the official result of the task.</p><p>The organiser team participated in both tasks <ref type="bibr" coords="7,354.10,333.15,14.61,8.74" target="#b15">[16]</ref>. The idea was to provide a baseline using only provided data. For both subtasks LIFER was used. LIFER is a interactive lifelog search engine that is able to solve different lifeloggin challenges.</p><p>The CIE@UTB <ref type="bibr" coords="7,221.46,381.01,10.52,8.74" target="#b2">[3]</ref> authors propose a content-context-based method to automatically create summaries for the ADLT task. The two main concepts used are a daily-normal environment panorama image which is used to detect events in known environments and a daily-abnormal environment taxonomy which is used to detect events in pre-defined taxonomy. The team only participated in the ADLT task.</p><p>CAMPUS-UPB <ref type="bibr" coords="7,220.86,452.79,10.52,8.74" target="#b3">[4]</ref> focused on LMRT. In their methods they analysed visual information, textual information and metadata. Visual concepts are extracted using a convolutional neural network (CNN) approach. Visual features are then clustered using K-means and reranked using the concepts and queried topics.</p><p>AILab-GTI <ref type="bibr" coords="7,203.88,500.66,10.52,8.74" target="#b8">[9]</ref> proposed a weakly supervised learning method for LMRT. The method consists of three different strategies. The Two-class strategy, is based on deep learning and presents each topic by two classes one described by the topic and the other by the absence of it. The second strategy, Ten-class strategy, considers all classes at the same time. The final strategy, called Eleven-Class strategy is similar to the previous one with one additional class for topics not belonging the the challenge.</p><p>The NLP-Lab <ref type="bibr" coords="7,215.85,584.39,15.50,8.74" target="#b9">[10]</ref> team tackled both subtask of the ImageCLEFlifeloggin task. The main idea was to reduce user involvement during the retrieval by using natural language processing. For both tasks specific approaches were presented based on the same methodology. Visual concepts are extracted from the images and combined with textual knowledge to get rid of the noise. For ADLT the images are ranked by time and frequency, whereas for LMRT ranking is performed exploiting similarity between image concepts and user queries. HCMUS <ref type="bibr" coords="8,190.66,339.85,15.50,8.74" target="#b11">[12]</ref> proposed a method based on visual concept fusion and textbased query expansion for both sub tasks. First concepts are extracted from the images. In addition textual descriptions of the images are created. These information are then combined in an inverted index for retrieval. To determine the similarity between words and phrases word embedding is used. Based on this and the users provided queries semantically similar concepts are recommended to the users.</p><p>The Regim Lab <ref type="bibr" coords="8,221.46,423.53,10.52,8.74" target="#b0">[1]</ref> team decided to work on the LMRT task. Combinations of visual features, textual features and a combination of both were used. For the visual features fine tuned CNN architectures were utilized. For the combination of visual and textual features the best visual run was combined with XQuery FLOWR results.</p><p>As mentioned before, for the ADLT task, four teams have been participated: CIE@UTB <ref type="bibr" coords="8,185.13,495.26,9.96,8.74" target="#b2">[3]</ref>, NLP-Lab <ref type="bibr" coords="8,245.60,495.26,14.61,8.74" target="#b9">[10]</ref>, HCMUS <ref type="bibr" coords="8,307.31,495.26,15.50,8.74" target="#b11">[12]</ref> and the Organisers team <ref type="bibr" coords="8,436.99,495.26,14.61,8.74" target="#b15">[16]</ref>.</p><p>The official results are summarised in Table <ref type="table" coords="8,345.43,507.22,3.87,8.74" target="#tab_2">2</ref>. The best run was submitted by CIE@UTB with a score of 0.556 which is also outperforming the organizers baseline approaches.</p><p>For LMRT, six teams have participated: AILab-GTI <ref type="bibr" coords="8,375.73,543.08,9.96,8.74" target="#b8">[9]</ref>, Regim Lab <ref type="bibr" coords="8,441.03,543.08,9.96,8.74" target="#b0">[1]</ref>, NLP-Lab <ref type="bibr" coords="8,155.51,555.04,14.61,8.74" target="#b9">[10]</ref>, HCMUS <ref type="bibr" coords="8,218.59,555.04,14.61,8.74" target="#b11">[12]</ref>, CAMPUS-UPB <ref type="bibr" coords="8,313.07,555.04,10.52,8.74" target="#b3">[4]</ref> and the Organisers team <ref type="bibr" coords="8,441.17,555.04,14.61,8.74" target="#b15">[16]</ref>. The results are presented in Table <ref type="table" coords="8,267.80,566.99,3.87,8.74">3</ref>. The best results were achieved by AILab-GTI with an F110 of 0.545. Major of the teams outperform the organisers baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>We learned that multi-modal data analysis has been explored and exploited this year, with the majority of the approaches combining visual, textual, location Table <ref type="table" coords="9,204.85,115.91,4.13,7.89">3</ref>. Submitted runs for ImageCLEFlifelog2018 LMRT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run Name Score (F1@10)</p><p>Organizers <ref type="bibr" coords="9,246.30,170.21,14.34,7.86" target="#b15">[16]</ref> Run and other information to solve the task. This was quite different from last year when often only one type of data was analysed. Furthermore, we learned that many approaches are based on deep neural networks, from standard CNN to specifically designed deep networks for lifelogging tasks. However, there are still rooms for improvement, since the best results are coming from the fine-tuned queries, which means we need more advanced techniques to bridging the gap between the abstract understanding of human needs and the multi-modal data. Furthermore, automatically "translate" the query into the retrieval criteria is still a challenge which requires further studies. Regarding the number of the signed-up teams and the submitted runs, we received a significant improvement compared to last year. This shows how interesting and challenging lifelog data is and that it holds much research potential. As next steps we do not plan to enrich the dataset but rather provide richer data and narrow down the application of the challenges (e.g., extend to health-care application).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,142.40,557.66,330.56,7.89;2,143.80,462.48,107.21,80.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example lifelogging information need: 'Show me all meeting moments'.</figDesc><graphic coords="2,143.80,462.48,107.21,80.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,139.85,117.75,335.65,95.51"><head>Table 1 .</head><label>1</label><figDesc>Statistics of ImageCLEFlifelog2018 Dataset.</figDesc><table coords="4,139.85,117.75,335.65,84.60"><row><cell>Size of the Collection</cell><cell>18.854 GB</cell></row><row><cell>Number of Images</cell><cell>80,440 images</cell></row><row><cell cols="2">Number of Known Locations 135 locations</cell></row><row><cell>Concepts</cell><cell>Fully annotated (by Microsoft Computer Vision API)</cell></row><row><cell>Biometrics</cell><cell>Fully provided (24 × 7)</cell></row><row><cell>Human Activities</cell><cell>Provided</cell></row><row><cell>Number of ADLT Topics</cell><cell>20 (10 for devset, 10 for testset)</cell></row><row><cell>Number of LMRT Topics</cell><cell>20 (10 for devset, 10 for testset)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,169.54,115.91,276.28,199.36"><head>Table 2 .</head><label>2</label><figDesc>Submitted runs for ImageCLEFlifelog2018 ADLT task. submissions from the organizer teams are just for reference.</figDesc><table coords="8,169.54,136.29,252.16,168.03"><row><cell>Team</cell><cell cols="2">Run Name Score (similarity)</cell></row><row><cell></cell><cell>Run 1 *</cell><cell>0.816</cell></row><row><cell></cell><cell>Run 2 *, †</cell><cell>0.456</cell></row><row><cell>Organizers [16]</cell><cell>Run 3 *, †</cell><cell>0.344</cell></row><row><cell></cell><cell>Run 4 *, †</cell><cell>0.481</cell></row><row><cell></cell><cell>Run 5 *, †</cell><cell>0.485</cell></row><row><cell>CIE@UTB [3]</cell><cell>Run 1</cell><cell>0.556</cell></row><row><cell></cell><cell>Run 1</cell><cell>0.243</cell></row><row><cell></cell><cell>Run 2</cell><cell>0.285</cell></row><row><cell>NLP-Lab [10]</cell><cell>Run 3</cell><cell>0.385</cell></row><row><cell></cell><cell>Run 4</cell><cell>0.459</cell></row><row><cell></cell><cell>Run 5</cell><cell>0.479</cell></row><row><cell>HCMUS [12]</cell><cell>Run 1</cell><cell>0.059</cell></row><row><cell>Notes:</cell><cell></cell><cell></cell></row></table><note coords="8,198.00,294.68,3.65,5.24;8,200.28,305.64,1.72,5.24;8,205.57,307.41,211.22,7.86"><p>* † submissions submitted after the official competition.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.44,137.01,7.47"><p>http://www.clef-initiative.eu</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Lifelog Ontology</head><p>They are the activities/facets of daily life and the environmental settings / contexts in which these activities take place. For the ADLT task, we selected a subset of ten of these activities and settings for evaluation.</p><p>A.1 Activities / Facets of Life Activity -Commuting (to work or other common venue) -Travelling (to a destination other than work, home or some other common social event) -Preparing meals (include making tea or coffee) -Eating/drinking -Taking care of children / playing with children -Sleeping -Praying / worshipping / meditating -Socialising / casual conversation -Relaxing / meditation -Reading</p><p>• reading a book • reading digital content -Gardening -Shopping</p><p>• retail shopping and purchasing • browsing for items in store • online shopping -Work meeting/interaction (one or more people)</p><p>• face-to-face </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.001 Public transportation</head><p>Description: Find the moments when I was taking public transportation Narrative: Moments in which the user was was taking any public transportation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.002 Eating Lunch</head><p>Description: Find the moments when I was eating lunch from 11: 00am to 3: 00pm Narrative: Moments in which the user was eating lunch are relevant regardless of where the lunch is eaten. Time is relevant T.003 Coffee Description: Find the moment(s) when I was drinking coffee in a cafe.</p><p>Narrative: Moments that show the user consuming coffee or tea in a cafe (outside of home or office) are considered relevant. The coffee can be hot in a cup or paper cup, or cold coffee in a plastic or paper cup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.004 Sunset &amp; Sunrise</head><p>Description: Find the moments when I was outside at sunset and sunrise. Narrative: To be considered relevant, the moment must show the sun setting or rising. This can be at night time and morning time, or can be when the sun is disappearing and appearing behind a mountain in the evening.</p><p>T.005 Presenting/Lecturing Description: Find the moments when I was lecturing to a group of people in a classroom environment. Narrative: A lecture can be in any classroom environment and must contain more than one person in the audience, who are sitting down. A classroom environment has desks and chairs clearly visible. Discussion or lecture encounters in which the audience are standing up, or outside of a classroom environment are not considered relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.006 Grocery Shopping</head><p>Description: Find all the moments when I was grocery shopping. Narrative: Any moment when the user was in a grocery store and visibly interacting with products is considered relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.007 Cooking</head><p>Description: Find the moments when I was cooking at home. Narrative: Cooking at home includes preparation of ingredients and cooking of the food. To be considered relevant the user must be seen to be preparing food.</p><p>Eating food at home is not considered relevant.</p><p>T.008 Having Beers in a Bar or restaurant Description: Find the moment when I had beer in a bar or in a restaurant. Narrative: To be considered relevant, the user must be clearly in a bar and having more than one drink. Black and light beers were consumed in this moment.</p><p>T.009 Working in a Coffee Shop Description: Find the moments in which I was working in a coffee shop. Narrative: To be considered relevant the user must be seen working with a laptop in a coffee shop. Relevant moments must show coffee on the table beside the laptop. Working in any place besides a coffee shop is not considered relevant. Socialising or relaxing in a coffee shop is not considered relevant if there is no laptop being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.010 Eating Pasta</head><p>Description: Find the moments when I was eating Pasta. Narrative: The user was eating pasta, either sitting at a table, an office desk or in a corridor outside an office. Sometimes pasta eating occurred with another person, sometimes it was in solitude. T.005 Dinner at Home Description: Find the moments when I was having dinner at home. Narrative: Moments in which the user was having dinner at home are relevant. Dinner in any other location is not relevant. Dinner usually occurs in the evening time T.006 Assembling Furniture Description: Find the moments when I was assembling a piece of furniture. Narrative: To be considered relevant, the moments must show some parts of the furniture being assembled.</p><p>T.007 Taking a coach/bus in foreign countries Description: Find the moments when I was taking a road vehicle in foreign countries. Narrative: To be considered relevant, the user must be taking road transport in a different country (i.e. not Ireland). Taking airplane, train or boat is not considered relevant.</p><p>T.008 Costa Coffee with friends Description: Find the moments when I was with friends in Costa coffee. Narrative: To be considered relevant, the moment must show at least a person together with the lifelogger in any Costa Coffee shop. Moments that show the user alone are not considered relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.009 Using mobile phone or tablets in a vehicle</head><p>Description: Find the moments in which I was using my mobile phone or tablets in a vehicle. Narrative: To be considered relevant the user must be seen using with a mobile phone or a tablet in a vehicle, as a driver or as a passenger.</p><p>T.010 Graveyard Description: Find the moments when I was at a graveyard. Narrative: The user must be in a graveyard or inside a church inside a graveyard. Passing or standing outside of the graveyard is not considered relevant.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,197.54,337.64,7.86;10,151.52,208.50,281.80,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,434.97,197.54,45.62,7.86;10,151.52,208.50,161.45,7.86">Regim Lab Team at ImageCLEFlifelog LMRT Task</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">B</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ezzarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,219.37,337.63,7.86;10,151.52,230.33,329.07,7.86;10,151.52,241.29,329.07,7.86;10,151.52,252.25,173.54,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,168.38,230.33,291.68,7.86">Overview of imagecleflifelog 2017: lifelog retrieval and summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,241.29,252.47,7.86">CLEF2017 Working Notes (CEUR Workshop Proceedings)</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,263.11,337.63,7.86;10,151.52,274.07,301.65,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,334.91,263.11,145.67,7.86;10,151.52,274.07,202.81,7.86">Leveraging Content and Context to Foster Understanding of Activities of Daily Living</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kasem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S H</forename><surname>Nazmudeen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,284.94,337.63,7.86;10,151.52,295.90,201.52,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,255.14,284.94,225.46,7.86;10,151.52,295.90,102.68,7.86">Multimedia Lab @ CAMPUS at ImageCLEFlifelog 2018 Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,306.76,337.63,7.86;10,151.52,317.72,329.07,7.86;10,151.52,328.68,329.07,7.86;10,151.52,339.64,329.07,7.86;10,151.52,350.60,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,239.87,317.72,240.73,7.86;10,151.52,328.68,162.53,7.86">Overview of the imageclef 2015 scalable image annotation, localization and sentence generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.33,328.68,146.26,7.86;10,151.52,339.64,162.05,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,361.46,337.64,7.86;10,151.52,372.42,329.07,7.86;10,151.52,383.38,329.07,7.86;10,151.52,392.07,272.66,10.13" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,287.50,372.42,193.09,7.86;10,151.52,383.38,117.61,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,293.35,383.38,187.25,7.86;10,151.52,394.34,110.74,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,405.21,337.63,7.86;10,151.52,416.17,329.07,7.86;10,151.52,427.12,329.07,7.86;10,151.52,438.08,329.07,7.86;10,151.52,449.04,25.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,402.96,416.17,77.63,7.86;10,151.52,427.12,182.07,7.86">Overview of imageclef 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.83,427.12,125.76,7.86;10,151.52,438.08,235.77,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="315" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,459.91,337.63,7.86;10,151.52,470.87,329.07,7.86;10,151.52,481.83,329.07,7.86;10,151.52,492.79,329.07,7.86;10,151.52,503.74,329.07,7.86;10,151.52,514.70,329.07,7.86;10,151.52,525.66,329.07,7.86;10,151.52,536.62,46.58,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,197.95,492.79,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,503.74,329.07,7.86;10,151.52,514.70,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="10,151.52,525.66,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,547.49,337.64,7.86;10,151.52,558.45,149.02,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,397.13,547.49,83.46,7.86;10,151.52,558.45,50.17,7.86">Retrieving Events in Life Logging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>García</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,569.31,337.98,7.86;10,151.52,580.27,329.07,7.86;10,151.52,591.23,329.07,7.86;10,151.52,602.19,46.58,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,418.44,569.31,62.15,7.86;10,151.52,580.27,329.07,7.86;10,151.52,591.23,279.50,7.86">NTU NLP-Lab at ImageCLEFlifelog 2018: Visual Concept Selection with Textual Knowledge for Understanding Activities of Daily Living and Life Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Fu1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,613.05,337.97,7.86;10,151.52,624.01,310.27,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,259.08,613.05,221.51,7.86;10,151.52,624.01,101.62,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.60,624.01,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,634.88,337.98,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,140.74,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,175.22,645.84,305.37,7.86;10,151.52,656.80,41.90,7.86">Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dinh-Duy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">A</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,119.67,337.98,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,46.58,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,257.71,119.67,222.89,7.86;11,151.52,130.63,65.14,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.38,130.63,45.21,7.86;11,151.52,141.59,223.39,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,163.51,337.98,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,329.07,7.86;11,151.52,196.39,22.02,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,262.58,163.51,218.02,7.86;11,151.52,174.47,96.12,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,271.70,174.47,107.11,7.86">CLEF2014 Working Notes</title>
		<title level="s" coord="11,387.41,174.47,93.18,7.86;11,151.52,185.43,31.91,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2014">September 15-18 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="308" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,207.34,337.97,7.86;11,151.52,218.30,329.07,7.86;11,151.52,229.26,301.98,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,315.57,207.34,165.02,7.86;11,151.52,218.30,167.36,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,342.84,218.30,137.75,7.86;11,151.52,229.26,133.89,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,240.22,337.97,7.86;11,151.52,251.18,329.07,7.86;11,151.52,262.14,95.77,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,468.56,240.22,12.03,7.86;11,151.52,251.18,329.07,7.86">An Interactive Lifelog Retrieval System for Activities of Daily Living Understanding</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
