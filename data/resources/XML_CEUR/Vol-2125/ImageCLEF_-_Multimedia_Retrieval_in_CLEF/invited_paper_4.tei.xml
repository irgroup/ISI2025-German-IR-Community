<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,187.48,115.96,240.41,12.62;1,217.98,133.89,179.40,12.62">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.07,171.56,106.97,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.60,171.56,72.45,8.74"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Brown University</orgName>
								<address>
									<settlement>Providence</settlement>
									<region>RI</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.54,183.51,92.20,8.74"><forename type="first">Vincent</forename><surname>Andrearczyk</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.91,183.51,68.10,8.74"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,187.48,115.96,240.41,12.62;1,217.98,133.89,179.40,12.62">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B84E8BB8A91763880B96A590CF308FA7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Caption prediction</term>
					<term>Image understanding</term>
					<term>radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The caption prediction task is in 2018 in its second edition after the task was first run in the same format in 2017. For 2018 the database was more focused on clinical images to limit diversity. As automatic methods with limited manual control were used to select images, there is still an important diversity remaining in the image data set. Participation was relatively stable compared to 2017. Usage of external data was restricted in 2018 to limit critical remarks regarding the use of external resources by some groups in 2017. Results show that this is a difficult task but that large amounts of training data can make it possible to detect the general topics of an image from the biomedical literature. For an even better comparison it seems important to filter the concepts for the images that are made available. Very general concepts (such as "medical image") need to be removed, as they are not specific for the images shown, and also extremely rare concepts with only one or two examples can not really be learned. Providing more coherent training data or larger quantities can also help to learn such complex models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The caption task described in this paper is part of the ImageCLEF<ref type="foot" coords="1,442.71,529.05,3.97,6.12" target="#foot_0">5</ref> benchmarking campaign <ref type="bibr" coords="1,219.26,542.58,7.75,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,227.00,542.58,3.87,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,227.00,542.58,3.87,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,230.88,542.58,7.75,8.74" target="#b3">[4]</ref>, a framework where researchers can share their expertise and compare their methods based on the exact same data and evaluation methodology in an annual rhythm. ImageCLEF is part of CLEF <ref type="foot" coords="1,420.59,564.91,3.97,6.12" target="#foot_1">6</ref> (Cross Language Evaluation Forum). More on the 2018 campaign in general is described in Ionescu et al. <ref type="bibr" coords="1,210.62,590.40,10.52,8.74" target="#b4">[5]</ref> and on the related medical tasks in <ref type="bibr" coords="1,387.53,590.40,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="1,399.71,590.40,7.01,8.74" target="#b6">7]</ref>. In general, Im-ageCLEF aims at building tasks that are related to clear information needs in medical or non-medical environments <ref type="bibr" coords="1,306.05,614.31,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="1,318.23,614.31,7.01,8.74" target="#b8">9]</ref>. Relationships also exist with the LifeCLEF and CLEFeHealth labs <ref type="bibr" coords="1,283.85,626.26,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,301.01,626.26,11.62,8.74" target="#b10">11]</ref>.</p><p>The caption task started in 2016 as a pilot task. In 2016, the task was part of the medical image classification task <ref type="bibr" coords="2,297.92,130.95,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,315.08,130.95,11.62,8.74" target="#b12">13]</ref>, although it unfortunately did not have any participants, also because the questions of the task were not strongly developed at the time. Since 2017, the caption task has been running in the current format. The motivation of this task is the strong increase in available images from the biomedical literature that is growing at an exponential rate and is made available via the PubMed Central R (PMC) <ref type="foot" coords="2,358.64,189.15,3.97,6.12" target="#foot_2">7</ref> repository. As the data set is dominated by compound figures and many general graphs, ImageCLEF has addressed the analysis of compound figures in the past <ref type="bibr" coords="2,370.45,214.64,14.61,8.74" target="#b12">[13]</ref>. To extract the image types a hierarchy was created <ref type="bibr" coords="2,263.70,226.59,14.61,8.74" target="#b13">[14]</ref>, and as training data for these image types are available the global data set of over 5 million images can be filtered to a more homogeneous set containing mainly radiology images as is described in the data preparation section (Section 3). The ImageCLEF caption task aims at better understanding the images in the biomedical literature and extract concepts and captions based only on the visual information of the images (see Figure <ref type="figure" coords="2,457.54,286.37,3.87,8.74" target="#fig_1">1</ref>). A further description of the task can be found in Section 2.</p><p>This paper presents an overview of the ImageCLEF caption task 2018 including the task and participation in Section 2, the dataset in Section 3 and an explanation of the evaluation framework in Section 4. The participant approaches are described in Section 5, followed by a discussion and the conclusions in Sections 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Participation</head><p>Following 2017 format, the 2018 caption task contains two subtasks, a concept detection subtask that aims at extracting UMLS (Unified Medical Language System R ) Concept Unique Identifiers (CUIs) from the images automatically based on the training data made available and a caption prediction subtask that requires to predict a precise text caption for the images in the test data set. Table <ref type="table" coords="2,162.52,481.23,4.98,8.74" target="#tab_0">1</ref> shows the 8 participants of the task who submitted 44 runs, 28 to the concept detection subtask and 16 to the caption prediction subtask. Three of the groups already participated in 2017, showing that the majority of the participant were new to the task.</p><p>It is interesting that despite the fact that the output of the concept detection task can be used for the caption prediction task, none of the participant used such an approach and only two groups participated in both tasks.</p><p>Concept Detection As a first step towards automatic image caption and scene understanding, this subtask aims at automatically extracting high-level biomedical concepts (CUIs) from medical images using only the visual content. This approach provides the participating systems with a solid initial building block for image understanding by detecting relevant individual components from which  full captions can be composed. The detected concepts are evaluated with a metric based on precision and recall using the concepts extracted from the ground truth captions (see <ref type="bibr" coords="3,220.14,517.08,42.61,8.74">Section 4)</ref>.</p><p>Caption Prediction In this subtask the participants need to predict a coherent caption for the entire medical image. The prediction can be based on the concepts detected in the first subtask as well as the visual analysis of their interaction in the image. Rather than the mere detection of visual concepts, this subtask requires to analyze the interplay of visible elements.</p><p>The evaluation of this second subtask is based on metrics such as BLEU scores independent from the first subtask and designed to be robust to variability in style and wording (see Section 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collection</head><p>Similarly to previous years, the experimental corpus is derived from scholarly biomedical articles on PMC from which we extract figures and their corresponding captions. As PMC contains many compound and non-clinical figures, we extract a subset of mainly clinical figures to remove noise from the data and focus the challenge on useful radiology/clinical images. The subset was created using a fully automated method based on deep multimodal fusion of Convolutional Neural Networks (CNNs) to classify all 5.8 million images of PMC from 2017 into image types, as described in <ref type="bibr" coords="4,300.35,408.80,14.61,8.74" target="#b20">[21]</ref>. This lead to a more homogeneous set of figures than in the 2017 ImageCLEF caption task but diversity still remained high. Besides the removal of many general graphs, also the number of compound figures (i.e. images containing more than one sub figure) was much lower than in 2017. Figure <ref type="figure" coords="4,202.88,456.62,4.98,8.74" target="#fig_3">2</ref> shows some examples of the images contained in the collection including some of the noise that still remained in the data. In total, the collection is comprised of 232,305 image-caption pairs <ref type="foot" coords="4,449.54,479.39,3.97,6.12" target="#foot_3">8</ref> . This overall set is further split into disjunct training (222,305 pairs) and test (10,000 pairs) sets. For the concept detection subtask, the QuickUMLS library <ref type="bibr" coords="4,465.09,504.87,15.50,8.74" target="#b21">[22]</ref> was used to identify UMLS concepts mentioned in the caption text. As a result 111,155 unique UMLS concepts were extracted from the training set.</p><p>Table <ref type="table" coords="4,178.10,541.17,4.98,8.74" target="#tab_1">2</ref> shows examples of the concepts. The average number of concepts per image in the training set is 30 varying between 1 and 1,276. In the training set 2,577 images are labeled with only 1 concept and 3,162 with only 2. Despite the collection being carefully created, there are still non-clinical images (see Figure <ref type="figure" coords="4,151.73,588.99,3.87,8.74" target="#fig_3">2</ref>), as all processing was automatic with only limited human checked. There are also non-relevant concepts for the task extracted, again linked to the fact that the data analysis was fully automatic with limited manual quality control. The concepts such as "and","medical image" or "image" are not relevant for  the task and not useful to predict from the visual information. Some of the concepts are redundant such as "Marrow -Specimen Source Codes" and "Marrow". Regardless of the limitations of the annotation the majority of concepts was of good quality and helps to understand the content of the images, as for example the concepts "Marrow" or "X-ray".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>The performance evaluation follows the approach in the previous edition <ref type="bibr" coords="5,465.10,524.61,15.50,8.74" target="#b22">[23]</ref> in evaluating both subtasks separately. For the concept detection subtask, the balanced precision and recall trade-off were measured in terms of F 1 scores. Python's scikit-learn (v0.17.1-2) library was used. Micro F 1 is calculated per image and then the average across all test images is taken as the final measure.</p><p>Caption prediction performance is assessed on the basis of BLEU scores <ref type="bibr" coords="5,465.10,584.39,15.50,8.74" target="#b23">[24]</ref> using the Python NLTK (v3.2.2) default implementation. Candidate captions are lower cased, stripped of all punctuation and English stop words. Finally, to increase coverage, Snowball stemming was applied. BLEU scores are computed per reference image, treating each entire caption as a sentence, even though it may contain multiple natural sentences. We report average BLEU scores across all test images. The source code of both evaluation scripts is available on the task Web page at http://imageclef.org/2018/caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This section shows the results achieved by the participants in both subtasks. Table <ref type="table" coords="7,161.15,201.27,4.98,8.74" target="#tab_2">3</ref> contains the results of the concept detection subtask and Table <ref type="table" coords="7,437.55,201.27,4.98,8.74" target="#tab_3">4</ref> contains the results of the caption prediction subtask. None of the participants used external data this year and despite less noise in the 2018 data, no better results were achieved in 2018 compared to 2017, maybe also due to the fact that no external data were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results for the Concept Detection subTask</head><p>28 runs were submitted by 5 groups (see Section 2) to the Concept detection subtasks. Table <ref type="table" coords="7,205.00,313.43,4.98,8.74" target="#tab_1">2</ref> shows the details of the results. Several approaches were used for the concept detection task, ranging from retrieval systems <ref type="bibr" coords="7,401.05,644.16,15.50,8.74" target="#b15">[16]</ref> to deep neural networks. Most research groups implemented at least one approach based on deep learning <ref type="bibr" coords="8,196.31,118.99,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="8,213.47,118.99,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="8,227.87,118.99,11.62,8.74" target="#b17">18]</ref>, including recurrent networks, various deep CNNs and generative adversarial networks. Best results were achieved by UA.PT Bioinformatics <ref type="bibr" coords="8,392.50,142.90,15.50,8.74" target="#b14">[15]</ref> by applying an adversarial auto-encoder for unsupervised feature learning. They also experimented with a traditional bag of words algorithm, using Oriented FAST and rotated BRIEF (ORB) key point descriptors. UA.PT employed two classification algorithms for concept detection over the learned feature spaces, namely a logistic regression and a variant of k-nearest neighbor (k-NN). Test results showed a best mean F1 score of 0.1102 for linear classifiers, by using the features of the adversarial auto-encoder.</p><p>ImageSem <ref type="bibr" coords="8,197.00,238.55,17.90,8.74" target="#b15">[16]</ref>was the group following UA.PT Bioinformatics in the ranking. ImageSem was the only group using a retrieval approach, which was more popular in 2017. This approach is based on the open-source Lucene Image Retrieval (LIRE) system used in combination with Latent Dirichlet Allocation (LDA) for clustering concepts of the similar images. ImageSem also experimented with a pre-trained CNN fine-tuned to predict a selected subset of concepts.</p><p>IPL <ref type="bibr" coords="8,169.91,310.28,15.50,8.74" target="#b16">[17]</ref> proposed a k-NN classifier using two image representation models. One of the methods used is a bag of visual words with dense Scale Invariant Feature Transform (SIFT) descriptors using 4,096 clusters. A second method uses a generalized bag of colors, dividing the image into a codebook of regions of homogeneous colors with 100 or 200 clusters.</p><p>The CS MS group <ref type="bibr" coords="8,236.90,370.05,15.50,8.74" target="#b17">[18]</ref> used an encoder-decoder model based on a multimodal Recurrent Neural Networks (RNNs). The encoded captions were the input to the RNN via word embedding, while deep image features were encoded via a pre-trained CNN. The combination of the two encoded inputs was used to generate the concepts.</p><p>The AILAB used a multimodal deep learning approach based. Instead of using the 220K images, AILAB only used a subset of 4,000 images with feature generation. The visual features are extracted by a pre-trained CNN, while the text features are obtained by word embedding, followed by a Long Short-Term Memory (LSTM) network. The two modalities are then merged and processed by a dense layer to make a final concept prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results for the Caption Prediction Task</head><p>16 runs were submitted by 5 groups (see Section 2) to the caption prediction subtask. Table <ref type="table" coords="8,201.29,548.52,4.98,8.74" target="#tab_3">4</ref> shows the details of the results.</p><p>ImageSem <ref type="bibr" coords="8,198.58,560.48,15.50,8.74" target="#b15">[16]</ref> achieved best results (0.2501 mean BLEU score) using the image retrieval method described in the previous section to combine captions of similar images. Preferred concepts, detected in the concept subtask by high CNN or LDA scores, were also used in other runs to improve the caption generation.</p><p>UMass <ref type="bibr" coords="8,181.79,608.30,15.50,8.74" target="#b18">[19]</ref> explored and implemented an encoder-decoder framework to generate captions. For the encoder, deep CNN features are used while an LSTM network is used for the decoder. The attention mechanism was also experimented on a smaller sample to evaluate its impact on the model fitting and prediction performance. As mentioned in the previous section for concept detection, CS MS <ref type="bibr" coords="9,445.22,355.32,15.50,8.74" target="#b17">[18]</ref> also used a similar multimodal deep learning method for caption prediction. A CNN feature extraction of the images was combined with an LSTM on top of word embeddings of the captions. A decoder made of two fully-connected layers produces the captions.</p><p>Instead of generating textual sequences directly from images, KU Leuven <ref type="bibr" coords="9,465.10,415.55,15.50,8.74" target="#b19">[20]</ref> first learn a continuous representation space for the captions. The representation space is learned by an adverserially regularized autoencoder (ARAE) <ref type="bibr" coords="9,462.33,439.46,14.61,8.74" target="#b24">[25]</ref>, combining a GAN and the auto-encoder. Subsequently, the task is reduced to learning the mapping from the images to the continuous representation, which is performed by a CNN. The decoder learned in the first step decodes the mapping to a caption for each image.</p><p>WHU also used a simple LSTM network that produces a caption by generating one word at every time step conditioned on a context vector, the previous hidden state and the previously generated words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusions</head><p>The 2018 caption prediction task of ImageCLEF attracted a similar number of participants compared to 2017. No external resources were used, making the task hard, which is also show in the results that overall were lower compared to 2017 despite the training and test data being more homogeneous, which should make the task slightly easier.</p><p>Most of the participants used deep learning approaches, but the used networks and architectures varied very strongly. This shows that there is still much research required and that the potential is high to improve results. Also more conventional features extraction and approaches based on retrieval delivered good results, showing also that there are many different ways for creating good models.</p><p>The limited participation was partly also linked to the large amount of data made available that caused problems for some research groups. The data set also remains noisy. Only more manual control can likely help creating cleaner data and thus maybe make results of automatic approaches more coherent. Even larger data sets could also help in this direction and really allow to create models for at least more frequently extracted concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,292.90,119.05,29.57,7.50;3,266.02,266.71,83.31,7.50;3,149.73,284.70,108.68,7.50;3,149.73,295.11,133.81,7.50;3,149.73,305.52,134.40,7.50;3,149.73,315.93,194.04,7.50;3,149.73,326.34,108.45,7.50;3,149.73,336.75,322.22,7.50;3,159.50,347.19,312.45,7.47;3,159.50,357.60,264.43,7.47;3,149.73,367.98,120.95,7.50;3,149.73,378.39,116.31,7.50;3,149.73,388.81,125.82,7.50;3,148.60,406.79,318.16,7.50"><head>-</head><label></label><figDesc>C0589121: Follow-up visit -C0018946: Hematoma, Subdural -C1514893: physiologic resolution -C0546674: Sorbitol dehydrogenase measurement -C4038402: Low resolution -C0374531: Postoperative follow-up visit, normally included in the surgical package, to indicate that an evaluation and management service was performed during a postoperative period for a reason(s) related to the original procedure -C0202691: CAT scan of head -C1320307: Urgent follow-up -C3694716: follow-up visit date Caption prediction: CT head at follow-up visit demonstrates resolution of SDH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,428.54,345.83,7.89;3,134.77,439.53,345.82,7.86;3,134.77,450.48,28.98,7.86;3,164.98,449.23,4.83,4.35;3,171.55,450.48,148.09,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of an image and the information provided in the training set in the form of the original caption and the extracted UMLS (Unified Medical Language System R ) Concept Unique Identifiers (CUIs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,270.88,228.39,73.60,7.86;5,268.96,355.28,77.43,7.86"><head>( a )</head><label>a</label><figDesc>Relevant images. (b) Irrelevant images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,134.77,380.16,345.83,7.89;5,134.77,391.15,120.74,7.86;5,185.50,242.72,78.87,107.73"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of (a) relevant images and (b) problematic images in the 2018 Image-CLEF caption task collection.</figDesc><graphic coords="5,185.50,242.72,78.87,107.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,345.83,153.62"><head>Table 1 .</head><label>1</label><figDesc>Participating groups in the 2018 ImageCLEF caption task. Participants marked with a star participated also in 2017.</figDesc><table coords="4,137.86,146.79,341.77,122.73"><row><cell>Team</cell><cell>Institution</cell><cell cols="2"># Concept detection # Caption prediction</cell></row><row><cell cols="2">UA.PT Bioinformatics [15] * DETI -Institute of Electronics and Informatics</cell><cell>9</cell><cell></cell></row><row><cell></cell><cell>Engineering, University of Aveiro, Portugal</cell><cell></cell><cell></cell></row><row><cell>ImageSem [16]</cell><cell>Institute of Medical Information, Chinese</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell>Academy of Medical Sciences/Peking Union</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Medical College, Beijing, China</cell><cell></cell><cell></cell></row><row><cell>IPL [17] *</cell><cell>Information Processing Laboratory, Athens Uni-</cell><cell>8</cell><cell></cell></row><row><cell></cell><cell>versity of Economics and Business, Athens,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Greece</cell><cell></cell><cell></cell></row><row><cell>CS MS [18] *</cell><cell>Computer Science Department, Morgan State</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>University, Baltimore, MD, USA</cell><cell></cell><cell></cell></row><row><cell>AILAB</cell><cell>University of the Aegean, Greece</cell><cell>3</cell><cell></cell></row><row><cell>UMass [19]</cell><cell>Umass Medical School, Worcester, MA, USA</cell><cell></cell><cell>5</cell></row><row><cell>KU Leuven [20]</cell><cell>Department of Computer Science, KU Leuven,</cell><cell></cell><cell>1</cell></row><row><cell></cell><cell>Leuven, Belgium</cell><cell></cell><cell></cell></row><row><cell>WHU</cell><cell>Wuhan University, Wuhan, Hubei, China</cell><cell></cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,227.64,345.83,319.42"><head>Table 2 .</head><label>2</label><figDesc>UMLS (Unified Medical Language System R ) Concept Unique Identifiers (CUIs) in the 2018 ImageCLEF caption and the number of images labeled with each of those concepts in the training set.</figDesc><table coords="6,173.91,271.18,267.54,275.88"><row><cell cols="2">CUI code concept</cell><cell>number of images</cell></row><row><cell cols="2">C1550557 RelationshipConjuntion -and</cell><cell>77,003</cell></row><row><cell cols="2">C1706368 And -dosing instruction fragment</cell><cell>77,003</cell></row><row><cell cols="2">C1704254 Medical Image</cell><cell>20,165</cell></row><row><cell cols="2">C1696103 image-dosage form</cell><cell>20,164</cell></row><row><cell cols="2">C1704922 image</cell><cell>20,164</cell></row><row><cell cols="2">C3542466 image (foundation metadata concept)</cell><cell>20,164</cell></row><row><cell cols="2">C1837463 Narrow face</cell><cell>19,491</cell></row><row><cell cols="2">C1546708 Marrow -Specimen Source Codes</cell><cell>19,253</cell></row><row><cell cols="2">C0376152 Marrow</cell><cell>19,253</cell></row><row><cell cols="2">C0771936 Yarrow flower extract</cell><cell>19,079</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">C0040405 X-Ray Computed Tomography</cell><cell>15,530</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">C1261259 Wright stain</cell><cell>12,217</cell></row><row><cell cols="2">C1510508 wright stain</cell><cell>12,137</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">C1306645 Plain x-ray</cell><cell>10,390</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">C0412620 CT of abdomen</cell><cell>8,037</cell></row><row><cell cols="2">C0202823 Chest CT</cell><cell>7,917</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">C0400569 Simple suture of pancreas</cell><cell>1</cell></row><row><cell cols="2">C0209088 4-methylcyclohexylamine</cell><cell>1</cell></row><row><cell cols="2">C0400569 Closed fracture of neck of femur</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,138.24,345.44,338.49,269.15"><head>Table 3 .</head><label>3</label><figDesc>Concept detection performance in terms of F1 scores.</figDesc><table coords="7,138.24,365.50,86.48,6.13"><row><cell>Team</cell><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,166.10,115.91,283.17,208.61"><head>Table 4 .</head><label>4</label><figDesc>Caption prediction performance in terms of BLEU scores.</figDesc><table coords="9,166.10,136.31,283.17,188.21"><row><cell>Team</cell><cell>Run</cell><cell>Mean BLEU</cell></row><row><cell cols="2">ImageSem run04Captionstraining</cell><cell>0.2501</cell></row><row><cell cols="2">ImageSem run09Captionstraining</cell><cell>0.2343</cell></row><row><cell cols="2">ImageSem run13Captionstraining</cell><cell>0.2278</cell></row><row><cell cols="2">ImageSem run19Captionstraining</cell><cell>0.2271</cell></row><row><cell cols="2">ImageSem run03Captionstraining</cell><cell>0.2244</cell></row><row><cell cols="2">ImageSem run07Captionstraining</cell><cell>0.2228</cell></row><row><cell cols="2">ImageSem run08Captionstraining</cell><cell>0.2221</cell></row><row><cell cols="2">ImageSem run06Captionstraining</cell><cell>0.1963</cell></row><row><cell>UMass</cell><cell>test captions output4 13 epoch</cell><cell>0.1799</cell></row><row><cell>UMass</cell><cell>test captions output2 12 epoch</cell><cell>0.1763</cell></row><row><cell>CS MS</cell><cell>result captio</cell><cell>0.1725</cell></row><row><cell>UMass</cell><cell>test captions output1</cell><cell>0.1696</cell></row><row><cell>UMass</cell><cell>test captions output5 13 epoch</cell><cell>0.1597</cell></row><row><cell>UMass</cell><cell>test captions output3 13 epoch</cell><cell>0.1428</cell></row><row><cell cols="2">KU Leuven 23 test valres 0.134779058389 out file greedy</cell><cell>0.1376</cell></row><row><cell>WHU</cell><cell>CaptionPredictionTesting-Results-zgb</cell><cell>0.0446</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="1,144.73,646.48,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="1,144.73,657.44,136.51,7.47"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="2,144.73,656.80,147.76,7.86"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="4,144.73,645.84,335.86,7.86;4,144.73,656.80,69.06,7.86"><p>Nine pairs were removed after the challenge started due to incorrect duplicates in the PMC figures.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,277.10,337.63,7.86;10,151.52,288.06,329.07,7.86;10,151.52,299.02,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,388.76,277.10,91.83,7.86;10,151.52,288.06,207.52,7.86">ImageCLEF -Experimental Evaluation in Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.66,288.06,53.93,7.86;10,151.52,299.02,182.45,7.86">The Springer International Series On Information Retrieval</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,309.42,337.64,7.86;10,151.52,320.38,329.07,7.86;10,151.52,331.34,329.07,7.86;10,151.52,342.27,269.83,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,264.01,320.38,216.58,7.86;10,151.52,331.34,324.67,7.86">Evaluating performance of biomedical image retrieval systems: Overview of the medical image retrieval task at ImageCLEF 2004-2014</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,342.30,183.45,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,352.71,337.64,7.86;10,151.52,363.67,329.07,7.86;10,151.52,374.63,329.07,7.86;10,151.52,385.58,329.07,7.86;10,151.52,396.54,258.61,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.13,352.71,159.46,7.86;10,151.52,363.67,55.90,7.86">The CLEF 2004 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.32,374.63,253.27,7.86;10,151.52,385.58,186.28,7.86">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="10,412.61,385.58,67.98,7.86;10,151.52,396.54,98.76,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,406.95,337.64,7.86;10,151.52,417.91,329.07,7.86;10,151.52,428.87,329.07,7.86;10,151.52,439.83,277.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,352.10,417.91,128.50,7.86;10,151.52,428.87,115.31,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.45,428.87,192.14,7.86;10,151.52,439.83,172.02,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="250" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,450.23,337.63,7.86;10,151.52,461.19,329.07,7.86;10,151.52,472.15,329.07,7.86;10,151.52,483.11,329.07,7.86;10,151.52,494.07,329.07,7.86;10,151.52,505.03,329.07,7.86;10,151.52,515.99,329.07,7.86;10,151.52,526.95,46.58,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,198.28,483.11,264.47,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,494.07,329.07,7.86;10,151.52,505.03,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="10,222.96,515.99,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,537.35,337.64,7.86;10,151.52,548.31,329.07,7.86;10,151.52,559.27,329.07,7.86;10,151.52,570.23,329.07,7.86;10,151.52,581.19,46.58,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,399.40,537.35,81.19,7.86;10,151.52,548.31,329.07,7.86;10,151.52,559.27,129.84,7.86">Overview of Image-CLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,301.52,559.27,179.07,7.86;10,151.52,570.23,46.41,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,591.60,337.64,7.86;10,151.52,602.56,329.08,7.86;10,151.52,613.51,329.07,7.86;10,151.52,624.47,187.76,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,417.04,591.60,63.56,7.86;10,151.52,602.56,263.03,7.86">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,437.46,602.56,43.13,7.86;10,151.52,613.51,188.04,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,634.88,337.64,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.77,235.44,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,151.52,645.84,325.36,7.86">A survey on visual information search behavior and requirements of radiologists</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dungs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kriewel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,656.80,145.99,7.86">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="539" to="548" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.64,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,306.13,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,151.52,130.63,232.20,7.86">Health care professionals&apos; image use and search behaviour</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Despont-Gros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,406.13,130.63,74.46,7.86;11,151.52,141.59,207.19,7.86">Proceedings of the Medical Informatics Europe Conference (MIE 2006)</title>
		<meeting>the Medical Informatics Europe Conference (MIE 2006)<address><addrLine>Maastricht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2006-08">aug 2006</date>
			<biblScope unit="page" from="24" to="32" />
		</imprint>
		<respStmt>
			<orgName>Studies in Health Technology and Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,163.52,337.98,7.86;11,151.52,174.48,329.07,7.86;11,151.52,185.44,319.45,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,353.22,174.48,127.36,7.86;11,151.52,185.44,158.32,7.86">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.90,185.44,106.21,7.86">Proceedings of CLEF 2017</title>
		<meeting>CLEF 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,196.41,337.98,7.86;11,151.52,207.36,329.07,7.86;11,151.52,218.32,329.07,7.86;11,151.52,229.28,53.11,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,241.77,207.36,238.82,7.86;11,151.52,218.32,193.82,7.86">Share/clef ehealth evaluation lab 2014, task 3: User-centred health information retrieval clef ehealth overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,366.31,218.32,73.68,7.86">CLEF Proceedings</title>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,240.25,337.98,7.86;11,151.52,251.21,329.07,7.86;11,151.52,262.17,329.07,7.86;11,151.52,273.13,225.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,348.75,251.21,131.84,7.86;11,151.52,262.17,71.21,7.86">General overview of imageclef at the clef 2016 labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,247.79,262.17,232.80,7.86;11,151.52,273.13,120.25,7.86">International conference of the cross-language evaluation forum for European languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,284.10,337.97,7.86;11,151.52,295.06,329.07,7.86;11,151.52,306.01,153.13,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,416.14,284.10,64.45,7.86;11,151.52,295.06,121.28,7.86">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">GarcÃ­a</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,293.48,295.06,187.11,7.86;11,151.52,306.01,74.23,7.86">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,316.98,337.98,7.86;11,151.52,327.94,329.07,7.86;11,151.52,338.90,122.35,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,437.12,316.98,43.47,7.86;11,151.52,327.94,310.29,7.86">Creating a classification of image types in the medical literature for visual categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,338.90,89.30,7.86">SPIE Medical Imaging</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,349.87,337.97,7.86;11,151.52,360.83,329.08,7.86;11,151.52,371.79,329.07,7.86;11,151.52,382.75,187.76,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,237.31,349.87,243.27,7.86;11,151.52,360.83,246.06,7.86">Feature learning with adversarial networks for concept detection in medical images: UA.PT Bioinformatics at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,437.46,360.83,43.13,7.86;11,151.52,371.79,188.04,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,393.72,337.98,7.86;11,151.52,404.67,329.07,7.86;11,151.52,415.63,329.07,7.86;11,151.52,426.59,95.77,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,317.08,393.72,163.52,7.86;11,151.52,404.67,168.75,7.86">ImageSem at ImageCLEF 2018 caption task: Image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,342.24,404.67,138.35,7.86;11,151.52,415.63,91.46,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,437.56,337.97,7.86;11,151.52,448.52,329.07,7.86;11,151.52,459.48,322.32,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,284.84,437.56,195.75,7.86;11,151.52,448.52,74.34,7.86">IPL at ImageCLEF 2018: A kNN-based concept detection approach</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,247.26,448.52,229.11,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,470.45,337.97,7.86;11,151.52,481.41,329.07,7.86;11,151.52,492.36,329.07,7.86;11,151.52,503.32,95.77,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,217.64,470.45,262.95,7.86;11,151.52,481.41,167.80,7.86">A cross modal deep learning based approach for caption prediction and concept detection by cs morgan state</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,341.73,481.41,138.86,7.86;11,151.52,492.36,91.46,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,514.29,337.97,7.86;11,151.52,525.25,329.07,7.86;11,151.52,536.21,187.76,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,213.92,514.29,203.16,7.86">UMass at ImageCLEF caption prediction 2018 task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,437.46,514.29,43.12,7.86;11,151.52,525.25,188.04,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,547.18,337.98,7.86;11,151.52,558.14,329.07,7.86;11,151.52,569.10,282.90,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,255.60,547.18,224.99,7.86;11,151.52,558.14,20.10,7.86">Generating text from images in a smooth representation space</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Spinks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,198.55,558.14,237.20,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,580.07,337.98,7.86;11,151.52,591.02,329.07,7.86;11,151.52,601.98,138.89,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,282.52,580.07,198.08,7.86;11,151.52,591.02,105.10,7.86">Deep multimodal classification of image types in biomedical journal figures</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,283.06,591.02,197.52,7.86;11,151.52,601.98,105.30,7.86">International Conference of the Cross-Language Evaluation Forum (CLEF)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,612.95,337.98,7.86;11,151.52,623.91,217.87,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,263.54,612.95,217.05,7.86;11,151.52,623.91,73.14,7.86">Quickumls: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,246.77,623.91,66.85,7.86">MedIR workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>sigir.</note>
</biblStruct>

<biblStruct coords="11,142.62,634.88,337.98,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,178.41,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,404.42,634.88,76.17,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,25.50,7.86">Overview of imageclefcaption 2017-image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÄ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,185.36,656.80,82.27,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,77.87,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,356.56,119.67,124.03,7.86;12,151.52,130.63,133.38,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.18,130.63,173.41,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,13.87,7.86">Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,163.51,337.98,7.86;12,151.52,174.47,316.78,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="12,380.30,163.51,100.29,7.86;12,151.52,174.47,187.70,7.86">Adversarially regularized autoencoders for generating discrete structures</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1706.04223</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
