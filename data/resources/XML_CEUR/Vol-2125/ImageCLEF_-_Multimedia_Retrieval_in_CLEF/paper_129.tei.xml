<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.50,152.67,318.27,12.64;1,199.37,170.67,196.51,12.64">ImageSem at ImageCLEF 2018 Caption Task: Image Retrieval and Transfer Learning</title>
				<funder ref="#_Cr8Jd6b #_aYr26wk">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Key Laboratory of Medical Information Intelligent Technology Chinese Academy of Medical Sciences</orgName>
				</funder>
				<funder>
					<orgName type="full">National Population and Health Scientific Data Sharing Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Knowledge Centre for Engineering Sciences and Technology (Medical Centre)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.57,209.70,40.23,8.96"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences/Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.17,209.70,55.12,8.96"><forename type="first">Xuwen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences/Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.55,209.70,37.78,8.96"><forename type="first">Zhen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences/Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,360.77,209.70,27.46,8.96"><forename type="first">Jiao</forename><surname>Li</surname></persName>
							<email>li.jiao@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences/Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.50,152.67,318.27,12.64;1,199.37,170.67,196.51,12.64">ImageSem at ImageCLEF 2018 Caption Task: Image Retrieval and Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B85D0BEBD35E0E8DAFBFDABA9C2D98B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>LDA</term>
					<term>Transfer Learning</term>
					<term>Multi-label Classification</term>
					<term>Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the Image Semantics group (ImageSem) of the Institute of Medical Information at the ImageCLEF 2018 caption task. We participated in both of the concept detection and the caption prediction tasks, with submitting 15 runs in total. In this study, we applied LIRE, an open source Lucene Image Retrieval, to index 222,314 images in training and 9,938 images in test sets. In concept detection subtask, we retrieved the similar images in the training set and applied Latent Dirichlet Allocation (LDA) for clustering concepts of the similar images. The transfer learning method was integrated to solve muti-label annotation in the concept detection task. In caption prediction, we used image retrieval strategies by tuning the parameters: the top similar images and number of candidate concepts. In the evaluation, ImageSem achieved the best F1 Score of 0.0928 in the concept detection subtask and the Mean BLEU score of 0.2501 in the caption prediction subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The corpus of annotated medical images, interpreting and summarizing the insights of images, are important for medical image processing and machine learning technology application <ref type="bibr" coords="1,172.06,539.75,10.88,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,182.94,539.75,7.25,8.96" target="#b1">2]</ref>. ImageCLEF task aims to promote the computational method development for machine understandable medical images, starting from visual content and textual descriptor alignment <ref type="bibr" coords="1,237.89,563.78,10.69,8.96" target="#b2">[3]</ref>. ImageCLEF 2018 caption task <ref type="bibr" coords="1,377.11,563.78,10.66,8.96" target="#b3">[4]</ref>, part of ImageCLEF 2018 <ref type="bibr" coords="1,149.30,575.78,10.69,8.96" target="#b4">[5]</ref>, includes two subtasks, namely concept detection and caption prediction <ref type="bibr" coords="1,124.70,587.78,12.74,8.96" target="#b5">[6]</ref>.Our team, ImageSem, participated in both tasks. Fig. <ref type="figure" coords="1,353.33,587.78,4.98,8.96">1</ref> shows our workflow in Im-ageCLEF 2018 Caption Task.</p><p>The concept detection subtask aims to identify the UMLS <ref type="bibr" coords="1,368.11,611.78,11.72,8.96" target="#b6">[7]</ref> Concept Unique Identifiers (CUIs) for a given medical image from the biomedical literature. We proposed approaches including multi-label classification, information retrieval and topic modeling. Convolutional Neural Networks (CNNs) is applied to train multi-label annotation of medical images <ref type="bibr" coords="2,201.65,150.18,10.91,8.96" target="#b7">[8,</ref><ref type="bibr" coords="2,212.56,150.18,7.27,8.96" target="#b8">9]</ref>. The LIRE search engine is employed for the information retrieval approach <ref type="bibr" coords="2,191.69,162.18,15.83,8.96" target="#b9">[10,</ref><ref type="bibr" coords="2,207.52,162.18,11.87,8.96" target="#b10">11]</ref>. The Latent Dirichlet Allocation (LDA) is used for CUIs topic modeling <ref type="bibr" coords="2,164.90,174.18,15.46,8.96" target="#b11">[12]</ref>.</p><p>The caption prediction subtask aims to predict and generate natural language caption for a given medical image. We proposed a retrieval-based method using LIRE on the training set and combined with preferred concepts recognized from the preceding subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Workflow of ImageSem team in the Caption Task</head><p>This paper is organized as follows: Section 2 introduces the task data and our data preprocessing method. Section 3 describes our methods for concept detection. Section 4 presents our methods for caption prediction. Section 5 summarizes all the runs submitted by our team. Section 6 makes a brief conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data overview</head><p>The training and test datasets contained 222,314 and 9,938 biomedical images respectively. The images were extracted from scholarly articles in PubMed Central (PMC)</p><p>[13]. In the concept detection subtask, a set of UMLS CUIs was provided for each image. The image captions were provided in caption generation task. Fig. <ref type="figure" coords="2,419.02,580.34,4.98,8.96">2</ref> shows two figures with captions in PMC and assigned concepts (note that the UMLS terms and semantic types were extracted by our team but were not provided by task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2.</head><p>The task images, its and pre-annotated concepts.</p><p>We firstly analyzed the annotated concept frequency distribution in order to better understand the task images. The distribution is important for multi-label training object selection and similar image measurement. The training data includes 222,314 images associated with 111,156 CUIs. Table <ref type="table" coords="3,277.59,437.87,4.98,8.96" target="#tab_0">1</ref> shows the concept distribution. It can be seen most annotated CUIs (92.19%) were used less than 100 times among 222,314 images. Thus, it is challenging to train a model to learn the annotation patterns of these 102,480 concepts. For the frequent concepts, there are 1312 concepts with frequency greater than 1000. Table <ref type="table" coords="3,195.24,485.87,4.98,8.96" target="#tab_1">2</ref> shows the top ranked concepts, their annotated image number, and their corresponding UMLS terms. Some general concepts like medical image (C1704254) and image (C1704922) were highly used but meaningless. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image indexing</head><p>We used LIRE <ref type="bibr" coords="4,186.29,398.99,16.78,8.96" target="#b9">[10,</ref><ref type="bibr" coords="4,203.07,398.99,12.58,8.96" target="#b10">11]</ref> to index the medical images released by ImageCLEF 2018. Table <ref type="table" coords="4,139.44,410.99,4.98,8.96" target="#tab_2">3</ref> shows the six features used in LIRE, including color and texture features. Further, after analyzing the training data, we found that a number of CUIs co-occur in almost the same set of medical images. To make use of this characteristic, we clustered the CUIs according to their similar scores based on their co-occurrence. The formulation of calculating the similar scores between CUIs is shown as follows:</p><formula xml:id="formula_0" coords="5,182.57,200.62,194.68,21.84">SIMILAR_SCORE (A, B) = 𝑖𝑚𝑎𝑔𝑒𝑠_𝐴 ∩ 𝑖𝑚𝑎𝑔𝑒𝑠_𝐵 𝑖𝑚𝑎𝑔𝑒𝑠_𝐴 ∪ 𝑖𝑚𝑎𝑔𝑒𝑠_𝐵</formula><p>Where, SIMILAR_SCORE(A,B) denotes the similar score between the CUI A and the CUI B. images_A and images_A separately represents the set of medical images in which the CUI A and the CUI B appears. CUIs with similar score more than 0.8 are clustered into the same group. Accordingly, 1312 CUIs are clustered into 459 groups and the first CUI of each group is selected as the representation CUI. Appearance of the representation CUI is the same as the appearance of all the CUIs in its group. Eventually, just the 459 representation CUIs are fed into the model for multi-label classification.</p><p>The medical images which contains at least one of the 1312 CUIs were selected for training. And for each image, we re-built its corresponding set of CUIs, only retaining the CUIs inside the 1312 CUIs and mapping them to the representation CUIs of their corresponding groups. Finally, 208595 medical images with 459 representation CUIs were used to train the transfer learning model for multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept Detection Methods</head><p>For the concept detection sub task, we employed three methods to find multiple CUIs for a specific image, including the multi-label classification method, retrieval-based method and the topic modeling method.</p><p>In the multi-label classification method, Convolutional Neural Networks (CNNs) was applied to assign one or multiple CUIs from the predefined CUIs label set.</p><p>In the retrieval-based method, we used LIRE (Lucene Image Retrieval) to retrieve the most similar images and corresponding CUIs from the training set.</p><p>In the topic modeling method, Latent Dirichlet Allocation (LDA) was used to analyze the topic distribution of CUIs from retrieved similar images and their CUIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>Multi-label classification with CNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Inception-v3</head><p>In recent years, deep neural network such as convolutional neural networks(CNN) and recurrent neural networks(RNN) have made great success in large-scale image processing, image content recognition, and image caption generation. Inception-v3, a convolutional neural network(CNN) model of Google, is an architecture that often achieves superior performance with low computational cost. The key advantage of Inception-v3 is the factorization of convolution kernel, for example, it can decom-pose a 7x7 convolution kernel into two one-dimensional kernels(a 1x7 kernel and a 7x1 kernel). Through the factorization of convolution kernels, it can accelerate the training and increase the depth of the network. In this study, the Inception-v3 model is pre-trained on the ImageNet datasets with more than 1 million images and 1000 classes <ref type="bibr" coords="6,400.32,162.18,15.68,8.96" target="#b12">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Transfer learning for concept detection</head><p>However, for our concept detection task which is treated as a multi-label classification problem, directly retraining the whole Inception-v3 model based on the training set needs to take at least a few days. Therefore, we used the pre-trained Inception-v3 based transfer learning method to identify the concepts from medical images. Specifically, we froze the parameters of all the previous layers, removed the last softmax layer and added a fully-connected layer and a sigmoid layer. While training, only the last two layers need to be trained to map the medical images to the CUIs. Totally, 208595 medical images with 459 representation CUIs were fed into the model. Eventually, after getting the predicting results of the test set, we extended the results through replacing the representation CUIs with all the CUIs in their corresponding groups according to the clustering result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image retrieval</head><p>LIRE is an open source Java library that provides a simple way to retrieve images and photos based on color and texture characteristics. We used LIRE to create a Lucene index of image features on the whole training set for content based image retrieval (CBIR). We submitted each query image from the test set to LIRE and selected top 50 visually similar images from the training set. For a given test image, we combined related CUIs of similar images as candidate concepts, then computed a concept score s(c) to determine which concepts to be assigned as semantic labels. In the following concept score equation, α 𝑗 denotes the normalization weight of similar figure j, P(j) denotes the probability of figure j and P(c|j) represents the probability of concept c that is assigned to figure j. Besides, we also considered the method of applying QuickUMLS or Metamap tools to label CUIs on retrieved similar images captions, but by testing, we found some difference between automatic tagging and original provided CUIs in the training set, which may due to different parameter settings or unknown concept expanding strategies. To avoid this uncontrollable noise, we focus on the analysis of provided CUIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image retrieval with topic model</head><p>On the basis of retrieved similar images and candidate CUIs, we employed topic modeling method to select more relevant concept for a given test image. Latent Dirichlet Allocation (LDA) is a widely used generative statistical topic model in natural language processing. In this subtask, we assume concepts related to each image are collected into documents, so each document is a mixture of a number of topics and each concept is attributable to one of the document's topics.</p><p>We applied Gensim, a topic modeling Python package to modeling topic distribution on retrieved similar images and candidate CUIs. For a given test image with its retrieved 50 similar images, we collected 50 documents of CUIs as the input of LDA model. According to the topic distribution θ of the current document set, we picked the topic with the highest probability p(z|D) as the candidate topic, and finally selected CUIs from the candidate topic that with probabilities p(c|z) above the threshold φ 0 as the final result.</p><p>Before submitting the runs we carried out experiments on the training data using highly related concepts detected in CNNs method as a hint for choosing better candidate topics. However, it didn't provide better results. So we submit the normal runs of topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Caption Prediction Methods</head><p>We used retrieval-based method in the caption prediction. The basic assumption is that similar images have similar lingual descriptions/captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Caption selection and combination</head><p>For each test image, we used LIRE to retrieve similar images from the training set, and combined the captions of similar images as a new caption of the test image. We tuned the parameter, the number of top similar images, to determine the candidate captions for further combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Concept selection and combination</head><p>We combined preferred concepts detected in the preceding concept detection subtask. The preferred concepts including CUIs from CNNs' high score output, as well as CUIs from the output of LDA model. We extracted all the UMLS terms of each CUIs, and combined them with captions generated in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Runs</head><p>This section provided a detailed description of our runs submitted to ImageCLEF 2018 caption task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs of concept detection</head><p>We submitted the following 7 runs to the Concept Detection subtask (see Table <ref type="table" coords="8,444.98,170.22,3.75,8.96" target="#tab_3">4</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept_Run1_submission_ID_5554:</head><p>We exploited the LIRE to retrieve the 50 most similar images to each medical image of the test set. Then, for a given test image, all the CUIs of its corresponding 50 most similar images were taken as the input of LDA model. Further, we pick the topic with the highest probability p(z|D) as the candidate topic. Finally, we select CUIs from the candidate topic that with probabilities p(c|z) above 0.005 as the CUIs of that test image. Concept_Run2_submission_ID_5556: Multi-label classification using transfer learning model, based on the pre-trained Inception-v3. The batch size was set to 20 and the learning rate was set to 0.003. While, the training steps were set to 15000. Finally, for a test image, top 10 representation CUIs of the predicting results were selected as the preliminary result, and we extended the preliminary result through replacing the representation CUIs with all the CUIs in their corresponding groups according to the clustering result as the final result of that test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept_Run3_submission_ID_5558:</head><p>The same as the Concept_Run1_submission_ ID_5554 except that, in the final submission file, all the CUIs were separated by com ma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept_Run4_submission_ID_5561:</head><p>The same as the Concept_Run1_submission_ ID_5554 except that of all the CUIs of the given test image's corresponding 50 most s imilar images, only the CUIs which appears in more than 1000 images of the training data were taken as the input of LDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept_Run6_submission_ID_5574:</head><p>The same as the Concept_Run2_submission_ ID_5556 except that the training steps were set to 5000 Concept_Run7_submission_ID_5575: The same as the Concept_Run2_submission_ ID_5556 except that for a test image, top 20 representation CUIs of the predicting resu lts were selected as the preliminary result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept_Run10_submission_ID_5583:</head><p>The same as the Concept_Run2_submission _ID_5556 except that the training steps were set to 25000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs of caption prediction</head><p>We submitted the following 8 runs to the Concept Detection subtask (see Table <ref type="table" coords="9,445.06,170.22,3.72,8.96" target="#tab_4">5</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presents the participation of the Image Semantics group (ImageSem) at the ImageCLEF 2018 caption task. We submitted 7 runs in the concept detection and 8 runs in the caption prediction tasks. The evaluation results showed that we achieved the best F1 Score of 0.0928 in the concept detection subtask and the Mean BLEU score of 0.2501 in the caption prediction subtask. Our methods mainly relied on image retrieval and transfer learning.</p><p>In our experiments, we found the ground truth concept annotations were not exactly represent the semantics of the images. It is difficult for error analysis from either computational view or clinical/biomedical view. In the future work, we would like to contribute the corpus construction together with the ImageCLEF committee.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,187.73,513.85,101.21,10.92;6,304.27,514.21,6.34,10.56;6,309.67,518.71,3.55,7.56;6,316.15,514.21,2.61,10.56;6,288.89,512.35,4.57,7.56;6,288.89,519.19,13.76,7.56;6,320.47,514.21,54.66,10.56;6,187.73,547.84,76.53,10.56;6,267.05,543.91,37.12,7.56;6,279.29,555.34,12.59,8.89;6,306.79,547.84,11.22,10.56;6,317.11,552.34,3.55,7.56;6,324.31,547.84,26.73,10.56;6,358.15,543.31,30.28,8.80;6,353.35,555.46,40.00,8.77;6,124.70,582.16,345.98,10.56;6,124.70,595.48,235.70,9.06"><head></head><label></label><figDesc>Concept score: s(c) = ∑ 𝛼 𝑗 • 𝑘 𝑗=1 𝑃(𝑗) • 𝑃(𝑐|𝑗) In which, P(c|j) = 𝑐𝑜𝑢𝑛𝑡(𝑐,𝑗) |𝐶 𝑗 | , α 𝑗 = 1 -𝑓 𝑗 -𝑓 𝑚𝑖𝑛 𝑓 𝑚𝑎𝑥 -𝑓 𝑚𝑖𝑛 Then candidate concepts were ranked according to their concept score s(c). We set a threshold τ and select top K CUIs as final related concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,124.70,147.40,345.89,210.55"><head></head><label></label><figDesc></figDesc><graphic coords="3,124.70,147.40,345.89,210.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,168.62,545.83,257.91,98.10"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the concepts in the medical images of training set.</figDesc><table coords="3,206.81,565.27,178.72,78.66"><row><cell>Frequency</cell><cell>Number</cell><cell>Proportion</cell></row><row><cell>0-100</cell><cell>102480</cell><cell>92.19%</cell></row><row><cell>100-500</cell><cell>6189</cell><cell>5.57%</cell></row><row><cell>500-1000</cell><cell>1175</cell><cell>1.06%</cell></row><row><cell>1000+</cell><cell>1312</cell><cell>1.18%</cell></row><row><cell>Total</cell><cell>111156</cell><cell>100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,154.70,186.11,291.41,168.21"><head>Table 2 .</head><label>2</label><figDesc>Top frequent concepts in the training set.</figDesc><table coords="4,154.70,205.55,291.41,148.77"><row><cell>CUI</cell><cell>Associated Image</cell><cell>UMLS Term</cell></row><row><cell>C1550557</cell><cell>77,003</cell><cell>Relationship Conjunction -and</cell></row><row><cell>C1706368</cell><cell>77,003</cell><cell>And -dosing instruction fragment</cell></row><row><cell>C1704254</cell><cell>20,165</cell><cell>Medical Image</cell></row><row><cell>C1696103</cell><cell>20,164</cell><cell>image -dosage form</cell></row><row><cell>C1704922</cell><cell>20,164</cell><cell>Image</cell></row><row><cell>C3542466</cell><cell>20,164</cell><cell>Image (foundation metadata concept)</cell></row><row><cell>C1837463</cell><cell>19,491</cell><cell>Narrow face</cell></row><row><cell>C0376152</cell><cell>19,253</cell><cell>Marrow</cell></row><row><cell>C1546708</cell><cell>19,253</cell><cell>Marrow -Specimen Source Codes</cell></row><row><cell>C0771936</cell><cell>19,079</cell><cell>Yarrow flower extract</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,124.70,446.92,345.98,237.06"><head>Table 3 .</head><label>3</label><figDesc>Descriptors of the features. CUIs in the training data). Therefore, we chose to only use the most frequent CUIs for training the model. Eventually, 1312 CUIs, each of which appears in more than 1000 images of the training data, were selected for the multi-label classification.</figDesc><table coords="4,124.70,466.36,345.94,181.62"><row><cell></cell><cell>Letter</cell><cell>Name</cell></row><row><cell></cell><cell>A</cell><cell>Color and edge directivity descriptor</cell></row><row><cell></cell><cell>B</cell><cell>Fuzzy color and texture histogram</cell></row><row><cell></cell><cell>C</cell><cell>Color histograms</cell></row><row><cell></cell><cell>D</cell><cell>Auto color correlation</cell></row><row><cell></cell><cell>E</cell><cell>Tamura texture features</cell></row><row><cell></cell><cell>F</cell><cell>Gabor texture features</cell></row><row><cell>2.3</cell><cell cols="2">Concept selection for transfer learning</cell></row><row><cell cols="3">As for transfer learning, the problem of detecting concepts from medical images was</cell></row><row><cell cols="3">treated as a multi-label classification task. However, too many CUIs without sufficient</cell></row><row><cell cols="3">medical images for training were not feasible for multi-label classification (222314</cell></row><row><cell cols="2">medical images with 111156</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,165.86,206.15,248.08,117.69"><head>Table 4 .</head><label>4</label><figDesc>ImageSem performance in the concept detection</figDesc><table coords="8,165.86,224.27,248.08,99.57"><row><cell>Run</cell><cell>Submission ID</cell><cell>Mean F1 Score</cell></row><row><cell>Concept_Run10</cell><cell>5583</cell><cell>0.092849</cell></row><row><cell>Concept_Run2</cell><cell>5556</cell><cell>0.090867</cell></row><row><cell>Concept_Run4</cell><cell>5561</cell><cell>0.090705</cell></row><row><cell>Concept_Run1</cell><cell>5554</cell><cell>0.089415</cell></row><row><cell>Concept_Run6</cell><cell>5574</cell><cell>0.082799</cell></row><row><cell>Concept_Run7</cell><cell>5575</cell><cell>0.066151</cell></row><row><cell>Concept_Run3</cell><cell>5558</cell><cell>0.000000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,124.70,194.15,345.99,493.91"><head>Table 5 .</head><label>5</label><figDesc>ImageSem performance in the caption generation We exploited the LIRE to retrieve the 50 most similar images to each medical image of the test set. Then, for a given test image, the captions of its top 2 most similar images were concatenated together as the result of that test image. Caption_Run4_submission_ID_5527: The same as the Caption_Run3_submission_I D_5526 except that for a given test image, the captions of its top 3 most similar image s were concatenated together as the result of that test image. The same as the Caption_Run3_submission_I D_5526 except that, for a given test image, we added the top 3 CUI of the predicting result of that test image from the concept detection task based on transfer learning into the final result. Caption_Run13_submission_ID_5548: The same as the Caption_Run3_submission _ID_5526 except that, for a given test image, we added the top 1 CUI of the predictin g result of that test image from the concept detection task based on the retrieval metho d and LDA into the final result. Caption_Run19_submission_ID_5552: The same as the Caption_Run3_submission _ID_5526 except that, for a given test image, both the top 1 CUI of the predicting resu lt of the concept detection task based on the retrieval method and LDA and the top 1 C UI of the predicting result based on transfer learning are added into the final result.</figDesc><table coords="9,124.70,210.95,345.81,369.11"><row><cell>Run</cell><cell>Submission ID</cell><cell>Mean BLEU Score</cell></row><row><cell>Caption_Run4</cell><cell>5527</cell><cell>0.250086</cell></row><row><cell>Caption_Run9</cell><cell>5546</cell><cell>0.234312</cell></row><row><cell>Caption_Run13</cell><cell>5548</cell><cell>0.227806</cell></row><row><cell>Caption_Run19</cell><cell>5552</cell><cell>0.227065</cell></row><row><cell>Caption_Run13</cell><cell>5526</cell><cell>0.22443</cell></row><row><cell>Caption_Run7</cell><cell>5531</cell><cell>0.222768</cell></row><row><cell>Caption_Run8</cell><cell>5545</cell><cell>0.222081</cell></row><row><cell>Caption_Run6</cell><cell>5528</cell><cell>0.196338</cell></row><row><cell cols="3">Caption_Run3_submission_ID_5526: Caption_Run6_submission_ID_5528: The same as the Caption_Run3_submission_I</cell></row><row><cell cols="3">D_5526 except that of the top 2 most similar images to the test image, only the captio</cell></row><row><cell cols="3">ns of images with similar distance less than 5 to the test image were concatenated toge</cell></row><row><cell cols="2">ther as the result of that test image.</cell><cell></cell></row><row><cell cols="3">Caption_Run7_submission_ID_5531: The same as the Caption_Run3_submission_I</cell></row><row><cell cols="3">D_5526 except that, for a given test image, we added the top 1 CUI of the predicting</cell></row><row><cell cols="3">result of that test image from the concept detection task based on transfer learning into</cell></row><row><cell>the final result.</cell><cell></cell><cell></cell></row><row><cell cols="3">Caption_Run8_submission_ID_5545: The same as the Caption_Run3_submission_I</cell></row><row><cell cols="3">D_5526 except that, for a given test image, we added the top 2 CUI of the predicting</cell></row><row><cell cols="3">result of that test image from the concept detection task based on transfer learning into</cell></row><row><cell>the final result.</cell><cell></cell><cell></cell></row><row><cell cols="2">Caption_Run9_submission_ID_5546:</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This study was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (Grant No. <rs type="grantNumber">2016YFC0901901</rs> and No. <rs type="grantNumber">2017YFC0907500</rs>), the <rs type="funder">Key Laboratory of Medical Information Intelligent Technology Chinese Academy of Medical Sciences</rs>, the <rs type="funder">National Population and Health Scientific Data Sharing Program of China</rs>, and the <rs type="funder">Knowledge Centre for Engineering Sciences and Technology (Medical Centre)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Cr8Jd6b">
					<idno type="grant-number">2016YFC0901901</idno>
				</org>
				<org type="funding" xml:id="_aYr26wk">
					<idno type="grant-number">2017YFC0907500</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,132.67,489.16,337.91,8.10;10,141.74,500.20,329.09,8.10" xml:id="b0">
	<monogr>
		<author>
			<orgName type="collaboration" coords="10,141.74,489.16,102.35,8.10">Interagency Working Group</orgName>
		</author>
		<title level="m" coord="10,230.11,500.20,202.90,8.10">Roadmap for medical imaging research and development</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
		<respStmt>
			<orgName>Medical Imaging Committee on Science, National Science and Technology Coucil</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,511.12,337.90,8.10;10,141.74,522.16,240.38,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,362.11,511.12,108.46,8.10;10,141.74,522.16,82.62,8.10">A survey on deep learning in medical image analysis</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,230.11,522.16,88.06,8.10">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,533.20,337.86,8.10;10,141.74,544.12,328.61,8.10;10,141.74,555.19,329.03,8.10;10,141.74,566.23,226.46,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,405.41,533.20,65.12,8.10;10,141.74,544.12,328.61,8.10;10,141.74,555.19,14.49,8.10">Overview of Im-ageCLEFcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,174.95,555.19,118.98,8.10">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="10,300.34,555.19,155.48,8.10">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,577.15,337.88,8.10;10,141.74,588.19,328.54,8.10;10,141.74,599.23,328.94,8.10;10,141.74,610.15,23.37,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,410.20,577.15,60.34,8.10;10,141.74,588.19,154.91,8.10">Overview of the ImageCLEF 2018 Caption Prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,315.11,588.19,98.86,8.10">CLEF 2018 Working Notes</title>
		<title level="s" coord="10,420.37,588.19,49.91,8.10;10,141.74,599.23,107.03,8.10">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,621.19,337.57,8.10;10,141.74,632.23,328.93,8.10;10,141.74,643.15,328.76,8.10;10,141.74,654.19,328.63,8.10;11,141.74,150.11,329.01,8.10;11,141.74,161.15,328.74,8.10;11,141.74,172.07,209.66,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,141.74,654.19,249.66,8.10">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,410.39,654.19,59.98,8.10;11,141.74,150.11,329.01,8.10;11,141.74,161.15,184.73,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,354.67,161.15,115.81,8.10;11,141.74,172.07,14.95,8.10">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-10">2018. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,183.11,337.96,8.10;11,141.74,194.15,38.88,8.10" xml:id="b5">
	<monogr>
		<ptr target="2018/5/30" />
		<title level="m" coord="11,141.74,183.11,114.60,8.10">ImageCLEFcaption Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,205.07,337.97,8.10;11,141.74,216.11,137.28,8.10" xml:id="b6">
	<monogr>
		<ptr target="https://www.nlm.nih.gov/re-search/umls/,lastaccessed2018/5/30" />
		<title level="m" coord="11,141.74,205.07,215.63,8.10">UMLS (Unified Medical Language System) Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,227.08,281.59,8.20" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,185.09,227.09,110.38,8.18">Special Issue: Digital Libraries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,301.27,227.08,56.04,8.19">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,238.10,337.99,8.10;11,141.74,249.14,328.86,8.10;11,141.74,260.18,328.93,8.10;11,141.74,271.10,26.37,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,351.06,238.10,119.60,8.10;11,141.74,249.14,131.64,8.10">CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,290.92,249.14,179.67,8.10;11,141.74,260.18,162.12,8.10">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,282.26,338.28,8.10;11,141.74,293.78,38.88,8.10" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><surname>Lire</surname></persName>
		</author>
		<ptr target="http://www.lire-project.net/" />
		<title level="m" coord="11,170.06,282.26,132.71,8.10">Lucene Image Retrieval) Homepage</title>
		<imprint>
			<date type="published" when="2018-05-30">2018/5/30</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,304.73,338.37,8.18;11,141.74,315.77,329.09,8.18;11,141.74,326.69,145.20,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,202.97,304.73,267.80,8.18;11,141.74,315.77,36.61,8.18">Using LIRe to Implement Image Retrieval System Based on Multi-feature Descriptor</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.13,315.77,257.24,8.18">Third International Conference on Digital Manufacturing &amp; Automation</title>
		<meeting><address><addrLine>Guilin, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1014" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,337.73,338.06,8.19;11,141.74,348.77,103.32,8.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,261.77,337.73,93.19,8.18">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,361.39,337.73,109.07,8.18;11,141.74,348.77,28.94,8.18">J Machine Learning Research Archive</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,370.82,338.29,8.10;11,141.74,381.86,328.84,8.10;11,141.74,392.80,212.90,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,255.89,381.86,179.61,8.10">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,442.16,381.86,28.42,8.10;11,141.74,392.80,123.42,8.10">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
