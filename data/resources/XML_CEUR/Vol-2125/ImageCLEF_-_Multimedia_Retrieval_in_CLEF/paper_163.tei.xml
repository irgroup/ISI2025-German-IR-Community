<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.32,115.96,336.72,12.62;1,189.94,133.89,235.47,12.62">UMass at ImageCLEF Medical Visual Question Answering(Med-VQA) 2018 Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.41,171.79,46.35,8.74"><forename type="first">Yalei</forename><surname>Peng</surname></persName>
							<email>ypeng5@wpi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
								<address>
									<postCode>01609</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.32,171.79,44.28,8.74"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
							<email>feifan.liu@umassmed.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Medical School</orgName>
								<address>
									<postCode>01655</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.54,171.79,60.94,8.74"><forename type="first">Max</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
							<email>max.rosen@umassmemorial.org</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Medical School</orgName>
								<address>
									<postCode>01655</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.32,115.96,336.72,12.62;1,189.94,133.89,235.47,12.62">UMass at ImageCLEF Medical Visual Question Answering(Med-VQA) 2018 Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77C52D65FDF7B879B2A24C2E6B1DDDE4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Attention Mechanism</term>
					<term>LSTM</term>
					<term>Residual Nets</term>
					<term>Multi-modal Fusion</term>
					<term>Topic Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the University of Massachusetts Medical School in the ImageCLEF 2018 Med-VQA task. The goal is to build a system that is able to reason over medial images and questions and generate the corresponding answers. We explored and implemented a co-attention based deep learning framework where residual networks is used to extract visual features from image that interact with the long-short term memory(LSTM) based question representation providing fine-grained contextual information for answer derivation. To efficiently integrate visual features from the image and textual features from the question, we employed Multi-modal Factorized Bilinear(MFB) pooling as well as Multi-modal Factorized High-order(MFH) pooling. In addition, we exploited transfer learning on pre-trained ImageNet model where embedding based topic model(ETM) is applied on the question texts of the training data and the corresponding topic labels are attached to each image for transfer learning. We submitted 3 valid runs for this task, and we found the ETM based transfer learning outperformed other models, achieving the best WBSS score of 0.186, which ranked first among participating groups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an image and a question in natural language, visual question answering (VQA) system is expected to reason over both visual and textual information to infer the correct answer. It is a challenging task that combines computer vision with natural language processing (NLP) and has received increasing attention. Various kinds of methods, like joint embedding approaches, attention mechanisms and compositional models, have been designed and practiced on this task. Meanwhile, data sets for learning VQA have also been evolving from simple image-QA datasets like COCO-QA to knowledge base-enhanced datasets like Visual Genome.</p><p>However, the study of VQA so far is mainly in general domain. There are few practice of VQA in other domain. With the increasing implementation of artificial intelligence (AI) into medical domain to support clinical decision making and improve patient engagement, the automation of medical image interpretation is becoming more and more desirable. The system is expected to help patients better understand their conditions regarding their available data which can be structured and unstructured, graphical and textual. Also the system, as a opinion machine, may enhance clinicians confidence in interpreting complex medical images. Motivated by this important need for automated image understanding in an advanced question answering manner for clinical domain, ImageCLEF 2018 <ref type="bibr" coords="2,157.67,226.59,10.52,8.74" target="#b0">[1]</ref> organized the inaugural edition of the Medical Domain Visual Question Answering(Med-VQA) Task <ref type="bibr" coords="2,259.99,238.55,9.96,8.74" target="#b1">[2]</ref>.</p><p>The implementation of VQA into medical domain is challenging not only because texts and images in medical domain are distinct from those in general domain, but also because the data resources in medical domain are limited compared with those in general domain. Thus, transfer learning from general domain is more promising than directly training from scratch.</p><p>Our main contributions in participating this challenge are as follows: First, we explored transfer learning on image channel to extract meaningful features from the medical images, where we present a novel approach of utilizing Embeddingbased topic modeling for transfer learning. Second, we implemented co-attention mechanism integrated with Multi-modal Factorized Bilinear Pooling (MFB) and Multi-modal Factorized High-order Pooling (MFH) to medical VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There Research on VQA has been showing increased interest due to methodological advances in both computer vision and NLP, and the availability of relevant large-scale datasets. The straightforward solution to VQA is the joint embedding method(e.g. <ref type="bibr" coords="2,194.08,488.75,10.30,8.74" target="#b6">[7]</ref>), where image and question are represented as global features which are merged to predict the answers. The limitation for this approach is that an image could contain more information than needed to answer a question, which may add noises to the classification model, making it difficult to answer questions pertaining to a specific part of the image. Therefore recent work on VQA explored attention mechanisms(e.g. <ref type="bibr" coords="2,363.62,548.52,10.79,8.74" target="#b2">[3]</ref>) to improve the performance by steering the model to specific sections of the input (image and/or question. The main idea is to replace the global image features with fine-grained spatial feature maps so that feature maps can interact with the given question to derive salient features for answer prediction.</p><p>Another line of work in VQA focuses on efficient ways for multi-modal feature fusion. A simple approach that has been widely used is linear fusion model, where visual features from image and textual features from question are concatenated or element-wise added. Due to the largely different distributions of two feature sets, the expressive power of the resulting fused representation is limited in terms of facilitating the final answer prediction. To address this issue, several approaches were proposed, such as Multi-modal Compact Bilinear (MCB) <ref type="bibr" coords="3,467.31,142.90,9.96,8.74" target="#b5">[6]</ref>, Multi-modal Low-rank Bilinear (MLB) <ref type="bibr" coords="3,311.15,154.86,9.96,8.74" target="#b3">[4]</ref>, and Multi-modal Factorized Bilinear pooling (MFB) <ref type="bibr" coords="3,223.90,166.81,9.96,8.74" target="#b4">[5]</ref>. In our medical VQA system, we integrated the MFB approach for multi-modal feature fusion which was shown to outperform both MCB and MLB in general domain VQA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Our system consists of four main components: Feature fusion, Co-attention mechanism, Transfer learning, and answer prediction, which are shown in Fig. <ref type="figure" coords="3,134.77,266.06,3.87,8.74" target="#fig_0">1</ref>. Specifically, visual context is extracted from the image facilitated by transfer learning, then fused with textual context from the question using co-attention mechanism and feature fusion techniques. Finally, answer is predicted based on the fused multi-modal contextual information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Fusion with Multi-modal Factorized Bilinear Pooling</head><p>We used MFB pooling method to merge the visual features from image and textual features from question, as it was shown to have dual benefits of compact output features of MLB and robust expressive capacity of MCB. For comparison, we also integrated multi-modal factorized high-order(MFH) pooling which consists of N MFB modules (N is a hyper-parameter).</p><p>Each MFB block contains two stages: expand and squeeze. In the expand stage, the textual context and the visual context are transformed into the same dimension by a fully-connected layer respectively for the next element-wise multiplication. Additionally, a dropout layer is next to the element-wise multiplication unit. Then, the fused context is further transformed in squeeze stage which contains sum pooling, power normalization and L2-normalization.</p><p>In the MFH module, the output from the dropout layer of the previous MFB block is fed into the next MFB block as additional input as shown in Fig. <ref type="figure" coords="4,472.84,142.90,3.87,8.74">2</ref>, and the output from multiple MFB blocks are merged together as a final fused feature representation.</p><p>Fig. <ref type="figure" coords="4,182.88,337.21,4.13,7.89">2</ref>. The high-order MFH model which consists of N MFB blocks <ref type="bibr" coords="4,442.37,337.24,9.73,7.86" target="#b4">[5]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Co-attention with MFB</head><p>Similar to <ref type="bibr" coords="4,187.27,416.38,9.96,8.74" target="#b4">[5]</ref>, we also implemented Co-attention mechanism for MED-VQA.</p><p>The pre-trained ResNet152 model of ImageNet (excluding the last 3 layers) is used as a image feature extractor, and a LSTM layer is used to encode the question into textual feature vectors. A pre-trained word-embedding (dimension of 200) on wikipedia, pubmed articles and Pittsburgh clinical notes is used as embedding input layer. MFB was used to fuse the the multi-modal features, followed by some feature transformations (e.g., 1 * 1 convolution and ReLU activation) and softmax normalization to predict the attention weight for each grid location. Based on the attention map, the attentional image features are obtained by the weighted sum of the spatial grid vectors. Multiple attention maps are generated to enhance the learned attention map, and these attention maps are concatenated to output the attentional image features. Next, the final attentional image features are merged with the question features using MFB for downstream answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transfer Learning to Tune Pretrained ResNet with ETM Labels</head><p>ImageNet data are very different from medical images in MED-VQA task, which motivates us to employ transfer learning to adapted pre-trained model to this task. Instead of fine tuning the pre-trained model on the fly, the off-line transfer learning based method can efficiently reduce the training time.</p><p>We explored topic analysis to derive semantic label for each image in order to enable transfer learning. The assumption is that the semantics of the question text should match the corresponding image. However, the question text is typically short which is challenging for traditional topic analysis approaches, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), to infer reliable topics as only very limited word co-occurrence information is available in short texts. Embedding-based topic model <ref type="bibr" coords="5,451.06,202.68,10.52,8.74" target="#b7">[8]</ref> not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo-texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic as shown in Fig. <ref type="figure" coords="5,299.52,250.50,3.87,8.74" target="#fig_1">3</ref>. First, short texts are merged into long pseudo-texts based on clustering methods using a word embedding pre-trained on a large relevant corpus. Then, embedding-based topic model is applied on the long pseudo-texts to generate latent topics.</p><p>Specifically, we applied ETM on question texts of the MED-VQA data, assigning a topic label to each question which can in turn be used as a semantic label for its corresponding image. We then performed transfer learning in a context of image classification, where the parameters of pre-trained residual nets were tuned with the goal of correctly classifying all the images to their corresponding topic labels. The fine-tuned network (removing the last convolution block, fullyconnected layer and softmax layer) was used as the static feature extractor in our system architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Prediction</head><p>The input to answer prediction is the attentional image features from Co-attention, fused with the LSTM based question representational features through MFB.</p><p>Here we employed a simple multi-label classification method where each unique word in the answer sentence is considered a answer label for the corresponding image-question pair. Based on distribution of all the answer labels, the final answer is generated using sampling method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing</head><p>Question-Answer Pair Preprocessing on question-answer pairs includes tokenization and lower casing, so that each word can be mapped to its dense representation by looking up pre-trained word embeddings.</p><p>Image Although original preprocessing procedure is recommended to better facilitate the transfer learning, we notice that lots of images in medical VQA data set are long shape consisting 2 -5 sub-images. Therefore, a lot of areas would be cut off, and features would be resized to be too small and blur if the original preprocessing is directly applied. Therefore, we reshape the long images into approximate squares by re-arranging the order of sub-images. Then, the original preprocessing when pre-training the ImageNet ResNet is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validation Runs</head><p>We experimented with the three co-attention systems with variant settings on feature fusion and transfer learning:  We submitted 3 valid runs based on the aforementioned system architectures, and the run from "ResNet152+ETM+MFH" achieved the best WBSS score of 0.186, and the run from "ResNet152+MHF" obtained the best BLEU score of 0.162 as shown in Table . 2. Note that the run (ID6091) is not a valid run due to a code error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">conclusions</head><p>We experimented with 3 different deep learning architectures for MED-VQA task 2018, where we proposed a novel method for transfer learning using embedding based topic analysis. We found that transfer learning and MFH based feature fusion is helpful on improving the system's performance. Due to time limitation, we didn't model the sequential information in the answer sequence which will be explored in the future to make the answer more natural and readable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,212.25,456.49,190.86,7.89;3,165.95,330.31,283.46,111.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our system architecture at MED-VQA.</figDesc><graphic coords="3,165.95,330.31,283.46,111.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,190.28,633.28,231.73,7.89;5,165.95,423.95,283.48,194.57"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Embedding based topic model for short texts [8].</figDesc><graphic coords="5,165.95,423.95,283.48,194.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,303.87,253.80,176.72,8.74;7,134.77,265.75,352.26,8.74;7,134.77,277.71,345.83,8.74;7,134.77,289.66,345.82,8.74;7,134.77,301.62,345.82,8.74;7,134.77,313.57,345.83,8.74;7,134.77,325.53,345.83,8.74;7,134.77,337.48,162.61,8.74"><head>( 1 )</head><label>1</label><figDesc>ResNet152+MFB which uses MFB for feature fusion and the pre-trained ResNet152 is directly used; (2)ResNet152+MFH which uses MFH for feature fusion and the pre-trained ResNet152 is directly used; (3)ResNet152+ETM+MFH which uses MFH for feature fusion, and the pre-trained ResNet152 is also tuned through transfer learning which is based on ETM topic modeling. In Fig.4, we shows the performance curves of 3 systems on validation dataset. We can see the MFH based feature fusion constantly outperforms the MFB based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,224.79,570.96,165.78,7.89;7,134.77,369.16,345.83,187.03"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Validation runs of 3 architectures</figDesc><graphic coords="7,134.77,369.16,345.83,187.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,230.21,345.83,335.72"><head>Table 1 .</head><label>1</label><figDesc>Statistics of Med-VQA dataset is shown in Table1. The training, validation and test data splits have 5413, 500 and 500 instances respectively. Both questions and answers are on average longer than those in VQA datasets in general domain. The word-embedding (dimension of 200) ,which was pretained on wikipedia, pubmed articles and Pittsburgh clinical notes, has a good coverage (roughly over 95%) on both question and answer words of each data split. Also, note that the number of images is less than the number of question-answer pairs, which means several question-answer pairs may share a common image. Especially in training dataset shown in Table1, there are 2278 images which are less than half of the number of question-answer pairs (5413). Statistics of Med-VQA datasets</figDesc><table coords="6,134.77,230.21,268.96,335.72"><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Train Valid Test</cell></row><row><cell></cell><cell>Num</cell><cell>5413</cell><cell>500</cell><cell>500</cell></row><row><cell></cell><cell>Max length</cell><cell>28</cell><cell>15</cell><cell>14</cell></row><row><cell>Question</cell><cell>Min length</cell><cell>3</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell>Avg length</cell><cell>9.63</cell><cell cols="2">7.38 6.968</cell></row><row><cell></cell><cell cols="4">Emd Coverage 94.99% 96.93% 95.52%</cell></row><row><cell></cell><cell>Num</cell><cell>5413</cell><cell>500</cell><cell>500</cell></row><row><cell></cell><cell>Max length</cell><cell>26</cell><cell>14</cell><cell>\</cell></row><row><cell>Answer</cell><cell>Min length</cell><cell>1</cell><cell>1</cell><cell>\</cell></row><row><cell></cell><cell>Avg length</cell><cell>6.03</cell><cell>4.06</cell><cell>\</cell></row><row><cell></cell><cell cols="3">Emd Coverage 95.05% 96.54%</cell><cell>\</cell></row><row><cell>Image</cell><cell>Num</cell><cell>2278</cell><cell>324</cell><cell>264</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,169.81,171.90,275.74,61.94"><head>Table 2 .</head><label>2</label><figDesc>Summary of submissions in ImageCLEF 2018 Run WBSS BLEU CBSS Type Models 6069 0.18616 0.15833 0.02295 automatic ResNet152 + ETM + MFH 6113 0.18455 0.16159 0.01649 automatic ResNet152 + MFH 5980 0.18445 0.15966 0.02053 automatic ResNet152 + MFB</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,417.38,342.24,7.86;8,146.91,428.34,333.68,7.86;8,146.91,439.30,333.68,7.86;8,146.91,450.26,333.68,7.86;8,146.91,461.22,333.68,7.86;8,146.91,472.18,333.68,7.86;8,146.91,483.13,48.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,208.39,461.22,268.02,7.86">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garca Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oladimeji</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,472.18,333.68,7.86">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,493.81,342.24,7.86;8,146.91,504.77,333.68,7.86;8,146.91,515.73,247.86,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,415.53,493.81,65.06,7.86;8,146.91,504.77,269.87,7.86">Overview of the ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,437.46,504.77,43.13,7.86;8,146.91,515.73,58.30,7.86">CLEF2018 Working Notes</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,526.41,342.24,7.86;8,146.91,537.37,185.47,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,263.03,526.41,217.56,7.86;8,146.91,537.37,59.10,7.86">A Focused Dynamic Attention Model for Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485[cs</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,548.05,342.24,7.86;8,146.91,559.01,280.46,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,438.34,548.05,42.26,7.86;8,146.91,559.01,154.42,7.86">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325[cs</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,569.69,342.25,7.86;8,146.91,580.65,328.34,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,278.34,569.69,202.25,7.86;8,146.91,580.65,201.96,7.86">Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01471[cs</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,591.32,342.24,7.86;8,146.91,602.28,333.68,7.86;8,146.91,613.24,333.67,7.86;8,146.91,624.20,333.67,7.86;8,146.91,635.16,54.32,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,455.25,591.32,25.34,7.86;8,146.91,602.28,333.68,7.86;8,146.91,613.24,11.14,7.86">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,178.75,613.24,301.84,7.86;8,146.91,624.20,67.71,7.86">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">457468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,645.84,342.24,7.86;8,146.91,656.80,307.40,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,146.91,656.80,179.93,7.86">Multimodal Residual Learning for Visual QA</title>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01455[cs</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.25,7.86;9,146.91,130.63,236.05,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08496[cs</idno>
		<title level="m" coord="9,308.08,119.67,172.52,7.86;9,146.91,130.63,109.42,7.86">Topic Modeling over Short Texts by Incorporating Word Embeddings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
