<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.78,115.96,311.80,12.62;1,224.47,133.89,166.43,12.62">Multimedia Lab @ ImageCLEF 2018 Lifelog Moment Retrieval Task</title>
				<funder ref="#_8pWSdAg">
					<orgName type="full">Ministry of Innovation and Research, UEFIS-CDI</orgName>
				</funder>
				<funder ref="#_gwEBh9q">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.29,172.44,63.26,8.74"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mdogariu@imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="department">CAMPUS</orgName>
								<orgName type="laboratory">Multimedia Lab</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.25,172.44,68.82,8.74"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bionescu@alpha.imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="department">CAMPUS</orgName>
								<orgName type="laboratory">Multimedia Lab</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.78,115.96,311.80,12.62;1,224.47,133.89,166.43,12.62">Multimedia Lab @ ImageCLEF 2018 Lifelog Moment Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18B367576EC89C4990E3CDE1498C007A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog</term>
					<term>CNN</term>
					<term>Imagenet</term>
					<term>Places365</term>
					<term>MSCOCO</term>
					<term>Food101</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Multimedia Lab team at the ImageCLEF 2018 Lifelog Moment Retrieval Task. Our method makes use of visual information, text information and metadata. Our approach consists of the following steps: we reduce the number of images to analyze by eliminating the ones that are blurry or do not meet certain metadata criteria, extract relevant concepts with several Convolutional Neural Networks, perform K-means clustering on the Oriented Gradients and Color Histograms features and rerank the remaining images according to a relevance score computed between each image concept and the queried topic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent technological advancements have resulted in the development of numerous wearable devices that can successfully help one track his own daily activity. Examples of such devices include wearable cameras, smart watches or fitness bracelets. Each of these provides information regarding its user's activity and combining the outputs of all such devices can result in a highly detailed description of the person's habits, schedule or actions. However, continuous acquisition of data can lead to cumbersome archives of information which, in term, can become too difficult to handle, up to the point where it becomes inefficient to try to use them. As part of ImageCLEF 2018 evaluation campaign <ref type="bibr" coords="1,416.75,534.81,9.96,8.74" target="#b6">[7]</ref>, the Lifelog Tasks <ref type="bibr" coords="1,162.55,546.77,10.52,8.74" target="#b3">[4]</ref> aim to solve these problems.</p><p>This paper presents our participation in the Lifelog Moment Retrieval (LMR) task, in which participants have to retrieve a number of specific moments in a lifeloggers life, given a text query. Moments are defined as semantic events, or activities that happened throughout the day. For each query, a total of 50 images are expected to be extracted, both relevant and diverse, with the official metric being F 1@10 measure.</p><p>The rest of the paper is organized as follows. In Section 2 we discuss related work from the literature, in Section 3 we present our proposed system, in Section 4 we discuss the results and in Section 5 we conclude the paper.</p><p>In this section we briefly discuss the recent results obtained in similar competitions. The organizing team of the ImageCELF 2017 Lifelog Tasks <ref type="bibr" coords="2,427.31,155.06,10.52,8.74" target="#b2">[3]</ref> proposed a pipeline in which they perform a segmentation of the dataset based on time and concepts metadata. In parallel, they analyzed each query and extracted the relevant information that can be applied on the given metadata. After extracting only the images that fit the previous criteria they performed an additional filtering of images and remove those that contain large objects or are blurry. The last step involves a diversification of images through hierarchical clustering.</p><p>A similar technique was used by <ref type="bibr" coords="2,296.88,238.79,15.50,8.74" target="#b11">[12]</ref> in their submission at the same competition. In addition, they also used the image descriptors obtained by running each image through different Convultional Neural Networks (CNN), i.e. they extracted object and place feature vectors to which they added a human detection CNN. Each image was assigned a relevance score obtained by comparing the feature vector to a reference vector on a per topic basis. Their chosen clustering approach was K-means <ref type="bibr" coords="2,267.35,310.52,14.61,8.74" target="#b10">[11]</ref>. The same authors use a very similar system in <ref type="bibr" coords="2,146.61,322.47,9.96,8.74" target="#b8">[9]</ref>, where they further add a temporal smoothing element. A somewhat different system was adopted in <ref type="bibr" coords="2,263.57,334.43,15.50,8.74" target="#b16">[17]</ref> where the authors combined a visual indexing method similar to the ones in <ref type="bibr" coords="2,266.91,346.39,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,284.06,346.39,7.75,8.74" target="#b8">9]</ref> with a location indexing method.</p><p>In our previous participation <ref type="bibr" coords="2,282.27,358.38,10.52,8.74" target="#b4">[5]</ref> we also applied a filtering procedure first based on the metadata and later on the similarity between the topic queries and the feature vector which consisted in detected concepts. This filtering was followed by a hierarchical clustering step. We learned that in order for this technique to work there has to be a strong correlation between the queries and the detected concepts. Also, enumeration of items that needed to be present in the image significantly improved the results.</p><p>This paper combines the benefits from <ref type="bibr" coords="2,318.06,442.11,15.50,8.74" target="#b11">[12]</ref> and <ref type="bibr" coords="2,355.28,442.11,10.52,8.74" target="#b2">[3]</ref> to which it adds two more feature vectors. Moreover, we explore the impact that supervised fine-tuning has on the final results and present the outcome of 5 different techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>Our approach involves the pipeline presented in Figure <ref type="figure" coords="2,373.65,522.21,3.87,8.74" target="#fig_0">1</ref>. Each of the processing steps is detailed in the following. The output of the system is a list of 50 images for each of the proposed 10 topics, which are both relevant and diverse with respect to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Blur filtering</head><p>We first apply a blur filtering over the entire dataset. We compute a focus measure for each image by using the variance of the Laplacian kernel. If an image has a focus measure below an imposed threshold then it is discarded from further processing. Choosing the threshold requires several trials to see what works best for the dataset at hand. Imposing a low value on the threshold results in a permissive filter, leading to a low number of discarded images, whereas a high threshold could wrongly discard images of acceptable quality. We found that a value of 60 for the threshold leads to satisfying results. We decided to allow the filter to be slightly permissive so that we do not reject true positives. In the end, from the total 80.5k images we discard 16.5k blurry images, leaving us with only 64k images to process. Another advantage of this technique is that it also filters out uninformative images that contain large homogeneous areas such as images where the camera was facing the ceiling or a wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Concepts extraction</head><p>In the second step of our algorithm we run each of the remaining 64k images through several classifiers and a detector. We use 3 image-level classifiers and one object detector, to which we also add the concept detector information provided by the organizers. All of these systems are implemented using CNNs as described below.</p><p>Imagenet classifier A common practice for detecting several concepts for an image is to run it through an image classifier trained on the popular Imagenet dataset <ref type="bibr" coords="3,170.28,571.78,9.96,8.74" target="#b7">[8]</ref>. This yields a 1000-D vector with values corresponding to the confidence level of associating the entire image with a certain concept. We use a ResNet50 <ref type="bibr" coords="3,179.51,595.69,10.52,8.74" target="#b5">[6]</ref> implementation trained on Imagenet.</p><p>However, there are 2 important aspects that need to be considered when implementing this technique. The first one is that the classifier is trained to predict a single concept for the entire image, whereas lifelog images contain numerous objects that might be of interest for the retrieval task. The second aspect is that out of the 1000 classes only a small part is relevant, with the vast majority of these concepts unlikely to be met in a person's daily routine. This leads to noisy classification, diminishing the usefulness of this classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places classifier</head><p>The second classifier that we implement is meant to predict the place presented in the image. We use the VGG16 <ref type="bibr" coords="4,374.77,172.58,15.50,8.74" target="#b13">[14]</ref> network, trained on the Places365 dataset <ref type="bibr" coords="4,235.90,184.53,14.61,8.74" target="#b17">[18]</ref>. The dataset consists of approximately 1.8 million images from 365 scene categories. The network outputs a 365-D vector with one confidence value for each scene category. The places classifier performs well with respect to the lifelogging tasks, being trained to distinguish between most of the backgrounds present in the competition's dataset. This comes especially useful as most topics require the lifelogger to be present in a certain place at the time when the image has been captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Food classifier</head><p>As some topics revolved around the lifelogger's eating and drinking habits we decided to also include a food classifier network. For this we use the InceptionV3 architecture <ref type="bibr" coords="4,282.36,309.85,15.50,8.74" target="#b14">[15]</ref> pre-trained on the Imagenet dataset and we fine-tune it on the Food101 dataset <ref type="bibr" coords="4,307.47,321.80,9.96,8.74" target="#b0">[1]</ref>. The result is a 101-D feature vector for each image. As the training dataset is composed of images where the labeled food takes up most of the image, when running our images through this classifier we extract 6 crops (upper left, upper right, center, lower left, lower middle, lower right) and their mirrored versions as well, which we pass through the network. Afterwards, we select the maximum activation for each food class from the 12 predictions and build the 101-D vector.</p><p>Object detector Additionally to the classifiers we also use a concept detector. This has the advantage that it locates more than one instance of the same object and each instance has its own attached confidence. Therefore, there will be no competition between detections when computing the final results. For this purpose we use a Faster R-CNN <ref type="bibr" coords="4,262.79,471.03,15.50,8.74" target="#b12">[13]</ref> implementation trained on the MSCOCO <ref type="bibr" coords="4,465.09,471.03,15.50,8.74" target="#b9">[10]</ref> dataset. Another advantage of this setup is that with object detection it also performs object counting. Therefore, we build two feature vectors for each image: one that retains the frequency of each detected object inside the image and one which sums up the confidences of all detected instances for each class inside the image. As the dataset also contains the class "person", we use its frequency to perform person counting. Also, many of the classes from the MSCOCO dataset can be found in daily scenarios, thus making it well-suited for the purpose of lifelog image retrieval.</p><p>Official concepts Apart from the previously mentioned systems there is one more feature extractor that we use, namely the one provided by the organizers. They released a set of results in which each image is described by a various number of concepts. The total number of possible classes is not known and their objective is also uncertain as they cover a broad range of concepts such as places, foods, actions, objects, adverbs etc. To cope with this we add each unique concept from the official feature results to a list that sums up 633 unique entries. In the end, we create a 633-D feature vector for each image, with nonzero entries only where the official concept detector triggered a detection. On this positions we retained the detector's confidence for the respective concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metadata processing</head><p>Apart from the concept detector, the organizers also released a file containing a large variety of metadata about each minute from the logged data. These metadata encompass a bundle of information such as biometric data, timestamps, locations, activities, geographical coordinates, food logs and even the music that the lifelogger was listening to at certain times. We use only a part of this set of metadata. The rest of it can be used as well, but it did not fit our proposed system, therefore we only extract these data but did not process it any further. A summary of all the information that we process for each image can be seen in Table <ref type="table" coords="5,162.16,306.21,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Refinement filtering</head><p>From previous experience we found that a key aspect of obtaining good results is to narrow down the set of images that are to be processed. This can be done by eliminating images that do not meet a certain set of minimum requirements.</p><p>In this sense we implement two types of filtering: one based on the metadata and one based on the soft values of the concepts mentioned in Table <ref type="table" coords="5,412.44,620.25,4.98,8.74" target="#tab_0">1</ref> and explained below. We select a random topic out of the 10 test ones, to serve as an example and we will discuss it throughout the rest of the paper. The topic consists of the following:</p><p>Title: My Presentations Description: Find the moments when I was giving a presentation to a large group of people. Narrative: To be considered relevant, the moments must show more than 15 people in the audience. Such moments may be giving a public lecture or a lecture in the university.</p><p>Metadata filtering Our general approach is to manually interpret the entire topic text and extract meaningful constraints on the metadata associated with each image. Those entries that do not satisfy the given constraints are eliminated from the processing pipeline. We prefer looser restrictions such that we lower the chance of removing images relevant to the query in question. For the above given topic we impose the following:</p><p>-Activity: if the activity is any of the {'airplane', 'transport', 'walking'} then remove image; -Location: if the location is anything different from {'Work', 'Dublin City University (DCU)'} then remove image; -Time: if the hour is not in the interval 9-19 then remove image; -Person count: if there are less than 10 persons detected then remove image.</p><p>Two remarks are in order here. First, even if the person count is not part of the metadata we treat it as such because of its 1-D nature and discrete values. Second, the minimum threshold on the person count is lower than the query asks for because the MSCOCO object detector can have difficulties in detecting overlapped persons in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft concepts filtering</head><p>In a similar manner we tackle the filtering based on the soft outputs of the concept detector/classifiers. If a certain object/concept is detected with a higher probability than a preset threshold in an image then that image is removed from the processing queue. Again, this process involves manual selection of concepts that should not be present in the images. As it would be a tedious work to select an exhaustive set of concepts for each classifier, we only select the ones which are most likely to appear in the lifelog dataset and would be in contradiction with the queried text, therefore the selection can greatly differ from one query to another. For the query in the above example we select the following:</p><p>-Places: if the probability to detect any of the places from the set of words {'car interior', 'living room', 'kitchen'} is greater than the threshold then remove image; -MSCOCO objects: if the probability to detect any of the objects from the set of words {'traffic light', 'cup'} is greater than the threshold then remove image; -Official concepts: if the probability to detect any of the concepts from the set of words {'blurry', 'blur', 'null', 'Null','wall', 'ceiling', 'outdoor', 'outdoor object'} is greater than the threshold then remove image;</p><p>We do not use the same technique for the Imagenet descriptor as it usually outputs low confidences and could thus have a great impact on the amount of images that would be removed. Also, the Food descriptor was not used for this topic as it is not relevant. Instead, its purpose is solely to classify food types for topics which implicitly ask for this.</p><p>We tried several values for the threshold and by visual inspection of the output we noticed that 0.3 offers a good trade-off between the probability of rejecting true positives and rejecting true negatives. Finding the best value for each concept detector and each topic requires many iterations, making this a costly process.</p><p>Relevance score After the blurred and irrelevant images have been filtered out we proceed into computing a relevance score for each image relative to the queried topic. In the same fashion as <ref type="bibr" coords="7,300.93,293.29,15.50,8.74" target="#b11">[12]</ref> we create a reference vector for each of the 5 concept detectors in Tabel 1 with higher values on the positions corresponding to concepts which are more likely to be found in relevant images and lower values on the other positions. The score associated to a certain concept detector is obtained by computing the dot product between the concept feature vector and its respective reference vector. The result is then weighted and added to the relevance score for each type of concept, as expressed in the equation below.</p><formula xml:id="formula_0" coords="7,175.69,399.77,304.90,169.04">score =w imagenet × 1000 i=1 [concept imagenet (i) * ref imagenet (i)]+ w places × 365 i=1 [concept places (i) * ref places (i)]+ w f ood × 101 i=1 [concept f ood (i) * ref f ood (i)]+ w mscoco × 80 i=1 [concept mscoco (i) * ref mscoco (i)]+ w of f icial × 633 i=1 [concept of f icial (i) * ref of f icial (i)],<label>(1)</label></formula><p>with concept &lt;dataset&gt; (i) being the confidence associated with the i -th detected concept from a dataset for the respective image, ref &lt;dataset&gt; (i) being the reference vector at position i for the given dataset and w &lt;dataset&gt; being the weight given to the respective dataset. The weights for each dot product have been manually adjusted for each topic by trial and error. The values for the reference vectors have been either set manually or automatically, depending on the submitted run. We discuss this at length in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Diversification</head><p>The submitted results are supposed to be both relevant and diverse. The relevance score should emphasize images that match the query description. For the diversity part we apply the K-means algorithm for all the images that are left after the filtering process. Each image is represented by the concatenation of two normalized vectors: a 1536-D vector representing the Histogram of Oriented Gradients (HOG) <ref type="bibr" coords="8,216.02,196.85,10.52,8.74" target="#b1">[2]</ref> feature vector and a 512-D vector representing the color histogram feature vector. This 2048-D vector should account for both shapes and colors inside images.</p><p>We run the K-means algorithm with either 5, 10, 25 or 50 clusters. For the final list of proposed images we select from each cluster the image with the highest relevance score in a round-robin manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We have submitted one run during the competition and 4 other runs after the competition ended. The official metric of the competition was F 1@X, which is computed as the harmonic mean between precision (P @X) and cluster recall (CR@X), with X representing the number of the top elements to be taken into consideration. In Table <ref type="table" coords="8,259.98,356.56,4.98,8.74">2</ref> we present the final F 1@X results that we have obtained for each run with best values in bold. Our last run is omitted when choosing the best results because it implied a highly supervised approach and would lead to an unfair comparison. In Figure <ref type="figure" coords="8,342.52,392.43,4.98,8.74" target="#fig_1">2</ref> we present the F 1@X results for individual topics. Next, we provide a detailed description of each run.</p><p>Table <ref type="table" coords="8,239.15,434.96,4.13,7.89">2</ref>. Official results for the submitted runs. Run F1@5 F1@10 F1@20 F1@30 F1@40 F1@50 Run 1 0.235 0.216 0.224 0.218 0.203 0.199 Run 2 0.154 0.169 0.215 0.21 0.207 0.199 Run 3 0.158 0.168 0.217 0.214 0.199 0.206 Run 4 0.129 0.166 0.184 0.184 0.178 0.188 Run 5 0.412 0.443 0.446 0.438 0.419 0.405 Run 1 This was the only run that we submitted during the competition and it follows the pipeline described in Section 3. We manually selected concepts from each training dataset that would be probable to appear in the images described by the queries. We set the reference vectors values to 1 on the positions corresponding to the selected concepts and to 0 elsewhere. This makes the dot product equivalent to an accumulation of confidences from a limited set of concepts for each image. The weights, w imagenet , w places , w f ood , w mscoco and w of f icial have been adjusted independently for each topic. The official F 1@10 value was 0.216 and this is the value that represents our position in the official standings.</p><p>Run 2 In addition to what was proposed for Run 1 we also applied another filtering of the results, this time after the clusterization part. While going through the clusters in the round robbin manner we also checked that the newly added images are not too visually similar to the ones already added to the list. For this purpose each new proposal would be compared one-on-one with the already added proposals. The comparison was done with two metrics: mean squared error (M SE) and structural similarity index (SSIM ). If for a pair of images M SE &lt; 2000 and SSIM &gt; 0.5 then they are considered to be too similar, the latter one is discarded and the round robin continues. We expected this technique to allow for more diversity in the proposed list of images and enhance the cluster recall. Instead, it turned out to eliminate a part of the correct predictions and lower the precision. The official F 1@10 value was 0.169.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3</head><p>For the 3rd run we proposed a different way of computing the reference vectors, the same technique that we used in <ref type="bibr" coords="9,336.28,291.92,9.96,8.74" target="#b4">[5]</ref>. Namely, instead of manually selecting the concepts that dictate whether an image is relevant or not from each dataset, we only selected the nouns that best describe the topic's description, obtaining a short set of key words, called "words to search". For the topic mentioned in Section 3.4 we have: words to search={'presentation', 'group', 'people', 'audience', 'public', 'lecture', 'conference', 'university', 'classroom'}. Starting from this set of words we computed the Wu-Palmer similarity measure <ref type="bibr" coords="9,465.09,363.65,15.50,8.74" target="#b15">[16]</ref> between each concept and all of the words from the "words to search" vector as described in the equation below.</p><formula xml:id="formula_1" coords="9,179.39,410.38,301.20,20.14">ref dataset (i) = w∈words to search d W U P (concept dataset (i), w),<label>(2)</label></formula><p>where dataset is any of the 5 datasets used in the concept detectors (Imagenet, Places-365, Food-101, MSCOCO, Official), d W U P (concept dataset , w) is the Wu-Palmer distance between one concept of the dataset and one word from the set of words to search for, "words to search" . This avoided the binary setting of the reference vector that was used in the previous runs but it lead to a decrease of the performance of the entire system. The official F 1@10 value was 0.168.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4</head><p>The 4th run was similar to Run 3, with the only difference being that all the weights w imagenet , w places , w f ood , w mscoco and w of f icial were set to 1, rendering them neutral to the reference score computation. This allows the reference score to stabilize solely according to the similarity measure between the words from the topic description and the labels of the concept detectors. From Table <ref type="table" coords="9,162.80,590.79,4.98,8.74">2</ref> we can see that this only lowers the results, suggesting that tweaking the weights for each dot-product is a better approach. This run was our closest submission to a fully automatic system. The official F 1@10 value was 0.166.</p><p>Run 5 Our last run was done with the same approach as Run 1, this time performing a fine-tuning of all system parameters for the topics that had bad results in the first run by trial and error. This approach leads to visibly better results. However, this is obtained after careful manual tuning, which makes the technique highly supervised and costly, as well, making it unfair to compare it with the previous runs, this being the reason why it is separated from the rest of the entries in Table <ref type="table" coords="10,234.19,166.81,3.87,8.74">2</ref>. The official F 1@10 value was 0.443. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discussion</head><p>From the results that we presented in Figure <ref type="figure" coords="10,337.38,452.82,4.98,8.74" target="#fig_1">2</ref> it can be seen that the F 1@X metric has high inter-topic variance. This does not come as a surprise since the topics approach different scenes, some of which are better represented in terms of number of images in the dataset or are better described in terms of the associated metadata. While some topics are easy to address (e.g. Topic 8:"Find the moments when I was with friends in Costa coffee." can be retrieved almost solely based on the location metadata) there are still topics for which retrieval is difficult (e.g. Topic 6:"Find the moments when I was assembling a piece of furniture.") mainly because of the difficulty of assigning distinctive concepts to their description. Except for the last run, it can be seen that all our approaches behave similarly for each individual topic, suggesting that there is no clear advantage in using one approach over the others. This is somewhat expected since they use the same data and almost the same degree of supervision. The only clear improvement can be seen when strong human input is involved. The part of the entire system which had the greatest impact on the final outcome was the metadata filtering. We argue that this is because this type of information has been specifically implemented for lifelogging purposes and therefore have the strongest contribution in the end. This was also proven by our 5th run where we paid more attention to fine-tuning the processing parameters, such as metadata, weights and set of query words, rather than on introducing a new system.</p><p>The way the F 1@X metric changes with X is also worth mentioning. We noticed that is more beneficial to focus on the cluster recall than on the precision. This comes straightforward from the definition of the F 1@X metric in which CR@X and P @X have equal contributions. As the topics cover an average of 5-6 different clusters (as per the development dataset) it is usually more productive to retrieve images even from at least two different clusters rather than retrieve all the images from a single cluster. This happens because the cluster recall can only increase with X, whereas the precision usually drops for the same number of images. However, the cluster recall usually compensates for the precision.</p><p>We also notice that almost all of our approaches have the highest F 1@X value for X = 20 and they slightly decrease with the increase of X which was rather inconvenient since the official metric accounts for X = 10. However, we have reported quite similar results for X = 10 and X = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we presented our approach for the LMR competition at the Im-ageCLEF Lifelog task. We have adopted a general framework that processes visual, text and meta information about images. We have extracted 5 concept vectors, 2 feature vectors and more than 10 metadata fields for each image. All of the proposed variants rely on metadata filtering and try to link each key-word from the search topics to the concepts detector labels. A relevance score which takes the aforementioned link into consideration is then computed and K-means algorithm is used for clustering the results for the final proposals.</p><p>The LMR task still poses numerous difficulties such as processing a great deal of multimodal data, adapting several multimedia retrieval systems to this type of task and integrating all the results. The diversity in the search queries is also to be taken into account, sometimes being quite easy to process (see results of 'Topic 8') but sometimes proving that there still is work to be done to find a solution that satisfies this type of generality (see results of 'Topic 7'). We found that manual fine-tuning of system parameters offers the best result, but this makes the system personalized for the given topics, lowering its scalability to other similar tasks.</p><p>As opposed to last year, we have implemented a significantly more complex system and the future challenge for us is to work towards a scalable system, not so much dependent on human input, to solve the LMR task. We believe that with the increasing interest in this type of competitions it is possible to achieve this perspective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,252.39,291.83,110.57,7.89;3,143.41,115.84,328.53,161.22"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Processing pipeline.</figDesc><graphic coords="3,143.41,115.84,328.53,161.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,212.26,378.57,190.83,7.89;10,134.77,196.44,345.79,167.36"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results for each topic from the test set.</figDesc><graphic coords="10,134.77,196.44,345.79,167.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,204.81,336.95,205.73,161.36"><head>Table 1 .</head><label>1</label><figDesc>Information used for individual images.</figDesc><table coords="5,204.81,357.75,205.73,140.57"><row><cell>Type</cell><cell>Content</cell><cell>Dimension</cell></row><row><cell></cell><cell>Activity</cell><cell>1-D</cell></row><row><cell>Metadata</cell><cell>Date Time (HH:MM:SS)</cell><cell>1-D 3-D</cell></row><row><cell></cell><cell>Location</cell><cell>1-D</cell></row><row><cell></cell><cell>Imagenet</cell><cell>1000-D</cell></row><row><cell></cell><cell>Places</cell><cell>365-D</cell></row><row><cell>Concepts</cell><cell>Food MSCOCO objects</cell><cell>101-D 80-D</cell></row><row><cell></cell><cell cols="2">MSCOCO person count 1-D</cell></row><row><cell></cell><cell>Official concepts</cell><cell>633-D</cell></row><row><cell>Feature vectors</cell><cell>HOG descriptor Color histogram</cell><cell>1536-D 512-D</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">Ministry of Innovation and Research, UEFIS-CDI</rs>, project <rs type="projectName">SPIA-VA</rs>, agreement <rs type="grantNumber">2SOL/2017</rs>, grant <rs type="grantNumber">PN-III-P2-2.1-SOL-2016-02-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8pWSdAg">
					<idno type="grant-number">2SOL/2017</idno>
					<orgName type="project" subtype="full">SPIA-VA</orgName>
				</org>
				<org type="funding" xml:id="_gwEBh9q">
					<idno type="grant-number">PN-III-P2-2.1-SOL-2016-02-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,220.11,337.63,7.86;12,151.52,231.07,329.07,7.86;12,151.52,242.03,60.92,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,324.96,220.11,155.63,7.86;12,151.52,231.07,115.57,7.86">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,288.77,231.07,171.48,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,252.82,337.63,7.86;12,151.52,263.78,329.07,7.86;12,151.52,274.74,329.07,7.86;12,151.52,285.70,40.44,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,242.26,252.82,219.47,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,263.78,329.07,7.86;12,151.52,274.74,141.99,7.86">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,296.49,337.63,7.86;12,151.52,307.45,329.07,7.86;12,151.52,318.41,329.07,7.86;12,151.52,329.37,254.98,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,167.90,307.45,308.16,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,168.00,318.41,296.33,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,340.16,337.64,7.86;12,151.52,351.12,329.07,7.86;12,151.52,362.08,329.07,7.86;12,151.52,373.04,322.32,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,442.93,340.16,37.66,7.86;12,151.52,351.12,329.07,7.86;12,151.52,362.08,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,197.14,362.08,232.60,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
			<biblScope unit="volume">11018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,383.83,337.64,7.86;12,151.52,394.79,329.07,7.86;12,151.52,405.75,317.71,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,255.75,383.83,224.85,7.86;12,151.52,394.79,73.29,7.86">A Textual Filtering of HOG-based Hierarchical Clustering of Lifelog Data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,246.06,394.79,234.53,7.86;12,151.52,405.75,43.39,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,416.54,337.64,7.86;12,151.52,427.50,329.08,7.86;12,151.52,438.46,98.94,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,299.05,416.54,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.33,427.50,310.07,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,449.25,337.63,7.86;12,151.52,460.21,329.07,7.86;12,151.52,471.17,329.07,7.86;12,151.52,482.12,329.07,7.86;12,151.52,493.08,329.07,7.86;12,151.52,504.04,329.07,7.86;12,151.52,515.00,329.07,7.86;12,151.52,525.96,95.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,197.95,482.12,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,493.08,329.07,7.86;12,151.52,504.04,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="12,197.87,515.00,169.61,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
			<biblScope unit="volume">11018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,536.75,337.63,7.86;12,151.52,547.71,329.07,7.86;12,151.52,558.67,100.85,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,328.09,536.75,152.50,7.86;12,151.52,547.71,103.36,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,274.48,547.71,206.12,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,569.46,337.63,7.86;12,151.52,580.42,329.07,7.86;12,151.52,591.38,329.07,7.86;12,151.52,602.34,83.87,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,423.38,569.46,57.21,7.86;12,151.52,580.42,196.57,7.86">VCI2R at the NTCIR-13 Lifelog-2 Lifelog Semantic Access Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,368.19,580.42,112.41,7.86;12,151.52,591.38,267.26,7.86">Proceedings of the 13th NT-CIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 13th NT-CIR Conference on Evaluation of Information Access Technologies<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">December 5-8 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,613.13,337.98,7.86;12,151.52,624.09,329.07,7.86;12,151.52,635.05,273.87,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,206.57,624.09,169.50,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,395.35,624.09,85.23,7.86;12,151.52,635.05,115.85,7.86">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zürich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,645.81,337.97,7.89;12,151.52,656.80,59.90,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,192.54,645.84,135.02,7.86">Least squares quantization in pcm</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,335.27,645.84,96.38,7.86">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="2006-09">Sep 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.98,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,288.03,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,464.21,119.67,16.39,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,41.44,7.86">VC-I2R@ImageCLEF2017: Ensemble of Deep Learned Features for Lifelog Video Summarization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="13,213.78,141.59,266.81,7.86;13,151.52,152.55,14.99,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,163.51,337.97,7.86;13,151.52,174.47,329.07,7.86;13,151.52,185.43,329.07,7.86;13,151.52,196.39,78.39,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,308.79,163.51,171.80,7.86;13,151.52,174.47,150.91,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,287.06,185.43,193.53,7.86;13,151.52,196.39,29.92,7.86">Advances in Neural Information Processing Systems 28</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,207.34,337.98,7.86;13,151.52,218.30,329.07,7.86;13,151.52,229.26,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,271.78,207.34,208.82,7.86;13,151.52,218.30,59.28,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,231.28,218.30,249.31,7.86">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,240.22,337.97,7.86;13,151.52,251.18,329.07,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.10,110.07,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,404.33,240.22,76.26,7.86;13,151.52,251.18,160.66,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,333.10,251.18,147.50,7.86;13,151.52,262.14,182.58,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,284.06,337.97,7.86;13,151.52,295.02,329.07,7.86;13,151.52,305.98,62.58,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,236.50,284.06,147.94,7.86">Verbs semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,405.71,284.06,74.88,7.86;13,151.52,295.02,272.00,7.86">Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 32Nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,316.93,337.97,7.86;13,151.52,327.89,329.07,7.86;13,151.52,338.85,329.07,7.86;13,151.52,349.81,111.12,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,460.56,316.93,20.03,7.86;13,151.52,327.89,232.14,7.86">PBG at the NTCIR-13 Lifelog-2 LAT, LSAT, and LEST Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Akagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Takimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,405.58,327.89,75.01,7.86;13,151.52,338.85,295.45,7.86">Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 13th NTCIR Conference on Evaluation of Information Access Technologies<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">December 5-8 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,360.77,337.97,7.86;13,151.52,371.73,329.07,7.86;13,151.52,382.66,205.81,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,399.52,360.77,81.07,7.86;13,151.52,371.73,145.67,7.86">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,303.98,371.73,176.62,7.86;13,151.52,382.69,82.44,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
