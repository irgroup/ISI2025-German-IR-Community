<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.68,116.95,333.99,12.62;1,192.73,134.89,229.90,12.62">Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.66,172.56,68.14,8.74"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Lab</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.35,172.56,45.52,8.74"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Lab</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.43,172.56,66.89,8.74"><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
							<email>dimeji.farri@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Lab</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.88,172.56,37.91,8.74"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Lab</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.35,172.56,68.11,8.74"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.89,184.51,78.48,8.74"><forename type="first">Matthew</forename><surname>Lungren</surname></persName>
							<email>mlungren@stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.68,116.95,333.99,12.62;1,192.73,134.89,229.90,12.62">Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6439AB6673E26E8B61E945AFBFC17026</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF 2018</term>
					<term>Visual Question Answering</term>
					<term>Medical Image Interpretation</term>
					<term>Question Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of the inaugural edition of the ImageCLEF 2018 Medical Domain Visual Question Answering (VQA-Med) task. Inspired by the recent success of visual question answering in the general domain, a pilot task was proposed this year to focus on visual question answering in the medical domain. Given medical images accompanied with clinically relevant questions, participating systems were tasked with answering the questions based on the visual image content. A dataset of 6,413 question-answer pairs accompanied with 2,866 medical images extracted from PubMed Central articles was provided; from which, 5,413 question-answer pairs with 2,278 medical images were used for training, 500 question-answer pairs with 324 medical images were used for validation, and 500 questions with 264 medical images were used for testing. Among 28 registered participants, 5 groups submitted a total of 17 runs, indicating a considerable interest in the VQA-Med task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing interest in artificial intelligence (AI) to support clinical decision making and improve patient engagement, opportunities to generate and leverage algorithms for automated medical image interpretation are currently being explored <ref type="bibr" coords="1,200.45,573.43,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="1,212.62,573.43,7.01,8.74" target="#b4">5]</ref>. Since patients may now access structured and unstructured data related to their health via patient portals, such access also motivates the need to help them better understand their conditions regarding their available data, including medical images.</p><p>The clinicians' confidence in interpreting complex medical images can be significantly enhanced by a "second opinion" provided by an automated system. In addition, patients may be interested in the morphology/physiology and diseasestatus of anatomical structures around a lesion that has been well characterized by their healthcare providers and they may not necessarily be willing to pay significant amounts for a separate office-or hospital visit just to address such questions. Although patients often turn to web search engines to disambiguate complex terms or obtain answers to confusing aspects of a medical image, results from search engines may be nonspecific, erroneous and misleading, or overwhelming in terms of the volume of information.</p><p>Visual Question Answering is a new and exciting problem that combines natural language processing and computer vision techniques. Inspired by the recent success of visual question answering in the general domain<ref type="foot" coords="2,410.82,214.17,3.97,6.12" target="#foot_0">4</ref>  <ref type="bibr" coords="2,417.66,215.74,9.96,8.74" target="#b6">[7]</ref>, we propose a pilot task as part of the ImageCLEF 2018 evaluation campaign<ref type="foot" coords="2,413.11,226.12,3.97,6.12" target="#foot_1">5</ref>  <ref type="bibr" coords="2,420.31,227.70,10.52,8.74" target="#b7">[8]</ref> to focus on visual question answering in the medical domain (VQA-Med). Given a medical image accompanied with a clinically relevant question, participating systems are tasked with answering the question based on the visual image content.</p><p>This paper presents an overview of the VQA-Med task at ImageCLEF 2018. Section 2 introduces the task and Section 3 presents details of the provided corpus. A description of the evaluation methodology is provided in Section 4. We discuss the participant submissions with results in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>In the inaugural edition we propose a pilot task of visual question answering in the medical domain (VQA-Med) as part of the ImageCLEF 2018 evaluation campaign <ref type="bibr" coords="2,179.87,404.23,9.96,8.74" target="#b7">[8]</ref>. Given medical images accompanied with clinically relevant questions, participating systems are tasked with answering the questions based on the visual image content. Figure <ref type="figure" coords="2,278.87,428.14,4.98,8.74" target="#fig_0">1</ref> shows a few example images with associated questions and ground truth answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>To create the datasets for the proposed VQA-Med task, we consider medical images along with their captions extracted from PubMed Central articles<ref type="foot" coords="2,456.98,507.35,3.97,6.12" target="#foot_2">6</ref> (essentially a subset of the ImageCLEF 2017 caption prediction task <ref type="bibr" coords="2,424.89,520.88,10.30,8.74" target="#b5">[6]</ref>).</p><p>We use a semi-automatic approach to generate question-answer pairs from captions of the medical images. First, we automatically generate all possible question-answer pairs from captions using a rule-based question generation (QG) system<ref type="foot" coords="2,164.49,567.24,3.97,6.12" target="#foot_3">7</ref>  <ref type="bibr" coords="2,172.97,568.81,9.96,8.74" target="#b3">[4]</ref>. The system consists of four modules to automate question generation: 1) sentence simplification, which utilizes clauses, subject, predicate, and verbs to split a long, complex sentence (i.e. the captions associated with the medical images) into multiple simple sentences via lexical alternation and appositive identification, 2) answer phrase identification, which identifies relevant phrases from the simple sentences such that corresponding questions can be generated, 3) question generation, where the answer phrases are used to generate possible question phrases through decomposition of the main verb, inversion of the subject and auxiliary verb, and inserting one of the possible question phrases in place of the answer phrases, and 4) candidate questions ranking, where a ranking model is trained to rank the generated candidate questions.</p><p>The candidate questions generated via the automatic approach may be noisy as the defined rules may not adequately capture the complex characteristics of medical domain terminologies (clinical concepts) and in particular, the unique writing style of the medical image captions in biomedical articles. Therefore, two expert human annotators manually check all generated question-answer pairs associated with the medical images in two passes. In the first pass, one annotator proofreads all question-answer pairs and resolves related noises accrued by the aforementioned four modules of the automatic QG system to ensure syntactic and semantic correctness. In the second pass, the other annotator, an expert in clinical medicine, verified all question-answer pairs to form well-curated validation and test sets by ensuring their clinical relevance with respect to associated medical images.</p><p>The final curated corpus is comprised of 6,413 question-answer pairs associated with 2,866 medical images. The overall set is split into 5,413 question-answer pairs (associated with 2,278 medical images) for training, 500 question-answer pairs (associated with 324 medical images) for validation, and 500 questions (associated with 264 medical images) for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>The evaluation of the participant systems of the VQA-Med task is conducted based on three metrics: BLEU, WBSS (Word-based Semantic Similarity), and CBSS (Concept-based Semantic Similarity).</p><p>BLEU <ref type="bibr" coords="4,180.99,368.72,10.52,8.74" target="#b0">[1]</ref> is used to capture the similarity between a system-generated answer and the ground truth answer. Each answer is converted to lower-case, all punctuations are removed, and the answer is tokenized 8 to individual words. Stopwords are removed using NLTK's 9 English stopword list. Snowball stemming 10 is applied to increase the coverage of overlaps. The overall methodology and resources for the BLEU metric are essentially similar to the ImageCLEF 2017 caption prediction task 11 .</p><p>Following a recent algorithm to calculate semantic similarity in the biomedical domain <ref type="bibr" coords="4,191.18,464.50,9.96,8.74" target="#b1">[2]</ref>, we create the WBSS metric based on Wu-Palmer Similarity (WUPS 12 ) <ref type="bibr" coords="4,184.65,476.45,10.52,8.74" target="#b2">[3]</ref> with WordNet ontology in the backend. WBSS computes a similarity score between a system-generated answer and the ground truth answer based on word-level similarity.</p><p>CBSS is similar to WBSS, except that instead of tokenizing the systemgenerated and ground truth answers into words, we use MetaMap 13 via the pymetamap wrapper 14 to extract biomedical concepts from the answers, and build a dictionary using these concepts. Then, we build one-hot vector representations of the answers to calculate their semantic similarity using the cosine similarity measure.</p><p>We received a total of 17 result submissions by 5 different teams from across the world. Table <ref type="table" coords="5,191.19,173.52,4.98,8.74" target="#tab_0">1</ref> gives an overview of all participants and the number of submitted runs. Note that, there was a limit of maximum 5 run submissions per team. All submitted runs were automatic runs denoting the fact that all participating systems automatically generated answers to the provided questions in the test set.</p><p>Overall, most participants used deep learning techniques to build their VQA-Med systems. In particular, participant systems <ref type="bibr" coords="5,341.14,248.79,13.05,8.74" target="#b13">[14]</ref><ref type="bibr" coords="5,354.19,248.79,4.35,8.74" target="#b14">[15]</ref><ref type="bibr" coords="5,358.54,248.79,13.05,8.74" target="#b15">[16]</ref><ref type="bibr" coords="5,373.24,248.79,12.73,8.74" target="#b17">18]</ref> leveraged sequence to sequence learning and encoder-decoder-based frameworks <ref type="bibr" coords="5,391.67,260.74,8.49,8.74" target="#b8">[9]</ref><ref type="bibr" coords="5,400.16,260.74,4.24,8.74" target="#b9">[10]</ref><ref type="bibr" coords="5,404.40,260.74,12.73,8.74" target="#b10">[11]</ref> utilizing deep convolutional neural networks (CNN) to encode medical images (with or without using pre-trained models such as VGG <ref type="bibr" coords="5,324.01,284.65,14.61,8.74" target="#b11">[12]</ref>, ResNet <ref type="bibr" coords="5,380.25,284.65,15.50,8.74" target="#b12">[13]</ref> etc.) and recurrent neural networks (RNN) to generate question encodings (with or without using pre-trained word embeddings). Some participants formulated the VQA-Med task as a multi-label multi-class classification problem <ref type="bibr" coords="5,348.05,320.52,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="5,365.21,320.52,12.73,8.74" target="#b16">17]</ref> while others considered it as a generation task <ref type="bibr" coords="5,236.52,332.48,14.60,8.74" target="#b17">[18]</ref>. Participants also used attention-based mechanisms <ref type="bibr" coords="5,134.77,344.43,13.05,8.74" target="#b14">[15]</ref><ref type="bibr" coords="5,147.81,344.43,4.35,8.74" target="#b15">[16]</ref><ref type="bibr" coords="5,152.16,344.43,13.05,8.74" target="#b16">[17]</ref> to identify relevant image features to answer the given questions. The submitted runs also varied with the use of various VQA networks such as stacked attention networks (SAN) <ref type="bibr" coords="5,253.53,368.34,14.61,8.74" target="#b14">[15]</ref>, the use of advanced techniques such as multimodal compact bilinear (MCB) pooling <ref type="bibr" coords="5,314.77,380.30,15.50,8.74" target="#b14">[15]</ref> or multimodal factorized bilinear (MFB) pooling <ref type="bibr" coords="5,202.63,392.25,15.50,8.74" target="#b16">[17]</ref> to combine multimodal features, the use of embedding based topic modeling (ETM) <ref type="bibr" coords="5,237.85,404.21,14.61,8.74" target="#b16">[17]</ref>, and the use of different hyperparameters etc. Participants did not use any additional datasets except the official training and validation sets to train their models.</p><p>The overall results of the participating systems are presented in Table <ref type="table" coords="5,463.04,443.61,4.98,8.74" target="#tab_1">2</ref> to Table <ref type="table" coords="5,162.62,455.56,4.98,8.74" target="#tab_3">4</ref> for the three different metrics in a descending order of the scores (the higher the better). The relatively low BLEU scores and WBSS scores of the runs denote the difficulty of the VQA-Med task in generating similar answers as the ground truth, while higher CBSS scores suggest that some participants were able to generate relevant clinical concepts in their answers similar to the clinical concepts present in the ground truth answers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented an overview of the inaugural Medical Domain Visual Question Answering (VQA-Med) challenge conducted as a part of the ImageCLEF 2018 evaluation campaign. We discussed participant submissions and results, which demonstrated the challenges and complexities of the VQA-Med task. In the future, we would consider the interesting data analyses and improvement suggestions presented in <ref type="bibr" coords="7,241.76,439.69,13.05,8.74" target="#b14">[15]</ref><ref type="bibr" coords="7,254.81,439.69,4.35,8.74" target="#b15">[16]</ref><ref type="bibr" coords="7,259.16,439.69,13.05,8.74" target="#b16">[17]</ref> and plan to increase the dataset size to leverage the power of advanced deep learning algorithms towards improving the state-ofthe-art in visual question answering in the medical domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,180.86,504.84,253.64,7.89;3,238.51,278.82,138.33,176.57"><head>Question:Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images with associated question-answer pairs.</figDesc><graphic coords="3,238.51,278.82,138.33,176.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,141.90,553.90,330.99,82.37"><head>Table 1 .</head><label>1</label><figDesc>Participating groups.</figDesc><table coords="5,141.90,573.95,87.40,6.14"><row><cell>Team</cell><cell>Institution</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,216.47,143.87,182.42,213.62"><head>Table 2 .</head><label>2</label><figDesc>BLEU scores of all submitted runs.</figDesc><table coords="6,258.21,162.90,98.93,194.59"><row><cell cols="3">Team Run ID BLEU</cell></row><row><cell cols="2">UMMS 6113</cell><cell>0.162</cell></row><row><cell cols="2">UMMS 5980</cell><cell>0.160</cell></row><row><cell cols="2">UMMS 6069</cell><cell>0.158</cell></row><row><cell cols="2">UMMS 6091</cell><cell>0.155</cell></row><row><cell>TU</cell><cell>5994</cell><cell>0.135</cell></row><row><cell>NLM</cell><cell>6084</cell><cell>0.121</cell></row><row><cell>NLM</cell><cell>6135</cell><cell>0.108</cell></row><row><cell>TU</cell><cell>5521</cell><cell>0.106</cell></row><row><cell>NLM</cell><cell>6136</cell><cell>0.106</cell></row><row><cell>TU</cell><cell>6033</cell><cell>0.103</cell></row><row><cell>NLM</cell><cell>6120</cell><cell>0.085</cell></row><row><cell>NLM</cell><cell>6087</cell><cell>0.083</cell></row><row><cell>JUST</cell><cell>6086</cell><cell>0.061</cell></row><row><cell>FSTT</cell><cell>6183</cell><cell>0.054</cell></row><row><cell>JUST</cell><cell>6038</cell><cell>0.048</cell></row><row><cell>JUST</cell><cell>6134</cell><cell>0.036</cell></row><row><cell>FSTT</cell><cell>6220</cell><cell>0.028</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,216.09,421.40,183.19,213.62"><head>Table 3 .</head><label>3</label><figDesc>WBSS scores of all submitted runs.</figDesc><table coords="6,257.58,440.42,100.19,194.59"><row><cell cols="3">Team Run ID WBSS</cell></row><row><cell cols="2">UMMS 6069</cell><cell>0.186</cell></row><row><cell cols="2">UMMS 6113</cell><cell>0.185</cell></row><row><cell cols="2">UMMS 5980</cell><cell>0.184</cell></row><row><cell cols="2">UMMS 6091</cell><cell>0.181</cell></row><row><cell>NLM</cell><cell>6084</cell><cell>0.174</cell></row><row><cell>TU</cell><cell>5994</cell><cell>0.174</cell></row><row><cell>NLM</cell><cell>6135</cell><cell>0.168</cell></row><row><cell>TU</cell><cell>5521</cell><cell>0.160</cell></row><row><cell>NLM</cell><cell>6136</cell><cell>0.157</cell></row><row><cell>TU</cell><cell>6033</cell><cell>0.148</cell></row><row><cell>NLM</cell><cell>6120</cell><cell>0.144</cell></row><row><cell>NLM</cell><cell>6087</cell><cell>0.130</cell></row><row><cell>JUST</cell><cell>6086</cell><cell>0.122</cell></row><row><cell>JUST</cell><cell>6038</cell><cell>0.104</cell></row><row><cell>FSTT</cell><cell>6183</cell><cell>0.101</cell></row><row><cell>JUST</cell><cell>6134</cell><cell>0.094</cell></row><row><cell>FSTT</cell><cell>6220</cell><cell>0.080</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,217.49,116.91,180.37,213.62"><head>Table 4 .</head><label>4</label><figDesc>CBSS scores of all submitted runs.</figDesc><table coords="7,259.23,135.94,96.90,194.59"><row><cell cols="3">Team Run ID CBSS</cell></row><row><cell>NLM</cell><cell>6120</cell><cell>0.338</cell></row><row><cell>TU</cell><cell>5521</cell><cell>0.334</cell></row><row><cell>TU</cell><cell>5994</cell><cell>0.330</cell></row><row><cell>NLM</cell><cell>6087</cell><cell>0.327</cell></row><row><cell>TU</cell><cell>6033</cell><cell>0.324</cell></row><row><cell>FSTT</cell><cell>6183</cell><cell>0.269</cell></row><row><cell>FSTT</cell><cell>6220</cell><cell>0.262</cell></row><row><cell>NLM</cell><cell>6136</cell><cell>0.035</cell></row><row><cell>NLM</cell><cell>6084</cell><cell>0.033</cell></row><row><cell>NLM</cell><cell>6135</cell><cell>0.032</cell></row><row><cell>JUST</cell><cell>6086</cell><cell>0.029</cell></row><row><cell cols="2">UMMS 6069</cell><cell>0.023</cell></row><row><cell cols="2">UMMS 5980</cell><cell>0.021</cell></row><row><cell cols="2">UMMS 6091</cell><cell>0.017</cell></row><row><cell cols="2">UMMS 6113</cell><cell>0.016</cell></row><row><cell>JUST</cell><cell>6038</cell><cell>0.015</cell></row><row><cell>JUST</cell><cell>6134</cell><cell>0.011</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,624.92,104.25,7.86"><p>http://www.visualqa.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,635.88,126.99,7.86"><p>http://www.imageclef.org/2018</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,144.73,646.84,147.76,7.86"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,144.73,657.79,210.60,7.86"><p>http://www.cs.cmu.edu/â¼ark/mheilman/questions/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,516.48,342.25,7.86;7,146.91,527.44,333.68,7.86;7,146.91,538.40,237.66,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,372.81,516.48,107.79,7.86;7,146.91,527.44,158.76,7.86">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,340.21,527.44,140.38,7.86;7,146.91,538.40,184.68,7.86">ACL-2002: 40th Annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">311318</biblScope>
		</imprint>
	</monogr>
	<note>PDF</note>
</biblStruct>

<biblStruct coords="7,138.35,549.07,342.24,7.86;7,146.91,560.03,305.52,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,314.75,549.07,165.84,7.86;7,146.91,560.03,176.88,7.86">BIOSSES: a semantic sentence similarity estimation system for the biomedical domain</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Soancolu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ztrk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,327.98,560.03,58.67,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,570.70,342.25,7.86;7,146.91,581.66,333.68,7.86;7,146.91,592.62,212.56,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,287.91,570.70,143.21,7.86">Verbs semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,436.92,570.70,43.68,7.86;7,146.91,581.66,317.78,7.86">InProceedings of the 32nd annual meeting on Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994-06">1994, June</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,603.29,342.24,7.86;7,146.91,614.25,333.68,7.86;7,146.91,625.21,161.34,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,292.20,603.29,188.39,7.86;7,146.91,614.25,94.52,7.86">Question Generation via Overgenerating Transformations and Ranking</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CMU-LTI-09-013</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Language Technologies Institute ; Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,138.35,635.88,342.25,7.86;7,146.91,646.84,333.68,7.86;7,146.91,657.79,333.68,7.86;8,146.91,120.67,333.68,7.86;8,146.91,131.63,333.68,7.86;8,146.91,142.59,246.79,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,146.91,120.67,271.81,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.43,120.67,54.17,7.86;8,146.91,131.63,333.68,7.86;8,146.91,142.59,119.31,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Association</title>
		<title level="s" coord="8,303.68,142.59,61.35,7.86">Springer LNCS</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,153.55,342.24,7.86;8,146.91,164.51,333.68,7.86;8,146.91,175.46,333.67,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,442.93,153.55,37.66,7.86;8,146.91,164.51,333.68,7.86;8,146.91,175.46,72.74,7.86">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garca Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,226.19,175.46,126.15,7.86">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="8,358.74,175.46,117.63,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,186.42,342.24,7.86;8,146.91,197.38,243.89,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,223.21,197.38,133.94,7.86">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,364.69,197.38,20.89,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,208.34,342.25,7.86;8,146.91,219.30,333.68,7.86;8,146.91,230.26,333.68,7.86;8,146.91,241.22,333.67,7.86;8,146.91,252.18,333.68,7.86;8,146.91,263.14,48.77,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,255.11,241.22,225.48,7.86;8,146.91,252.18,41.77,7.86">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garca Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,195.98,252.18,284.61,7.86;8,146.91,263.14,14.75,7.86">Proceedings of the 9th International Conference of the CLEF Association</title>
		<meeting>the 9th International Conference of the CLEF Association</meeting>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,274.09,342.24,7.86;8,146.91,285.05,153.18,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,333.07,274.09,147.52,7.86;8,146.91,285.05,121.22,7.86">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,296.01,337.97,7.86;8,146.91,306.97,163.80,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,352.82,296.01,127.77,7.86;8,146.91,306.97,86.53,7.86">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>NIPS: 3104-3112</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,317.93,337.97,7.86;8,146.91,328.89,333.68,7.86;8,146.91,339.85,263.83,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,258.69,328.89,221.90,7.86;8,146.91,339.85,174.94,7.86">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Glehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,328.96,339.85,30.18,7.86">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,350.81,337.97,7.86;8,146.91,361.77,197.16,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,322.65,350.81,157.94,7.86;8,146.91,361.77,122.81,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,372.73,337.98,7.86;8,146.91,383.68,118.91,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,343.25,372.73,137.34,7.86;8,146.91,383.68,46.11,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,200.28,383.68,23.34,7.86">CVPR</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,394.64,337.98,7.86;8,146.91,405.60,333.68,7.86;8,146.91,416.56,232.96,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,369.17,394.64,111.43,7.86;8,146.91,405.60,302.02,7.86">Deep Neural Networks and Decision Tree classifier for Visual Question Answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ben Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,455.89,405.60,24.70,7.86;8,146.91,416.56,102.00,7.86">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,256.21,416.56,119.44,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,427.52,337.98,7.86;8,146.91,438.48,333.67,7.86;8,146.91,449.44,286.33,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,178.60,438.48,301.99,7.86;8,146.91,449.44,18.02,7.86">NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,172.52,449.44,129.76,7.86">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,309.59,449.44,119.43,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,460.40,337.98,7.86;8,146.91,471.36,333.68,7.86;8,146.91,482.31,152.12,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,319.78,460.40,160.81,7.86;8,146.91,471.36,220.36,7.86">Employing Inception-Resnet-v2 and Bi-LSTM for Medical Domain Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,374.87,471.36,105.72,7.86;8,146.91,482.31,21.16,7.86">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,175.38,482.31,119.43,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,493.27,337.98,7.86;8,146.91,504.23,333.68,7.86;8,146.91,515.19,123.66,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,332.71,493.27,147.88,7.86;8,146.91,504.23,192.91,7.86">UMass at ImageCLEF Medical Visual Question Answering (Med-VQA) 2018 Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.09,504.23,129.27,7.86">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,146.91,515.19,119.44,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,526.15,337.97,7.86;8,146.91,537.11,291.45,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,323.16,526.15,157.43,7.86;8,146.91,537.11,23.04,7.86">JUST at VQA-Med: A VGG-Seq2Seq Model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,177.63,537.11,129.77,7.86">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,314.70,537.11,119.44,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
