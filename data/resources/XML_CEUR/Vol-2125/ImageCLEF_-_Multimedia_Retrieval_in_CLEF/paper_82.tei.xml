<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.68,115.96,304.00,12.62;1,176.25,133.89,262.86,12.62">ImageCLEF 2018: Semantic descriptors for Tuberculosis CT Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,147.20,171.74,95.64,8.74"><forename type="first">Abdelkader</forename><surname>Hamadi</surname></persName>
							<email>abdelkader.hamadi@univ-mosta.dzdjamel.ed.y@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.87,171.74,112.29,8.74"><forename type="first">Djamel</forename><forename type="middle">Eddine</forename><surname>Yagoub</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.68,115.96,304.00,12.62;1,176.25,133.89,262.86,12.62">ImageCLEF 2018: Semantic descriptors for Tuberculosis CT Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">416AB27E8B0E6AE59213E74D818681A3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Tuberculosis Task</term>
					<term>Deep Learning</term>
					<term>CT Image</term>
					<term>Tuberculosis CT Image Classification</term>
					<term>Tuberculosis Severity Scoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we present our methodologies used in our participation at the two sub-tasks of the ImageCLEF 2018 Tuberculosis Task (TBT and SVR task). We proposed to extract a single semantic descriptor of 3D CT image to describe each patient rather than using all his slices as separate samples. In TBT task, the resulting descriptors are then exploited in a second learning stage to identify the type of tuberculosis among five given classes. In SVR task, the same experimental design is used to predict the degree of severity of the disease. We reached a Kappa coefficient value of about 0.0629 in TBT sub-task, and our best run on SVR was ranked 12 th out of 36 submission and 5 th out of 7 participant teams. We believe that our approach could give better results if applied properly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis is an infectious disease caused by a bacterium called Bacillus microbacterium tuberculosis. With a high mortality rate in the world, this disease remained one of the top ten causes of death in the world in 2015. Diagnosing this sickness quickly and accurately is a vital goal that would limit its invasion and damage. One of the major problems of this disease is that traditional tests produce inaccurate or too long results. For these reasons, researchers have been interested in this disease diagnosis, particularly in the context of the international challenge ImageCLEF 2017 <ref type="bibr" coords="1,285.07,608.30,10.52,8.74" target="#b1">[3]</ref> and ImageCLEF 2018 <ref type="bibr" coords="1,397.45,608.30,10.52,8.74" target="#b7">[9]</ref> where two tasks (three tasks in ImageCLEF 2018) have been reserved for it. The first aims to detect multi-drug resistant (MDR) status of patients. The goal of the second task is to identify the type of tuberculosis. A third task has been introduced in ImageCLEF 2018 <ref type="bibr" coords="1,227.87,656.12,10.51,8.74" target="#b3">[5]</ref> which consists to predict the degree of severity of the patient's case. In all the three tasks, the predictions are based on 3D CT scans images. Algorithms involving deep learning have been tested to diagnose the presence or the absence of tuberculosis. The results obtained were interesting. However, they must be improved for better control and effective diagnosis, helping doctors to make the decisions and to choose the necessary treatments at the right time.</p><p>We can summarize the objectives of the Tuberculosis task through the following points:</p><p>-Helping medical doctors in the diagnosis of drug-resistant TB and TB type identification through image processing techniques; -Introducing work towards inexpensive and quick methods for early detection of the MDR status and TB types in patients; -Predicting quickly the type of TB and its severity degree to help doctors to make quick decisions and give the effective treatments.</p><p>We present in the following our work that has been made in the context of our participation to the two sub-tasks of ImageCLEF 2018 Tuberculosis Task: Tu-Berculosis Types classification (TBT) and Tuberculosis Severity Scoring (SVR).</p><p>The remainder of this article is organized as follows. Section 2 describes the two tasks to which we had participated. In section 3, we present our contribution by detailing the system deployed to complete our submissions. Section 4 details our experimental protocols followed to generate our predictions. We detail and analyze in the same section the results obtained. We conclude in the last section by presenting our perspectives and future works.</p><p>2 Participation to imageCLEF 2018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks description</head><p>In this paper, we focus on our participation in the TBT and the SVR sub-tasks that we describe in the following sections.</p><p>In both tasks the data is provided as 3D CT scans. For some patients several 3D CT scans are given while for some others only one is provided. All the CT images are stored in NIFTI file format with .nii.gz extension file (g-zipped .nii files). For each of the 3-dimensions of the CT image, we find a number of slices varying from about 50 to 400. Each slice has a size of about 512×512 pixels.</p><p>A training collection is provided at the beginning of the task with its groundtruth (labels of samples). Participants prepare and train their systems on this dataset. A test collection is provided at a later date. Participants interrogate their system and return their predictions to the organizers' committee. An evaluation is performed by the latter to compare the performance of the systems.</p><p>TBT task consists of the automatic categorization of TB cases in 5 target classes based on CT scans of patients. The five types considered are:</p><p>1. Infiltrative 2. Focal, 3. Tuberculoma 4. Miliary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fibro-cavernous</head><p>The results will be evaluated using unweighted Cohens Kappa and accuracy.</p><p>SVR task aims to predict the degree of severity of TB cases. Given a TB patient, the main goal is to predict its severity score based on his 3D CT scan.The degree of severity is modeled according to 5 discrete values : from 1 ("critical/very bad") to 5 ("very good"). The score value is simplified so that values 1, 2 and 3 correspond to "high severity" class, and values 4 and 5 correspond to "low severity".</p><p>The classification problem are evaluated using ROC-curves (AUC) produced from the probabilities provided by the participants. For the regression problem, the root mean square error (RMSE) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our contribution</head><p>We proposed to extract semantic descriptors from 3D CT scans. We noticed that participants of the ImageCLEF TBT 2017 task used each extracted slice as a separate sample. Thus, hundreds of slices are considered as separate learning samples while these slices represent the same patient. This introduces a lot of noise. In addition, each slice will be assigned the label of the patient (its type) even those whose content does not present any information to identify the type of TB case. This introduces more noise. The majority of the participants <ref type="bibr" coords="3,453.92,497.42,15.50,8.74" target="#b9">[11]</ref> of ImageCLEF 2017 highlighted this problem and its impact on the results.</p><p>To overcome this problem, we believe that the simplest solution is to produce a single descriptor for each patient. This constitutes the key idea of our contribution.</p><p>Our proposed system goes through three main stages:</p><p>1. Input data pre-processing 2. Features extraction 3. Learning a classification model We will detail each step in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input data pre-processing</head><p>We remind that in both tasks, 3D CT scans are provided in compressed Nifti format. Firstly, we decompress the files and extract the slices. At the end, we have three sets of slices corresponding to the three dimensions of the 3D image. For each dimension and for each Nifti image we obtain a number of slices ranging from 50 to 400 jpeg images.</p><p>The visual content of the images extracted from the different dimensions is not similar. Indeed, the images of each dimension are taken with from a different angle of view.We noticed from our experiments that the slices of the -Ydimension give better results compared to the two others (X and Z). However, the following steps can be applied to slices of any of the three dimensions. On the other hand, not all slices necessarily contain relevant information that can be useful to identify types of TB. This is why, it is essential to filter slices by keeping only those that can be informative and may contain relevant information. Moreover, since we want to extract a single descriptor per patient, it is essential to keep the same number of slices for each patient. We found that there is usually a maximum of 60 slices visually informative. Since the slices are ordered, the 60 most informative are usually at the center of the list. We propose then to keep the 60 middle slices. This is not optimal but we opted for this choice for a fully automatic approach. This choice can be improved by performing a manual filtering with the intervention of a human expert, preferably with medical skills on TB disease. Figure <ref type="figure" coords="4,258.44,570.41,4.98,8.74" target="#fig_0">1</ref> summarizes the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features extraction</head><p>After slices extraction and filtering, we propose to extract a single descriptor per patient. The transfer learning presents in this context an interesting track that can be exploited. The results of SGEast <ref type="bibr" coords="4,313.21,644.16,15.50,8.74" target="#b9">[11]</ref> and even other teams in the same task of ImageCLEF 2017 proved the efficiency of this approach <ref type="bibr" coords="4,417.36,656.12,10.52,8.74" target="#b2">[4,</ref><ref type="bibr" coords="4,429.53,656.12,11.62,8.74" target="#b9">11]</ref>. Indeed, SGEast opted for the transfer learning where they exploited the output of a Resnet-50 <ref type="bibr" coords="5,181.58,130.95,10.52,8.74" target="#b6">[8]</ref> deep learner layer. However, this idea presents a problem of the resulting descriptor size. Indeed, for example, SGeast considered a descriptor per slice and not per patient. However, since we want to have a single descriptor, it is important that the information extracted from each slice must not be very large. Therefore, we propose to describe each slice by semantic information. This idea is inspired by the work presented in <ref type="bibr" coords="5,315.17,190.72,9.96,8.74" target="#b5">[7]</ref>.</p><p>So, we choose to exploit the probabilities predicted by a deep learner trained on the set of slices. If K is the number of classes considered, this information typically corresponds to the K predicted probability values for the K classes (five probabilities of the five types for the TBT task, or the five severity degrees for the SVR task). We obtain then for each slice K values corresponding to the number of the considered classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slices, labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning a classification model</head><p>In this step, we propose to exploit the semantic descriptors of patients obtained in the previous step. Any approach of supervised classification can be applied as shown in figure <ref type="figure" coords="6,204.31,160.06,3.87,8.74" target="#fig_2">3</ref>. We recommend for this step some ideas:</p><p>-To use a deep learner having as input the semantic descriptors of patients and their labels. As an alternative, we propose to use a bagging method that collaborates several learners and sub-samples the train collection. This would lead to better results as our experiments showed. -To apply a samples selection, especially in the TBT task where several CT images were provided for some patients. We noticed in our experiments that using all the images for each patient introduces a lot of noise and would give less good results than using only one image per patient. An alternative consists of creating multiple sub-collections where each one contains a different single CT image per patient, and generating then a learner on each sub-collection to aggregate finally their results. This would probably lead to a much more robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>We describe in the following sections our runs submitted to the TBT and SVR tasks.</p><p>We implemented the semantic descriptor approach described in section 3. We used for that the following tools:</p><p>-The Caffe frawework <ref type="bibr" coords="7,245.30,148.37,15.50,8.74" target="#b8">[10]</ref> for deep learning; -Weka <ref type="bibr" coords="7,178.54,159.82,10.52,8.74" target="#b4">[6]</ref> for testing several learning and classification algorithms; -med2image [1] for the conversion of nifti medical images to the classic Jpeg format.</p><p>We chose to use slices of the -Y-dimension because our experiments showed that they are more suitable than those of the two others and got better results.</p><p>For descriptors extraction, our approach consists to learn a deep model to generate semantic information. Unfortunately, we had problems with our machines deployed for training our deep learner. Due to lack of time, we could not achieve the learning process. As an alternative to this step, we deployed the same model as the one proposed by the SGeast team <ref type="bibr" coords="7,378.70,282.84,15.50,8.74" target="#b9">[11]</ref> at the CLEF 2017 TBT Task. The model is accessible from the following link <ref type="bibr" coords="7,395.63,294.79,9.96,8.74" target="#b0">[2]</ref>. It is based on a Resnet-50 <ref type="bibr" coords="7,180.65,306.75,10.51,8.74" target="#b6">[8]</ref> and got the best results at the TBT task of 2017 edition. We have therefore exploited the outputs of the last layer (named prob) of the Resnet-50 corresponding to the probabilities of the 5 considered classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TBT task</head><p>Dataset: The dataset used in TBT tasks includes chest CT scans of TB patients along with the TB type. Some patients include more than one scan. All scans belonging to the same patient present the same TB type. Table <ref type="table" coords="7,421.23,400.37,4.98,8.74" target="#tab_0">1</ref> summarizes the distribution of CT scans according to the five types of TB considered. Experimental protocol: We used the train collection provided by the organizers and we split it into two sub-collections: 80% for training and 20% as validation set. We have exploited in all our runs the semantic descriptors generated as previously described. We tested several learners in the classification step. We finaly submitted three main runs. The other submissions are some variants or are generated through the fusion of some of these three runs:</p><p>-Run 1 (TBT mostaganemFSEI run1): random forest as supervised classifier.</p><p>We tuned the two parameters referring to the number of iterations performed and the number of features selected randomly; -Run 2 (TBT mostaganemFSEI run2): bagging of a set of random forest learners. We tuned the number of learners for the bagging and the same two parameters as Run1 for random forest; -Run 4 (TBT mostaganemFSEI run4): A hierarchical classification. We organized the five 5 classes into a hierarchical structure as described in figure <ref type="figure" coords="8,472.84,202.94,3.87,8.74" target="#fig_3">4</ref>.</p><p>We have created two new virtual classes V -1 and V -2. V -2 regroups the three classes Type Type 2, and V -2 contains the classes Type 3, Type 4 and Type 5. We have reorganized our collections in order to achieve a classification on two different levels. In the first stage, we classify the samples into two virtual classes V -1 and V -2. In the second level of classification, we performed a classification of the samples regarding the set of classes of the predicted class in the previous stage. In two classification process we used a random forest learner by tuning its two parameters as described for Run1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V-1 V-2</head><p>Second level : Type1 Type 2 Type3 Type 4 Type 5</p><p>First level : Results: Table <ref type="table" coords="8,205.47,524.11,4.98,8.74" target="#tab_1">2</ref> shows the results obtained by our runs on validation collection. Table <ref type="table" coords="8,177.09,644.16,4.98,8.74" target="#tab_2">3</ref> shows the results obtained by our runs on the evaluation performed by the ImageCLEFcommittee. As shown on validation results, Run 4 has been our best submission and got also the best results on test collection compared to run 1 and run 2. Although the results achieved by our submissions are not well ranked compared to those of the top of the list, we can notice that several runs belong to the same teams that had good results, and they probably do not differ too much. On the other hand, we recall that our semantic descriptors were extracted using a model that was not very well trained. In fact, we met problems with our machines during the training of our deep learner. Indeed, although SGEast's deployed model got the best results at ImageCLEF 2017 Tuberculosis TBT task, we did not have the ability to perform exactly the same pre-processing performed by this team as described in <ref type="bibr" coords="9,257.95,632.21,14.61,8.74" target="#b9">[11]</ref>. We believe that our semantic descriptors could give better results if they are extracted from a more adapted and well-developed deeper model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SVR task</head><p>Dataset: The dataset for SVR task includes chest CT scans of TB patients along with the corresponding severity score (1 to 5). Scores from 1 to 3 correspond to the "High" severity whereas the two scores 4 and 5 refer to the "Low" degree of severity. Table <ref type="table" coords="10,213.86,375.31,4.98,8.74" target="#tab_3">4</ref> summarizes the distribution of CT scans according to two severity classes. Experimental protocol: We generated in a first step the semantic descriptors following the approach described in the section 3. For the prediction of TB severity scores, we treated the problem as a classification problem. We used for this two approaches :</p><p>1. Multi-class classification problem: we considered the five scores as separate classes. We then tested several classifiers. We selected two that have been most effective compared to those tested: Random forest, bagging of a set of random forest learners. 2. Hierarchical classification: We organized our data in order to carry out a hierarchical classification. We considered the hierarchy described in figure <ref type="figure" coords="10,472.84,644.16,3.87,8.74" target="#fig_6">7</ref>.</p><p>Then, a two-level hierarchical classification is carried out. In the first level the samples are classified into "High" or "Low" classes. In the second level, the samples are reclassified into the descending classes of the one predicted in the first level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High Low</head><p>Second level : 1 2 3 4 5</p><p>First level : We submitted five runs:</p><p>1. Run 1 (SVR mostaganemFSEI run1): Multi-class model using Random forest as classifier. We tuned the two parameters : the number of iterations performed and the number of features randomly chosen; 2. Run 2 (SVR mostaganemFSEI run2) : Multi-class model using a bagging of a set of random forest learners with sub-sampling of the main train collection. We created two sub-collections by balancing the number of samples for the 5 classes. We then merged the results obtained by the two sub-collections; 3. Run 3 (SVR mostaganemFSEI run3): Hierarchical classification using a Bagging of a set of Random forest learners in each level of the hierarchical classification process. 4. Run (SVR mostaganemFSEI run4): fusion of Run 1 and Run 2 5. Run 6 (SVR mostaganemFSEI run6): fusion of Run 3 and Run 1 Results: Table <ref type="table" coords="11,205.47,533.62,4.98,8.74" target="#tab_4">5</ref> shows the results obtained by our runs on validation collection. Table <ref type="table" coords="12,177.09,118.99,4.98,8.74" target="#tab_5">6</ref> shows the results obtained by our runs on the evaluation performed by the ImageCLEF committee on test collection. We can see that our Run 3 got best results in terms of RMSE compared to our other runs on validation collection and even on test data. However, in terms of AUC, Run 2 seems to be more efficient.  We can see that our best run is ranked 12 th out of 36 submissions. However, the difference between the performances of the 12 best runs is not very significant. We recall that our best result is achieved by a hierarchical classification approach using a bagging of random forest learners at each level of the hierarchy. We believe that our approach could give better results using a well-trained deep model in the semantic features extraction step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future works</head><p>We have described in this article our contributions to the TBT and SVR tasks of ImageCLEF Tuberculosis 2018. We proposed an approach that consists in extracting a single semantic descriptor for each CT image / patient instead of considering all the slices as separate samples. Unfortunately, we could not achieve the training of our deep learner. However, the results obtained show that this approach could be much more efficient and give more interesting results if it is applied properly.</p><p>As perspectives, we plan to adopt enrichment strategies and learning samples selection. Indeed, one of the characteristics of the problematic addressed in the SVR and TBT tasks is the nature of the provided data collections, which are of a small size and are noisy because of the presence of many slices that do not contain useful information. Our bagging and sub-sampling strategies adopted in our experiments confirmed this. In addition, we noticed during the sub-sampling of our data that the deletion or addition of some samples had an impact on the results. On the other hand, filtering slices effectively to keep only those that are truly informative is a key idea that could further improve system performance as reported by several participating teams <ref type="bibr" coords="13,322.41,546.91,14.61,8.74" target="#b9">[11]</ref>. Furthermore, we noticed in our experiments that there is a difference in terms of precision achieved for each studied class. Indeed, some classes are more difficult to identify than others. This is also an interesting track to study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,233.70,417.07,147.97,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pre-processing of input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,208.60,556.94,198.17,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our semantic features extraction process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,159.08,414.25,297.19,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Learning a classification model based on the semantic descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,206.18,471.61,203.00,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Hierarchical re-organization of TBT types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,149.71,254.67,330.88,8.74;9,134.77,266.63,263.16,8.74"><head>Figures 5 and 6 Fig. 5 .</head><label>65</label><figDesc>Figures5 and 6describes the results and ranking of all submissions on TBT task in terms of kappa coefficient and accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,152.51,286.51,310.33,7.89"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results and ranking in terms of accuracy on test data for TBT Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,191.36,312.45,232.64,7.89"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The hierarchy of classes considered for SVR Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="12,139.40,520.85,336.57,7.89"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results and ranking in terms of Root Mean Square Error on test collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="12,149.71,562.92,330.88,8.74;12,134.77,574.88,234.12,8.74"><head>Figures 8 and 9</head><label>9</label><figDesc>Figures 8 and 9 describes the results and ranking of all submissions on SVR task in terms of RMSE and AUC values, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,142.03,300.68,331.30,7.89"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Results and ranking in terms of Area Under ROC curve on test collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,197.20,443.32,220.95,110.26"><head>Table 1 .</head><label>1</label><figDesc>Dataset given for Tuberculosis TBT task<ref type="bibr" coords="7,405.87,443.35,9.22,7.86" target="#b7">[9]</ref>.</figDesc><table coords="7,218.07,464.22,179.22,89.36"><row><cell></cell><cell>Train</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell cols="5">TB types #Patients #CTs #Patients #CTs</cell></row><row><cell>Type 1</cell><cell>228</cell><cell>376</cell><cell>89</cell><cell>176</cell></row><row><cell>Type 2</cell><cell>210</cell><cell>273</cell><cell>80</cell><cell>115</cell></row><row><cell>Type 3</cell><cell>100</cell><cell>154</cell><cell>60</cell><cell>86</cell></row><row><cell>Type 4</cell><cell>79</cell><cell>106</cell><cell>50</cell><cell>71</cell></row><row><cell>Type 5</cell><cell>60</cell><cell>99</cell><cell>38</cell><cell>57</cell></row><row><cell>Total</cell><cell>677</cell><cell>1008</cell><cell>317</cell><cell>505</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,197.47,555.87,220.42,60.19"><head>Table 2 .</head><label>2</label><figDesc>Results on validation set for TBT task.</figDesc><table coords="8,197.47,574.92,220.42,41.14"><row><cell>Runs</cell><cell cols="2">Kappa Accuracy</cell></row><row><cell cols="2">Run 1 (TBT mostaganemFSEI run1) 0.21</cell><cell>0.38</cell></row><row><cell cols="2">Run 2 (TBT mostaganemFSEI run2) 0.25</cell><cell>0.41</cell></row><row><cell cols="2">Run 4 (TBT mostaganemFSEI run4) 0.26</cell><cell>0.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,171.31,115.91,272.73,60.19"><head>Table 3 .</head><label>3</label><figDesc>Results on test set for TBT task.</figDesc><table coords="9,171.31,134.97,272.73,41.14"><row><cell>Runs</cell><cell cols="3">Kappa Rank Accuracy Rank</cell></row><row><cell cols="2">Run 1 (TBT mostaganemFSEI run1) 0.0412 28</cell><cell>0.2650</cell><cell>29</cell></row><row><cell cols="2">Run 2 (TBT mostaganemFSEI run2) 0.0275 29</cell><cell>0.2555</cell><cell>32</cell></row><row><cell cols="2">Run 4 (TBT mostaganemFSEI run4) 0.0629 25</cell><cell>0.2744</cell><cell>27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,197.72,419.08,219.93,64.83"><head>Table 4 .</head><label>4</label><figDesc>Dataset given for Tuberculosis SVR task<ref type="bibr" coords="10,405.36,419.10,9.22,7.86" target="#b7">[9]</ref>.</figDesc><table coords="10,258.86,439.97,97.63,43.93"><row><cell></cell><cell>Train Test</cell></row><row><cell cols="2">Low severity 90 62</cell></row><row><cell cols="2">High severity 80 47</cell></row><row><cell>Total</cell><cell>170 109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,565.66,345.83,95.31"><head>Table 5 .</head><label>5</label><figDesc>Results on validation set for SVR task in terms of Accuracy and Root Mean Square Error (RMSE).</figDesc><table coords="11,197.98,597.92,219.39,63.06"><row><cell>Runs</cell><cell cols="2">Accuracy RMSE</cell></row><row><cell cols="2">Run 1 (SVR mostaganemFSEI run1) 0.41</cell><cell>0.37</cell></row><row><cell cols="2">Run 2 (SVR mostaganemFSEI run2) 0.36</cell><cell>0.45</cell></row><row><cell cols="2">Run 3 (SVR mostaganemFSEI run3) 0.56</cell><cell>0.3</cell></row><row><cell cols="2">Run 4 (SVR mostaganemFSEI run4) 0.42</cell><cell>0.36</cell></row><row><cell cols="2">Run 6 (SVR mostaganemFSEI run6) 0.48</cell><cell>0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,177.72,171.24,259.91,82.11"><head>Table 6 .</head><label>6</label><figDesc>Results on test set for SVR task.</figDesc><table coords="12,177.72,190.29,259.91,63.06"><row><cell>Runs</cell><cell>RMSE Rank AUC Rank</cell></row><row><cell cols="2">Run 1 (SVR mostaganemFSEI run1) 1.0227 19 0.5971 26</cell></row><row><cell cols="2">Run 2 (SVR mostaganemFSEI run2) 1.0837 22 0.6127 22</cell></row><row><cell cols="2">Run 3 (SVR mostaganemFSEI run3) 0.9721 12 0.5987 25</cell></row><row><cell cols="2">Run 4 (SVR mostaganemFSEI run4) 1.0137 18 0.6107 24</cell></row><row><cell cols="2">Run 6 (SVR mostaganemFSEI run6) 1.0046 16 0.6119 23</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,645.84,34.21,7.86;13,198.44,645.84,24.33,7.86;13,244.03,645.84,11.03,7.86;13,276.33,645.84,37.13,7.86;13,334.71,645.84,18.43,7.86;13,374.41,645.84,44.40,7.86;13,440.08,645.84,16.69,7.86;13,478.03,645.84,2.56,7.86;13,151.52,656.80,327.27,7.86" xml:id="b0">
	<monogr>
		<ptr target="check:30/05/2018" />
		<title level="m" coord="13,151.53,645.84,25.64,7.86;13,198.44,645.84,24.33,7.86;13,244.03,645.84,11.03,7.86;13,276.33,645.84,37.13,7.86;13,334.71,645.84,18.43,7.86;13,374.41,645.84,44.40,7.86;13,440.08,645.84,16.69,7.86">Sgeast model for imageclef 2017 tubeculosis task</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.63,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,100.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,431.14,119.67,49.45,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,23.74,7.86">Overview of the imageclef 2017 tuberculosis task -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/invitedpaper1.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="14,196.07,141.59,284.53,7.86;14,151.52,152.55,24.01,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,174.47,337.64,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.35,288.03,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,442.93,174.47,37.66,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,23.74,7.86">Overview of ImageCLEFtuberculosis 2017 -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,200.37,196.39,108.50,7.86">CLEF2017 Working Notes</title>
		<title level="s" coord="14,318.18,196.39,162.42,7.86;14,151.52,207.35,14.99,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,218.30,337.64,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,329.07,7.86;14,151.52,251.18,329.07,7.86;14,151.52,262.14,34.31,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,378.22,218.30,102.37,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,108.19,7.86">Overview of ImageCLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,279.94,240.22,103.81,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="14,390.71,240.22,89.89,7.86;14,151.52,251.18,82.70,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,273.10,337.63,7.86;14,151.52,284.03,329.07,7.89;14,151.52,295.02,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,151.52,284.06,178.30,7.86">The WEKA data mining software: update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,336.78,284.06,90.64,7.86">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,305.98,337.64,7.86;14,151.52,316.91,329.07,7.89;14,151.52,327.89,329.07,7.86;14,151.52,338.85,26.37,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,321.11,305.98,159.49,7.86;14,151.52,316.93,114.00,7.86">Extended conceptual feedback for semantic multimedia indexing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quénot</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-014-1937-y</idno>
		<ptr target="https://doi.org/10.1007/s11042-014-1937-y" />
	</analytic>
	<monogr>
		<title level="j" coord="14,273.97,316.93,98.91,7.86">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,349.81,337.64,7.86;14,151.52,360.77,159.05,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="14,299.05,349.81,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,371.73,337.63,7.86;14,151.52,382.69,329.07,7.86;14,151.52,393.65,329.07,7.86;14,151.52,404.61,329.07,7.86;14,151.52,415.56,329.07,7.86;14,151.52,426.52,329.07,7.86;14,151.52,437.48,329.07,7.86;14,151.52,448.44,46.58,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,197.95,404.61,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,415.56,329.07,7.86;14,151.52,426.52,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="14,151.52,437.48,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,459.40,337.97,7.86;14,151.52,470.36,329.07,7.86;14,151.52,481.32,154.44,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="14,237.79,470.36,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,492.28,337.98,7.86;14,151.52,503.24,329.07,7.86;14,151.52,514.19,329.07,7.86;14,151.52,525.15,174.43,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,329.02,492.28,151.57,7.86;14,151.52,503.24,112.73,7.86">Imageclef 2017: Imageclef tuberculosis task -the sgeast submission</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper130.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="14,286.45,503.24,194.14,7.86;14,151.52,514.19,123.03,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
