<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.82,152.67,321.32,12.64;1,149.66,170.67,295.82,12.64;1,240.77,188.67,109.23,12.64;1,350.11,187.07,4.50,8.10">Visual Concept Selection with Textual Knowledge for Understanding Activities of Daily Living and Life Moment Retrieval *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.78,227.58,69.67,8.96"><forename type="first">Tsun-Hsien</forename><surname>Tang</surname></persName>
							<email>thtang@nlg.csie.ntu.edu.tw</email>
						</author>
						<author>
							<persName coords="1,268.95,227.58,54.76,8.96"><forename type="first">Min-Huan</forename><surname>Fu</surname></persName>
							<email>mhfu@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.89,227.58,69.80,8.96"><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
							<email>hhhuang@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.61,239.25,59.08,8.96"><forename type="first">Kuan-Ta</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.33,239.25,59.82,8.96"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@ntu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.82,152.67,321.32,12.64;1,149.66,170.67,295.82,12.64;1,240.77,188.67,109.23,12.64;1,350.11,187.07,4.50,8.10">Visual Concept Selection with Textual Knowledge for Understanding Activities of Daily Living and Life Moment Retrieval *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F2DCB2E447FA78A7CDE946206BF944D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Concept Selection</term>
					<term>Distributed Word Representation</term>
					<term>Lifelog</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our approach to the task of ImageCLEFlifelog 2018. Two subtasks, activities of daily living understanding (ADLT) and life moment retrieval (LMRT) are addressed. We attempt to reduce the user involvement during the retrieval stage by using natural language processing technologies. The two subtasks are conducted with dedicated pipelines, while similar methodology is shared. We first obtain visual concepts from the images with a wide range of computer vision tools and propose a concept selection method to prune the noisy concepts with word embeddings in which textual knowledge is inherent. For ADLT, the retrieved images of a given topic are sorted by time, and the frequency and duration are further calculated. For LMRT, the retrieval is based on the ranking of similarity between image concepts and user queries. In terms of the performance, our systems achieve 47.87% of percentage dissimilarity in ADLT and 39.5% of F1@10 in LMRT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wearable devices for personalized multimodal recording, together with dedicated lifelogging applications for smartphones, become more popular nowadays. For example, gadgets like GoPro and Google Lens have already attracted consumers' attention, and new kinds of media like Video Weblog (VLog) emerged on Youtube heavily rely on these devices. On the other hand, numerous personalized data that are acquired, recorded, and stored still remain challenging to access by their owners. As a result, a system that supports human to make summarization and recap precious life moments is highly demanded.</p><p>In ImageCLEFlifelog 2018 <ref type="bibr" coords="2,247.13,150.18,10.91,8.96" target="#b0">[1,</ref><ref type="bibr" coords="2,258.03,150.18,7.27,8.96" target="#b1">2]</ref>, two subtasks are conducted to address the issue of image-based lifelog retrieval. The first subtask, activities of daily living understanding (ADLT), is aimed at providing a summarization of certain life events for a lifelogger. The second subtask, lifelog moment retrieval (LMRT), is aimed at retrieving specific moments in a lifelog such as shopping in a wine store. A key challenge in both subtasks is the semantic gap between the textual user queries and the visual lifelog data. Users tend to express their information needs in higher-level, abstract descriptions such as shopping, dating, and having a coffee, while the visual concepts that computer vision (CV) tools extract from images are usually a set of concrete objects such as cup, table, and television. The approaches proposed by previous work <ref type="bibr" coords="2,362.59,258.21,11.54,8.96" target="#b2">[3,</ref><ref type="bibr" coords="2,374.13,258.21,7.69,8.96" target="#b3">4]</ref> focus on dealing with the visual information. In this work, we attempt to reduce the semantic gap by using both visual information and textual knowledge. We propose a framework that integrates visual and textual information extracted from advanced CV and natural language processing (NLP) technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly discuss recent works on lifelog retrieval. For a retrieval model, relevance and diversity are two major criteria to achieve. The retrieval of relevant lifelog data is usually based on modeling the similarity between the textual concepts, from the user query, and the visual features, from the lifelog data. Diversity, on the other hand, can be improved by image clustering. For example, Liting et al. <ref type="bibr" coords="2,459.10,411.23,11.60,8.96" target="#b2">[3]</ref> propose a method based on textual concept matching and hierarchical agglomerative clustering. Ana et al. <ref type="bibr" coords="2,212.93,435.23,11.84,8.96" target="#b3">[4]</ref> propose a method based on both visual and metadata information along with clustering on results. So far, various techniques for image processing and image retrieval have been applied to lifelog retrieval, but relatively few NLP techniques are explored in this area.</p><p>As deep neural networks have achieved remarkable success in computer vision, it is also tempting to use deeply learned features in lifelog retrieval. For example, features generated from CNN are adopted to the lifelog retrieval task <ref type="bibr" coords="2,377.18,507.23,10.81,8.96" target="#b2">[3]</ref>. Models for textual concept extraction include image classification model <ref type="bibr" coords="2,349.49,519.23,11.85,8.96" target="#b4">[5]</ref> or object detection model <ref type="bibr" coords="2,124.70,531.23,10.90,8.96" target="#b5">[6,</ref><ref type="bibr" coords="2,135.60,531.23,7.27,8.96" target="#b6">7]</ref>. For textual knowledge modeling, deep neural networks also benefit distributed word representations, also known as word embeddings, where every word is represented in a vector in a dense space. There are different implementations for learning word embeddings <ref type="bibr" coords="2,197.95,567.26,11.99,8.96" target="#b7">[8,</ref><ref type="bibr" coords="2,209.94,567.26,7.99,8.96" target="#b8">9,</ref><ref type="bibr" coords="2,217.93,567.26,11.99,8.96" target="#b9">10,</ref><ref type="bibr" coords="2,229.92,567.26,11.99,8.96" target="#b10">11,</ref><ref type="bibr" coords="2,241.92,567.26,11.99,8.96" target="#b11">12,</ref><ref type="bibr" coords="2,253.91,567.26,11.99,8.96" target="#b12">13]</ref>, which can be further used in multi-modal applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval Framework</head><p>Fig. <ref type="figure" coords="2,143.63,636.26,4.98,8.96" target="#fig_0">1</ref> illustrates our proposed system. The two subtasks, ADLT and LMRT, share a similar framework. The details of our framework are introduced in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Concept Extraction</head><p>Both subtasks rely on the information from the provided image set. We extract the visual concepts from each image by using a wide range of image recognition tools. Before that, preprocessing is performed to improve the image recognition. The images in the lifelog data are automatically taken with a wearable camera so that many of them suffer from the poor quality such as overexposed, underexposed, out of focus, or ill-composed. We apply blurriness detection and pixel-wise color histogram <ref type="bibr" coords="3,408.93,230.22,11.81,8.96" target="#b3">[4]</ref> to filter out those uninformative images. For the qualified images, several image recognition tools are integrated to extract visual concepts from a variety of aspects. Image Filtering. We prune low quality images with blurriness and color diversity detection. The blurriness metric is defined based on the variation of the Laplacian. We perform convolution on each image with the Laplacian filter (3x3 kernel), and calculate the blurriness score as the variance of the convolved result. The images with a variance below a threshold are considered blurry and undesirable. Moreover, images with a high color homogeneity are also considered uninformative, and can be detected with quantized color histograms. Concept Labeling. In order to retrieve lifelog data according to the query style defined in the two subtasks, effective textual representation for images is crucial. For a given photo, we would like to know where it was being taken, what objects are in it, and even what action the lifelogger took at that moment. To extract visual concepts from different aspects, deep learning models have shown breakthrough results in recent years. Basically, general concepts and scene of images can be captured by two DenseNet <ref type="bibr" coords="3,124.70,677.78,11.72,8.96" target="#b4">[5]</ref> classifiers pre-trained on ImageNet1K <ref type="bibr" coords="3,300.37,677.78,16.82,8.96" target="#b13">[14]</ref> and Place365 <ref type="bibr" coords="3,379.40,677.78,15.42,8.96" target="#b14">[15]</ref>, respectively. We consider classes with output probability beyond the threshold as labels. The threshold is moderate to ensure the recall rate. For details present in images, object detection techniques Yolo-v2 <ref type="bibr" coords="4,207.15,174.18,11.74,8.96" target="#b4">[5]</ref> and Faster RCNN <ref type="bibr" coords="4,298.29,174.18,11.80,8.96" target="#b6">[7]</ref> are used. Both tools are pre-trained on MS COCO <ref type="bibr" coords="4,171.84,186.18,16.78,8.96" target="#b15">[16]</ref> and Open Images <ref type="bibr" coords="4,263.41,186.18,16.80,8.96" target="#b16">[17]</ref> datasets.</p><p>Open Images dataset, which consists of 15,440,132 boxes on 600 categories in a domain closer to human daily life, covers most of the topics in the subtasks. MS COCO Dataset consists of 91 objects types with 2.5 million labels in 328k images. We also utilize the image analysis function provided by Google Cloud Vision API <ref type="foot" coords="4,419.35,233.07,3.24,5.83" target="#foot_0">1</ref> . The online service provides not only fruitful labels but also supports optical character recognition (OCR), which helps detect and extract text from images. Note that the image concepts provided by organizer are also added in. After going through the above tools, an image would be tagged with concepts present in various aspects, as shown in Fig. <ref type="figure" coords="4,426.38,282.21,3.76,8.96" target="#fig_1">2</ref>. Image recognizers are hardly to perfectly label the concepts. For instance, most of the deep learning tools cannot capture the place of Fig. <ref type="figure" coords="4,345.31,634.10,4.03,8.96" target="#fig_0">1</ref>(a) except OCR, which identifies the keyword Cafe. In our framework, the ensemble of the outputs of all image recognizers is considered as a set of candidate visual concepts.</p><p>Concept Filtering. We leverage a number of state-of-the-art tools to depict an image in a set of candidate visual concepts. However, false positive concepts generated by those tools result in redundancy and noise. For example, an image relevant to interior like "bedroom" or "living room" would be supported by the visual concepts such as "couch", "bed", and "table". By contrast, the concept "church" would have lower similarities to those terms. Based on this idea, we prune the set of candidate visual concepts by removing the concepts that are less supported by other concepts, and produce a set of visual concepts for each image. We compute the semantic similarity between candidate visual concepts by using pre-trained word embeddings and construct a similarity matrix, which represents the similarity of each concept pair. We discard those concepts that would accumulate low correlations with other concepts. The procedure is illustrated in Fig. <ref type="figure" coords="5,178.46,282.21,4.98,8.96" target="#fig_2">3</ref> with a real example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing</head><p>Our framework fetches images based on a given query. In ADLT, we require users to specify concepts that highly related to the given topics and keep off confusing terms as a query term set for each topic. Moreover, time span according to the topics would be considered. To ensure the quality and usability of the retrieval system in practice, preprocessing on textual data and the retrieval algorithms are other crucial issues. Metadata Preprocessing. For generalization of the retrieval task, tags available in metadata like "Home" and "Work" in the location field and "transport" and "walking" in the activity field are extracted as attributes of images, instead of using all the location information. To proceed further with locational information, we calculate the average moving speed according to GPS coordinates to infer the type of transportation (e.g. car, airplane). For any pair of points {(lat1, lon1), (lat2, lon2)} on the geographical coordinate system, the distance d is given by the great-circle distance formula as follows.</p><formula xml:id="formula_0" coords="5,127.22,609.34,340.93,10.44">ùëë = cos -1 (sin(ùëôùëéùë°1) sin(ùëôùëéùë°2) + cos(ùëôùëéùë°1) cos(ùëôùëéùë°2) cos(|ùëôùëúùëõ2 -ùëôùëúùëõ1|)) √ó 6371ùëòùëö (1)</formula><p>and the average speed is calculated according to the distance d and the difference between timestamps.</p><p>Retrieval Model. The similarity between the user query and an individual image, represented as a set of visual concepts, is measured with three schemes as follows.</p><p>Exact Matching. Given a list of concepts in the user query that are combined with logical operators AND, OR, and NOT, the visual concepts of the image should meet the condition. This approach returns accurate results if the topic is explicit, e.g., watching TV or use cell phone in the car. BM25 Model. Exact matching suffers from low recall rate. Here, we perform the partial matching by using a classic retrieval model, BM25 <ref type="bibr" coords="6,332.32,210.18,15.54,8.96" target="#b17">[18]</ref>. The BM25 scheme measures the relatedness between two sets of concepts based on term frequency (TF) and inverse document frequency (IDF). In this way, the specific concepts are more likely to be extracted. For example, the concept "grocery", which provides specific information than the general concept "indoor" does, has a higher BM25 score due to its higher inverse document frequency. Word Embedding. Word embeddings (distributed word representations) have been widely used in text similarity measurement. For fuzzy matching, we adopt the word embeddings to measure the semantic relatedness between the concepts in the query and the concepts extracted from an image. The information of semantic relatedness would be helpful when similar but not identical concepts are present in both sides. We first obtain distributed representations of concepts with word embeddings that are pretrained on a large-scale corpus, and aggregate concept-level semantics by taking the element-wise mean for each query/image. In this way, the relatedness between the query and an image can be computed by using the cosine similarity. Note that by using the pre-trained word embeddings, external knowledge is inherent in the retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Stream Segmentation</head><p>Deeply Learned Features. Pre-trained convolutional neural networks have been shown to be beneficial for various computer vision tasks as generic image feature extractors. In this sense, we apply pre-trained CNN tools <ref type="bibr" coords="6,344.11,464.27,11.71,8.96" target="#b4">[5]</ref> to extract dense feature vectors, and estimate the level of change between consecutive images by measuring the dissimilarity of their representations using Euclidean distance and cosine similarity. Another neural network-based approach for this purpose is to compress the image by autoencoders. These methods are tempting that feature extraction can be done automatically, and the obtained feature can be integrated with other features simply.</p><p>In this work, we obtain the dense vector for each image with a pre-trained DenseNet and with a deep autoencoder trained on the provided images. For each pair of two consecutive images, a threshold of dissimilarity is heuristically tuned to determine whether the two images belong to the same event cluster or not. Besides, smoothing methods such as moving average should also be adopted to prevent the consecutive boundary occur in a short time period. Event Interval. In addition to the features learned by models, human knowledge is also involved. Due to the property of lifelog data and given topics, human beings can easily figure out how long it takes for a daily activity. Therefore, our other approach is grouping the images by reasonable interval of shooting times. That is, two consecutively retrieved images would treat as a single event if the difference of their timestamps is smaller than a threshold according to the event topic. The thresholds for each topic are intuitive defined by human. For example, the reasonable event interval would be about 60 minutes for having a lunch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Daily Activities Summarization</head><p>For ADLT, the system output should be two real values indicating the frequency and total duration of the given topic, respectively. The frequency could be calculated by summing up the number of segmented retrieved events. The duration is obtained by summing up the time differences between the first frame and last frame of each event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Life Moment Retrieval</head><p>For LMRT, the model is designed to retrieve a number of images indicating specific life moments with respect to the target query. There are total 10 topics, each consists of a title, a short description, and a longer narrative in detail. For query processing, we extract key terms in the title and in the description only, since the intention in the narrative is often too complicated to extract without human assistance. This can be done by simply removing function words in each query. Note that further processing such as stemming is unnecessary due to the use of word embeddings. The resulting concepts can be further improved by human, according to the narrative or human's general knowledge. Finally, the query is transformed into vector representation and compared with pre-computed vector representation of each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For the two subtasks, automatic runs are submitted. For each of the subtasks, we first describe the parameter settings, and show the experimental results. We only report the execution time of our system for testing since we exploit pre-trained tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Activities of Daily Living Understanding Task</head><p>In the first trial of ADLT, we use the query consisting of the concepts automatically parsed from the given topics. We seize nouns and gerunds as concepts and list them for query. For the time condition, information is directly provided with the &lt;span&gt; tag. However, some of the required concepts like socializing or having party seldom appear in the visual concept sets since most of CV tools are trained with shallow semantic image descriptions. To deal with this issue, the topics consisting of abstract ideas are refined by human.</p><p>As mentioned in previous sections, visual concepts of each image labeled by CV tools are far from perfect. For this reason, visual concept filtering is applied to discard the concepts with a low relevance to other concepts in the same image. After we obtain a list of visual concepts sorted by the row sum of similarity matrix for each image, the top half of the concepts are retained. To our observation, this is a flexible threshold. Besides, two kinds of pre-trained word embeddings are brought in here, including GloVe <ref type="bibr" coords="8,153.73,150.18,11.72,8.96" target="#b8">[9]</ref> trained on Common Crawl with 840B tokens and ConceptNet Numberbatch <ref type="bibr" coords="8,124.70,162.18,10.68,8.96" target="#b7">[8]</ref>. The comparison in percentage dissimilarity <ref type="bibr" coords="8,316.17,162.18,11.69,8.96" target="#b0">[1]</ref> is shown in Table <ref type="table" coords="8,403.36,162.18,3.71,8.96" target="#tab_0">1</ref>, where (G) and (N) denote GloVe and ConceptNet Numberbatch word vectors, respectively. For summarization, two types of image stream segmentation are tried on the development set. We out that the segmentation based on deeply learned features results unstable boundaries and comes out a limited performance. Basically, it is hard to determine the threshold of dissimilarity between two images. On the other hand, using human defined reasonable interval for each event type, deemed an intuitive parameter for daily activities, provides more sensible boundaries. As a result, we apply the latter method on the test set.</p><p>In general, our framework achieves a percentage dissimilarity of 0.3850 with human refined query and visual concept filtering reported by the online evaluation platform. The inference time for each topic is 5.59s on average. Error analysis shows that our system fetches a lot of unnecessary images for topic 1 because the CV tools cannot identify "coffee cup" in the lifelog data. For this reason, we refine the query by using only "cup", instead of "coffee cup", to ensure the recall rate. It turns out that images with bottles or mug are retrieved (under the Office scenario).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4. Examples of coffee capturing based on RGB constraints</head><p>To enhance the precision of topic 1, an ad-hoc method is further introduced to filter out surplus retrieved items. First, we observe that the coffees might be bought by the lifelogger from the same shop within the same red cup. So we specify the upper and the lower bounds of RGB value to capture red objects in a given shoot. For the consideration of spatial verification, we preserve images containing red area larger than a threshold. The procedures of the above operations are shown in Fig. <ref type="figure" coords="9,374.95,186.18,3.76,8.96">4</ref>. By distinguishing the red coffee cup from other types of cup, the results of topic 1 are greatly enhanced, and the overall performance is also increased as shown in Table <ref type="table" coords="9,365.75,210.18,3.76,8.96" target="#tab_0">1</ref>.</p><p>The best result among our all submissions, i.e., percentage dissimilarity of 0.4787, is achieved by the combination of fine-tuned query, concept selection mechanism with ConceptNet Numberbatch word embedding and coffee capturing trick. The ad-hoc image filtering for topic 1 produces a significant improvement. For the construction of the similarity matrix used in visual concept filtering, the ConceptNet Numberbatch embeddings, built using an ensemble that combines data from ConceptNet <ref type="bibr" coords="9,393.67,282.21,10.77,8.96" target="#b7">[8]</ref>, word2vec <ref type="bibr" coords="9,451.44,282.21,15.41,8.96" target="#b9">[10]</ref>, GloVe <ref type="bibr" coords="9,154.92,294.21,10.69,8.96" target="#b8">[9]</ref>, and OpenSubtitles2016 <ref type="bibr" coords="9,270.84,294.21,15.46,8.96" target="#b18">[19]</ref>, provide better results comparing to GloVe. The effectiveness of concept filtering is shown in Fig. <ref type="figure" coords="9,342.91,306.21,3.77,8.96" target="#fig_3">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Life Moment Retrieval Task</head><p>For LMRT, we submit a total of four successful runs, which are either fully automatic or semi-automatic with human fine-tuned queries. In this subtask, we use a subset of CV tools based on a preliminary evaluation on the development set. The relatedness between images and queries are measured with the word embedding method as described in Section 3.2. We employ the 300-dimenstional word embeddings pre-trained on 16B tokens with fastText <ref type="bibr" coords="9,242.18,538.31,15.55,8.96" target="#b10">[11]</ref>. Out-of-vocabulary words are ignored. The first run is using automatically extracted query terms to match image concepts. We consider this as the baseline method.</p><p>In the rest of runs, we add time and location constraints to each query and re-rank the retrieved images using provided metadata. This information can be either inferred automatically by NLP APIs<ref type="foot" coords="9,235.49,597.23,3.24,5.83" target="#foot_1">2</ref> in Run 3, or given by human in Run 4 and Run 5. We use a heuristic method for re-ranking that increases the score by a weight wl if the location constraints are satisfied, and decreases the score by a weight wt if the time constraints are not satisfied. The weight wl is set as 0.8, and wt is 0.1 in the experiments.</p><p>In Run 4, we fine-tune the queries to get better results. Each query is rewritten into the form consisting of the following fields: positively related concepts, negatively related concepts, location constraint, and time constraint. For negatively related concepts, we assign a weight of -1 to their word representations before vector aggregation. Table <ref type="table" coords="10,124.70,198.18,4.98,8.96" target="#tab_1">2</ref> shows an example that the automatic one is extracted from the title and the description. The fine-tuned one is obtained by removing relatively useless query terms and manually adding additional concepts, according to narrative and retrieved images. Once the query is modified, the system retrieves new results, and the query can be further improved based on new results. In the last run, we perform clustering on the retrieved images based on the event intervals described in Section 3.3.   <ref type="table" coords="10,161.89,423.23,4.98,8.96">3</ref> and Fig. <ref type="figure" coords="10,206.09,423.23,4.98,8.96">6</ref> show the performance of our method for LMRT. The inference time (without clustering) for each topic is 2.46s on average. The best result on the test set is achieved by the combination of the fine-tuned query with time/location constraints, showing that it is crucial to have human to give more precise query expressions. We also notice that there is no improvement with clustering. A possible reason is that some queries in the test set ask for rare events such as assembling furniture. Under the scoring metric, cluster recall at 10 (CR@10) <ref type="bibr" coords="10,302.15,495.23,10.75,8.96" target="#b0">[1]</ref>, used in this subtask, there is no benefit to cluster events that occur less than ten times. Instead, inaccurate results may be introduced and decrease the score.</p><p>For LMRT, we employ pre-trained fastText for the official runs. As word embedding playing a crucial role in this task, we try a number of off-the-shelf word embeddings for similarity computation. Details of these embeddings can be found in <ref type="bibr" coords="10,426.85,555.23,11.96,8.96" target="#b7">[8,</ref><ref type="bibr" coords="10,438.81,555.23,7.98,8.96" target="#b8">9,</ref><ref type="bibr" coords="10,446.78,555.23,11.96,8.96" target="#b9">10,</ref><ref type="bibr" coords="10,458.75,555.23,11.96,8.96" target="#b10">11,</ref><ref type="bibr" coords="10,124.70,567.26,12.18,8.96" target="#b11">12,</ref><ref type="bibr" coords="10,136.88,567.26,12.18,8.96" target="#b12">13]</ref>. These models are designed in quite different ways, but all produce semantic representations for individual words in a latent space. Fig. <ref type="figure" coords="10,363.34,579.26,4.98,8.96" target="#fig_6">7</ref> compares different word embeddings. We report F1@10 scores per query in the development set with a fully automated approach. The results suggest that it would be beneficial to use word embeddings that associate with additional contextual information such as syntactic dependency or lexical ontology.</p><p>Another advantage of adopting word embeddings is that sometimes query words are absent while relative or similar concepts are present in the desired images. With word embeddings, we have an opportunity to capture this kind of relations. For example, it is possible to match an image with the concepts {bowl, food, knife, meal, person, salad, table} to the query {salad, kitchen} with a high similarity, though the keyword "kitchen" is absent from the set of visual concepts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents our approaches to daily activities summarization and moment retrieval for lifelogging. In both subtasks, we introduce the external textual knowledge to reduce the semantic gap between the user query and the visual concepts extracted by the latest CV tools. Experimental results show the ensemble distributed word model, ConceptNet Numberbatch, provides effective word embeddings in both two subtasks. Experimental results also suggest that better performances can be achieved by using fine-tuned queries. That means there still exists a room for improvement on bridging the gap between the abstract human intentions and the concrete visual concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,249.41,492.52,96.35,8.10;3,139.08,275.40,330.84,208.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System framework.</figDesc><graphic coords="3,139.08,275.40,330.84,208.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,137.18,575.83,320.95,8.10;4,261.89,586.87,71.43,8.10;4,136.08,482.64,327.12,70.20"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Lifelog images with their corresponding visual concepts labeled by various image recognition models.</figDesc><graphic coords="4,136.08,482.64,327.12,70.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,228.05,405.52,139.07,8.10;5,127.32,303.36,340.56,93.48"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of concept filtering.</figDesc><graphic coords="5,127.32,303.36,340.56,93.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,170.18,429.04,254.90,8.10;9,178.56,327.36,249.48,92.64"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of concept filtering on image 20160830_181833_000.</figDesc><graphic coords="9,178.56,327.36,249.48,92.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,126.26,299.33,7.44,9.72;10,150.26,300.21,146.12,8.96;10,126.26,311.33,7.44,9.72;10,150.26,312.21,304.12,8.96;10,126.26,323.33,7.44,9.72;10,150.26,324.21,324.86,8.96;10,150.26,336.21,324.92,8.96;10,150.26,348.21,22.41,8.96;10,137.66,361.29,42.10,8.96;10,204.41,361.29,220.11,8.96;10,204.41,373.29,95.62,8.96;10,137.66,386.25,43.11,8.96;10,204.41,386.25,176.19,8.96;10,204.41,398.27,142.44,8.96;10,136.10,423.23,22.90,8.96"><head>ÔÅ¨</head><label></label><figDesc>Title: Interviewed by a TV presenter ÔÅ¨ Description: Find all the moments when I was interviewed by TV presenter. ÔÅ¨ Narrative: The moment must show the cameras or cameramen in front of the lifelogger. The interviews can occur at the lifelogger's home or in the office environment. Automatic positive = {interviewed, TV, presenter}; negative = {}; location = {}; time = {} Fine-tuned positive = {camera, person}; negative = {}; location = {home, work}; time = {} Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,230.93,396.64,133.43,8.10;11,135.00,294.42,325.20,93.43"><head>Table 3 . 354 Fig. 6 .</head><label>33546</label><figDesc>Fig. 6. Official results on the test set.</figDesc><graphic coords="11,135.00,294.42,325.20,93.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,156.14,533.68,282.98,8.10;11,174.96,417.00,256.68,108.00"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. F1@10 Scores on the development set with different embeddings used.</figDesc><graphic coords="11,174.96,417.00,256.68,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,136.10,197.99,335.35,88.74"><head>Table 1 .</head><label>1</label><figDesc>Performances in ADLT.</figDesc><table coords="8,136.10,216.66,32.72,8.96"><row><cell>Pipeline</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,225.29,282.02,144.50,8.10"><head>Table 2 .</head><label>2</label><figDesc>Query Formulation for LMRT.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,136.10,686.23,116.41,8.10"><p>https://cloud.google.com/vision/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="9,129.98,686.23,154.91,8.10"><p>https://cloud.google.com/natural-language/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,132.67,176.99,337.62,8.10;12,141.74,188.03,328.71,8.10;12,141.74,199.07,217.87,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,424.96,176.99,45.33,8.10;12,141.74,188.03,311.99,8.10">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,141.74,199.07,97.52,8.10">CLEF2018 Working Notes</title>
		<title level="s" coord="12,246.87,199.07,105.16,8.10">CEUR Workshop Proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,209.99,338.05,8.10;12,141.74,221.03,328.74,8.10;12,141.74,232.07,328.70,8.10;12,141.74,243.02,328.76,8.10;12,141.74,254.06,328.76,8.10;12,141.74,265.10,125.76,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,425.45,232.07,44.98,8.10;12,141.74,243.02,194.75,8.10">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,351.64,243.02,118.86,8.10;12,141.74,254.06,224.30,8.10">proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF 2018)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,276.02,338.00,8.10;12,141.74,287.06,328.96,8.10;12,141.74,298.10,115.66,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,412.32,276.02,58.35,8.10;12,141.74,287.06,324.85,8.10">Organizer Team at ImageCLEFlifelog 2017: Baseline Approaches for Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.56,298.10,77.30,8.10">proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,309.02,337.89,8.10;12,141.74,320.06,328.66,8.10;12,141.74,331.10,158.70,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,141.74,320.06,328.66,8.10;12,141.74,331.10,37.04,8.10">VC-I2R@ImageCLEF2017: Ensemble of Deep Learned Features for Lifelog Video Summarization</title>
		<author>
			<persName coords=""><forename type="first">Garcia</forename><surname>Del Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hwee Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,194.48,331.10,77.43,8.10">proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,342.02,337.97,8.10;12,141.74,353.06,143.88,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,325.15,342.02,145.48,8.10;12,141.74,353.06,20.09,8.10">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,177.98,353.06,79.02,8.10">proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,364.10,329.61,8.10" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m" coord="12,232.45,364.10,119.83,8.10">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,375.02,337.97,8.10;12,141.74,386.06,328.87,8.10;12,141.74,397.12,178.58,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,283.67,386.06,186.94,8.10;12,141.74,397.12,55.51,8.10">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,212.71,397.12,79.02,8.10">proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,408.04,338.05,8.10;12,141.74,419.08,162.19,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,257.78,408.04,212.94,8.10;12,141.74,419.08,39.27,8.10">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,197.48,419.08,77.72,8.10">proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,430.12,337.67,8.10;12,141.74,441.04,142.92,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,305.81,430.12,164.52,8.10;12,141.74,441.04,13.02,8.10">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,170.06,441.04,85.84,8.10">proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,452.08,338.40,8.10;12,141.74,463.12,281.16,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,362.38,452.08,108.42,8.10;12,141.74,463.12,161.72,8.10">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,319.25,463.12,75.08,8.10">proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,474.04,337.85,8.10;12,141.74,485.08,181.94,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,328.00,474.04,142.25,8.10;12,141.74,485.08,26.41,8.10">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,496.12,338.20,8.10;12,141.74,507.04,151.56,8.10" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,328.35,496.12,142.25,8.10;12,141.74,507.04,41.52,8.10">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,518.08,338.33,8.10;12,141.74,529.12,26.37,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,235.23,518.08,140.49,8.10">Dependency-Based Word Embeddings</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,394.75,518.08,75.98,8.10">proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,540.04,338.15,8.10;12,141.74,551.08,226.34,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,377.80,540.04,92.74,8.10;12,141.74,551.08,102.58,8.10">ImageNet: A LargeScale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,260.45,551.08,79.09,8.10">proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,562.15,337.98,8.10;12,141.74,573.07,206.78,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,337.91,562.15,132.46,8.10;12,141.74,573.07,78.88,8.10">Places: A 10 Million Image Database for Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Agata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Aude</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Antonio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.48,573.07,83.47,8.10">proceedings of TPAMI</title>
		<meeting>TPAMI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,584.11,338.02,8.10;12,141.74,595.15,319.73,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,172.30,595.15,165.34,8.10">Microsoft COCO: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,353.23,595.15,79.57,8.10">proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,606.07,338.13,8.10;12,141.74,617.11,328.84,8.10;12,141.74,628.15,328.61,8.10;12,141.74,639.07,307.25,8.10" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,413.48,628.15,56.87,8.10;12,141.74,639.07,275.30,8.10">OpenImages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,650.11,337.99,8.10;12,141.74,661.15,323.57,8.10" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,247.90,650.11,218.20,8.10">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,154.10,661.15,203.71,8.10">Foundations and Trends in Information Retrieval archive</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,672.07,337.94,8.10;12,141.74,683.11,182.30,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,232.14,672.07,238.20,8.10;12,141.74,683.11,59.90,8.10">OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,217.00,683.11,78.36,8.10">proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
