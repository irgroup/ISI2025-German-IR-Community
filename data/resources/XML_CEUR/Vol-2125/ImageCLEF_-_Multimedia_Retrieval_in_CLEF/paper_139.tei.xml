<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.85,115.99,339.99,12.60;1,144.11,133.92,327.70,12.60;1,182.86,151.85,250.03,12.60">Feature Learning with Adversarial Networks for Concept Detection in Medical Images: UA.PT Bioinformatics at ImageCLEF 2018</title>
				<funder ref="#_3pwHCqS">
					<orgName type="full">FCT</orgName>
				</funder>
				<funder ref="#_ZNnJFtR">
					<orgName type="full">ERDF -European Regional Development Fund</orgName>
				</funder>
				<funder ref="#_Upb5MPN">
					<orgName type="full">National Funds</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.23,194.97,66.17,8.80"><forename type="first">Eduardo</forename><surname>Pinho</surname></persName>
							<email>eduardopinho@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.09,194.97,56.04,8.80"><forename type="first">Carlos</forename><surname>Costa</surname></persName>
							<email>carlos.costa@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.85,115.99,339.99,12.60;1,144.11,133.92,327.70,12.60;1,182.86,151.85,250.03,12.60">Feature Learning with Adversarial Networks for Concept Detection in Medical Images: UA.PT Bioinformatics at ImageCLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43CB4AFC69B234E33A6086510BBC468E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Representation Learning</term>
					<term>Deep Learning</term>
					<term>Generative Adversarial Networks</term>
					<term>Auto-Encoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the subjects of representation learning and generative adversarial networks become increasingly attractive to the scientific community, they also bring an exciting perspective towards their application in the digital medical imaging domain. In particular, the ImageCLEF caption challenge is focused on automatic medical image understanding. This paper describes a set of feature learning approaches for the concept detection sub-task of ImageCLEFcaption 2018. The first approach consists on a traditional bag of words algorithm, using ORB keypoint descriptors. The remaining two methods are based on a variant of generative adversarial networks with an auto-encoding process. Subsequently, two kinds of classification algorithms were employed for concept detection over the feature spaces learned. Test results showed a best mean F1 score of 0.110176 for linear classifiers, by using the features of the adversarial autoencoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF initiative <ref type="bibr" coords="1,255.66,519.01,10.73,8.80" target="#b0">[1]</ref> has launched the second edition of the caption challenge <ref type="bibr" coords="1,179.58,530.97,10.16,8.80" target="#b1">[2]</ref>, aiming for the automatic information extraction from medical images. This challenge is divided into two sub-tasks: concept detection and caption prediction. The concept detection sub-task is the first part of the caption prediction task, in which the goal is to automatically recognize certain concepts from the UMLS vocabulary <ref type="bibr" coords="1,253.09,578.79,10.31,8.80" target="#b2">[3]</ref> in biomedical images. The obtained list of concepts would then be used in the caption prediction sub-task, where a small humanreadable description of the image is generated.</p><p>For this challenge, we reiterate on the lessons learned from the ImageCLEF 2017 concept detection task <ref type="bibr" coords="1,237.59,632.15,10.16,8.80" target="#b3">[4]</ref>, by experimenting with new representation learning methods, through which other machine learning tasks can be employed more efficiently, including biomedical concept detection. This is also part of a wider vision of improving a system's information retrieval capabilities over non-annotated data.</p><p>This paper presents our solution proposal for the concept detection sub-task, and describes our methods of image feature extraction for the purpose of biomedical concept recognition, followed by their evaluation under the ImageCLEF 2018 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head><p>This task was accompanied with two data sets containing various images from biomedical literature: the training set, comprising 223,859 images, included the list of concepts from the UMLS dictionary associated to each image. The testing set, composed of 9938 images, had its annotations hidden from the participants.</p><p>We have addressed the concept detection task in two phases. First, mid-level representations of the images were chosen and built:</p><p>-In Section 2.1, as a classical approach, bags of visual words were used as image descriptors, obtained from the clustering of visual keypoints; -In Section 2.2, two kinds of deep neural networks for unsupervised feature learning were designed and trained for the purpose of visual feature extraction.</p><p>Afterwards, as described in Section 2.4, the concept detection problem were treated as a multi-label classification problem: the three representations were validated by training classifiers of low complexity over the new representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bags of Visual Words</head><p>After converting the images to grayscale, without resizing, visual keypoint descriptors were extracted using Oriented FAST and Rotated BRIEF (ORB) <ref type="bibr" coords="2,469.01,506.64,10.13,8.80" target="#b4">[5]</ref>. The implementation in OpenCV <ref type="bibr" coords="2,279.54,518.60,10.55,8.80" target="#b5">[6]</ref> was used for ORB keypoint extraction and descriptor computation. Each image would yield a variable number of descriptors of size 64. In cases where the ORB algorithm did not retrieve any keypoints, the algorithm's parameters were adjusted to loosen edge detection criteria. As a last resort, images without any meaningful features (e.g. images of solid or gradiented color) were given an empty bag of words.</p><p>From the training set, five thousand files were randomly chosen and their respective keypoints collected to serve as a template keypoint set. A visual vocabulary (codebook) of size k = 4096 was then obtained by performing k-means clustering on all template keypoints and retrieving the centroids of each cluster, yielding a list V = {V i }, of 4096 64-dimensional vectors. We used the k-means clustering implementation from the Faiss library <ref type="bibr" coords="2,303.63,656.06,9.96,8.80" target="#b6">[7]</ref>.</p><p>Once a visual vocabulary was available, each image's bag of visual words (BoW) was constructed by determining the closest visual vocabulary point and incrementing the corresponding position in the BoW for each image keypoint descriptor. In other words, for an image's BoW B = {o i }, for each image keypoint descriptor d j , o i is incremented when the smallest Euclidean distance from d j to all other visual vocabulary points in V is the distance to V i . Finally, each BoW was normalized so that the maximum value of the elements in each BoW equals 1. The visual BoWs method has been widely experimented in content based image retrieval (CBIR) for representing visual content, including medical images <ref type="bibr" coords="3,423.09,214.58,10.06,8.80" target="#b7">[8]</ref>. From our prior experience in visual BoWs, we deem ORB as a competitive, open, and faster alternative to more frequently employed keypoint detection algorithms, such as Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="3,321.18,250.44,9.96,8.80" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Auto-encoding networks for feature learning</head><p>Generative adversarial networks (GANs) <ref type="bibr" coords="3,322.32,312.21,15.81,8.80" target="#b9">[10]</ref> are a strong target of research nowadays. These adversarial networks are primarily composed of a generator which learns to produce samples from a distribution, and a discriminator that learns to distinguish real samples from generated ones. Well trained GANs for images can produce visually appealing samples, sometimes unrecognizable from real content. This breakthrough led the scientific community into devising new GAN variants and applications to this adversarial loss, including for unsupervised representation learning <ref type="bibr" coords="3,238.13,395.90,14.61,8.80" target="#b10">[11]</ref>.</p><p>One of the shortcomings of the traditional GAN schematic is the lack of a function mapping the sample distribution to the code feature space. One of the possible solutions in literature to this issue is the expansion of the GAN to hybrid architectures with an auto-encoding procedure. The following sections describe two GAN-based deep neural network models for the unsupervised extraction of visual features from biomedical images. Both architectures will result in an encoder of samples into a latent code z, which is subsequently used as a global descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Encoder / Decoder Specification</head><p>The networks presented here abide to similar specifications: encoders and discriminators were built according to Table <ref type="table" coords="3,163.51,557.29,3.95,8.80">1</ref>. The code discriminator is an exception to these two forms, and is instead specified and explained in Section 2.2.2. Both architectures are composed of a sequence of blocks of convolutional layers, where each are followed by a normalization procedure and leaky Rectified Linear Unit (LReLU) activations with a leakiness factor of 0.2. All convolutional layers relied on a kernel of size 3x3. The encoding network ends with a fully connected layer, where h is equal to the size of z for the encoder, and 1 for the discriminator.</p><p>Tbl. The decoding network, which is used for decoders and generators, replicate the encoding process in inverse order (Table <ref type="table" coords="4,321.48,337.74,3.95,8.80">2</ref>). It starts with a mapping of the prior (or latent) code features to a feature space of dimensions 4x4x512 using a fully connected network. Each 2-layer block is composed by a convolutional layer, followed by a transposed convolution of stride 2 (also called fractionallystrided convolution). Each block doubles the height and width of the output, as a consequence of the strided convolution, until the intended image output dimensions are reached. The last convolution maps these activations to the RGB pixel value domain.</p><p>Tbl. 2: A tabular representation of the sequential composition of the decoders and generators in the networks. layer kernel size/stride output shape</p><formula xml:id="formula_0" coords="4,168.89,498.10,267.90,140.30">fc(nb = 8192, LRelU) N/A 8192 reshape(4x4) N/A 4x4x512 conv(nb = 512, LRelU) 3x3 /1 4x4x512 dconv(nb = 512, LRelU) 3x3 /2 8x8x512 conv(nb = 256, LReLU) 3x3 /1 8x8x256 dconv(nb = 256, LReLU) 3x3 /2 16x16x256 conv(nb = 128, LReLU) 3x3 /1 16x16x128 dconv(nb = 128, LReLU) 3x3 /2 32x32x128 conv(nb = 64, LReLU) 3x3 /1 32x32x64 dconv(nb = 64, LReLU) 3x3 /2 64x64x64 conv(nb = 32, LReLU) 3x3 /1 64x64x64 conv(nb = 3, tanh) 1x1 /1 64x64x3 x z encoder decoder x MSE ε discriminator + - adversarial loss</formula><p>Fig. <ref type="figure" coords="5,215.22,313.79,3.87,8.80">1</ref>: Architecture of the adversarial autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Adversarial Autoencoder</head><p>The adversarial autoencoder makes use of the same loss function in GAN training, but to the latent vector space instead of the sample distribution <ref type="bibr" coords="5,252.35,376.09,14.81,8.80" target="#b11">[12]</ref>. As an autoencoder, its reconstruction criterion is to learn the pair of functions (E, D) so that x = E(G(x)) is closest to the original sample x. In practice, this translated into a loss function for minimizing the mean squared error (MSE) between the original image and the generated one:</p><formula xml:id="formula_1" coords="5,241.87,456.11,238.72,30.32">L rec(E,G) = 1 2N N i (x i -x i ) 2<label>(1)</label></formula><p>Additionally, the bottleneck vector is regularized with an additional code discriminator network. The adversarial loss function is similar to the original solution to the min-max game <ref type="bibr" coords="5,233.12,528.34,14.82,8.80" target="#b9">[10]</ref>, where the value function V (E, D) is solved instead: the discriminated distribution learns to distinguish the distribution q(z ∼ E(x)) from a prior distribution p(z).</p><formula xml:id="formula_2" coords="5,171.31,587.55,309.28,14.64">V (E, D) = min E max D E z∼pz [log D(z)] + E x∼p(x) [log 1 -D(E(x))]<label>(2)</label></formula><p>Instead of convolutional layers, the code discriminator is composed of 3 fully connected networks of 1024 neurons each, with layer normalization <ref type="bibr" coords="5,444.12,632.15,15.81,8.80" target="#b12">[13]</ref> and LReLU after each one. The fourth layer, as in the remaining discriminators, ends with a single output neuron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Flipped Adversarial Autoencoder</head><p>When the mappings of the AAE are inverted, this results in the flipped adversarial autoencoder (F-AAE) <ref type="bibr" coords="6,450.33,130.89,14.36,8.80" target="#b13">[14]</ref>. In this architecture, a basic GAN is augmented with an encoding network E(x ) = z , which learns to reconstruct the original prior code leading to the given generated sample (Figure <ref type="figure" coords="6,203.42,166.75,3.88,8.80" target="#fig_0">2</ref>). As a means of stabilizing the the GAN training process, the architecture was adjusted to handle two levels of samples at different levels of the network. The generator produces two images x and x small , the latter 4 times smaller in side (16x16), whereas the discriminator receives both images for discriminating the sample as a whole. The adversarial training formula is equivalent to the original GAN's: This two-level GAN is influenced by the progressive GAN <ref type="bibr" coords="6,382.43,596.28,14.32,8.80" target="#b14">[15]</ref>. After two convolutional blocks of the generator, a 1x1/1 convolution of 3 kernels is used to convert the feature maps into a smaller RGB image. The network progresses as usual to the other two blocks in order to produce the 64x64 image. At the two-level discriminator, the smaller image is mapped to a 64-channel feature map with another 1x1 convolution, and concatenated with the features after the second convolutional block. Unlike the progressive GAN, the two levels are generated and updated simultaneously.</p><formula xml:id="formula_3" coords="6,171.47,261.51,309.12,14.64">V (G, D) = min G max D E x∼px [log D(x)] + E z∼p(z) [log 1 -D(G(z))]<label>(3)</label></formula><p>This idea has been reiterated in literature: the original work on GANs <ref type="bibr" coords="7,440.90,149.50,15.30,8.80" target="#b9">[10]</ref> hints towards an approximate inference component for predicting the prior code from a sample. A very similar concept was also presented by Donahue et al <ref type="bibr" coords="7,439.57,173.41,15.30,8.80" target="#b15">[16]</ref> when proposing a latent regressor between the prior z and an encoder network's output E(G(z)), while also exposing it as a major drawback in feature learning. In this architecture, the encoder never gets to see real data, and the generated samples, while potentially making great approximations of the intended distribution, will usually not achieve a perfect result. The application of the F-AAE in this task was not envisioned as a potentially competitive solution to feature learning, but rather as a means to obtain quantitative results in contrast to the AAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Image Preprocessing and Augmentation</head><p>Training samples were preprocessed in the following fashion: images were resized to the square resolution of 96 pixels. Afterwards, each incoming sample was cropped to a random square of size 64x64, yielding the final real samples. Images in the testing set, on the other hand, were only resized to fit the 64x64 dimensions. For all cases, the images' pixel RGB values were normalized with the formula n(v) = v / 127.5 -1, thus sitting in the range [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Network Training Details</head><p>The GANs were trained with sequential 3-step iterations of stochastic gradient descent: (1) reconstruction, (2) generator training, and (3) discriminator training. The prior codes were sampled from a 1024-dimensional surface of a hypersphere. The parameters were initialized with a random Gaussian distribution with a standard deviation of 0.002. Training took place through stochastic gradient descent, using the Adam optimizer <ref type="bibr" coords="7,464.86,464.40,15.73,8.80" target="#b16">[17]</ref> with a learning rate of 10 -4 and the hyperparameters β 1 = 0.5 and β 2 = 0.99 for all three optimization steps. The AAE was trained for 140000 iterations, the F-AAE for 210000 iterations, with a mini-batch size of 32. This is approximately 20 epochs over the training data for the AAE and 30 epochs for the F-AAE.</p><p>Depending on its purpose, specific regularizers and stabilization methods were also added to the network:</p><p>-In the (convolutional) discriminator, batch normalization <ref type="bibr" coords="7,402.55,572.37,15.37,8.80" target="#b17">[18]</ref> was employed after every convolutional layer of the encoding blocks. As a technique devised in <ref type="bibr" coords="7,163.25,596.28,15.36,8.80" target="#b14">[15]</ref> to prevent mode collapse, the across-minibatch standard deviation of the last convolutional layer activations was injected into the same activations as an additional feature map, yielding a new tensor of output 4x4x513 (and a flattened layer of 8208 features). Moreover, we included an auxiliary loss component 0.001 × E[D(x) 2 ] to prevent the output from drifting too far away from zero <ref type="bibr" coords="7,196.31,656.06,14.61,8.80" target="#b14">[15]</ref>.</p><p>-In the final activations of the encoder, a regularization loss was employed so that the codes would approach a unit norm, which is an invariant in the hypersphere distribution: sphere × | z 2 -1|, where sphere was set to the constant 0.001. -In the generators of samples, pixelwise feature vector normalization <ref type="bibr" coords="8,446.35,166.75,15.30,8.80" target="#b14">[15]</ref> was added immediately after every convolutional layer in the decoding blocks (before the leaky ReLU): b x,y = a x,y / 1 N N -1 j=0 (a j x,y ) 2 + , where = 10 -8 . -The introduction of some noise in GANs is known to stabilize the training process and contribute to an easier convergence <ref type="bibr" coords="8,371.13,219.79,14.90,8.80" target="#b18">[19]</ref>. We added drop-out layers <ref type="bibr" coords="8,178.40,231.74,15.19,8.80" target="#b19">[20]</ref> with a 50% drop rate at the second to last layer of the discriminator, and after the fully connected layer in the generator (25% drop rate).</p><p>TensorFlow <ref type="bibr" coords="8,188.50,272.20,15.64,8.80" target="#b20">[21]</ref> with GPU support was used to train both neural networks, as well as to retrieve the final features of each image in the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Qualitative GAN Results</head><p>The original concept of GAN shows ground-breaking results in the field of data synthesis: as the generator learns to create realistic samples (in this case, images in the biomedical domain), retrieving new content from a trained generator can be done by feeding it with prior codes. The F-AAE benefits from this perk, as it is an extension to the original GAN. The AAE, on the other hand, presents blurrier images as a consequence of mean squared error being used as the reconstruction loss. This can be seen in Figure <ref type="figure" coords="8,275.01,419.11,3.85,8.80">3</ref>, where a few samples were retrieved nearly at the end of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: An example of samples produced by the AAE and the F-AAE during the training process.</head><p>Other patterns of the learned representation can be obtained with an observation of samples in a close manifold. Figure <ref type="figure" coords="8,297.05,644.10,4.88,8.80">4</ref> show one generated sample of the trained F-AAE which has been moved in its own latent space around one of the circles of the hypersphere. This exploration of the feature space does not ensure that the images to stay in the same modality or retain semantic concepts, but unlike an interpolation in pixel space, intermediate positions still exhibit sharp visual features, and can be considered as "real-looking" as the starting image, within the capabilities of this GAN. Fig. <ref type="figure" coords="9,153.78,337.76,3.95,8.80">4</ref>: A sequence of generated samples from the F-AAE by moving the prior code around a circle in the hypersphere, column-first (from left to right, then top to bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-label Classification</head><p>For each of the three representations learned, a multi-label classifier was applied to the resulting features, where the concepts of the image associated to the feature vector were treated as labels. With the purpose of evaluating the descriptiveness of these features, algorithms of low complexity were chosen. We experimented with logistic regression and a variant of k-nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Logistic regression</head><p>After mapping all images in the training and testing sets into the latent feature space, the training set was split to create a fine-tuning (or validation) set containing approximately 10% of the data points. Afterwards, linear classifiers were built for multi-label classification of biomedical concepts. As the number of possibly existing concepts in the images is very high, only the 500 terms most frequently occurring in the training set were considered. The label vectors were built based on a direct mapping from the UMLS term identifier to an index in the vector. The reverse mapping was kept for producing the textual list of concepts.</p><p>The linear classifiers were trained for each of the three representations, using FTRL-Proximal optimization <ref type="bibr" coords="9,270.36,656.06,15.81,8.80" target="#b21">[22]</ref> with a base learning rate of 0.05, L 1 -and L 2 -norm of 0.01, and a batch size of 64. After each epoch, the classifiers were evaluated based on their precision, recall, and mean F 1 score averaged against the samples in the separate fine-tuning set, with respect to multiple fixed operating point thresholds: 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, and 0.2. Classifiers were trained until the best score among these thresholds would no longer improve, and the model with the maximizing threshold was then used to predict the concepts in the testing set. These procedures were implemented in TensorFlow.</p><p>After the classifiers were experimented for all three representations, an attempt to recognize more concepts was made with the AAE representation: a set of new linear classifiers were trained, as above, but for the following 1000 most frequent concepts (after the 500), with even lower thresholds: 0.01, 0.0124, 0.025, 0.0375, 0.05, 0.075, and 0.1. The same procedure was performed again, for the 1000 most frequent concepts following the other 1500. The three prediction files were merged with a concatenation of the concept lists to form this final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Similarity search</head><p>In this classification method, the images' features were placed in a dense feature index, and the k most similar points were used to determine which concepts are present in each sample. While the use of similarity searching techniques is a naive approach to classification, it enables a rough assessment of whether the representation would fare well in retrieval tasks where similarity metrics were not previously learned, which is the case for the Euclidean distance between features.</p><p>Like in logistic regression, the training set was split into two portions 90%/10%, leaving the latter for fine tuning and validation. The Euclidean (L 2 ) distance was mainly used for determining the similarity between data points. However, as an attempt to exploit the hyperspherical vector space in the features emerging from the AAE and the F-AAE, we have also tested cosine similarity for these representations. For this particular metric, features were linearly normalized to unit norm, so that the internal product could be employed by the index. Faiss <ref type="bibr" coords="10,134.77,488.44,10.52,8.80" target="#b6">[7]</ref> was used for the vector similarity search.</p><p>A modified form of the k-nearest neighbors (k-NN) algorithm was used: a positive prediction for a given label was made when at least one of the captured neighbors is positive for that label, and false otherwise. This tweak makes the algorithm much more sensitive to very sparse classification labels. The hyperparameter k was exhaustively searched in 1.2, 3, 4, 5 to yield the highest F 1 score against the validation set. Analogous to the threshold in logistic regression, the optimal k was used to build the predictions on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The methods described were combined into a total of 9 official submissions from the team, as listed in Table <ref type="table" coords="10,254.52,656.06,3.80,8.80">3</ref>. Additional details regarding each run are provided further below, as separate tables. Rank is the final position out of all graded runs from this year's participations in the task. The Kind of representation and classifier type is specified. Kind corresponds to the feature extractor used. Classifier is either linear(C) for logistic regression on the C most frequent concepts, or k-NN(M ) for similarity search with the given metric. All F 1 scores stated are the micro F 1 scores of each sample averaged across the corresponding set (validation or testing). F-AAE k-NN(L 2 ) 0.027188 Table <ref type="table" coords="11,162.20,414.20,5.06,8.80">4</ref> shows the validation metrics and accompanying final score of the runs based on logistic regression, including which of the tested thresholds yielded the highest score. The validation F 1 score, which was obtained from evaluating the model against the validation set, only assumes the existence of the respective target concepts (i.e. the 500 most frequent in most runs). Nonetheless, these metrics were assumed to be acceptable for an objective comparison among local runs, and have indeed defined the same ranking order as the final Test F 1 score.</p><p>The adversarial auto-encoder resulted in better quality features than the ORB bags of words or the flipped adversarial auto-encoder. Although the optimal thresholds were low, representations with more descriptive power required less threshold lowering. The second row shows the final outcome of merging the three portions of the multi-label classifiers together (500 + 1000 + 1000). Locally, they were evaluated independently. The extended label set classifiers in this case, albeit covering more biomedical concepts, were less informative on the rarer labels, which ended up crippling the final score.</p><p>The work by Lipton et al <ref type="bibr" coords="11,245.44,608.24,15.19,8.80" target="#b22">[23]</ref> provide some relevant insights on classifiers aiming to maximize F 1 score. First, that the optimal threshold in a probabilistic classifier which maximizes the score s will be max E p(y|s) [ F1  2 ]. Notably, the thresholds identified in our experiments would usually be slightly lower than half the obtained validation F 1 score. The same work presents an algorithm to further fine-tune these thresholds to attain a more ideal operating point. However, given the higher risks of batch observation (since the thresholds are fitting a portion of the data) and uninformative classifier observation (from the difficuly of classifying rare concepts), we chose to avoid overfitting the validation set by selecting a few thresholds within the interval known to contain the optimal threshold. Tbl. <ref type="bibr" coords="12,156.02,189.02,3.88,8.80" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents the methods used to obtain unsupervised representations for medical images in literature. We show that the use of deep learning methods can surpass more traditional representations, such as the bags of visual words, in terms of descriptive power. These representations were evaluated by treating the concept detection as a multi-label classification problem, and attained a best mean F 1 score of 0.110176 with logistic regression, ranking first in this year's edition of the concept detection task. A score of 0.056958 was also attained with parameterless vector searching alone. No external sources of evidence were used for any of the presented methods.</p><p>On the other hand, these results may not seem to provide a substantial jump when compared to the initial iteration of the ImageCLEF caption challenge. For instance, the best run of the 2017 concept detection task which did not rely on any external resources had a mean F 1 score of 0.1436 <ref type="bibr" coords="13,387.71,347.55,14.85,8.80" target="#b23">[24]</ref>, and the use of a pre-trained neural network had achieved a score of 0.1583 <ref type="bibr" coords="13,378.56,359.51,14.32,8.80" target="#b24">[25]</ref>. Granted, the scores are not directly comparable due to a variety of factors which could influence the performance of concept detection. As a new and disjoint set of medical images and concepts, the quality of the latest data set was slightly improved with the exclusion of multi-figures, and the overall size of the training set was increased by roughly 28%. On the other hand, the number of different concept identifiers presented in the training set's ground truth increased significantly, which may also make the detection task more difficult. The sample-averaged F 1 score for evaluating these solutions is certainly preferable over the macro F 1 score, which would have skewn the scores heavily due to the excessive weight applied to the rare labels <ref type="bibr" coords="13,183.16,479.06,14.55,8.80" target="#b22">[23]</ref>. Nevertheless, we find that most concepts in the set do not have enough images with a positive label for a valuable classifier, and that the obtained performance measurements are within the expected range of scores in this task, as the classified labels are very varied and scarce.</p><p>Multiple roads to future work can be outlined from this year's participation:</p><p>-Generative adversarial networks still make a hot topic, but it is likely to bring remarkable breakthroughs in feature learning. With the emergence of promising techniques for improving the quality and training process of GANs to this purpose, they should likewise be considered for this task and potentially other similar problems. -There is an open opportunity to learn richer representations with semisupervised learning, by taking advantage of the concepts availble in the training set. The original paper on the adversarial auto-encoder contemplates one form of incorporating label information in the regularization process <ref type="bibr" coords="13,464.08,656.06,14.32,8.80" target="#b11">[12]</ref>,</p><p>but the currently known approaches to semi-supervised GANs are much more diverse at the time of writing <ref type="bibr" coords="14,282.71,130.89,15.50,8.80" target="#b25">[26,</ref><ref type="bibr" coords="14,301.53,130.89,11.62,8.80" target="#b26">27]</ref>. -We do not exclude the possibility of attaining better scores with other classification algorithms, potentially those which are more adapted to an extreme multi-label classification scenario. The decisions made in this work were based on the desire to test the representation's descriptiveness without learning another complex feature space.</p><p>Finally, we find of significant importance that past efforts in the ImageCLEF challenges make a smooth transition to future editions, so as to obtain more competitive baselines and enable new research groups to join in with less technical debt. Delivering the necessary software components to reproduce the results as open source software would contribute to this cause.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,559.52,345.77,8.80"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture of the flipped adversarial autoencoder with two image levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,134.41,213.48,346.19,8.80;11,134.77,225.43,139.20,8.80;11,159.13,261.49,88.92,8.80;11,329.74,261.49,21.59,8.80;11,372.02,261.49,39.44,8.80;11,430.55,249.43,21.24,8.90;11,435.78,261.55,10.38,9.65;11,167.53,278.34,118.07,8.90;11,329.48,278.44,21.72,8.80;11,368.20,278.44,92.60,8.80;11,167.68,290.30,134.67,8.90;11,329.48,290.40,131.32,8.80;11,167.68,302.25,129.52,8.90;11,329.46,302.35,22.14,8.80;11,368.20,302.35,92.61,8.80;11,168.19,314.31,119.88,8.80;11,324.76,314.31,136.05,8.80;11,165.45,326.26,139.48,8.80;11,329.48,326.26,131.32,8.80;11,165.45,338.22,118.57,8.80;11,329.48,338.22,21.72,8.80;11,370.47,338.22,34.18,8.80;11,404.65,336.66,3.97,6.16;11,409.12,338.22,51.68,8.80;11,165.45,350.17,121.77,8.80;11,329.46,350.17,22.14,8.80;11,370.48,350.17,34.17,8.80;11,404.65,348.62,3.97,6.16;11,409.12,350.17,51.68,8.80;11,165.58,362.13,133.45,8.80;11,324.76,362.13,136.05,8.80;11,165.58,374.08,124.11,8.80"><head>Tbl. 3 : 1 1</head><label>31</label><figDesc>List of submitted runs from the UA.PT Bioinformatics research group to the concept detection challenge. Rank Run file name Kind Classifier Test F aae-500-o0-2018-04-30_1217 AAE linear(500) 0.110176 2 aae-2500-merge-2018-04-30_1812 AAE linear(2500) 0.108229 3 lin-orb-500-o0-2018-04-30_1142 ORB linear(500) 0.097769 9 faae-500-o0-2018-04-27_1744 F-AAE linear(500) 0.082475 11 knn-ip-aae-train-2018-04-27_1259 AAE k-NN(cosine) 0.056958 12 knn-aae-all-2018-04-26-1233 AAE k-NN(L 2 ) 0.055936 19 knn-orb-all-2018-04-24_1620 ORB k-NN(L 2 ) 0.031376 21 knn-ip-faae-all-2018-04-27_1512 F-AAE k-NN(cosine) 0.027978 22 knn-faae-all-2018-04-26_0933</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,117.34,345.82,181.26"><head></head><label></label><figDesc>1: A tabular representation of the sequential composition of the encoders and discriminators in the networks.</figDesc><table coords="4,188.95,153.29,238.04,145.31"><row><cell></cell><cell>kernel</cell><cell></cell></row><row><cell>Layer</cell><cell cols="2">size/stride Output shape</cell></row><row><cell>conv(nb = 64, LReLU)</cell><cell>3x3 /2</cell><cell>32x32x64</cell></row><row><cell>conv(nb = 128, LReLU)</cell><cell>3x3 /1</cell><cell>32x32x128</cell></row><row><cell>conv(nb = 128, LReLU)</cell><cell>3x3 /2</cell><cell>16x16x128</cell></row><row><cell>conv(nb = 256, LReLU)</cell><cell>3x3 /1</cell><cell>16x16x256</cell></row><row><cell>conv(nb = 256, LReLU)</cell><cell>3x3 /2</cell><cell>8x8x256</cell></row><row><cell>conv(nb = 512, LReLU)</cell><cell>3x3 /1</cell><cell>8x8x512</cell></row><row><cell>conv(nb = 512, LReLU)</cell><cell>3x3 /2</cell><cell>4x4x256</cell></row><row><cell>conv(nb = 512, LReLU)</cell><cell>3x3 /1</cell><cell>4x4x512</cell></row><row><cell>flatten</cell><cell>N/A</cell><cell>8192</cell></row><row><cell>fc(nb=h)</cell><cell>N/A</cell><cell>h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,134.26,189.02,348.27,475.84"><head></head><label></label><figDesc>: Results obtained from the submissions to the ImageCLEF 2018 concept detection task with logistic regression.With the k-nearest neighbors algorithm, five runs were submitted Table5. It is understandable that logistic regression has a strong vantage point over this method, since it learns a linear vector space that is more ideal for classification, whereas k -N N is restricted to a set of fixed metrics. The resulting scores were significantly lower, but were closed to the final score against the testing set. It is also worth emphasizing that cosine similarity over the features of the AAE and the F-AAE resulted in slightly better metrics. Mode collapse is one of the major issues that may arise in GAN training, and is still heavily tackled in recent literature.</figDesc><table coords="12,134.41,224.97,348.12,439.89"><row><cell cols="2">generated samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Target</cell><cell cols="3">Optimal</cell><cell>Val.</cell><cell>Val.</cell></row><row><cell>Kind</cell><cell cols="3">Concepts</cell><cell cols="2">Thres.</cell><cell>Precision</cell><cell>Recall</cell><cell>Val. F 1 Test F 1</cell></row><row><cell>AAE</cell><cell cols="2">500</cell><cell></cell><cell>0.1</cell><cell></cell><cell cols="2">0.216225 0.291748 0.248372 0.110176</cell></row><row><cell>AAE</cell><cell cols="2">2500</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.108229</cell></row><row><cell cols="3">ˆ1000</cell><cell></cell><cell cols="4">0.05 0.107612 0.100736 0.104060</cell><cell>-</cell></row><row><cell cols="3">ˆ1000</cell><cell></cell><cell cols="4">0.025 0.079174 0.089273 0.083921</cell><cell>-</cell></row><row><cell>ORB</cell><cell cols="2">500</cell><cell></cell><cell>0.1</cell><cell></cell><cell cols="2">0.213660 0.254010 0.232094 0.097769</cell></row><row><cell cols="3">F-AAE 500</cell><cell></cell><cell cols="4">0.075 0.182600 0.147899 0.163428 0.082475</cell></row><row><cell cols="8">Tbl. 5: Results obtained from the submissions to the ImageCLEF 2018 concept</cell></row><row><cell cols="7">detection task with similarity search.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Val.</cell><cell>Val.</cell></row><row><cell cols="5">Kind Metric k</cell><cell cols="2">Precision</cell><cell>Recall</cell><cell>Val. F 1 Test F 1</cell></row><row><cell cols="8">AAE cosine 2 0.065085 0.120610 0.073711 0.056958</cell></row><row><cell cols="2">AAE</cell><cell cols="6">L 2 2 0.062775 0.116644 0.071168 0.055936</cell></row><row><cell cols="2">ORB</cell><cell cols="6">L 2 4 0.023813 0.080327 0.032593 0.031376</cell></row><row><cell cols="8">F-AAE cosine 3 0.065085 0.069767 0.031575 0.027978</cell></row><row><cell cols="2">F-AAE</cell><cell cols="6">L 2 3 0.021505 0.062078 0.028007 0.027188</cell></row><row><cell cols="8">The low performance obtained with the F-AAE in both classification algorithms</cell></row><row><cell cols="8">was empirically justified in Section 2.2.3. However, these results were as low</cell></row><row><cell cols="8">as to suggest that the model would still greatly benefit from better training.</cell></row><row><cell cols="8">From observing generated samples side by side with real images, we have noticed</cell></row><row><cell cols="8">that certain image categories were very underrepresented to non-existent in the</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is financed by the <rs type="funder">ERDF -European Regional Development Fund</rs> through the <rs type="programName">Operational Programme for Competitiveness and Internationalisation -COMPETE 2020 Programme</rs>, and by <rs type="funder">National Funds</rs> through the <rs type="programName">FCT -Fundação para a Ciência e a Tecnologia</rs> within project <rs type="grantNumber">PTDC/EEI-ESS/6815/2014</rs>. <rs type="person">Eduardo Pinho</rs> is funded by the <rs type="funder">FCT</rs> under the grant <rs type="grantNumber">PD/BD/105806/2014</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZNnJFtR">
					<orgName type="program" subtype="full">Operational Programme for Competitiveness and Internationalisation -COMPETE 2020 Programme</orgName>
				</org>
				<org type="funding" xml:id="_Upb5MPN">
					<idno type="grant-number">PTDC/EEI-ESS/6815/2014</idno>
					<orgName type="program" subtype="full">FCT -Fundação para a Ciência e a Tecnologia</orgName>
				</org>
				<org type="funding" xml:id="_3pwHCqS">
					<idno type="grant-number">PD/BD/105806/2014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,138.22,457.24,343.76,8.80;14,134.77,469.19,347.21,8.80;14,134.77,481.15,347.21,8.80;14,134.77,493.10,347.22,8.80;14,134.77,505.06,347.21,8.80;14,134.77,517.01,347.22,8.80;14,134.77,528.97,62.71,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,291.44,493.10,190.54,8.80;14,134.77,505.06,98.61,8.80">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,255.17,505.06,226.80,8.80;14,134.77,517.01,65.55,8.80">Experimental ir meets multilinguality, multimodality, and interaction</title>
		<title level="s" coord="14,207.83,517.01,184.65,8.80">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.31,547.42,342.63,8.80;14,134.77,559.38,347.75,8.80;14,134.77,571.33,143.19,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,441.06,547.42,39.88,8.80;14,134.77,559.38,209.64,8.80">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws" />
	</analytic>
	<monogr>
		<title level="m" coord="14,366.93,559.38,111.37,8.80">CLEF 2018 working notes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.46,589.78,342.13,8.80;14,134.77,601.74,318.17,8.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,222.46,589.78,258.13,8.80;14,134.77,601.74,99.06,8.80">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,241.61,601.74,93.91,8.80">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.21,620.19,342.38,8.80;14,134.77,632.15,345.82,8.80;14,134.77,644.10,345.83,8.80;14,134.77,656.06,134.96,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,337.25,620.19,143.34,8.80;14,134.77,632.15,345.82,8.80;14,134.77,644.10,64.14,8.80">Towards Representation Learning for Biomedical Concept Detection in Medical Images: UA. PT Bioinformatics in ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,243.21,644.10,237.38,8.80;14,134.77,656.06,22.36,8.80">Working notes of conference and labs of the evaluation forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.56,118.93,342.03,8.80;15,134.77,130.89,347.76,8.80;15,134.77,142.84,125.78,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,359.71,118.93,120.88,8.80;15,134.77,130.89,72.42,8.80">ORB: an efficient alternative to SIFT or SURF</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,230.94,130.89,246.54,8.80">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.65,160.72,342.19,8.80;15,133.60,172.68,30.44,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,202.13,160.72,117.12,8.80">Others: The opencv library</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,326.46,160.72,95.60,8.80">Doctor Dobbs Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="126" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.40,190.56,344.13,8.80;15,134.77,202.51,177.50,8.80" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m" coord="15,300.15,190.56,176.60,8.80">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,138.72,220.39,341.87,8.80;15,134.77,232.35,334.41,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,332.59,220.39,148.00,8.80;15,134.77,232.35,186.07,8.80">Large-scale Retrieval for Medical Image Analytics: A Comprehensive Review</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,329.03,232.35,102.00,8.80">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.58,250.23,343.68,8.80;15,134.77,262.18,246.55,8.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,200.36,250.23,249.98,8.80">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,458.27,250.23,23.99,8.80;15,134.77,262.18,159.21,8.80">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.92,280.06,339.06,8.80;15,134.77,292.02,336.14,8.80" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,293.25,292.02,121.13,8.80">Generative Adversarial Nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.92,309.89,339.33,8.80;15,134.77,321.85,346.09,8.80;15,134.77,333.80,134.53,8.80" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,318.52,309.89,163.74,8.80;15,134.77,321.85,274.07,8.80">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434.1-16</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.92,351.68,337.67,8.80;15,134.39,363.64,95.84,8.80" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,430.22,351.68,50.38,8.80;15,134.39,363.64,57.29,8.80">Adversarial Autoencoders</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.75,381.52,290.19,8.80" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="15,307.10,381.52,87.47,8.80">Layer Normalization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.79,399.40,339.47,8.80;15,134.77,411.35,211.07,8.80" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E.-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04504</idno>
		<title level="m" coord="15,355.16,399.40,127.10,8.80;15,134.77,411.35,25.93,8.80">Flipped-Adversarial AutoEncoders</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.91,429.23,337.69,8.80;15,134.77,441.19,236.14,8.80" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="15,347.81,429.23,132.78,8.80;15,134.77,441.19,198.06,8.80">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.64,459.07,338.20,8.80;15,134.77,471.02,149.79,8.80" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="15,331.10,459.07,118.23,8.80">Adversarial feature learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.92,488.90,339.05,8.80;15,134.77,500.86,259.83,8.80" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,259.34,488.90,202.40,8.80">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,134.77,500.86,226.07,8.80">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.92,518.73,337.93,8.80;15,134.41,530.69,346.18,8.80;15,134.77,542.64,165.79,8.80" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,255.03,518.73,225.83,8.80;15,134.41,530.69,202.92,8.80">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,360.03,530.69,120.55,8.80;15,134.77,542.64,72.50,8.80">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.58,560.52,338.01,8.80;15,134.77,572.48,272.62,8.80" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="15,261.38,560.52,219.22,8.80;15,134.77,572.48,87.21,8.80">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,143.03,590.36,338.95,8.80;15,134.77,602.31,345.83,8.80;15,134.77,614.27,232.30,8.80" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,134.77,602.31,286.41,8.80">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,428.33,602.31,52.26,8.80;15,134.77,614.27,129.73,8.80">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.83,632.15,339.14,8.80;15,134.77,644.10,347.21,8.80;15,134.77,656.06,347.21,8.80;16,134.51,118.93,347.46,8.80;16,134.77,130.89,347.21,8.80;16,134.39,142.84,347.59,8.80;16,134.39,154.80,346.21,8.80;16,134.77,166.75,71.78,8.80" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="16,151.33,154.80,329.26,8.80;16,134.77,166.75,33.27,8.80">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,143.17,184.69,338.81,8.80;16,134.77,196.64,345.83,8.80;16,134.51,208.60,346.09,8.80;16,134.77,220.55,347.76,8.80" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,348.02,196.64,132.58,8.80;16,134.51,208.60,101.02,8.80">Others: Ad click prediction: a view from the trenches</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,259.56,208.60,221.04,8.80;16,134.77,220.55,217.96,8.80">Proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 19th acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,143.17,238.49,337.42,8.80;16,134.77,250.44,347.76,8.80;16,134.77,262.40,104.61,8.80" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,360.50,238.49,120.09,8.80;16,134.77,250.44,82.54,8.80">Thresholding Classifiers to Maximize F1 Score</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Narayanaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,224.99,250.44,252.79,8.80">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8725</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.94,280.33,337.65,8.80;16,134.41,292.28,347.57,8.80;16,134.77,304.24,100.34,8.80" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,288.99,280.33,191.61,8.80;16,134.41,292.28,18.26,8.80">IPL at ImageCLEF 2017 Concept Detection Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,175.29,292.28,260.25,8.80">Working notes of conference and labs of the evaluation forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,143.17,322.17,337.42,8.80;16,134.77,334.13,345.83,8.80;16,134.77,346.08,222.75,8.80" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,263.05,322.17,217.54,8.80;16,134.77,334.13,120.46,8.80">Concept detection on medical images using Deep Residual Learning Network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ergina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,280.59,334.13,200.00,8.80;16,134.77,346.08,71.65,8.80">Working notes of conference and labs of the evaluation forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,143.05,364.01,339.20,8.80;16,134.77,375.97,212.30,8.80" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="16,238.25,364.01,244.01,8.80;16,134.77,375.97,173.73,8.80">Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.89,393.90,339.08,8.80;16,134.77,405.86,345.83,8.80;16,134.77,417.81,183.50,8.80" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,317.02,393.90,164.96,8.80;16,134.77,405.86,190.27,8.80">Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,347.01,405.86,133.59,8.80;16,134.77,417.81,79.88,8.80">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5540" to="5550" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
