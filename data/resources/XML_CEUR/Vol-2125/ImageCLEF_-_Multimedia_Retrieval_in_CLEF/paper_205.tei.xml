<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.37,114.32,342.62,14.35;1,236.64,132.25,142.08,14.35">IPL at imageCLEF 2018: A kNN-based Concept Detection Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,204.28,170.48,82.72,9.96"><forename type="first">Leonidas</forename><surname>Valavanis</surname></persName>
							<email>valavanisleonidas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.69,170.48,101.38,9.96"><forename type="first">Theodore</forename><surname>Kalamboukis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.37,114.32,342.62,14.35;1,236.64,132.25,142.08,14.35">IPL at imageCLEF 2018: A kNN-based Concept Detection Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32B4FC81B2E8EC843B195CD1F9A1A709</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bag of Visual Words</term>
					<term>Bag of Colors</term>
					<term>image annotation</term>
					<term>knn</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the methods and techniques performed by the IPL Group for the Concept Detection subtask of the Im-ageCLEF 2018 Caption Task. In order to automatically predict multiple concepts in medical images, a k-NN based concept detection algorithm was used. The visual representation of images was based on the bagof-visual-words and bag-of-colors models. Our proposed algorithm was ranked 13th among 28 runs and our top run achieved F1 score 0.0509.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Concept Detection or Automated Image Annotation of medical images has become a very challenging task, due to the increasing number of images in the medical domain. This immense number of medical images leads to a situation where automatic image annotation becomes more and more important. It has gained a lot of attention through various challenges and researchers that try to automate the understanding of the image and its content and provide useful insights that could be beneficial for clinicians. Detecting image concepts can be particularly useful in Content Based Image Retrieval (CBIR) because it allows us to annotate images with semantically meaningful information bridging the gap between low level visual features and high level semantic information and improving the effectiveness of CBIR. The main idea of image annotation techniques is to learn automatically semantic concept from a large number of training samples, and use them to label new images. As the amount of visual information increases, the need for new methods for searching it, also increases. This year our group participated only in the concept detection task. Details of this task and the data can be found in the overview papers <ref type="bibr" coords="1,399.28,623.88,9.97,9.96" target="#b0">[1]</ref>, <ref type="bibr" coords="1,415.25,623.88,10.52,9.96" target="#b6">[7]</ref> and the web page of the contest 1 . In the next section we present a detailed description of the modelling techniques. In section 3, the data representation and preprocessing is presented as well as the results of our submitted runs. Finally, Section 4 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A k-NN based concept detection algorithm</head><p>kNN is one of the simplest and very effective algorithm in classification problems. Given an unknown item, J, (testing image) we calculate its distance from all images in a training set and assign the item to its nearest category. The decision is based on the number of neighbours we take to assign a score to categories. This is a difficult decision and a weak point of the algorithm.</p><p>In the following we give the definition of the annotation problem and a brief description of the algorithm we used in our submitted runs.</p><p>Let X = {x 1 , x 2 , ..., x N } a set of images and Y = {y 1 , y 2 , ..., y l } the set of concepts (labels or annotations). Consider a train set T = {(x 1 , Y 1 ), ..., (x N , Y N )} where Y i ⊆ Y is a subset of concepts assigned to image x i . For each concept y i we define its semantic group by the set</p><formula xml:id="formula_0" coords="2,294.25,314.16,186.34,10.88">T i ⊆ T , such that T i = {x k ∈ T : y k ∈ Y i }.</formula><p>The annotation problem is then defined by the probability p(y i |J). Given an unannotated image, J, the best label will be</p><formula xml:id="formula_1" coords="2,263.09,357.35,217.51,16.50">y * = arg max i p(y i |J)<label>(1)</label></formula><p>Several relations have been used to define the probability p(y i |J) <ref type="bibr" coords="2,434.98,382.11,9.97,9.96" target="#b1">[2]</ref>. In the following it is defined by the score:</p><formula xml:id="formula_2" coords="2,240.87,406.85,239.72,30.87">score(y k , J) = ∑ xj ∈T k dist(J, x j )<label>(2)</label></formula><p>were dist(.) can be any distance (L1, L2 etc) between the visual representations of the images J and x j . If we sort the images inside T k , with the highest score on the top, we can take any number of images in the summation 2. Due to the imbalance property of the images between the semantic groups of the concepts usually we consider only a subset of the nearest neighbours in the summation of equation 2. The scores in (2) are converted to probabilities using a soft-max function with a decay parameter, w, which nullifies the distances of most of the images and only few of them, the nearest ones, contribute in the summation.</p><p>In our experiments the value of the parameters, like the parameter w, were estimated on experiments with the CLEF 2017 data set. The algorithm we have described in matrix form is written by the matrix multiplication:</p><formula xml:id="formula_3" coords="2,257.17,586.10,223.42,12.69">score(y k , J) = J T X T v Y (3)</formula><p>were X v is a matrix N ×m v with N the size of the train set and m v the number of visual features and Y a N ×numOf Concepts binary and very sparse matrix. The entry Y {i}(j) denotes the j -th conceptID of the image i. For computational efficiency, eq. 3 was implemented with a very fast Matlab function using the cosine distance -equivalent to Euclidean distance-for normalized to unity vectors.</p><p>As we already mention due to the imbalance property of the annotation problem the algorithm benefits those concepts with high frequency occurrence in the training images. From several experiments with smaller data, plotting the distribution of relevance of the concepts, sorted by their values of DF (Document Frequency) versus the distribution of retrieval we observed that, concepts with low frequency in the train set are downgraded while concepts with high frequency are benefited by the algorithm. Thus the algorithm was modified by normalizing the outcome from eq. 3 by the value DF (y k )/avgDF . This last step improved the performance of the algorithm significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Visual Representation</head><p>One important step in the process of concept detection is the visual representation of images. Images are represented using two models, the Bag-of-visual Words (BoVW) model and a generalized version of the Bag-of-Colors (QBoC) model based on the quad tree decomposition of the image. The BoC model was used for classification of biomedical images in <ref type="bibr" coords="3,338.42,326.87,10.52,9.96" target="#b2">[3]</ref> and it was combined successfully with the BoVW-SIFT model in a late fusion manner. In a similar vein, we based our approach to the BoVW and QBoC models for the concept detection of images. In this section, we give a brief insight of these descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag-of-visual Words (BoVW)</head><p>The BoVW model has shown promising results in the field of classification and image retrieval. The DenseSIFT visual descriptor was used to implement the BoVW model in our runs. This process includes the extraction of the SIFT keypoints <ref type="bibr" coords="3,337.09,425.13,10.52,9.96" target="#b3">[4]</ref> from a dense grid of locations at a fixed scale and orientation of the images. The extracted interest points are clustered, by k-means, to form a visual codebook of a predefined size. In our runs the size of the codebook was 4,096. The final representation of an image is created by performing a vector quantization which assigns each extracted key-point of an image to its closest cluster in the codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Bag-of-Colors Model(QBoC)</head><p>A generalized version of the BoC model was proposed in <ref type="bibr" coords="3,264.80,523.39,9.97,9.96" target="#b4">[5]</ref>. The approach introduces spatial information in the representation of an image by finding homogeneous regions based on some criterion. A common data structure which has been used for this purpose is the quadtree. A quad-tree recursively divides a square region of an image into four equal size quadrants until a homogeneous quadrant was found or a stopping criterion is met. This approach uses the simple BoC model <ref type="bibr" coords="3,394.31,583.16,10.52,9.96" target="#b5">[6]</ref> to form a vocabulary or palette of colors, which is then used to extract the color histograms for each image. Similar colors within a sub-region of the image are quantized into the same color, which is the closest color(visual word) in the visual codebook. In our runs we have used two different palettes of size 100 and 200. These palettes result to 1500 and 3000 total color features depending on the number of levels in the quad-tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing and normalization</head><p>The TF-IDF weights of visual words were calculated for each model separately and the corresponding visual vectors were normalized using the Euclidean norm. The similarities between test and train images were combined for both representations of the images in a late fusion manner using weights w1=0.65 for the DenseSIFT descriptor and w2=0.35 for the QBoC. The values of these parameters were chosen based on our experiments on several other image collections. Finally another parameter of our algorithm is the decay parameter (w) of the softmax function. Several values of w were used in our runs based on experimentation with the CLEF-2017 data set. This year's data contain a set of 223305 training images and a test set of 10000 images with 111156 discrete concepts.</p><p>The training data are represented with two matrices, one of 223305 × 4096 for the dense SIFT representation and the other of 223305 × 3000 for the QBoC representation. Similarly are represented the images in the test set. These data demand more that 13GB of memory. To overcome our memory limitations we implemented a parallel knn algorithm splitting the matrices into 10 blocks that are accommodated in RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted Runs and Results</head><p>To determine the algorithm's optimal parameters we experimented with the ImageCLEF 2017 caption task dataset. In this year's contest we submitted eight visual runs for the concept detection task. For all runs we used Dense Sift with 4.096 clusters and GBoC with 200 clusters. The parameter w is between 200 and 300. The parameter annot denotes the number of predicted concepts. The results are presented in table <ref type="table" coords="4,264.23,459.19,3.88,9.96" target="#tab_0">1</ref>. The choice of the parameters w, and number of concepts (annot) is a matter for further investigation. It seems that there is a trade off between the values of these parameters which are set experimentally. A large value of w, may lead the model to over-fitting while a large value of annot reduces the accuracy. Our choice of annot was based on the observation that on average each image in the train set contains 30 concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run_ID</head><p>In this paper we presented the automated image annotation experiments performed by the IPL Group for the concept detection task at ImageCLEF 2018 Caption task. A k-NN based concept detection algorithm was used for the automatic Image Annotation of medical images. A normalization step was proposed on the scores in eq. ( <ref type="formula" coords="5,229.02,189.50,4.24,9.96">3</ref>) which improved significantly the performance of kNN. The results so far with our new knn approach are encouraging and several new directions have emerged which are currently under investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,166.22,497.12,282.91,68.98"><head>Table 1 .</head><label>1</label><figDesc>IPL submitted visual runs on Concept Detection Task.</figDesc><table coords="4,170.17,497.12,275.01,48.22"><row><cell cols="2">F1 Score Annot Parameter</cell></row><row><cell>DET_IPL_CLEF2018_w_300_gboc_200 0.0509</cell><cell>70</cell></row><row><cell>DET_IPL_CLEF2018_w_300_gboc_200 0.0406</cell><cell>40</cell></row><row><cell>DET_IPL_CLEF2018_w_300_gboc_200 0.0351</cell><cell>30</cell></row><row><cell>DET_IPL_CLEF2018_w_200_gboc_200 0.0307</cell><cell>30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,655.95,207.12,8.97"><p>http://http://www.imageclef.org/2018/caption</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,269.01,342.23,8.97;5,146.92,279.97,326.23,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,414.16,269.01,66.42,8.97;5,146.92,279.97,157.12,8.97">Overview of the imageclef 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,326.00,279.97,112.63,8.97">CLEF working notes, CEUR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,290.93,342.24,8.97;5,146.92,301.88,333.67,8.97;5,146.92,312.84,333.67,8.97;5,146.92,323.80,60.94,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,265.30,290.93,215.29,8.97;5,146.92,301.88,61.73,8.97">Image annotation using metric learning in semantic neighbourhoods</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,233.44,301.88,247.15,8.97;5,146.92,312.84,82.85,8.97">Computer Vision -ECCV 2012 -12th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">October 7-13, 2012. 2012</date>
			<biblScope unit="page" from="836" to="849" />
		</imprint>
	</monogr>
	<note>Part III.</note>
</biblStruct>

<biblStruct coords="5,138.35,334.76,342.23,8.97;5,146.92,345.72,333.67,8.97;5,146.92,356.68,137.02,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,338.18,334.76,142.40,8.97;5,146.92,345.72,98.54,8.97">Bag-of-colors for biomedical document image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,265.63,345.72,214.96,8.97;5,146.92,356.68,30.72,8.97">Medical Content-Based Retrieval for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,367.64,342.24,8.97;5,146.92,378.60,197.76,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,197.48,367.64,223.77,8.97">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,428.33,367.64,52.27,8.97;5,146.92,378.60,112.88,8.97">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,389.56,342.23,8.97;5,146.92,400.51,333.67,8.97;5,146.92,411.47,333.67,8.97;5,146.92,422.43,138.74,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,346.45,389.56,134.14,8.97;5,146.92,400.51,167.60,8.97">Fusion of bag-of-words models for image classification in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,336.76,400.51,143.83,8.97;5,146.92,411.47,219.76,8.97">Advances in Information Retrieval -39th European Conference on IR Research, ECIR 2017</title>
		<meeting><address><addrLine>Aberdeen, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2017">April 8-13, 2017. 2017</date>
			<biblScope unit="page" from="134" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,433.39,342.23,8.97;5,146.92,444.35,333.67,8.97;5,146.92,455.31,252.13,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,297.77,433.39,162.21,8.97">Bag-of-colors for improved image search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,146.92,444.35,282.53,8.97">Proceedings of the 19th International Conference on Multimedia 2011</title>
		<meeting>the 19th International Conference on Multimedia 2011<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12-01">November 28 -December 1, 2011. 2011</date>
			<biblScope unit="page" from="1437" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,466.27,342.23,8.97;5,146.92,477.23,333.68,8.97;5,146.92,488.19,333.67,8.97;5,146.92,499.14,333.67,8.97;5,146.92,510.10,333.67,8.97;5,146.92,521.06,333.67,8.97;5,146.92,532.02,333.67,8.97;5,146.92,542.98,185.06,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,249.12,510.10,231.47,8.97;5,146.92,521.06,43.41,8.97">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oladimeji</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,208.01,521.06,272.58,8.97;5,146.92,532.02,90.45,8.97">Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="5,268.51,532.02,168.68,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF 2018)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1437">2018. September 10-14, 1437-1440</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
