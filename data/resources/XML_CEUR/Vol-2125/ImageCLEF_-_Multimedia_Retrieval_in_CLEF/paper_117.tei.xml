<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.98,115.96,283.40,12.62;1,205.64,133.89,204.09,12.62">Regim Lab Team at ImageCLEF Lifelog Moment Retrieval Task 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.29,171.68,48.15,8.74"><forename type="first">Ben</forename><surname>Fatma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.76,171.68,74.79,8.74"><forename type="first">Ghada</forename><surname>Abdallah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.88,171.68,67.11,8.74"><forename type="first">Mohamed</forename><surname>Feki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.31,171.68,80.34,8.74"><roleName>Anis</roleName><forename type="first">Ben</forename><surname>Ezzarka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,421.96,171.68,29.77,8.74;1,301.66,183.64,17.02,8.74"><forename type="first">Ben</forename><surname>Ammar</surname></persName>
							<email>anis.ben.ammar@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.00,183.64,24.66,8.74"><surname>Amar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory" key="lab1">Regim-LAB</orgName>
								<orgName type="laboratory" key="lab2">REsearch Groups in Intelligent Machines</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.98,115.96,283.40,12.62;1,205.64,133.89,204.09,12.62">Regim Lab Team at ImageCLEF Lifelog Moment Retrieval Task 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23C3AEA30F56DDEC75231185D2C479C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep-learning</term>
					<term>CNN</term>
					<term>LSTM</term>
					<term>fine-tuning</term>
					<term>lifelog</term>
					<term>moments retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our approach for the ImageCLE-Flifelog Moment Retrieval task. A total of five runs were submitted, which used visual features, textual features or combination. The first run was based only on the concepts gived by the organizers. In the second and third runs, we used respectively fine-tuned Googlenet and Alexnet for images description. The fourth run was based on the fusion of the two previous runs. For the fifth run, we crossed the results of our best run (based on Alexnet model) with the result of XQuery FLWOR expression applied to the XML file containing the semantic location and activities data. Our architecture is implemented using Neural Network Toolbox, Parallel Computing Toolbox and GPU coder which generates CUDA from MATLAB. The results obtained are promising for a first participation to such a task, with F1-measure@10=0.424 which placed us at third behind AILabGTi Team with 0.545 and HCMUS Team with 0.479.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the ImageCLEFlifelog LMRT task is to retrieve a number of specific moments in a lifelogger's life. Moments are semantic events, or activities that happened throughout one or several days. For example, participants in this subtask should return the relevant moments for the query 'Find the moment(s) when I was shopping for wine in the supermarket' <ref type="bibr" coords="1,356.41,572.31,10.52,8.74" target="#b5">[6]</ref>  <ref type="bibr" coords="1,370.24,572.31,14.61,8.74" target="#b17">[18]</ref>. ImageCLEFlifelog data consists of anonymised lifelogs gathered by one user over 50 days. The dataset is based on the data available for the NTCIR-13-Lifelog 2 <ref type="bibr" coords="1,134.77,608.17,14.61,8.74" target="#b15">[16]</ref>.</p><p>The ImageCLEFlifelog LMRT task can best be compared to a known-item search task with one (or more) relevant items per topic <ref type="bibr" coords="1,377.97,632.21,14.61,8.74" target="#b15">[16]</ref>. By analysing more closely the topics, we find that the query is based on finding a location, an activity or both. The major difficulty to retrieve relevant images to a specific query from the ImageCLEFlifelog dataset is to analyse the multiple multimodal source of information : the images, the semantic content and the biometric information. There is no precise format for the data. The accuracy of the images returned depends extremely on the exploitation of these data. The main objective of experiments is to find an automatic way to extract and analyse these multimodal data. Also, it is necessary to find a way to translate each query (topic) into a set of concepts to match with image concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches used and progress beyond state-of-the-art</head><p>Despite the rapid increase in the number of publications in multimedia retrieval <ref type="bibr" coords="2,134.77,257.27,12.25,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,147.01,257.27,8.16,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,155.17,257.27,8.16,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,163.34,257.27,12.25,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,175.58,257.27,12.25,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,187.83,257.27,12.25,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,200.08,257.27,12.25,8.74" target="#b12">13,</ref><ref type="bibr" coords="2,212.32,257.27,12.25,8.74" target="#b14">15,</ref><ref type="bibr" coords="2,224.57,257.27,12.25,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,236.81,257.27,12.25,8.74" target="#b28">29,</ref><ref type="bibr" coords="2,249.06,257.27,12.25,8.74" target="#b30">31]</ref>, the problems related to the semantic gap are not yet solved. Use low-level descriptors via sophisticated algorithms can not model in an efficient way the semantics of an image or a video <ref type="bibr" coords="2,367.94,281.18,14.61,8.74" target="#b18">[19]</ref>. Indeed, this approach has many limitations especially when it is dealing with large dataset <ref type="bibr" coords="2,428.94,293.14,15.50,8.74" target="#b22">[23]</ref> because there is no direct link between the low level and the semantic level <ref type="bibr" coords="2,433.04,305.09,15.50,8.74" target="#b24">[25]</ref> . Deep learning have exposed encouraging results and performance in several multimedia research domain <ref type="bibr" coords="2,209.84,329.00,10.96,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,220.80,329.00,7.31,8.74" target="#b6">7,</ref><ref type="bibr" coords="2,228.11,329.00,7.31,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,235.41,329.00,7.31,8.74" target="#b4">5]</ref>. Convolutional neural networks (CNN) are nowadays the most powerful models for classifying images <ref type="bibr" coords="2,341.75,340.96,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="2,357.24,340.96,11.62,8.74" target="#b19">20]</ref>. Given this fact, we focus on works which used deep-learning to retrieval egocentric images. Authors in <ref type="bibr" coords="2,465.09,352.91,15.50,8.74" target="#b29">[30]</ref> adopted the text retrieval method where each document has a document ID and its content is a set of labels assigned to the corresponding image. They counted the document frequency for each distinct label to get the idf (inverse document frequency). The image labels were stem from Deep Neural Network (DNN). They used GoogleNet <ref type="bibr" coords="2,206.92,412.69,15.50,8.74" target="#b27">[28]</ref> and AlexNet <ref type="bibr" coords="2,284.24,412.69,15.50,8.74" target="#b19">[20]</ref> trained on the Imagenet<ref type="foot" coords="2,408.68,411.11,3.97,6.12" target="#foot_0">1</ref> dataset for the object recognition. They used GoogleNet, AlexNet, VGG <ref type="bibr" coords="2,392.15,424.64,15.50,8.74" target="#b25">[26]</ref> and Resnet <ref type="bibr" coords="2,465.09,424.64,15.50,8.74" target="#b16">[17]</ref> trained on Places365<ref type="foot" coords="2,227.21,435.02,3.97,6.12" target="#foot_1">2</ref> for scene recognition. <ref type="bibr" coords="2,334.60,436.60,15.50,8.74" target="#b21">[22]</ref> based their approach on the use of convolutional neural networks to transform egocentric lifelog images into concepts. To generate this transformation, they used Resnet152 trained on Im-agenet1K and Places365 and a fast region-based convolutional network method <ref type="bibr" coords="2,134.77,484.42,15.50,8.74" target="#b23">[24]</ref> with Inception-Resnet <ref type="bibr" coords="2,253.91,484.42,15.50,8.74" target="#b26">[27]</ref> pre-trained on MSCOCO <ref type="foot" coords="2,383.43,482.84,3.97,6.12" target="#foot_2">3</ref> . The important concepts were then learned with the conditional random field. <ref type="bibr" coords="2,387.19,496.37,15.50,8.74" target="#b31">[32]</ref> based their works on translating manually (by a human-in-the-loop) the query into specific required pieces of information. Thus, the proposed works depends on the concepts gave by benchmarks' organizers. They make use of manual annotation to overcome this gap. Some of them, use only CNN trained on IMAGENET to extract concepts. According to <ref type="bibr" coords="2,134.77,568.10,9.96,8.74" target="#b0">[1]</ref>, performance can be enhanced when the CNN is retrained on images that are more related to the retrieval dataset. Creating a new convolutional neural network is expensive in terms of expertise, equipment and the amount of annotated data needed. The training can take several weeks for the best CNNs, with many GPUs working on hundreds of thousands of annotated images. The complexity of creating CNN can be avoided by adapting publicly available pre-trained networks. We exploit the knowledge acquired on a general classification problem to apply it again to the lifelog context. We choose Alexnet and Googlenet and adapt the last three layer. The proposed approaches which is based on five phases follow the schema as illustrated in Figure <ref type="figure" coords="3,228.48,178.77,3.87,8.74" target="#fig_0">1</ref>. It is divided into two parts: one online and the other offline. The offline part contains the (1)query analysis, (2)the fine-tuning CNN, (3)the data extraction and the (4)image inverted index generation. The online part use these several steps to (5)retrieve relevant images according to a specific query. The first phase consists in analyzing the query using LSTM to match concepts with queries. The second phase is based on fine-tuning CNN to improve the search performance of the neural network. In the third phase, we use XQuery FLWOR expression to extract relevant images related to location, activity or time. The fourth phase consists in image inverted index generation to facilitate and speed up the processing time of the retrieval. The fifth phase which based on doc2sequence aims at retrieving the data that is matching the query. We detailed in the following each of these phases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Analysis</head><p>As first step, we build a document which contains labeled textual descriptions of queries moments. Then we convert the words to numeric vectors by training a word embedding with dimension 100 and 50 epochs. After that, we create and train an LSTM network based on the sequences of word vectors using the stochastic gradient descent with momentum (SGDM) optimizer with learning rate 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Tuned CNN</head><p>In this section, we describe how we fine-tuned Googlenet and Alexnet. We organize 2184 images into 48 classes. To retrain GoogLeNet to classify new images, we replace the last three layers of the network: a fully connected layer, a softmax layer, and a classification output layer. We set the final fully connected layer to have the same size as the number of classes in the new data set. We freeze the weights of the first 110 layers in the network by setting the learning rates in those layers to zero to speed up network training and to prevent those layers from overfitting to the new data set. We also generate data augmentation to prevent the network from overfitting and memorizing the exact details of the training images. We use 70% of the images for training and 30% for validation. To retrain Alexnet, we followed the same steps as Googlenet except for freezing initial layers. Finally, we classify each images from the IMAGECLEFLifelog2018 dataset using the fine-tuned network and generate a csv file which contains image, concepts and scores. The scores are obtained from the classification of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extract data</head><p>We need to extract the location, the activity or the time coming from sensor readings on mobile devices described in XML format. Indeed, some topics like 'Find the moments when I was taking a road vehicle in foreign countries' or 'Find the moments when I was having dinner at home' need other information than those contained in the images themselves. Therefore, we use XQuery FLWOR expression to extract relevant images related to location (home and longitude/latitude), to activity (transport) and to time (night).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Image Inverted Index Generation</head><p>Building an image inverted index is an important step in our approach. In this file, the images are organized in a matrix which represents the occurrence (or not) for each concept. Indeed, each image contains one or more concepts which describes the content. If a concept is contained in the image, a score between 0 and 1 is assigned to the image. Since the Narrative Clip wearable camera captures one image every 30 seconds, we obtain at the end of each day about 1440 images. So, we chose to generate an image inverted index for each day of the lifelogger. Create one image inverted index for all the images will cause a considerable loss of time in the generation of the file and also a slowness in the retrieval. To build this matrix, we first extracted all the concepts contained in the dataset and we sorted them alphabetically. Then, we generated a matrix with images names as column and concepts as rows. For our first run, we used the data provided by the organizers to build the image inverted index. For the other runs, we generated the matrix using the output file of the fine-tuned CNN step previously described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Retrieval</head><p>All the steps described above are done offline. Only the retrieval is online. The proposed approach for the retrieval process is based on the trained LSTM networks, the fine-tuning and the XQuery results. Firstly, we classify the query using the trained LSTM network. We then obtain the concepts that we are working on in the inverted index. Secondly, we search the concepts in the inverted index then we extract the relevant images with scores. After that, we realized an aggregation between the results obtained by the fine-tuning and those obtained by Xquery. Finally, we sort the results based on highest scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resources</head><p>Our approach is implemented using Intel(R) Core(TM) i5-4430 CPU @3.00Ghz with 16Go RAM. We work on Windows 10 Professional using Matlab 2018a. We use Neural Network Toolbox with GPU coder which generates CUDA from MATLAB code for deep learning. We train the fine-tuned Alexnet and Googlenet using Intel(R) Core(TM) i5-4200M CPU @2.50Ghz with 6Go RAM. It lasts respectively 215 and 751 minutes for a dataset containing 2184 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results obtained</head><p>We submitted 5 runs on the retrieval LMRT subtask summarized in Table <ref type="table" coords="5,462.81,524.61,3.87,8.74">1</ref>. The first run is exploiting only the concepts provided by the ImageCLEFlifelog organizers. We generate an inverted index for each lifelogger's day based on the information gived by the organizers of ImageCLEFlifelog2018. The retrieval returns the images that contains the concepts extracted from the topics based on the inverted index and the trained LSTM. The second run is using the Googlenet network fine-tuned with batch size 10 and learning rate 0.0001. We train the network for 6 epochs with 912 iterations. The third run is using Alexnet network fine-tuned with same parameters as Googlenet. The fourth run is based on the result of the two previous runs. We merged the two results, sorted the scores from highest to lowest and take the n first results where n=50. For the fifth run, we crossed the results of our best run (that of Alexnet) with the result of XQuery FLWOR expression applied to the XML file containing the semantic location and activities data. The figure <ref type="figure" coords="6,182.46,142.90,4.98,8.74" target="#fig_2">2</ref> shows the official ranking metrics F1-measure@10 for the five runs, which gives equal importance to diversity (via CR@10) and relevance (via P@10) <ref type="bibr" coords="6,134.77,166.81,9.96,8.74" target="#b5">[6]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of the results</head><p>The tables 2, 3 and 4 show the results of the runs submitted to the retrieval LMRT subtask. The results confirm that relying only on the textual concepts provided by the organizers does not give good results. We then focus on working directly on images so we choose to apply We first fine-tune with AlexNet, then with Googlenet which improved significantly retrieval performance. After that, we compare the results from the two previous runs and we take the images with highest scores. The fifth run which gave the best result in term of F1@10 is based on fine tuning with Alexnet and extracted information from the XML files which contains semantic locations and semantic activities. We extracted this information using XQuery.</p><p>Table <ref type="table" coords="7,272.28,210.38,4.13,7.89">2</ref>. Precision at X (P@X) Run RunID Name @5 @10 @20 @30 @40 @50 Run1 #Run4 Baseline-concepts of the organizers 0.1 0.06 0.03 0.02 0.015 0.012 Run2 #Run2 Fine-Tuning with Googlenet 0.38 0.34 0.295 0.263 0.227 0.198 Run3 #Run5 Fine-Tuning with Alexnet 0.48 0.39 0.315 0.28 0.24 0.212 Run4 #Run1 Fine-Tuning with Alexnet/Googlenet 0.48 0.39 0.305 0.273 0.237 0.21 Run5 #Run3 Fine-Tuning with Alexnet + XQuery 0.46 0.43 0.33 0.28 0.24 0.214 Table <ref type="table" coords="7,258.45,343.44,4.13,7.89">3</ref>. Cluster Recall at X (CR@X) Run RunID Name @5 @10 @20 @30 @40 @50 Run1 #Run4 Baseline-concepts of the organizers 0.05 0.083 0.083 0.083 0.083 0.083 Run2 #Run2 Fine-Tuning with Googlenet 0.438 0.487 0.512 0.537 0.557 0.557 Run3 #Run5 Fine-Tuning with Alexnet 0.5 0.567 0.64 0.653 0.657 0.661 Run4 #Run1 Fine-Tuning with Alexnet/Googlenet 0.5 0.567 0.616 0.648 0.648 0.672 Run5 #Run3 Fine-Tuning with Alexnet + XQuery 0.45 0.578 0.624 0.648 0.657 0.681 Table <ref type="table" coords="7,265.11,476.50,4.13,7.89">4</ref>. F1-measure at X (F1@X) Run RunID Name @5 @10 @20 @30 @40 @50 Run1 #Run4 Baseline-concepts of the organizers 0.067 0.065 0.042 0.031 0.025 0.02 Run2 #Run2 Fine-Tuning with Googlenet 0.359 0.364 0.324 0.299 0.268 0.24 Run3 #Run5 Fine-Tuning with Alexnet 0.413 0.411 0.365 0.326 0.288 0.261 Run4 #Run1 Fine-Tuning with Alexnet/Googlenet 0.413 0.411 0.352 0.321 0.285 0.259 Run5 #Run3 Fine-Tuning with Alexnet + XQuery 0.36 0.424 0.368 0.324 0.286 0.263</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Perspectives</head><p>This paper focuses on the problem of retrieving specific moment in lifelogger's life during ImageCLEFlifelog2018 LMRT task. We proposed a deep-learning based approach established on five phases using fine-tuning and LSTM. The first phase consists in analyzing the query using LSTM to match concepts with queries. The second phase is based on fine-tuning CNN to improve the search performance of the neural network. In the third phase, we use XQuery FLWOR expression to extract relevant images related to location, activity or time. The fourth phase consists in image inverted index generation to facilitate and speed up the processing time of the retrieval. The fifth phase which based on doc2sequence aims at retrieving the data that is matching the query. Promising results has been officially reported, demonstrating the effectiveness of the proposed retrieval approach. As future work, we will focus on fine-tuning with other CNNs like Inception or Resnet. Moreover, we will consider face detection based on training cascade object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>The research leading to these results has received funding from the Ministry of Higher Education and Scientific Research of Tunisia under the grant agreement number LR11ES48.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,248.04,610.23,119.27,7.89;3,134.77,375.77,345.83,219.70"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed architecture</figDesc><graphic coords="3,134.77,375.77,345.83,219.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,254.71,198.99,105.93,7.89;6,136.16,218.05,79.82,7.86;6,345.42,218.05,30.15,7.86;6,390.96,218.05,81.17,7.86;6,136.16,229.41,285.52,7.86;6,136.16,240.76,170.63,7.86;6,345.42,240.76,70.93,7.86;6,136.16,252.12,161.09,7.86;6,345.42,252.12,70.93,7.86;6,136.16,263.48,280.19,7.86;6,136.16,274.84,324.22,7.86"><head>Table 1 .</head><label>1</label><figDesc>Submitted Runs Run RunID Name Parsing Type of information Run1 #Run4 Baseline-concepts of the organizers Automatic Textual Run2 #Run2 Fine-Tuning with Googlenet Automatic Visual Run3 #Run5 Fine-Tuning with Alexnet Automatic Visual Run4 #Run1 Fine-Tuning with Alexnet/Googlenet Automatic Visual Run5 #Run3 Fine-Tuning with Alexnet + XQuery Automatic Visual + Textual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,260.74,542.47,93.88,7.89;6,138.15,333.58,345.82,194.11"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. F1-measure@10</figDesc><graphic coords="6,138.15,333.58,345.82,194.11" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,634.88,81.94,7.86"><p>www.image-net.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,645.84,174.95,7.86"><p>http://places2.csail.mit.edu/download.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,656.80,91.46,7.86"><p>http://cocodataset.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,364.64,337.63,7.86;8,151.52,375.60,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,385.39,364.64,95.20,7.86;8,151.52,375.60,32.07,7.86">Neural codes for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.42,375.60,158.91,7.86">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,386.03,337.64,7.86;8,151.52,396.99,329.07,7.86;8,151.52,407.92,185.04,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,365.50,386.03,115.10,7.86;8,151.52,396.99,275.55,7.86">Facial expression recognition based on a mlp neural network using constructive training algorithm</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Boughrara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chtourou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,434.25,396.99,46.34,7.86;8,151.52,407.95,93.02,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="709" to="731" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,418.37,337.63,7.86;8,151.52,429.33,329.07,7.86;8,151.52,440.29,329.07,7.86;8,151.52,451.25,199.26,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,356.93,418.37,123.66,7.86;8,151.52,429.33,167.61,7.86">A hypergraph-based reranking model for retrieving diverse social images</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bouhlel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,341.60,429.33,139.00,7.86;8,151.52,440.29,212.53,7.86">Computer Analysis of Images and Patterns -17th International Conference, CAIP 2017</title>
		<meeting><address><addrLine>Ystad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 22-24, 2017. 2017</date>
			<biblScope unit="page" from="279" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,461.67,337.63,7.86;8,151.52,472.63,329.07,7.86;8,151.52,483.59,329.07,7.86;8,151.52,494.55,68.27,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,312.35,461.67,168.24,7.86;8,151.52,472.63,124.40,7.86">Very deep recurrent convolutional neural network for object recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Aoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,299.72,472.63,180.87,7.86;8,151.52,483.59,25.39,7.86">Ninth International Conference on Machine Vision</title>
		<meeting><address><addrLine>ICMV</addrLine></address></meeting>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="volume">10341</biblScope>
			<biblScope unit="page">1034107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,504.97,337.63,7.86;8,151.52,515.93,329.07,7.86;8,151.52,526.87,329.07,7.89;8,151.52,537.85,79.35,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,438.97,504.97,41.62,7.86;8,151.52,515.93,329.07,7.86;8,151.52,526.89,143.55,7.86">Prediction of visual attention with deep cnn on artificially degraded videos for studies of attention of patients with dementia</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chaabouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zemmari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,303.10,526.89,144.26,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="22527" to="22546" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,548.28,337.64,7.86;8,151.52,559.24,329.07,7.86;8,151.52,570.19,329.07,7.86;8,151.52,581.15,292.63,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,442.93,548.28,37.66,7.86;8,151.52,559.24,329.07,7.86;8,151.52,570.19,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,200.54,570.19,108.44,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="8,318.26,570.19,162.34,7.86;8,151.52,581.15,14.99,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,591.58,337.63,7.86;8,151.52,602.54,329.07,7.86;8,151.52,613.50,281.88,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,343.96,591.58,136.62,7.86;8,151.52,602.54,124.86,7.86">Deep learning with shallow architecture for image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eladel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ejbali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,299.15,602.54,181.44,7.86;8,151.52,613.50,170.15,7.86">High Performance Computing &amp; Simulation (HPCS), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="408" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,623.92,337.64,7.86;8,151.52,634.88,329.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,100.85,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,326.84,623.92,153.76,7.86;8,151.52,634.88,63.30,7.86">Fuzzy user profile modeling for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,235.55,634.88,245.04,7.86;8,151.52,645.84,196.88,7.86">KDIR 2014 -Proceedings of the International Conference on Knowledge Discovery and Information Retrieval</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-24">21 -24 October, 2014. 2014</date>
			<biblScope unit="page" from="431" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.63,7.86;9,151.52,130.63,329.07,7.86;9,151.52,141.59,329.07,7.86;9,151.52,152.55,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,362.86,119.67,117.73,7.86;9,151.52,130.63,202.55,7.86">Personalizing information retrieval: a new model for user preferences elicitation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,374.03,130.63,106.56,7.86;9,151.52,141.59,51.30,7.86">Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2091" to="002096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,163.52,337.98,7.86;9,151.52,174.48,329.07,7.86;9,151.52,185.44,266.50,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,302.59,163.52,174.78,7.86">Towards diverse visual suggestions on flickr</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,166.14,174.48,263.20,7.86">Ninth International Conference on Machine Vision (ICMV 2016)</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10341</biblScope>
			<biblScope unit="page">103411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,196.41,337.98,7.86;9,151.52,207.36,329.07,7.86;9,151.52,218.32,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,361.51,196.41,119.09,7.86;9,151.52,207.36,162.17,7.86">Knowledge structures: Which one to use for the query disambiguation?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,329.81,207.36,150.79,7.86;9,151.52,218.32,58.37,7.86;9,238.56,218.32,131.21,7.86">Intelligent Systems Design and Applications (ISDA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
	<note>15th International Conference on</note>
</biblStruct>

<biblStruct coords="9,142.62,229.29,337.97,7.86;9,151.52,240.25,329.07,7.86;9,151.52,251.21,324.71,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,415.86,229.29,64.73,7.86;9,151.52,240.25,143.84,7.86">REGIM @ 2016 retrieving diverse social images task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bouhlel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.47,240.25,164.12,7.86;9,151.52,251.21,85.03,7.86">Working Notes Proceedings of the Medi-aEval 2016 Workshop</title>
		<meeting><address><addrLine>Hilversum, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 20-21, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,262.18,337.98,7.86;9,151.52,273.14,329.07,7.86;9,151.52,284.10,314.18,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,345.46,262.18,135.13,7.86;9,151.52,273.14,170.00,7.86">Improving image search effectiveness by integrating contextual information</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,343.00,273.14,137.60,7.86;9,151.52,284.10,45.08,7.86">Content-Based Multimedia Indexing (CBMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
	<note>11th International Workshop on</note>
</biblStruct>

<biblStruct coords="9,142.62,295.07,337.97,7.86;9,151.52,306.02,329.07,7.86;9,151.52,316.96,99.91,7.89" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,151.52,306.02,295.88,7.86">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Orts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Rodríguez</surname></persName>
		</author>
		<idno>CoRR abs/1704.06857</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,327.95,337.97,7.86;9,151.52,338.91,329.07,7.86;9,151.52,349.87,140.41,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,297.75,327.95,164.48,7.86">Indexing and images retrieval by content</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guedri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,338.91,215.58,7.86">High Performance Computing and Simulation (HPCS)</title>
		<title level="s" coord="9,395.75,338.91,84.84,7.86;9,151.52,349.87,17.41,7.86">International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="369" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,360.84,337.98,7.86;9,151.52,371.80,329.07,7.86;9,151.52,382.76,241.58,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,185.43,371.80,134.71,7.86">Overview of ntcir-13 lifelog-2 task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,326.93,371.80,153.66,7.86;9,151.52,382.76,92.41,7.86">Proceedings of the Thirteenth NTCIR conference (NTCIR-13)</title>
		<meeting>the Thirteenth NTCIR conference (NTCIR-13)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">December 5-8 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,393.73,337.97,7.86;9,151.52,404.68,329.07,7.86;9,151.52,415.64,76.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,290.95,393.73,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,404.68,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,426.61,337.97,7.86;9,151.52,437.57,329.07,7.86;9,151.52,448.53,329.07,7.86;9,151.52,459.49,329.07,7.86;9,151.52,470.45,329.07,7.86;9,151.52,481.41,329.07,7.86;9,151.52,492.36,329.07,7.86;9,151.52,503.32,46.58,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,197.95,459.49,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,470.45,329.07,7.86;9,151.52,481.41,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="9,151.52,492.36,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,514.29,337.97,7.86;9,151.52,525.25,329.07,7.86;9,151.52,536.21,216.63,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,269.00,514.29,211.59,7.86;9,151.52,525.25,61.41,7.86">Content based image retrieval using quantitative semantic features</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khodaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ladhake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,235.10,525.25,245.50,7.86;9,151.52,536.21,93.11,7.86">International Conference on Human Interface and the Management of Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,547.18,337.98,7.86;9,151.52,558.14,329.07,7.86;9,151.52,569.10,329.07,7.86;9,151.52,580.06,144.83,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,328.09,547.18,152.50,7.86;9,151.52,558.14,106.13,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,280.02,558.14,200.57,7.86;9,151.52,569.10,180.67,7.86;9,444.63,569.10,31.47,7.86">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct coords="9,142.62,591.02,337.98,7.86;9,151.52,601.98,329.07,7.86;9,151.52,612.94,252.55,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,339.04,591.02,141.55,7.86;9,151.52,601.98,147.35,7.86">Effective diversification for ambiguous queries in social image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.60,601.98,158.99,7.86;9,151.52,612.94,129.13,7.86">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="571" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,623.91,337.98,7.86;9,151.52,634.87,329.07,7.86" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Del Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<title level="m" coord="9,231.07,634.87,221.82,7.86">Vci2r at the ntcir-13 lifelog-2 lifelog semantic access task</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,645.84,337.97,7.86;9,151.52,656.80,329.07,7.86;10,151.52,119.67,329.07,7.86;10,151.52,130.63,169.66,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,300.66,645.84,179.93,7.86;9,151.52,656.80,68.68,7.86">Capturing image semantics with lowlevel descriptors</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2001.958942</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2001.958942" />
	</analytic>
	<monogr>
		<title level="m" coord="9,251.82,656.80,228.78,7.86;10,151.52,119.67,165.39,7.86">Proceedings 2001 International Conference on Im-age Processing (Cat. No.01CH37205)</title>
		<meeting>2001 International Conference on Im-age Processing (Cat. No.01CH37205)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,141.59,337.98,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,104.35,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,301.55,141.59,179.05,7.86;10,151.52,152.55,137.84,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,310.29,152.55,170.29,7.86;10,151.52,163.51,29.48,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,174.47,337.98,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,329.07,7.86;10,151.52,207.34,129.52,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,292.98,174.47,187.61,7.86;10,151.52,185.43,144.83,7.86">Mining association rules between low-level image features and high-level concepts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">L</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,318.41,185.43,162.18,7.86;10,151.52,196.39,139.56,7.86">Data Mining and Knowledge Discovery: Theory, Tools, and Technology III</title>
		<imprint>
			<biblScope unit="volume">4384</biblScope>
			<biblScope unit="page" from="279" to="291" />
			<date type="published" when="2001">2001</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,218.30,337.97,7.86;10,151.52,229.26,231.27,7.86" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="10,278.92,218.30,201.67,7.86;10,151.52,229.26,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.62,240.22,337.97,7.86;10,151.52,251.18,329.07,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,360.37,240.22,120.22,7.86;10,151.52,251.18,202.47,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,374.90,251.18,21.29,7.86">AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,262.14,337.98,7.86;10,151.52,273.10,329.07,7.86;10,151.52,284.06,193.34,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,286.34,273.10,130.01,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.12,273.10,40.47,7.86;10,151.52,284.06,164.67,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,295.02,337.97,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.93,323.33,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,407.00,295.02,73.58,7.86;10,151.52,305.98,196.22,7.86">A new system for event detection from video surveillance sequences</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Aoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Karray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,368.60,305.98,112.00,7.86;10,151.52,316.93,199.60,7.86">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.98,7.86;10,151.52,338.85,329.07,7.86;10,151.52,349.81,52.86,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="10,464.60,327.89,16.00,7.86;10,151.52,338.85,185.47,7.86">Pbg at the ntcir-13 lifelog-2 lat, lsat, and lest tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Akagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Takimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,343.80,338.85,101.64,7.86">Proceedings of NTCIR-13</title>
		<meeting>NTCIR-13<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,360.77,337.98,7.86;10,151.52,371.70,325.88,7.89" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="10,319.01,360.77,161.58,7.86;10,151.52,371.73,116.80,7.86">Fuzzy reasoning framework to improve semantic video interpretation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,275.26,371.73,96.30,7.86">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5719" to="5750" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,382.69,337.98,7.86;10,151.52,393.65,81.32,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="10,370.88,382.69,109.71,7.86;10,151.52,393.65,15.40,7.86">Dcu at the ntcir-13 lifelog-2 task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,173.85,393.65,30.33,7.86">NTCIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
