<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.33,115.96,302.70,12.62;1,186.38,133.89,242.59,12.62">NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain</title>
				<funder>
					<orgName type="full">Doris Duke Charitable Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Foundation for the NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Library of Medicine</orgName>
					<orgName type="abbreviated">NLM</orgName>
				</funder>
				<funder>
					<orgName type="full">Genentech</orgName>
				</funder>
				<funder>
					<orgName type="full">Lister Hill National Center for Biomedical Communications</orgName>
					<orgName type="abbreviated">LHNCBC</orgName>
				</funder>
				<funder>
					<orgName type="full">Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Colgate-Palmolive Company, Elsevier</orgName>
				</funder>
				<funder>
					<orgName type="full">American Association for Dental Research</orgName>
				</funder>
				<funder ref="#_vy2WqAG #_8pRqkGV">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,160.31,171.56,78.71,8.74"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.gov</email>
						</author>
						<author>
							<persName coords="1,247.39,171.56,62.55,8.74"><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
							<email>soumya.gayen@nih.gov</email>
						</author>
						<author>
							<persName coords="1,318.31,171.56,50.94,8.74"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
						</author>
						<author>
							<persName coords="1,377.45,171.56,77.60,8.74;1,220.03,183.51,46.23,8.74"><forename type="first">Sivaramakrishnan</forename><surname>Rajaraman</surname></persName>
						</author>
						<author>
							<persName coords="1,294.09,183.51,101.23,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Lister Hill National Center for Biomedical Communications</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.33,115.96,302.70,12.62;1,186.38,133.89,242.59,12.62">NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1A03E4E61D9250A530132F1126C6377</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Deep Learning</term>
					<term>Medical Images</term>
					<term>Medical Questions and Answers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the U.S. National Library of Medicine (NLM) in the Visual Question Answering task (VQA-Med) of ImageCLEF 2018. We studied deep learning networks with stateof-the-art performance in open-domain VQA. We selected Stacked Attention Network (SAN) and Multimodal Compact Bilinear pooling (MCB) for our official runs. SAN performed better on VQA-Med test data, achieving the second best WBSS score of 0.174 and the third best BLEU score of 0.121. We discuss the current limitations and future improvements to VQA in the medical domain. We analyze the use of automatically generated questions and images selected from the literature based on ImageCLEF data. We describe four areas of improvements dedicated to medical VQA: (i) designing goal-oriented VQA systems and datasets (e.g. clinical decision support, education), (ii) generating and categorizing medical/clinical questions, (iii) selecting (clinically) relevant images, and (iv) capturing the context and the medical knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the U.S. National Library of Medicine 1 (NLM) in the Visual Question Answering task 2 (VQA-Med) of ImageCLEF 2018 <ref type="bibr" coords="1,157.95,533.12,9.96,8.74" target="#b0">[1]</ref>. ImageCLEF is an evaluation campaign that is being organized as part of the CLEF 3 initiative labs since 2003.</p><p>This year, the VQA-Med task <ref type="bibr" coords="1,284.11,557.03,10.52,8.74" target="#b1">[2]</ref> was introduced for the first time, inspired by the open-domain VQA challenges 4 that started in 2015. Given a medical image and a natural language question about the image, participating systems are tasked with answering the question based on the visual image content. Three datasets were provided for training, validation and testing.</p><p>In our experiments, we used two deep learning VQA models: Stacked Attention Network (SAN) and Multimodal Compact Bilinear pooling (MCB). For image processing, both VQA models use Convolutional Neural Networks (CNNs). SAN uses VGG-16 and MCB uses ResNet-152 and ResNet-50, pre-trained on the ImageNet database <ref type="foot" coords="2,219.21,165.24,3.97,6.12" target="#foot_0">5</ref> . Image features are extracted from the last pooling layer of the CNNs. For question processing, both VQA models use LSTMs without pre-trained embeddings. Question vectors are extracted from the final hidden layer of the LSTMs. We submitted five runs using these two VQA models <ref type="foot" coords="2,456.59,201.11,3.97,6.12" target="#foot_1">6</ref> .</p><p>The rest of the paper is organized as follows: Section 2 describes the datasets provided in the scope of the VQA-Med challenge. Section 3 presents the deep learning networks that we selected and used for VQA in the medical domain. Section 4 describes our method to fine-tune pre-trained CNNs on modality classification. Section 5 provides a description of the submitted runs. Section 6 presents the official results. Finally, we discuss the VQA task, the data, and future improvements in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Description</head><p>Given an image and a natural language question, the VQA task consists in providing an accurate natural language answer based on the content of the image. Figure <ref type="figure" coords="2,166.20,368.79,4.98,8.74" target="#fig_1">1</ref> shows an example from VQA-Med data.</p><p>In the scope of the VQA-Med challenge, three datasets were provided:</p><p>- By analyzing the questions manually, four main types of questions could be identified:</p><p>1. Location, e.g. where is the lesion located? where is the abnormality found?</p><p>where is the lesion seen?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Question Answering Networks</head><p>Recently, VQA has been widely addressed with the introduction of new datasets such as the open-domain VQA 1.0 dataset of 250K images, over 760K questions, and around 10M answers <ref type="bibr" coords="3,252.81,456.36,9.96,8.74" target="#b2">[3]</ref>, and new applications such as answering visual questions for blind people <ref type="bibr" coords="3,250.31,468.32,9.96,8.74" target="#b3">[4]</ref>.</p><p>Different deep networks and attention mechanisms have been applied to opendomain VQA <ref type="bibr" coords="3,194.80,492.77,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,206.97,492.77,7.01,8.74" target="#b5">6]</ref>. We studied several VQA networks and selected the following models for our participation in VQA-Med 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stacked Attention Network</head><p>The Stacked Attention Network (SAN) <ref type="bibr" coords="3,315.90,559.94,10.52,8.74" target="#b6">[7]</ref> was proposed to allow multi-step reasoning for answer prediction. SAN includes three components: (i) the image model based on a CNN to extract high level image representations, (ii) the question model using an LSTM to extract a semantic vector of the question and (iii) the stacked attention model which locates the image regions that are relevant to answer the question. The SAN model achieves an Accuracy of 57.3% on VQA 1.0 test data.</p><p>For the image model, we used the last pooling layer of VGG-16 pre-trained on imageNet as image features. For the question model, we used the last LSTM layer as question features. The image features and the question vector were used to generate the attention distribution over the regions of the image.</p><p>The first attention layer of the SAN is then computed to capture the correlations between the tokens of the question and the regions in the image. Multimodal pooling is performed to generate a combined question and image vector that is then used as the query for the image in the next layer. We used two attention layers, as it showed better results in open-domain VQA. The last step is answer prediction. For a set of N answer candidates, the answer prediction task is modeled as N-class classification problem and performed using a one-layer neural network. Answers are predicted using Softmax probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Compact Bilinear pooling</head><p>Multimodal Compact Bilinear pooling (MCB) <ref type="bibr" coords="4,342.33,275.97,10.52,8.74" target="#b7">[8]</ref> is the winner of the CVPR-2016 VQA Workshop challenge using an attention mechanism that implicitly computes the outer product of visual and textual vectors. MCB architecture contains: (i) a CNN image model, (ii) an LSTM question model, and (iii) MCB pooling that first predicts the spatial attention and then combines the attention representation with the textual representation to predict the answers.</p><p>For the image model, we used ResNet-152 and ResNet-50 pre-trained on imageNet. For the question model, a 2-layer (1024 units in each layer) LSTM model is used. Concatenated output from both layers (2048 units) forms the input to the next pooling layer. MCB pooling is then used to combine both image and textual vectors to produce a multimodal representation. To incorporate attention, MCB pooling is used again to merge the multimodal representation with the textual representation for each spatial grid location. We also fine-tuned ResNet-50 on modality classification. Our method is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fine-tuning pre-trained CNNs on modality classification</head><p>CNNs are shown to deliver promising results with an increase in the availability of annotated data and computational resources. However, with the scarcity of annotated data, especially in the case of medical imagery, transfer learning is preferred where the CNNs are pre-trained on a huge selection of stock photographic images like ImageNet that contains more than 15 million annotated images belonging to 20K categories. These pre-trained models learn generic features from these large-scale collections, the knowledge can be transferred to the current task. This transfer of knowledge is generic rather than unique to the task under study. Pre-trained CNNs are fine-tuned <ref type="bibr" coords="4,365.17,608.30,10.52,8.74" target="#b8">[9]</ref> and/or used as feature extractors <ref type="bibr" coords="4,181.64,620.25,15.50,8.74" target="#b9">[10]</ref> for a variety of visual recognition tasks to improve performance.</p><p>For ImageClef, we selected five categories of modalities that are the most relevant to VQA-Med data. We used a set of 54,200 medical images associated with the categories: CT (17k images), MRI (12.7k images), XRAY (20k images), COMPOUND (1,1k images), and Other (3.4k images). We evaluated the performance of a pre-trained ResNet-50 (winner of ILSVRC 2015) toward classifying these modalities. The hyper-parameters were optimized by a method based on randomized grid search. The search ranges were initialized to [1e-7 1e-2], [0.8 0.95] and [1e-10 1e-1] for the learning rate, momentum and L2-weight decay parameters, respectively. The convolutional part of the pre-trained CNNs was instantiated, the pre-trained weights were loaded, a fully connected model was added, the pre-trained layers were frozen up to the deepest convolutional layer and the fully connected model was trained on the extracted features. We then fine-tuned the model from the layer (res5c-branch2c) rather than the entire model to prevent overfitting since the entire CNN has an extremely high entropic capacity and the tendency to overfit. We empirically found that fine-tuning the model from the aforementioned layer improved the classification performance. The model achieved an accuracy of 99.19% in classifying the different imaging modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>In VQA-Med 2018, two metrics were used to evaluate the submitted runs: (i) a new metric called Word-based Semantic Similarity (WBSS), inspired by previous efforts <ref type="bibr" coords="5,165.01,546.58,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="5,182.17,546.58,12.73,8.74" target="#b11">12]</ref> and (ii) the BLEU score <ref type="bibr" coords="5,307.20,546.58,14.61,8.74" target="#b12">[13]</ref>.</p><p>Table <ref type="table" coords="5,176.56,558.54,4.98,8.74" target="#tab_1">1</ref> shows our official results in the VQA-Med task. The SAN model provided the best results with 0.174 WBSS score and 0.121 BLEU score. Changing the number of iterations had a small impact on the results (Run1, Run2 and Run3). The fourth run used the MCB model and ResNet-152 pre-trained on imageNet for image features. Fine-tuning ResNet-50 on modality classification (Run5) had slightly improved the results of the MCB model.</p><p>Figure <ref type="figure" coords="5,180.79,630.27,4.98,8.74" target="#fig_3">2</ref> presents the results of the five participating teams. Our best overall result was obtained by Run1-SAN, achieving the second best WBSS score of 0.1744 and the third best BLEU score of 0.121 in the VQA-Med challenge.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Systems capable of understanding clinical images to the extent of answering questions about the images could support clinical education, clinical decisions and patient education. ImageCLEF VQA-Med is an essential step towards achieving these goals. Our goal for this first challenge was to evaluate the currently existing VQA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data-Driven Analysis</head><p>While ImageCLEF VQA-Med represents one of the first attempts for VQA to enter the medical domain, and is an excellent starting point, the dataset has several limitations, illustrated by the following example from VQA-Med training set:</p><p>-Question: who does ct chest demonstrate interval resolution of without cardiophrenic sparing?</p><p>-Answer: pneumothoraces and persistent -Image: Figure <ref type="figure" coords="7,215.52,131.30,8.86,8.74" target="#fig_4">3b</ref>. This example illustrates some of the issues facing medical VQA:</p><p>(1) Tradeoffs between time-consuming manual construction of datasets and automatically constructing datasets using readily-available resources. Artificial questions do not always make sense, to the point where we cannot reason what the question was trying to ask. Searching PubMed, we found that the original caption for this image is:</p><p>-"CT chest (axial slices in lung window) demonstrating interval resolution of pneumothoraces and persistent, diffuse numerous thin-walled pulmonary cysts without cardiophrenic sparing."</p><p>Maybe the question was supposed to be 'what does the ct chest demonstrate interval resolution of ?' A more natural way to ask the question might be 'the ct chest demonstrates resolution of what?'.</p><p>(2) The context and domain knowledge needed to answer the question. A radiologist may not have been able to answer the above question given only Figure <ref type="figure" coords="7,167.63,528.20,10.52,8.74" target="#fig_4">3b</ref> because so much context was lost. The article is a case report of a patient experiencing acute respiratory distress syndrome and Figure <ref type="figure" coords="7,450.28,540.15,10.52,8.74" target="#fig_4">3b</ref> is a follow-up image taken 8 months after the patient was discharged. Figure <ref type="figure" coords="7,460.10,552.11,9.96,8.74" target="#fig_4">3a</ref> is needed to correctly answer the question with pneumothoraces.</p><p>(3) The clinical task that VQA aims to support. The purpose of radiology images and captions in PubMed are to summarize and present an interesting case to the scientific community. This differs from clinical radiology reports in hospitals where they assist clinical decision making and direct patient care. If VQA tools are to assist clinical processes, the design of the dataset needs to have aligned goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Challenges and Future Improvements</head><p>The VQA community has greatly benefited from large datasets such as Ima-geNet <ref type="bibr" coords="8,163.36,149.26,15.50,8.74" target="#b13">[14]</ref> and MS-COCO <ref type="bibr" coords="8,253.03,149.26,15.50,8.74" target="#b14">[15]</ref> used to construct public domain VQA datasets. Similarly, specialized medical image resources can be leveraged for VQA, however, there are some fundamental challenges that need to be addressed to apply VQA to a more specialized domain such as Medicine.</p><p>If we are to develop tools for VQA to assist clinicians, we will need to better understand what clinincians might ask and how clinical questions are naturally phrased. We also need insight to what questions are relevant in different contexts. Just like a question about weather would not make sense for an image of an indoor space, a question about the brain would not make sense for a CT of the chest.</p><p>Categorization of question and answer types will help both to direct clinical usefulness and support evaluation of different algorithm designs. Question categorization can help give insight if certain algorithms are good at characterizing the intensity of a tumor while others may be better at counting the number of ribs.</p><p>Clinically relevant images are also needed. If the goal of medical VQA is to assist clinical processes, then we need to have images used in clinical settings. Many of the PubMed images are designed for the scientific community. Composite images of multiple images or 3D reconstructions may rarely be relevant for clinical decision-making and supporting direct patient care.</p><p>Finally, context and background knowledge might play a more important role in medicine than in the open domain. For example, radiologists will know to look for internal bleeding in a head CT scan with contrast even if they are not given the patient's history or symptoms. This context is implicit yet will influence the types of clinically relevant questions and answers. Understanding how we can capture these types of context and the necessary minimum amount are important future goals for benefiting clinical processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper describes our participation in the ImageCLEF 2018 VQA-Med task. We tested two publicly available VQA networks. We achieved the second best WBSS score in the challenge using the SAN model. Our results also showed the positive impact of (i) performing visual attention to learn image regions relevant to answer the clinical questions, and (ii) using a multiple-layer SAN to query an image multiple times and infer the answer progressively. This first VQA challenge in the medical domain allowed testing the combination of language models and vision models in a restricted domain.</p><p>Several tracks can be investigated for future improvement including adding questions asked by health care professionals, and using medical terminologies in the linguistic analysis of the questions in deep networks. Based on a new clinical VQA dataset that was recently created <ref type="bibr" coords="8,348.24,644.16,14.61,8.74" target="#b15">[16]</ref>, we plan to study further the automatic construction of clinical images and related question-answer pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,151.70,404.14,328.89,8.74;2,151.70,416.10,69.82,8.74;2,140.99,428.27,339.60,8.77;2,151.70,440.26,78.38,8.74;2,140.99,452.43,307.05,8.77"><head></head><label></label><figDesc>The training set contains 5,413 question-answer pairs associated with 2,278 training images. -The validation set contains 500 question-answer pairs associated with 324 validation images. -The test set contains 500 questions associated with 264 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,356.58,345.83,8.74;3,134.77,368.53,208.88,8.74;3,199.18,135.48,218.37,164.97"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of a medical image and the associated question and answer from the training set of ImageCLEF 2018 VQA-Med.</figDesc><graphic coords="3,199.18,135.48,218.37,164.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,337.01,291.13,8.74;5,140.99,355.03,339.60,8.77;5,151.70,367.01,328.89,8.74;5,151.70,378.97,328.89,8.74;5,151.70,390.92,276.94,8.74;5,140.99,402.21,253.47,8.77;5,140.99,413.52,248.49,8.77;5,140.99,424.83,339.60,8.77;5,151.70,436.81,328.89,8.74;5,151.70,448.77,150.80,8.74;5,140.99,460.05,339.60,8.77;5,151.70,472.03,227.56,8.74"><head></head><label></label><figDesc>We submitted five automatic runs to ImageCLEF 2018 VQA-Med: -Run1-SAN: This run used the stacked attention network (SAN) with 2 attention layers. The VQA model was trained on imageClef VQA-Med training and validation datasets (10k iterations). VGG-16 pre-trained on ImageNet was used for image features. No additional resources were used. -Run2-SAN: Same as run number 1, with 12k iterations. -Run3-SAN: Same as run number 1, with 4k iterations. -Run4-MCB: This run used the MCB model trained on the training and validation datasets (50k iterations). ResNet-152 was used for image features. No additional resources were used. -Run5-MCB: Similar to run number 4 (20k iterations). For image features, ResNet-50 was fine-tuned on modality classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,146.31,428.16,322.74,8.74;6,152.06,232.68,331.65,180.90"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Official Results of ImageClef 2018 VQA-Med: Participating Teams.</figDesc><graphic coords="6,152.06,232.68,331.65,180.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,209.07,304.75,192.75,8.74;7,401.82,303.18,3.97,6.12;7,315.49,174.43,141.74,92.89"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Images extracted from a PMC article 7</figDesc><graphic coords="7,315.49,174.43,141.74,92.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,138.97,532.36,341.62,92.95"><head></head><label></label><figDesc>2. Finding, e.g. what does the mri show? what does the ct scan confirm? what does the image demonstrate? what does the ct chest reveal? what is the</figDesc><table coords="2,138.97,556.30,341.62,69.01"><row><cell>lesion suggestive of?</cell></row><row><cell>3. Yes/No questions, e.g. is the brain magnetic resonance imaging normal?</cell></row><row><cell>are the opacities in both lungs? does the image demonstrate cerebral infarc-</cell></row><row><cell>tion? Could the recess be mistaken for subcarinal lymphadenopathy?</cell></row><row><cell>4. Other questions, e.g. what kind of image is this? what is the mass invading?</cell></row><row><cell>what other abnormality can be seen in the image?</cell></row></table><note coords="3,141.03,119.12,30.19,7.65;3,141.03,312.85,219.01,7.65;3,141.03,323.48,334.67,7.65;3,141.03,334.14,39.30,7.63"><p><p>Image:</p>Question: what does transverse ct image demonstrate? Answer: focal defect in inflamed appendiceal wall and periappendiceal inflammatory stranding.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,161.82,117.75,291.71,88.26"><head>Table 1 :</head><label>1</label><figDesc>Official Results of ImageClef 2018 VQA-Med: NLM Runs.</figDesc><table coords="6,182.88,117.75,219.20,64.68"><row><cell>NLM Runs</cell><cell>WBSS Score</cell><cell>BLEU Score</cell></row><row><cell>Run1-SAN</cell><cell>0.174</cell><cell>0.121</cell></row><row><cell>Run2-SAN</cell><cell>0.168</cell><cell>0.108</cell></row><row><cell>Run3-SAN</cell><cell>0.157</cell><cell>0.106</cell></row><row><cell>Run4-MCB</cell><cell>0.130</cell><cell>0.083</cell></row><row><cell>Run5-MCB</cell><cell>0.144</cell><cell>0.085</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,144.73,634.88,106.26,7.86"><p>http://www.image-net.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="2,144.73,645.84,335.87,7.86;2,144.73,656.80,335.86,7.86"><p>For the SAN model, we utilized Torch and the computational resources of the NIH HPC Biowulf cluster (http://hpc.nih.gov). We thank Wolfgang Resch for his support.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="7,144.73,656.80,234.73,7.86"><p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4638149</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by the <rs type="programName">Intramural Research Program</rs> of the <rs type="funder">National Institutes of Health (NIH)</rs>, <rs type="funder">National Library of Medicine (NLM)</rs>, and <rs type="funder">Lister Hill National Center for Biomedical Communications (LHNCBC)</rs>. This research was partially made possible through the <rs type="funder">NIH</rs> <rs type="programName">Medical Research Scholars Program</rs>, a public-private partnership supported jointly by the <rs type="funder">NIH</rs> and generous contributions to the <rs type="funder">Foundation</rs> for the <rs type="funder">NIH</rs> from the <rs type="funder">Doris Duke Charitable Foundation</rs>, <rs type="funder">Genentech</rs>, the <rs type="funder">American Association for Dental Research</rs>, the <rs type="funder">Colgate-Palmolive Company, Elsevier</rs>, alumni of student research programs, and other individual supporters via contributions to the <rs type="funder">Foundation for the NIH</rs>. For a complete list, please visit the Foundation website 8 .</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vy2WqAG">
					<orgName type="program" subtype="full">Intramural Research Program</orgName>
				</org>
				<org type="funding" xml:id="_8pRqkGV">
					<orgName type="program" subtype="full">Medical Research Scholars Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,319.52,337.63,7.86;9,151.52,330.48,329.07,7.86;9,151.52,341.44,329.07,7.86;9,151.52,352.39,329.07,7.86;9,151.52,363.35,329.07,7.86;9,151.52,374.31,329.07,7.86;9,151.52,385.27,329.07,7.86;9,151.52,396.23,46.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,198.28,352.39,264.47,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,363.35,329.07,7.86;9,151.52,374.31,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="9,222.96,385.27,167.97,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,407.29,337.64,7.86;9,151.52,418.25,329.08,7.86;9,151.52,429.21,329.07,7.86;9,151.52,440.16,187.76,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,417.04,407.29,63.56,7.86;9,151.52,418.25,263.03,7.86">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,437.46,418.25,43.13,7.86;9,151.52,429.21,188.04,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,451.22,337.63,7.86;9,151.52,462.18,329.07,7.86;9,151.52,473.14,80.94,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,151.52,462.18,134.19,7.86">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,308.67,462.18,171.92,7.86;9,151.52,473.14,47.51,7.86">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,484.20,337.63,7.86;9,151.52,495.16,329.07,7.86;9,151.52,506.09,99.91,7.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,172.69,495.16,275.76,7.86">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<idno>CoRR abs/1802.08218</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,517.17,337.64,7.86;9,151.52,528.11,174.70,7.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,240.57,517.17,240.02,7.86;9,151.52,528.13,38.91,7.86">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno>CoRR abs/1610.01465</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,539.19,337.63,7.86;9,151.52,550.15,329.07,7.86;9,151.52,561.11,329.07,7.86;9,151.52,572.07,181.13,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,305.86,539.19,174.73,7.86;9,151.52,550.15,100.10,7.86">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,272.62,550.15,207.97,7.86;9,151.52,561.11,282.41,7.86">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,583.13,337.64,7.86;9,151.52,594.09,329.07,7.86;9,151.52,605.04,329.07,7.86;9,151.52,616.00,23.04,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,356.13,583.13,124.46,7.86;9,151.52,594.09,103.30,7.86">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.05,594.09,199.54,7.86;9,151.52,605.04,133.40,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,627.06,337.64,7.86;9,151.52,638.02,329.07,7.86;9,137.50,655.03,3.65,5.24;9,144.73,656.80,304.37,7.86;10,151.52,119.67,329.07,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,13.82,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,455.25,627.06,25.34,7.86;9,151.52,638.02,324.77,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<ptr target="http://fnih.org/what-we-do/current-education-and-training-programs/mrsp" />
	</analytic>
	<monogr>
		<title level="m" coord="10,164.90,119.67,315.69,7.86;10,151.52,130.63,101.98,7.86">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,152.55,337.64,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,329.07,7.86;10,151.52,185.43,134.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,391.05,152.55,89.54,7.86;10,151.52,163.51,179.46,7.86">CNN features off-theshelf: An astounding baseline for recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.01,163.51,126.58,7.86;10,151.52,174.47,232.11,7.86">IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">June 23-28, 2014. 2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,196.39,337.97,7.86;10,151.52,207.34,329.07,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.24,88.64,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,250.44,207.34,230.15,7.86;10,151.52,218.30,324.82,7.86">Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poostchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Silamut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Maude</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,229.26,22.55,7.86">PeerJ</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4568</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,237.96,337.98,10.13;10,151.52,251.15,329.07,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,315.77,240.22,164.82,7.86;10,151.52,251.18,175.47,7.86">BIOSSES: a semantic sentence similarity estimation system for the biomedical domain</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sogancioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Öztürk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Özgür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,334.41,251.18,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,262.14,337.98,7.86;10,151.52,273.10,329.07,7.86;10,151.52,284.06,329.07,7.86;10,151.52,295.02,13.82,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,252.03,262.14,148.58,7.86">Verb semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,427.60,262.14,53.00,7.86;10,151.52,273.10,232.55,7.86">32nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Las Cruces, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="1994-06-30">27-30 June 1994. 1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,305.98,337.98,7.86;10,151.52,316.93,329.07,7.86;10,151.52,327.89,329.07,7.86;10,151.52,338.85,85.50,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,338.90,305.98,141.69,7.86;10,151.52,316.93,121.20,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,299.55,316.93,181.04,7.86;10,151.52,327.89,184.53,7.86">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">July 6-12, 2002. 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,349.81,337.98,7.86;10,151.52,360.77,329.07,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.69,85.50,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,359.00,349.81,121.59,7.86;10,151.52,360.77,85.74,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,259.05,360.77,221.54,7.86;10,151.52,371.73,186.32,7.86">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,393.65,337.98,7.86;10,151.52,404.61,329.07,7.86;10,151.52,415.56,329.07,7.86;10,151.52,426.52,172.11,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,209.09,404.61,180.92,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,411.81,404.61,68.78,7.86;10,151.52,415.56,170.99,7.86">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Part V.</note>
</biblStruct>

<biblStruct coords="10,142.62,437.48,337.98,7.86;10,151.52,448.44,329.07,7.86;10,151.52,459.40,87.67,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,394.09,437.48,86.50,7.86;10,151.52,448.44,262.19,7.86">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,426.02,448.44,54.57,7.86;10,151.52,459.40,59.01,7.86">Submitted to Scientific Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
