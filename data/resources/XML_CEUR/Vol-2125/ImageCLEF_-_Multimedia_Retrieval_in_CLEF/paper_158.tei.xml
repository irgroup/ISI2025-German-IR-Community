<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.37,115.96,250.62,12.62;1,163.54,133.89,288.28,12.62">Leveraging Content and Context in Understanding Activities of Daily Living</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.64,171.74,63.23,8.74"><forename type="first">Minh-Son</forename><surname>Dao</surname></persName>
							<email>minh.son@utb.edu.bnwww.utb.edu.bn</email>
							<affiliation key="aff0">
								<orgName type="institution">Universiti Teknologi Brunei</orgName>
								<address>
									<country>Brunei</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Information Technology</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.77,171.74,56.85,8.74"><forename type="first">Asem</forename><surname>Kasem</surname></persName>
							<email>asem.kasem@utb.edu.bnwww.utb.edu.bn</email>
							<affiliation key="aff0">
								<orgName type="institution">Universiti Teknologi Brunei</orgName>
								<address>
									<country>Brunei</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.55,171.74,76.65,8.74"><forename type="first">Mohamed</forename><surname>Saleem</surname></persName>
							<email>mohamed.saleem@utb.edu.bnwww.utb.edu.bn</email>
						</author>
						<author>
							<persName coords="1,391.52,171.74,74.72,8.74"><forename type="first">Haja</forename><surname>Nazmudeen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universiti Teknologi Brunei</orgName>
								<address>
									<country>Brunei</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.37,115.96,250.62,12.62;1,163.54,133.89,288.28,12.62">Leveraging Content and Context in Understanding Activities of Daily Living</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF604339826DBD5D8A0FA11410705164</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>lifelog data analysis</term>
					<term>image alignment and stitching</term>
					<term>semantic taxonomy</term>
					<term>heterogeneous data segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a content-context-based method to automatically provide a summarization of lifelog data based on selected concepts of Activities of Daily Living (ADL). The main idea of the proposed method is to create a so-called (1) Daily-Normal Environment Panorama (DNEP) image, and a (2) Daily-Abnormal Environment (DAE) Taxonomy. The former is used to detect events that happen in known environments such as in a house, in an office and on the way from a parking lot to an office. The latter is used to detect events whose concepts can be detected by a pre-defined taxonomy such as in a restaurant, in a church, and on a street. The proposed method is evaluated by using the data and evaluation tool offered by organizers of imageCLEFlifelog2018 -subtask Activities of Daily Living understanding (ADLT). The experiments show that the proposed method works better than methods proposed by other participants of the imageCLEFlifelog2018.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelogging is an interesting topic that can help to understand the daily living activities and to recall moments of interest that happened in the past. The scope of applications that utilize lifelog data extends to human-supported scenarios such as personal healthcare and personal assistants <ref type="bibr" coords="1,341.93,560.48,9.96,8.74" target="#b0">[1]</ref>. Since lifelog data is a timeseries data, events detection in lifelog data can be considered as scene detection in video data where data segmentation plays an essential role <ref type="bibr" coords="1,402.84,584.39,9.96,8.74" target="#b1">[2]</ref>. Therefore, the task of segmentation in lifelog data analysis is crucial, and recently many works have focused on lifelog data segmentation <ref type="bibr" coords="1,316.84,608.30,10.35,8.74" target="#b2">[3]</ref>[4] <ref type="bibr" coords="1,337.53,608.30,10.35,8.74" target="#b4">[5]</ref> <ref type="bibr" coords="1,347.88,608.30,10.35,8.74" target="#b5">[6]</ref>. Most of these methods utilized visual features only for video segmentation. However, lifelog data contains not only visual information, but also heterogeneous data such as textual data (e.g. tags, comments), geo data (e.g. GPS, places name), and physiological data (e.g. heartbeat, skin temperature). Hence, it is essential to have a method that can analyze such heterogeneous and big data to extract the information people may need.</p><p>The imageCLEFlifelog2018 (the organizer hereafter) <ref type="bibr" coords="2,382.95,143.14,10.51,8.74" target="#b6">[7]</ref>[8] organizes a challenge to encourage participants to propose methods to automatically analyze such data towards categorizing, summarizing, and retrieving information of interest. We have joined the challenge with the subtask "Activities of Daily Living understanding" (ADLT), and this paper reports our work and discusses its evaluation on this subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we introduce what we call DNEP image and DAE taxonomy and how to leverage them to detect events in lifelog data. The idea behind these image and taxonomy concepts is based on "Content without Context is meaningless" <ref type="bibr" coords="2,195.39,296.97,9.96,8.74" target="#b8">[9]</ref>. It means that we use not only the content of lifelog data to understand an event, but also the context where that event happened. This is expected to improve event detection as well as decrease the missing/redundant information of events boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Daily-Abnormal Environment Taxonomy (Contexts and Activities)</head><p>Most of the events that happen daily have their own unspoken/spoken rules by which we can build a suitable taxonomy. For example, when visiting a church the environment must contain salient and typical symbols of a church such as the cross and Saint statutes, while the activities could be: slowly walking, quietly sitting or kneeling. Therefore, based on the events concept we can build a taxonomy for the specific event to further detect that event in lifelog data.</p><p>Another example is socialising in a restaurant. The visual taxonomy of a meal table, especially a menu and a counter (with or without a queue), can be integrated with GPS and/or the restaurants name tagged by people to distinguish whether the event happened in a restaurant or in a relatives house.</p><p>In fact, each daily activity can be determined when knowing the environment where such an activity happens. These environments again can be determined by "scene recognition" and "visual concepts detection" tasks. The former names a place and the latter labels all objects appeared inside the place.</p><p>The work carried out by Zhou et al. <ref type="bibr" coords="2,313.67,564.85,15.50,8.74" target="#b12">[13]</ref> is a good example of scene recognition. In this work, the scene hierarchy defined by the authors has two levels, and each scene is located to a suitable slot in this scene hierarchy. For example, the conference room scene is located at (level 1: indoor → level 2: workplace (office building, factory, lab, etc.)) <ref type="foot" coords="2,273.06,611.10,3.97,6.12" target="#foot_0">3</ref> . The organizer also offered the scene ontology described in NTCIR-13 Lifelog Ontology <ref type="bibr" coords="2,317.08,624.63,9.96,8.74" target="#b7">[8]</ref>. This ontology gives the summary of scenes appeared in the dataset. By integrating the scene ontology and scene Fig. <ref type="figure" coords="3,177.26,380.41,4.13,7.89">1</ref>. An example of DAE taxonomy of topic 3 "preparing meals, home" hierarchy, we build DAE taxonomy from the root to level 3, as illustrated in Fig. <ref type="figure" coords="3,134.77,424.24,3.87,8.74">1</ref>.</p><p>When successfully locating scene A (in level 3), the next question is "what kind of activities normally happens and which visual concepts always appear in the scene A?". Such a question leads to the need of building level 4 of DAE taxonomy. In this level, each level-3 scene has two same-level categories activities and visual concepts. The former is built based on the Activities/Facets of Life activity defined in NTCIR-13 Lifelog Ontology; and the latter is constructed by utilizing the visual concepts and food-logs and drink-logs described in <ref type="bibr" coords="3,448.58,507.93,9.96,8.74" target="#b7">[8]</ref>. For example, in a kitchen, preparing a meal often happens and definitely there must be kitchen appliance and foods, illustrate in Fig. <ref type="figure" coords="3,347.95,531.84,3.87,8.74">1</ref>.</p><p>Algorithm 1: DAE taxonomy building 1. Repeat Alg.1-step.1 to the ground-truth data to generate the training data. 2. Use the context, physical activities tags, physiological data, and image/visual concepts provided by the organizer and scene hierarchy offered by <ref type="bibr" coords="3,452.09,596.81,15.50,8.74" target="#b12">[13]</ref> to build the DAE taxonomy. In our case, the physical activities tags and physiological data are utilized to determine two categories (1) moving (2) nomoving. 3. Build a lookup location table (DAE-LT) for determining the geofencing of the event using GPS and places name tags.</p><p>The moving of people inside a scene also can give rich semantic cues for understanding their activities. For example, the loop of long standing (no-moving) and short walking (moving) inside a kitchen can differ from the loop of long sitting (no-moving) and short walking/turning (moving) inside an office. These information can be captured by using biometrics:(grs, steps) entry in the metadata of dataset with the human activities recognizer developed in <ref type="bibr" coords="4,423.44,178.77,14.61,8.74" target="#b14">[15]</ref>.</p><p>The Algorithm 1 summarizes the content of this subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Daily-Normal Environment Panorama Image (Visual Background)</head><p>One of the characteristics of lifelog data is the repeated routine. People usually have at least one place to visit almost every day and the environment of this place rarely changes, such as at home, a relatives house, an office, a favorite restaurant, and a familiar supermarket. Therefore, if we can accumulate all images captured from those places, we can build a panorama image. Consequently, if we can successfully project a lifelog image onto a panorama image (e.g. using image alignment, object detection, image segmentation) with a known concept, we can assign the right event label for that image and further detect the boundary of that event.</p><p>The Image Alignment and Stitching have been researched for over a decade now <ref type="bibr" coords="4,156.57,372.66,14.61,8.74" target="#b9">[10]</ref>. Two popular approaches are applied to align and stitch images (1) features-based, and (2) direct (or global) methods. While the former uses images features (e.g. points, edges), the latter utilizes the whole image to estimate the transformation between images. We utilized two methods introduced by Meneghetti et al. <ref type="bibr" coords="4,211.32,420.48,15.50,8.74" target="#b10">[11]</ref> and Poleg and Peleg <ref type="bibr" coords="4,320.29,420.48,15.50,8.74" target="#b11">[12]</ref> to create our DNEP images. The former can deal with the sparsely structured environment where not enough distinct features can be detected such as in the case of uniform walls, floors and ceilings in indoor scenarios, and sky and sea in outdoor scenarios. The latter can deal with non-overlapping images issue that happens due to non-continuous recording if lifelog data.</p><p>After creating a DNEP image, this image will be located into the DAE taxonomy by using scene recognition tools <ref type="bibr" coords="4,293.81,504.22,14.60,8.74" target="#b12">[13]</ref>. The Fig. <ref type="figure" coords="4,354.25,504.22,4.98,8.74">2</ref> illustrates the DNEP image of in a living room scene that created by aligning and stitching all developing data containing living room images.</p><p>The Algorithm 2 denotes the way we create DNEP image.</p><p>Algorithm 2: DNEP image building 1. Use the ground-truth data to build the training data of a given event.</p><p>2. Assign the suitable hierarchical context (e.g. indoor → in a house → in a kitchen, indoor → in a house → in a living room, indoor → in a working place → in an office) to the training data. 3. Build a lookup location table (DNEP-LT) for determining the geofencing of the event using GPS and places name tags. 4. Utilize algorithms introduced in <ref type="bibr" coords="4,294.33,656.12,15.50,8.74" target="#b10">[11]</ref> and <ref type="bibr" coords="4,332.53,656.12,15.50,8.74" target="#b11">[12]</ref> to build events DNEP image. In fact, DNEP image is a set of panorama images created by aligning and stitching all images contained in the training data. In the beginning, there could be several panorama images due to not having enough images to reflect the surrounding environment. Nevertheless, these panorama images will be merged if more images are added to the training dataset. This process can be done either by manually adding more ground-truth data or by automatically importing the images generated by Algorithm 3.</p><p>The Fig. <ref type="figure" coords="5,190.80,512.83,4.98,8.74" target="#fig_1">3</ref> and<ref type="figure" coords="5,218.75,512.83,4.98,8.74">4</ref> illustrate one example of DNEP image. In this case, there are two DNEP images that express the same environment office. Due to lack of suitable images they have temporally not yet merged together. Nevertheless, there is a ghost laptop-monitor that could be a good cue for merging two DNEP images. In this paper, this problem has not yet solved and is served for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Activities of Daily Living Understanding Using DNEP image and DAE taxonomy</head><p>The idea of our content-context-based segmentation algorithm is quite simple. We divide the environment into two distinguish categories: (1) daily-normal environment, and (2) occasionally-abnormal environment. For example, the in a home and in an office environments are assigned to the former, and the in a publicly-accessible building and in an open space are classified to the latter one.</p><p>With the daily-normal environment category, we create DNEP images and treat them as the visual background of activities, and access DAE taxonomy by top-bottom direction. Giving an image a, we simply use the object detection or template matching to justify whether this image belongs to the DNEP image of the required environment A. We than use DAE taxonomy to check how many visual concepts and activities of the current scene A the image a can satisfy to get the conclusion of what activity it is.</p><p>With the occasionally-abnormal environment, we apply DAE taxonomy in the bottom-up manner. It means that, first we try to extract as many visual concepts as possible from a given image to fulfil visual concepts of level 4. Then, we recognize the scene of the given image to match level 3 to the root. Other tasks such as location name extraction and activities detection are carried out parallel to fill the activities of level 4. Finally, we check whether the new taxonomy (just created) is matched with the DAE taxonomy of the required task.</p><p>The Algorithm 3 abstractly describes how we can carry out the contentcontext-base segmentation to meet the requirement of the challenges. (c) with each lifelog image that falls in the temporal frame defined by the event (e.g. watching TV before 7am) determine whether this image belongs to the DNEP image by applying object detection (with occlusion option). i. if successful, assign the event label to this image, and add to the buffer CANDIDATE-A. ii. if this image does not appear in the DNEP image but its location is still in the DNEP images geofencing, then use the algorithm in <ref type="bibr" coords="6,465.09,524.02,15.50,8.74" target="#b11">[12]</ref> to align and stitch this image to the DNEP image. Next, assign the event label to this image, and add it to the buffer CANDIDATE-A. iii. if this image and its location do not belong the DNEP image, then it is assigned non-event label and add to the buffer CANDIDATE-A. (d) repeat (c) until all temporal frame is scanned. (e) merge all consecutive (image-id, event label) of CANDIDATE-A if a certain number of non-event labels lay between two event clusters. In our case, this number is set to be less than 1/10 of total time when merging them together. (f) those images that are assigned as non-event-label before merging and as event-label after merging will be sent to the CANDIDATE-A set.</p><p>Further, they will be manually confirmed and automatically aligned and stitched to the relative DNEP image. This will help to decrease the number of panorama images as well as to increase the coverage of the DNEP image. 4. IF using DAE taxonomy (a) create the CANDIDATE-B buffer as {(image -id, event/non -eventlabel)} (b) load the related DAE taxonomy. (c) with each lifelog image that falls in the temporal frame defined by the event, i. detect all objects and scenes defined in the DAE taxonomy. NOTE:</p><p>In our case, we used visual concepts that contain both objects and scenes names, provided by the organizer for this task. ii. extract information of locations (GPS, place's name), and physical activities (action's names, moving/no-moving). iii. check whether this information satisfies the DAE taxonomy. If yes, add it to the CANDIDATE-B buffer as event-label. If not, add it as non-event-label. (d) merge all consecutive (image-id, event label) of CANDIDATE-B similarly to 3(e) above. 5. IF both DNEP images and DAE taxonomies are used (a) since we treat a DAE taxonomy as a foreground and a DNEP image as a background, we merge the CANDIDATE-A and CANDIDATE-B so that CANDIDATE-A should cover CANDIDATE-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The data and metrics offered by imageCLEFlifelog2018 -subtask Activities of Daily Living understanding (ADLT) are utilized to evaluate the proposed method. Ten events with given concepts are required to be detected; each detected event must be reported in the form of a triplex (topic-id, number-of-times, number-of-minutes) where topic-id is the number of the queried topic, numberoftimes reports how many times the event occurred, and number-of-minutes tells us for how long (in minutes) the event lasted. Equation ( <ref type="formula" coords="7,380.74,509.22,4.24,8.74" target="#formula_0">1</ref>) is the metric used to evaluate our results.</p><formula xml:id="formula_0" coords="7,169.78,541.60,310.81,23.23">ADL score = 1 2 (max(0, 1 - |n -n gt | n gt ) + max(0, 1 - |m -m gt | m gt ))<label>(1)</label></formula><p>where (n, n gt ) and (m, m gt ) are the (submitted value, ground-truth value) for how many times the events occurred, and for how long (in minutes) the events lasted, respectively. Based on the testset provided by the organizer, we assigned categories of DNEP image and DAE taxonomy to the ten required queries as denoted in Table <ref type="table" coords="7,162.16,632.21,4.98,8.74" target="#tab_0">1</ref> We used the same parameters of the methods we have utilized. Table <ref type="table" coords="7,442.52,644.16,4.98,8.74" target="#tab_1">2</ref> reports the results of participants in this subtask. In fact, the proposed method works case-by-case since it heavily depends on the content and context of a given event. The training phase is vital and needs manual intervention to build a suitable taxonomy depending on given activities and contexts. While the DNEP image can be generally generated without or with little manual support, the DAE taxonomy is established using peoples knowledge about the event and how many objects the system can detect and recognize from images. Thus, if the person lacks events knowledge to build the DAE taxonomy, the result of event segmentation could degrade.</p><p>For some events, both DNEP image and DAE taxonomy did not work well, e.g. in the case of "Find how many times and how long the user is having coffee in the office. Having coffee at the bars at the workplace is not considered." (topic 1 -subtask ADLT). The DNEP image successfully detects an office scene, and the DAE taxonomy can recognize a coffee cup on a table. Nevertheless, it is hard to find a suitable hint to know exactly when the person drinks the coffee. Although that person already tagged the time for drinking coffee, visual and other cues say nothing about that activity. In this case, our method almost failed to detect the right boundary of this event</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we introduce a content-context-based method to automatically detect events with given concepts from lifelog data. Data and metrics offered by imageCLEFlifelog2018 are used to evaluate the proposed method. The dailynormal environment panorama image and the daily-abnormal environment taxonomy are created to detect events. The events content (e.g. visual, textual, physiological, and GPS features) and context (e.g. concepts and taxonomy) are carefully taken into account to create DNEP image and DAE taxonomy as well as to detect events. Both DNEP image and DAE taxonomy have the ability to evolve themselves along the lifelog data time. It means that the more data gets recorded, the larger the scope of events DNEP image and DAE taxonomy can cover. In future, post-processing to polish events boundaries will be investigated. Moreover, fusion of features <ref type="bibr" coords="9,320.68,190.72,15.50,8.74" target="#b13">[14]</ref> will be evaluated to seek better evaluation. Currently, we only use features provided by the organizer. These features are somehow not enough for building a strong DAE taxonomy as well as successfully projecting an image into DNEP images. Consequently, we will develop our own features extractors to fulfil our requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,259.55,253.70,96.25,7.89;5,324.97,273.01,155.60,108.72"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig. 2. In a living room</figDesc><graphic coords="5,324.97,273.01,155.60,108.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,149.71,346.71,256.10,8.77;6,138.97,367.26,295.83,8.74;6,138.97,379.36,341.62,8.74;6,151.70,391.32,255.88,8.74;6,138.97,403.42,109.65,8.74;6,150.93,415.82,347.55,8.74;6,150.37,427.93,150.43,8.74"><head>Algorithm 3 :</head><label>3</label><figDesc>Content-Context-based Segmentation 1. Prepare DNEP images and DAE taxonomies based on the quest. 2. Pick an event. Based on the events concept, it could have a DNEP image and/or DAE taxonomy that can be used for segmentation. 3. IF using DNEP image (a) create the CANDIDATE-A buffer as {(image -id, event/non -eventlabel)} (b) load the related DNEP image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.77,115.84,345.83,249.81"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.77,115.84,345.83,249.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,136.16,115.91,358.45,147.86"><head>Table 1 .</head><label>1</label><figDesc>Dividing ten required queries into DNEP image and DAE taxonomy</figDesc><table coords="8,136.16,145.90,358.45,117.88"><row><cell cols="2">Query ID Query (ADL, context)</cell><cell>Category</cell></row><row><cell>1</cell><cell>(Drinking coffee, in an Office)</cell><cell>DNEP image, DAE taxonomy</cell></row><row><cell>2</cell><cell>(Shopping, outside Office)</cell><cell>DAE taxonomy</cell></row><row><cell>3</cell><cell>(Preparing meals, Home)</cell><cell>DNEP image, DAE taxonomy</cell></row><row><cell>4</cell><cell>(Watching TV, Home)</cell><cell>DNEP image, DAE taxonomy</cell></row><row><cell>5</cell><cell cols="2">(Listening/Watching Presentations, at Work) DAE taxonomy</cell></row><row><cell>6</cell><cell>(Using mobile device, In a vehicle)</cell><cell>DAE taxonomy</cell></row><row><cell>7</cell><cell>(Not using computers, In an office)</cell><cell>DNEP image, DAE taxonomy</cell></row><row><cell>8</cell><cell>(Walking, on the street)</cell><cell>DAE taxonomy</cell></row><row><cell>9</cell><cell>(, In a church)</cell><cell>DAE taxonomy</cell></row><row><cell>10</cell><cell cols="2">(Socialising/Eating/Drinking, In a restaurant) DAE taxonomy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,157.84,276.86,299.54,61.94"><head>Table 2 .</head><label>2</label><figDesc>Activites of Daily Living Understanding Competitive Results</figDesc><table coords="8,157.84,297.63,299.54,41.17"><row><cell>Group name</cell><cell cols="2">Percentage Dissimilarity Rank Dissimilarity</cell></row><row><cell cols="2">CIE@UTB (our group) 0.556</cell><cell>1</cell></row><row><cell>NLP-Lab</cell><cell>0.479</cell><cell>2</cell></row><row><cell>HCMUS</cell><cell>0.059</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,656.80,184.26,7.86"><p>http : //places2.csail.mit.edu/download.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,294.59,342.24,7.86;9,146.91,305.55,104.64,7.86;9,254.63,303.78,5.28,5.24;9,263.48,305.52,176.22,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,319.05,294.59,123.91,7.86">LifeLogging: Personal Big Data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,450.24,294.59,30.35,7.86;9,146.91,305.55,104.64,7.86;9,254.63,303.78,5.28,5.24;9,263.48,305.55,96.38,7.86">Journal of Foundations and Trend R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,316.56,342.24,7.86;9,146.91,327.49,294.32,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,273.80,316.56,206.79,7.86;9,146.91,327.52,73.94,7.86">State-of-the-art and future challenges in video scene detection: a survey</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Del Fabro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Böszörmeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,227.88,327.52,124.11,7.86">Journal of Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="427" to="454" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,338.54,342.24,7.86;9,146.91,349.49,333.68,7.86;9,146.91,360.45,224.71,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,367.32,338.54,113.28,7.86;9,146.91,349.49,42.64,7.86">Multimodal segmentation of lifelog data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.54,349.49,271.05,7.86;9,146.91,360.45,94.58,7.86">Procs. Large Scale Semantic Access to Content (Text, Image, Video, and Sound) (RIAO &apos;07)</title>
		<meeting>s. Large Scale Semantic Access to Content (Text, Image, Video, and Sound) (RIAO &apos;07)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="21" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,371.47,342.24,7.86;9,146.91,382.43,333.68,7.86;9,146.91,393.36,73.26,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,233.63,371.47,246.96,7.86;9,146.91,382.43,109.57,7.86">SR-clustering: Semantic regularized clustering for egocentric photo streams segmentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,262.94,382.43,213.24,7.86">Journal of Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,404.40,342.24,7.86;9,146.91,415.36,333.68,7.86;9,146.91,426.32,171.84,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,271.44,404.40,209.15,7.86;9,146.91,415.36,85.12,7.86">Approaches for Event Segmentation of Visual Lifelog Data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<idno type="DOI">10.1007/9783319736037-47</idno>
		<ptr target="https://doi.org/10.1007/9783319736037-47" />
	</analytic>
	<monogr>
		<title level="m" coord="9,271.79,415.36,94.74,7.86">MultiMedia Modeling</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="581" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,437.34,342.24,7.86;9,146.91,448.30,333.68,7.86;9,146.91,459.23,222.52,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,318.95,437.34,161.64,7.86;9,146.91,448.30,235.29,7.86">Personal-Location-Based Temporal Segmentation of Egocentric Videos for Lifelogging Applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,388.56,448.30,92.04,7.86;9,146.91,459.25,151.91,7.86">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,470.27,342.24,7.86;9,146.91,481.23,333.67,7.86;9,146.91,492.19,333.68,7.86;9,146.91,503.15,333.68,7.86;9,146.91,514.11,333.68,7.86;9,146.91,525.06,118.93,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,196.44,503.15,279.98,7.86">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,161.26,514.11,319.33,7.86;9,146.91,525.06,16.79,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction, CLEF 2018</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,536.08,342.24,7.86;9,146.91,547.04,333.68,7.86;9,146.91,558.00,333.68,7.86;9,146.91,568.96,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,442.93,536.08,37.65,7.86;9,146.91,547.04,333.68,7.86;9,146.91,558.00,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.53,558.00,212.03,7.86">Procs. CEUR Workshop, CLEF2018 Working Notes</title>
		<meeting>s. CEUR Workshop, CLEF2018 Working Notes<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,579.97,342.24,7.86;9,146.91,590.93,174.22,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,227.05,579.97,161.66,7.86">Content without Context is Meaningless</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,410.45,579.97,70.14,7.86;9,146.91,590.93,16.79,7.86">Procs. ACM MM 2010</title>
		<meeting>s. ACM MM 2010<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,601.95,337.97,7.86;9,146.91,612.91,41.75,7.86;9,191.74,611.14,5.28,5.24;9,200.59,612.88,216.70,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,202.72,601.95,176.01,7.86">Image Alignment and Stitching: A Tutorial</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,386.24,601.95,94.35,7.86;9,146.91,612.91,41.75,7.86;9,191.74,611.14,5.28,5.24;9,200.59,612.91,135.22,7.86">Journal of Foundations and Trend R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,623.92,337.97,7.86;9,146.91,634.88,333.68,7.86;9,146.91,645.84,333.67,7.86;9,146.91,656.80,171.84,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,423.89,623.92,56.70,7.86;9,146.91,634.88,287.69,7.86">Image Alignment for Panorama Stitching in Sparsely Structured Environments</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Menegetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nordberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/9783319196657-36</idno>
		<ptr target="https://doi.org/10.1007/9783319196657-36" />
	</analytic>
	<monogr>
		<title level="m" coord="9,294.12,645.84,42.94,7.86">SCIA 2015</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Paulsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Pedersend</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9127</biblScope>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.97,7.86;10,146.91,130.63,285.50,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,239.58,119.67,221.29,7.86">Alignment and Mosaicing of Non-Overlapping Images</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.91,130.63,256.84,7.86">Procs. IEEE Int. Conf. on Computational Photography (ICCP)</title>
		<meeting>s. IEEE Int. Conf. on Computational Photography (ICCP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,141.59,337.97,7.86;10,146.91,152.55,333.68,7.86;10,146.91,163.51,135.17,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,399.52,141.59,81.07,7.86;10,146.91,152.55,151.66,7.86">Places: A 10 million Image Database for Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,305.36,152.55,175.23,7.86;10,146.91,163.51,81.29,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,174.47,337.98,7.86;10,146.91,185.43,333.68,7.86;10,146.91,196.39,95.87,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,385.84,174.47,94.75,7.86;10,146.91,185.43,259.40,7.86">A Context-Aware Late-Fusion Approach for Disaster Image Retrieval from Social Media</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">N M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kasem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Nazmudeen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM ICMR</publisher>
			<pubPlace>Yokoham, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86;10,146.91,218.30,267.85,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,384.47,207.34,96.12,7.86;10,146.91,218.30,170.26,7.86">Smart Lifelogging: Recognizing Human Activities using PHASOR</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,326.20,218.30,38.39,7.86">ICPRAM</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
