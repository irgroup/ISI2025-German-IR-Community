<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.88,115.96,325.59,12.62;1,138.45,133.89,338.45,12.62">Employing Inception-Resnet-v2 and Bi-LSTM for Medical Domain Visual Question Answering</title>
				<funder ref="#_wCbRK8H">
					<orgName type="full">Ministry of Education, Science, Sports and Culture of Japan, Grantin-Aid for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.88,171.56,65.86,8.74"><forename type="first">Yangyang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.05,171.56,39.91,8.74"><forename type="first">Xin</forename><surname>Kang</surname></persName>
							<email>kang-xin@is.tokushima-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.85,171.56,37.64,8.74"><forename type="first">Fuji</forename><surname>Ren</surname></persName>
							<email>ren@is.tokushima-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.88,115.96,325.59,12.62;1,138.45,133.89,338.45,12.62">Employing Inception-Resnet-v2 and Bi-LSTM for Medical Domain Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67FD0202A044BF27888E089E3EACB7BA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-Med</term>
					<term>Inception-Resnet-v2</term>
					<term>Bi-LSTM</term>
					<term>Attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our method for generating the answers for questions based on medical images, in the ImageCLEF VQA-Med 2018 task [7][5]. Firstly, we use some image enhancement methods like clipping and questions preprocessing methods like lemmatization. Secondly, we use Inception-Resnet-v2 model (CNN) to extract image features, and use Bi-LSTM model (RNN) to encode the questions. Finally, we concatenate the coded questions with the image features to generate the answers. Our result was ranked secondly based on the BLEU, WBSS and CBSS metrics for evaluating semantic similarity, which suggests that our method is effective for generating answers from medical images and related questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual question answering (VQA) is the task of generating textual answers for questions based on the contents of images. The VQA system takes images and questions as input, and combines the information of the input to generate readable answers as output. To generate the answers of specific questions, the VQA system needs to understand the content of the images and to get related background knowledge, which involves natural language processing and computer vision techniques. On the other hand, with the increasing of pouring attention into the medical domain, the combination of VQA and medical domain has become an extremely interesting challenge. It can not only provide a reference of diagnosis to the doctor, but also allow the patient to obtain health information directly, thereby improving the efficiency of diagnosis and treatment. Existing systems like MYCIN <ref type="bibr" coords="1,227.99,584.39,15.50,8.74" target="#b12">[13]</ref> have been able to simulate the diagnostic process and generate treatment plans based on relevant medical knowledge and a series of rules.</p><p>This paper aims to generate readable answers in the ImageCLEF VQA-Med 2018 task. The dataset involves a variety of medical images, related questions and answers. We divide the data into two parts as input. We use some image enhancement methods, and generate the image features by pre-trained CNN model. As for the part of the questions, we use kinds of text preprocessing methods like lemmatization, and after that, encode the questions by RNN model. Then, we add attention mechanism to the model. At last, we formulate simple rules on the output and generate reliable answers.</p><p>The rest of this paper is organized as follows. Section 2 briefly reviews the related work of VQA-Med task. Section 3 describes the analysis of data sets and methods used for generation in details during the experiment. We report our experiment result and evaluation in section 4, and conclude this paper in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A very close study to the VQA-Med task is the VQA challenge<ref type="foot" coords="2,404.57,273.51,3.97,6.12" target="#foot_0">1</ref> . The VQA challenge has been held every year since 2016. The data set is based on open domain and includes more than 260 thousand images and 5.4 questions on average per image.</p><p>Kafle K et al. <ref type="bibr" coords="2,212.01,323.33,10.52,8.74" target="#b7">[8]</ref> and other researchers summarized quite a few methods for VQA. The majority of them used recurrent neural networks such as LSTM to encode questions, and used deep convolutional neural networks such as VGG-16 to focus on image recognition in advance. On the basis of these, there were variant models such as attention mechanisms <ref type="bibr" coords="2,335.58,371.15,14.61,8.74" target="#b16">[17]</ref>, neural modules <ref type="bibr" coords="2,427.07,371.15,9.96,8.74" target="#b0">[1]</ref>, dynamic memory <ref type="bibr" coords="2,172.81,383.10,14.61,8.74" target="#b9">[10]</ref>, and even the addition of external knowledge bases <ref type="bibr" coords="2,413.10,383.10,14.61,8.74" target="#b15">[16]</ref>, to improve the accuracy of the answers.</p><p>Deep convolutional neural networks <ref type="bibr" coords="2,312.30,407.44,10.52,8.74" target="#b8">[9]</ref> (CNN) can be used to extract the features of an image and identify the objects in it. The Inception-Resnet-v2 model <ref type="bibr" coords="2,164.47,431.35,15.50,8.74" target="#b14">[15]</ref> is one kind of advanced convolutional neural network that combines the inception module with ResNet. The remaining connections allow shortcuts in the model to make the network more efficient.</p><p>Elman J L <ref type="bibr" coords="2,202.85,467.64,10.52,8.74" target="#b2">[3]</ref> first used a recurrent neural network (RNN) to handle sequences problems. Nevertheless, context information is easily ignored when RN-N processes long sequences. The proposal of LSTM <ref type="bibr" coords="2,367.02,491.55,10.52,8.74" target="#b5">[6]</ref> alleviated the problem of long-distance dependence. Furthermore, the researchers also found that if the input sequence is reversed, the corresponding path from the decoder to the encoder will be shortened, contributing to network memory. The Bi-LSTM model <ref type="bibr" coords="2,145.27,539.37,10.52,8.74" target="#b3">[4]</ref> combines the two points above, and makes the result better.</p><p>On the other hand, there have been many Computer-aided diagnosis systems in medical imaging <ref type="bibr" coords="2,221.67,563.70,9.96,8.74" target="#b1">[2]</ref>. However, the majority of them are dealing with singledisease problems, and mainly concentrated on easily-determined regions such as the lungs and skins. The progress of the complex parts is slow. Compared with detection technology, the global lesions and structural lesions are still intensely difficult for the machines to learn.</p><p>The VQA-Med task differs from the VQA challenge in that it requires the understanding of different kinds of medical images with different body parts. The dataset of VQA-Med task consists of more than two thousand images, containing several kinds of medical images, such as computed tomography, magnetic resonance imaging, positron emission tomography, etc. However, compared to the open field VQA dataset, the number of training examples in the VQA-Med task is very small. For the deep learning models of VQA, which usually contain millions of parameters, the learning process would converge quickly with high bias, i.e. overfitting. Table <ref type="table" coords="3,255.56,349.49,4.98,8.74" target="#tab_0">1</ref> shows the statistics of the data. From the training set, there are an average of 2.4 questions per image, and a maximum of 7 questions per image. This ratio is even smaller in the validation set and test set. Additionally, there is only one reference answer for each question, which has a great limitation for answers generation. Fig. <ref type="figure" coords="3,167.31,644.17,4.40,8.74" target="#fig_0">1</ref> shows statistics about different types of questions. The number of questions started with word "what" is large in three datasets, while the questions asking positions and other questions, including Yes-no questions, occupy relatively small proportions. Moreover, the proportions of questions in the three datasets are also quite different. Therefore, it is difficult for computers to learn the characteristics from the questions in small proportion in the course of training, and the performance in validation and test may not be as good as we expected. We count the sequences of questions and answers. As shown in Fig. <ref type="figure" coords="4,454.23,572.44,4.13,8.74" target="#fig_1">2</ref>, the sequence length of the questions is obviously longer than the answers. In fact, many of the answers are just phrases and cannot form complete sentences. Through horizontal comparison, we can find that the length of sentences in the training set is longer than that of the validation set. To prevent too slow training due to the long sequences, and to prevent loss of information due to the short sequences, we fix the length of the training sentences to 9 words. This length of sentences allows us to reserve the contents of most of the questions and answers. After merging all the questions and answers separately, we calculate the word frequency in Fig. <ref type="figure" coords="5,209.31,428.25,4.13,8.74" target="#fig_2">3</ref>. In order to ensure the effectiveness of training, we plan to remove low-frequency words. Considering that it is appropriate to control the dictionary size of questions or answers within one thousand, we eventually set the words whose frequency is less than 5 as low-frequency words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing</head><p>For images, we use Inception-Resnet-v2 models to generate their features. In order to reduce the overfitting case, we adopt some image enhancement methods. Considering there are position judgments in the task, we reconstruct the picture with exceedingly small random rotations, offsets, scaling, clipping, and increase to 20 images per image (Fig. <ref type="figure" coords="5,259.50,560.48,4.06,8.74" target="#fig_3">4</ref>).</p><p>For questions, we adopt some methods like stemming and lemmatization to alter verbs, nouns, and other words into original forms, to prevent overfitting. Furthermore, there is a situation that both full name and abbreviation coexist, like "inferior vena cava" and "IVC". We have changed all these medical terms into abbreviation. There are also a lot of pure numbers and combinations of numbers and letters. Therefore, the combinations of letters and numbers used to represent positions are mapped to an "pos" token, and the pure numbers are mapped to an "num" token, so as to reduce information complexity. In addition, we try to remove useless information such as stop words. According to the word frequency distribution in data analysis, we remove the lowfrequency words to ensure training efficiency. In the meanwhile, we establish the dictionaries separately and make sure that the sizes of the dictionaries are both within one thousand.</p><p>There are some high-frequency verbs like "show" that emerge in almost every question. Several less useful adjectives like "large" also appear in questions from time to time. To cooperate with image enhancement methods, these verbs and adjectives are removed in the questions each time, so that each question is enhanced to 20 questions, and the answer remains unchanged at the same time.</p><p>The preprocessing of the answers is simpler than that of the questions. We use lemmatization and removing stop words. Besides, we create dictionaries separately and make sure that the sizes of them are within one thousand, just like questions part. However, the difference is that the low-frequency words in answers would be replaced by "abnormality" instead of simply removing them. Words with numbers have not been replaced. And the output sequences are the same as input sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VQA-Med model</head><p>The basic model we build is to combine Inception-Resnet-v2 with Bi-LSTM. Firstly, as shown in Fig. <ref type="figure" coords="6,245.45,549.19,4.13,8.74">5</ref>, the medical images are transformed into the features through the Inception-Resnet-v2 network. The pre-training weights of the Inception-Resnet-v2 are based on the Apache License<ref type="foot" coords="6,368.51,571.52,3.97,6.12" target="#foot_1">2</ref> . Secondly, the questions are fed to the embedding layer and the Bi-LSTM layer. The last time step output of the Bi-LSTM layer is reserved as the question encoding features. Thirdly, after concatenating the features of images and questions, we use another Bi-LSTM layer. And this time the output returns decoded sequences. Finally the fully connected layer outputs the predicted sequence with "softmax" activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Initial model without attention mechanism</head><p>The loss function of the model we selected is categorical cross entropy, using the following formula:</p><formula xml:id="formula_0" coords="7,245.09,297.04,231.26,30.32">H(T, q) = - n ∑ i=1 1 N log 2 q(x i ) (<label>1</label></formula><formula xml:id="formula_1" coords="7,476.35,307.45,4.24,8.74">)</formula><p>where N is the size of validation set, and q(x) is the probability of event x estimated from the training set.</p><p>Considering that the overfitting is severe with small amount of training data, we adopt a dropout value of 50%, a L2 regularization in the Bi-LSTM layers and a batch normalization after the Bi-LSTM layers. However, we find that there are some problems with the syntax and semantics of the generated answers, which is not satisfactory. In particular, the overfitting problem still exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6. Improved model with attention mechanism</head><p>To solve this problem, we add attention mechanism and modify the model. As shown in Fig. <ref type="figure" coords="7,190.94,596.35,4.13,8.74">6</ref>, the image features are converted by Inception-Resnet-v2 network, as in the previous model. Then, we use a dense layer and a repeat vector layer to deal with the image features. The questions are trained in a Bi-LSTM layer after an embedding layer and return the sequences directly, with a 50% dropout rate and a batch normalization. We adopt the attention module to integrate the features of the images and the questions. After that, we concatenate the outcomes of attention module with the question features. Eventually, the fullconnected layer outputs the predicted sequences with "softmax" activation.</p><p>We also added several simple rules to the output to make the generated answers more reasonable. It may be due to the fact that the word frequency of prepositions is relatively high, some of the generated answers have successive and repetitive prepositions outputs. Thus, we choose to delete these extra prepositions. In addition, for the answers of Yes-no questions, there is a case in which "yes" or "no" is output at the same time with other words unrelated. We choose to delete these extra words as well. Based on the performance of VQA-Med on the validation set, the parameters are set as follows. The size of dictionary is 1000, and the length of sequences is 9. The hidden size of Bi-LSTM is 128. And the batch size of training is 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model selection</head><p>The metrics method is categorical accuracy. We use the ADAM optimizer with β1 = 0.9, β2 = 0.999, ε = 10 -8 .</p><p>We set the epoch to 300, and the training process is shown in Fig. <ref type="figure" coords="9,450.92,143.39,4.13,8.74" target="#fig_4">7</ref>. The accuracy of the validation set and the degree of overfitting are both better than that without attention mechanism. The final loss of no-attention-model is over 8 while that of attention-model is about 4.5, which means it is effective to add attention mechanism.</p><p>The final result we submitted is using training set and the validation set to participate in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>The following evaluation methods are employed for evaluating the VQA-Med results.</p><p>Bilingual evaluation understudy <ref type="bibr" coords="9,296.32,294.64,15.50,8.74" target="#b11">[12]</ref> is an auxiliary tool for assessing the quality of bilingual translations. It is used to determine the degree of similarity between sentences translated by machines and by humans. BLEU uses the matching rule of N-gram to calculate the proportion of similarity between two sentences. Actually, it is to calculate the frequency of two sentences co-occurrence words. This tool is fast, and the results are also close to human evaluation scores. Nevertheless, there are also deficiencies. For instance, it is easily interfered by frequent words, cannot consider synonym expression, and do not consider grammatical accuracy. In this task, the method is used to compare the similarity between the generated answers and the referenced answers.</p><p>Word-based Semantic Similarity method is used to measure the semantics similarity between the generated answers and the factual answers at the word level by tokenizing predictions and real answers as words. This algorithm is recently used to calculate the semantic similarity in the biomedical domain <ref type="bibr" coords="9,462.34,450.54,14.61,8.74" target="#b13">[14]</ref>.</p><p>Concept-based Semantic Similarity is similar to WBSS as described above. The difference is that this metric is to extract the biomedical concepts in the predictions and the real answers respectively, then construct a dictionary. After vectorizing the words and calculating the cosine between them, the similarity could be expressed. The scores of our task submissions are shown in Table <ref type="table" coords="9,400.44,644.17,3.87,8.74" target="#tab_1">2</ref>. As can be seen from the table, the use of output rules is crucial: the scores of all three evalu-ation methods drop if it is removed. Attention mechanism is also a significant component that improves BLEU and WBSS scores.</p><p>Most of the generated results are phrases, such as "right region" and "anterior part bladder". However, since there are no medical imaging professionals who can provide suggestions for the improvement of our process, the results may differ from the actual situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we described our participation in ImageCLEF VQA-Med 2018 task, which is a problem of answering questions for the medical domain. We use images and questions enhancement preprocessing. We adopt the VQA-Med model introduced above during the training. Our result has a BLEU score of 0.135, a WBSS score of 0.174 and a CBSS score of 0.330. As can be seen, due to the small number of datasets, it is difficult to generate highly accurate answers without using external data.</p><p>Our future work will focus on making the answers more accurate. In the preprocessing section, we can classify the medical images and train them separately. External data and relevant medical knowledge can be used in data enhancement. As for the model, we consider to use other new methods, such as Hierarchical Co-Attention model <ref type="bibr" coords="10,224.98,367.09,15.50,8.74" target="#b10">[11]</ref> to improve the accuracy of answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,209.62,610.97,196.11,7.89;3,134.76,427.98,345.88,168.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of VQA-Med question types</figDesc><graphic coords="3,134.76,427.98,345.88,168.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,209.61,535.47,196.13,7.89;4,137.60,201.28,340.45,319.42"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of VQA-Med text sequences</figDesc><graphic coords="4,137.60,201.28,340.45,319.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,195.89,382.83,223.57,7.89;5,138.22,165.78,338.65,202.27"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distribution of VQA-Med text Word Frequency</figDesc><graphic coords="5,138.22,165.78,338.65,202.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,151.48,245.54,312.39,7.89;6,134.76,115.42,346.04,115.35"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of original (left) and enhanced (middle and right) images</figDesc><graphic coords="6,134.76,115.42,346.04,115.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,134.76,583.97,345.82,7.89;8,134.76,594.96,36.61,7.86;8,137.61,316.83,340.54,252.40"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Accuracy and loss of initial model(top); Accuracy and loss of improved model (bottom)</figDesc><graphic coords="8,137.61,316.83,340.54,252.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,190.11,180.88,195.65,61.61"><head>Table 1 .</head><label>1</label><figDesc>Statistics of VQA-Med data</figDesc><table coords="3,190.11,180.88,195.65,41.14"><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Images</cell><cell>2278</cell><cell>324</cell><cell>264</cell></row><row><cell>Questions</cell><cell>5413</cell><cell>500</cell><cell>500</cell></row><row><cell>Answers</cell><cell>5413</cell><cell>500</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,136.16,545.73,334.79,61.61"><head>Table 2 .</head><label>2</label><figDesc>Scores of VQA-Med task submissions</figDesc><table coords="9,136.16,545.73,334.79,41.14"><row><cell></cell><cell>BLEU</cell><cell>WBSS</cell><cell>CBSS</cell></row><row><cell cols="2">Improved model without output rules 0.103070853</cell><cell>0.147733901</cell><cell>0.3236155</cell></row><row><cell>Basic model with output rules</cell><cell>0.106454315</cell><cell>0.159756011</cell><cell>0.334431201</cell></row><row><cell>Improved model with output rules</cell><cell>0.134830654</cell><cell>0.173731936</cell><cell>0.329503441</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,124.74,7.86"><p>http://visualqa.org/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,656.80,254.05,7.86"><p>https://github.com/tensorflow/models/blob/master/LICENSE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research has been partially supported by the <rs type="funder">Ministry of Education, Science, Sports and Culture of Japan, Grantin-Aid for Scientific Research</rs>(A), <rs type="grantNumber">15H01712</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wCbRK8H">
					<idno type="grant-number">15H01712</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,492.00,337.63,7.86;10,151.52,502.96,329.06,7.86;10,151.52,513.91,67.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,362.90,492.00,98.40,7.86">Neural module networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,502.96,324.86,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,524.96,337.64,7.86;10,151.52,535.89,329.07,7.89;10,151.52,546.88,60.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,187.56,524.96,293.04,7.86;10,151.52,535.92,107.15,7.86">Computer-aided diagnosis in medical imaging: historical review, current status and future potential</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,265.57,535.92,179.30,7.86">Computerized medical imaging and graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="198" to="211" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,557.89,330.86,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,204.63,557.92,100.83,7.86">Finding structure in time</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,312.62,557.92,69.18,7.86">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,568.96,337.63,7.86;10,151.52,579.89,329.07,7.89;10,151.52,590.88,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,271.09,568.96,209.49,7.86;10,151.52,579.92,180.30,7.86">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,339.60,579.92,68.28,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,601.92,337.62,7.86;10,151.52,612.88,329.06,7.86;10,151.52,623.84,329.07,7.86;10,151.52,634.80,203.50,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,416.91,601.92,63.67,7.86;10,151.52,612.88,264.15,7.86">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,437.45,612.88,43.13,7.86;10,151.52,623.84,61.28,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="10,223.09,623.84,179.08,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,645.84,337.62,7.86;10,151.52,656.77,92.84,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,288.75,645.84,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,398.69,645.84,81.89,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,119.68,337.63,7.86;11,151.52,130.64,329.06,7.86;11,151.52,141.60,329.06,7.86;11,151.52,152.55,329.06,7.86;11,151.52,163.51,329.06,7.86;11,151.52,174.47,329.06,7.86;11,151.52,185.43,329.06,7.86;11,151.52,196.39,46.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,197.94,152.55,265.23,7.86">Overview of ImageCLEF 2018: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,163.51,329.06,7.86;11,151.52,174.47,324.97,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Ninth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="11,151.52,185.43,167.96,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,207.35,337.63,7.86;11,151.52,218.28,294.24,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,240.03,207.35,240.55,7.86;11,151.52,218.31,38.92,7.86">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,197.40,218.31,176.52,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,229.27,337.63,7.86;11,151.52,240.23,329.06,7.86;11,151.52,251.19,86.01,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,328.08,229.27,152.50,7.86;11,151.52,240.23,103.95,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,275.63,240.23,200.73,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,262.14,337.97,7.86;11,151.52,273.10,329.05,7.86;11,151.52,284.06,329.06,7.86;11,151.52,295.02,47.09,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,256.57,273.10,224.00,7.86;11,151.52,284.06,78.00,7.86">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.04,284.06,184.94,7.86">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,305.98,337.97,7.86;11,151.52,316.94,329.06,7.86;11,151.52,327.90,76.79,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,305.71,305.98,174.87,7.86;11,151.52,316.94,99.52,7.86">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,270.76,316.94,205.42,7.86">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,338.86,337.97,7.86;11,151.52,349.82,329.06,7.86;11,151.52,360.77,329.06,7.86;11,151.52,371.73,98.01,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,348.36,338.86,132.22,7.86;11,151.52,349.82,130.71,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,304.31,349.82,176.27,7.86;11,151.52,360.77,162.46,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,382.69,337.97,7.86;11,151.52,393.65,25.60,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,211.66,382.69,195.59,7.86">Computer-based medical consultations: MYCIN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shortliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,402.34,337.98,10.13;11,151.52,415.54,329.06,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,321.33,404.61,159.26,7.86;11,151.52,415.57,174.32,7.86">Biosses: a semantic sentence similarity estimation system for the biomedical domain</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sogancıoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Öztürk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Özgür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,332.52,415.57,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,426.53,337.97,7.86;11,151.52,437.49,329.06,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,360.35,426.53,120.23,7.86;11,151.52,437.49,202.47,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,374.90,437.49,21.29,7.86">AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,448.45,337.98,7.86;11,151.52,459.40,322.63,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,378.06,448.45,102.54,7.86;11,151.52,459.40,156.29,7.86">Explicit knowledge-based reasoning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02570</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.61,470.36,337.96,7.86;11,151.52,481.32,329.06,7.86;11,151.52,492.28,329.06,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,201.52,481.32,279.06,7.86;11,151.52,492.28,35.26,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,206.33,492.28,183.19,7.86">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
