<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.14,152.67,306.76,12.64;1,125.06,170.67,345.17,12.64;1,282.41,188.67,30.45,12.64">A Cross Modal Deep Learning Based Approach for Caption Prediction and Concept Detection by CS Morgan State</title>
				<funder ref="#_VAATq7C">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,249.29,227.70,96.56,8.96"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<postCode>21251</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Morgan State</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.14,152.67,306.76,12.64;1,125.06,170.67,345.17,12.64;1,282.41,188.67,30.45,12.64">A Cross Modal Deep Learning Based Approach for Caption Prediction and Concept Detection by CS Morgan State</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF91B261CEAA9EDF813C9021DE0A404C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Caption generation</term>
					<term>Concept Detection</term>
					<term>Cross Modality</term>
					<term>Deep Learning</term>
					<term>Encoder-Decoder</term>
					<term>Merge Architecture</term>
					<term>Performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in the ImageCLEFcaption under ImageCLEF 2018. The problem of automatic image caption prediction involves outputting a human-readable and concise textual description of the contents of a figure appeared in a biomedical journal. It is a challenging problem in biomedical literature as it requires techniques from both the fields of Computer Vision to understand the visual contents of the image and Natural Language Processing (NLP) to turn the understanding of the image into words in the right order to generate the textual description for caption. Recently, deep learning methods have achieved state-of-the-art results on this challenging problem in general domain of natural photographic images. For the tasks of caption prediction and concept detection, we used the merge architectures for the encoder-decoder recurrent neural network (RNN) due to it's success over inject for caption generation. The merge model combines both the encoded form of the image input with the encoded form of the text description generated so far. The combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence. In this article, we present main objectives of experiments, overview of these approaches, resources employed, and describe our submitted runs and results with conclusions and future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes our second year's participation in ImageCLEF 2018 <ref type="bibr" coords="2,428.98,210.18,11.69,8.96" target="#b0">[1]</ref> for the ImageCLEF caption track which consists of both Concept Detection and Caption Prediction Tasks <ref type="bibr" coords="2,194.23,234.18,10.75,8.96" target="#b1">[2]</ref>. For the Concept Detection, participating systems are tasked with identifying the presence of relevant UMLS concepts in images appeared in biomedical journal articles (PubMed Central). For the Caption Prediction, participating systems are tasked with composing coherent captions for the entirety of an image based on the interaction of visual information content and the detected concepts from the first task. There has been a lot of interest recently from Computer Vision, Machine Learning and NLP community in developing deep learning-based approaches for automatic caption generation of natural photographic images <ref type="bibr" coords="2,333.55,330.21,10.90,8.96" target="#b2">[3,</ref><ref type="bibr" coords="2,344.46,330.21,7.27,8.96" target="#b3">4]</ref>. The problem of image caption generation involves outputting a readable and concise description of the contents of a photograph. It is a challenging artificial intelligence problem as it requires both techniques from computer vision to interpret the contents of the photograph and techniques from natural language processing to generate the textual description. Recently, deep learning methods have achieved state of the art results on examples of this problem. In general, a standard encoder-decoder recurrent neural network architecture is used to address the image caption generation problem. Generally, a pre-trained convolutional neural network (CNN) is used to encode the images and a recurrent neural network, such as a Long Short-Term Memory (LSTM) network, is used to either encode the text sequence generated so far, and/or generate the next word in the sequence <ref type="bibr" coords="2,124.70,462.23,10.90,8.96" target="#b4">[5,</ref><ref type="bibr" coords="2,135.61,462.23,7.27,8.96" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach of Developing Deep Learning Model</head><p>The provided datasets have separate training and test sets, which are really just different groups of image identifiers in separate text files respectively (such as Caption-Training2018-List.txt and CaptionTesting2018-List.txt in case of caption prediction task). From these file names, we extracted the photo identifiers and use these identifiers as keys to filter photos and descriptions for each set. The merge model (Fig. <ref type="figure" coords="3,231.25,332.01,4.17,8.96" target="#fig_0">1</ref>) combines both the encoded form of the image input with the encoded form of the text description generated so far, which is handled by Recurrent Neural Networks (RNNs). This separates the concern of modeling the image input, the text input and the combining and interpretation of the encoded inputs <ref type="bibr" coords="3,447.58,368.01,10.69,8.96" target="#b7">[8]</ref>. It is experimentally found <ref type="bibr" coords="3,223.35,380.01,11.85,8.96" target="#b8">[9]</ref> that the merge architecture is more effective compared to the inject approach, where both image and word information is injected into the RNN. Hence, we followed this approach for both tasks in ImageCLEFcaption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Plot of the Deep Learning Model for Caption/Concept Generation</head><p>The entire process of the model generation is shown as a plot in Fig. <ref type="figure" coords="4,418.82,463.07,4.98,8.96">2</ref> and can be described in three parts as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Feature Extractor</head><p>We loaded all the images from a subset of the original training set, extract their features using a pre-trained 16-layer VGG model <ref type="bibr" coords="4,321.97,537.11,10.72,8.96" target="#b4">[5]</ref>, and store the extracted features keyed on the image id to a new file that is later loaded and used as input for training a language model. We pre-processed the images with the VGG model (without the output layer) trained on ImageNet dataset <ref type="bibr" coords="4,296.32,573.11,11.69,8.96" target="#b6">[7]</ref> and used the extracted features predicted by this model as input. Keras also provides tools for reshaping the loaded photo into the preferred size for the model (e.g. 3 channel 224 x 224 pixel image). The Visual Feature Extractor model expects input photo features to be a vector of 4,096 elements. These are processed by a Dense layer to produce a 256-element representation of the photo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Feature Extractor and Sequence Processor</head><p>This is a word embedding layer for handling the text input, followed by a LSTM layer. The model expects input sequences with a pre-defined length which are fed into an embedding layer that uses a mask to ignore padded values. This is followed by an LSTM layer with 256 memory units. Both the input models produce a 256-element vector. Further, both input models use regularization in the form of 50% dropout. This is to reduce overfitting the training dataset, as this model configuration learns very fast. The Decoder model merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then to a final output Dense layer that makes a softmax prediction over the entire output vocabulary for the next word in the sequence <ref type="bibr" coords="5,231.53,398.25,10.69,8.96" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoder</head><p>Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction. The model we will develop will generate a caption given a photo, and the caption will be generated one word at a time. The sequence of previously generated words will be provided as input. Therefore, we will need a first word to kick-of the generation process and a last word to signal the end of the caption. We used the strings startseq and endseq for this purpose. Each caption of an associated image is tokenized into words after performing some text processing and cleaning operations. The model is provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. For example, the input sequence "Mandibular true occlusal radiograph" would be split into five input-output pairs to train the model:</p><p>The Decoder model merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then to a final output Dense layer that makes a softmax prediction over the entire output vocabulary for the next word in the sequence. Later, when the model is used to generate descriptions, the generated words are concatenated and recursively provided as input to generate a caption for an image. The Keras functional API <ref type="bibr" coords="6,319.75,222.18,16.76,8.96" target="#b9">[10]</ref> is used to define the model (Figure <ref type="figure" coords="6,141.59,234.18,4.17,8.96">2</ref>) as it provides the flexibility needed to define a model that takes two input streams and combines them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Software and Experimental Environment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs and Results</head><p>This section provides descriptions of our submitted runs and analysis of the result. We (morgan_cs) tried to submit few runs for each task of caption prediction and concept detection. However, only a single run for each task was graded successfully, whereas others were failed due to processing errors.</p><p>Both runs are generated based on the deep learning method (merge model) described in previous sections. We almost implemented the same process of feature (textual caption and visual) extraction and model generation for training for both tasks. Results are generated based on the test sets (9,938 images) for captions and concepts. However, for training we only used only a small subset (around 4K images) of the original training set (&gt; 222K images) as provided by the organizers. This limitation is due to the limited resources (both memory and computing power) that was available during the model generation and result submission steps in this evaluation campaign.  <ref type="table" coords="7,150.48,240.18,3.76,8.96" target="#tab_0">1</ref>. We also submitted mistakenly a run for caption prediction (which was graded successfully with a 0.0 score although it was meant for the submission of concept detection task). We failed to submit few other runs. However, there were some problems in our runs (result files) and received errors while the files were parsed by the online evaluation tool under crowdAI provided by the CLEF organizer.</p><p>For the sole submission (ID: 6216) of concept detection, we received a F1 score of 0.0417 (ranked 4th group wise) which is comparatively lower compared to the leading group (UA.PT_Bioinformatics) with a score of 0.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This article describes the strategies of the second year's participation of the Morgan CS group for the concept detection and caption prediction tasks of ImageCLEFcaption <ref type="bibr" coords="7,143.90,425.37,10.69,8.96" target="#b1">[2]</ref>. Motivated by the state-of-the-art results on caption generation problems in general domain, we developed a cross modal deep learning model by using both image and text features in a merge architecture to accomplish these tasks. We achieved comparable results (ranked in the middle group wise for both tasks) considering the limited resources (computing and memory power) we had at the time of the submission. In future, we plan to experiment with larger pre-trained models (such as, Inception, ResNet, Google Net, etc.) for photo feature extraction that offer better performance on the ImageNet dataset <ref type="bibr" coords="7,258.11,509.39,10.81,8.96" target="#b6">[7]</ref>. Also, better performance might be achieved by using word vectors trained on a much larger corpus of text, such as news articles or Wikipedia. We will also perform our experiments (e.g. model generation and prediction) in a cloud infrastructure (such, as Amazon AWS) by exploring alternate configuration and exploiting to full capacity of the ground truth dataset (e.g., training sets).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,188.57,308.90,218.18,8.10;3,124.70,147.35,366.60,157.70"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Merge Architecture for Encoder-Decoder Model [9]</figDesc><graphic coords="3,124.70,147.35,366.60,157.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,129.86,599.20,335.48,8.10;5,158.75,525.85,277.70,71.25"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of transforming into input and output Sequences from image description<ref type="bibr" coords="5,454.90,599.20,10.44,8.10" target="#b5">[6]</ref> </figDesc><graphic coords="5,158.75,525.85,277.70,71.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,124.70,314.61,345.44,8.96;6,124.70,326.13,345.97,8.96;6,124.70,337.65,345.71,8.96;6,124.70,349.17,345.70,8.96;6,124.70,360.69,345.89,8.96;6,124.70,372.09,345.97,8.96;6,124.70,383.61,345.69,8.96;6,124.70,395.13,345.79,8.96;6,124.70,406.65,345.68,8.96;6,124.70,418.17,260.18,8.96"><head>For</head><label></label><figDesc>implementation of our deep learning model and submission of the runs, we used Keras (version 2.1.5) library in a Python 3 (version 3.5.2) SciPy environment on top of TensorFlow (version 1.1.0) backend. Keras provides a clean and convenient way to create a range of deep learning models on top of both Theano or TensorFlow backend which can be executed on both GPUs and CPUs given the underlying frameworks. Other necessary libraries, such as scikit-learn, OpenCV, Pandas, NumPy pickle, and Matplotlib are also installed and used to support visual and text feature extraction for model inputs as well as model generation, and saving and plotting the model. For training, the model was fitted for 30 epochs and given the amount of training data each epoch took around 3 hours in a window-based workstation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,139.20,147.35,328.20,288.85"><head></head><label></label><figDesc></figDesc><graphic coords="4,139.20,147.35,328.20,288.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,155.30,660.55,280.25,27.66"><head>Table 1 .</head><label>1</label><figDesc>Results (Top 3) of the Caption Prediction Task.</figDesc><table coords="6,155.30,680.11,280.25,8.10"><row><cell>Group ID</cell><cell>Entries</cell><cell>BLUE</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is supported by an <rs type="funder">NSF</rs> <rs type="grantName">HBCU-UP Research Initiation Award</rs> (<rs type="grantNumber">ID. 1601044</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VAATq7C">
					<idno type="grant-number">ID. 1601044</idno>
					<orgName type="grant-name">HBCU-UP Research Initiation Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,132.67,291.02,338.04,8.10;8,141.74,302.06,328.66,8.10;8,141.74,313.10,282.05,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,141.74,302.06,253.16,8.10">Overview of ImageCLEF 2018: Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,401.67,302.06,68.73,8.10;8,141.74,313.10,203.04,8.10">Proceedings of the Ninth International Conference of the CLEF Association</title>
		<meeting>the Ninth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,324.02,337.78,8.10;8,141.74,335.06,328.60,8.10;8,141.74,346.10,54.84,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,157.45,335.06,224.29,8.10">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,389.06,335.06,77.73,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,357.02,337.58,8.10;8,141.74,368.06,209.81,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,379.37,357.02,90.88,8.10;8,141.74,368.06,89.90,8.10">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,237.77,368.06,23.63,8.10">CVPR</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="3156" to="3164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,379.10,337.86,8.10;8,141.74,390.02,328.86,8.10;8,141.74,401.06,129.01,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,382.15,379.10,88.37,8.10;8,141.74,390.02,228.01,8.10">Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge</title>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,376.15,390.02,94.45,8.10;8,141.74,401.06,46.24,8.10">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,412.01,337.95,8.19;8,141.74,422.93,252.15,8.18;8,408.94,422.93,26.11,8.18;8,450.06,422.93,20.38,8.18;8,141.74,434.06,214.13,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,299.49,412.01,171.13,8.18;8,141.74,422.93,114.42,8.18">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014. 32, 86, 171</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,445.10,338.07,8.10;8,141.74,456.02,133.55,8.10" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Brownlee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blog</forename></persName>
		</author>
		<ptr target="https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,467.08,337.93,8.10;8,141.74,478.12,157.86,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,363.67,467.08,106.92,8.10;8,141.74,478.12,93.58,8.10">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,251.32,478.12,20.61,8.10">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,489.04,329.09,8.10;8,141.74,500.08,198.62,8.10;8,141.74,511.12,158.04,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,312.53,489.04,149.23,8.10;8,141.74,500.08,50.66,8.10">Where to put the Image in an Image Caption Generator</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Tanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">P</forename><surname>Camilleri</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324918000098</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,198.53,500.08,109.41,8.10">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">03</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,522.04,316.53,8.10;8,141.74,533.08,321.32,8.10;8,141.74,544.12,316.49,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,314.79,522.04,134.41,8.10;8,141.74,533.08,181.76,8.10">What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Tanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">P</forename><surname>Camilleri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02043</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.99,533.08,131.07,8.10;8,141.74,544.12,156.73,8.10">Proc. 10th International Conference on Natural Language Generation (INLG&apos;17)</title>
		<meeting>10th International Conference on Natural Language Generation (INLG&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct coords="8,132.40,555.04,203.94,8.10" xml:id="b9">
	<monogr>
		<ptr target="https://keras.io/applications/" />
		<title level="m" coord="8,141.74,555.04,84.13,8.10">Keras Applications API</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
