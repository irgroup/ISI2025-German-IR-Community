<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.27,115.96,272.81,12.62;1,144.29,133.89,326.78,12.62;1,144.79,151.82,325.78,12.62">Aristotle University&apos;s Approach to the Technologically Assisted Reviews in Empirical Medicine Task of the 2018 CLEF eHealth Lab</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.29,189.49,81.74,8.74"><forename type="first">Adamantios</forename><surname>Minas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.50,201.45,91.88,8.74"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.27,115.96,272.81,12.62;1,144.29,133.89,326.78,12.62;1,144.79,151.82,325.78,12.62">Aristotle University&apos;s Approach to the Technologically Assisted Reviews in Empirical Medicine Task of the 2018 CLEF eHealth Lab</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85CE160785E38CA49CDE4271AEF00B1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Topic field Document field(s) 1, 2 48 log(BM25) Protocol -Reference Standards Title,</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Systematic reviews are literature reviewing processes that aim to retrieve all relevant content based on a specific topic, in an exhaustive manner. Such reviews are particularly useful in healthcare, where decision making must take into account all possible evidence, and are usually done by constructing a boolean query and submitting it to a database, and then screening the retrieved documents for relevant ones. Task 2 of CLEF 2018 eHealth lab focuses on automating this process on two fronts: Sub-Task 1 is about bypassing the construction of the boolean query, retrieving relevant documents and ranking them by relevance based on a protocol that describes a topic, and Sub-Task 2 is about ranking the documents retrieved by an already constructed query by Cochrane experts. We present our approaches for both sub-tasks, which combine a learning-to-rank model trained on multiple reviews with a model incrementally trained on each individual review using relevance feedback.</p><p>Table <ref type="table" coords="6,184.28,115.91,4.13,7.89">1</ref>. Set of features employed by the inter-topic model for Sub-Task 1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Systematic reviews are a crucial part of Evidence-Based Medicine, which uses any current evidence to support a decision on how a patient will be treated. These reviews aim to find the aforementioned evidence, which should fit some criteria in order to take part in the final decision-making. Systematic reviews can be broken down into a 3-step process:</p><p>1. Document Retrieval: An expert builds a boolean query that describes their review topic, which is later submitted to a medical database. Boolean queries are queries that define if a document is relevant by the existence (or not) of user-specified terms in the document. By using boolean logic, complex queries with multiple rules can be constructed in order to filter through large amounts of information. 2. Title and Abstract Screening: After the possibly relevant documents have been retrieved, they must be screened to find the truly relevant ones. Screening takes part in two stages: in the first stage, experts review each retrieved document's title and abstract, and decide if it is non-relevant, or if it is possibly relevant and must be read in full to decide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Document Screening:</head><p>The second stage of screening is reading the full text of the document that passed through the first screening stage, and deciding if it should take part in the review. Document screening is the most time-consuming task of this process. Medical databases are expanding rapidly -PubMed counts 26,759,399<ref type="foot" coords="2,422.89,178.86,3.97,6.12" target="#foot_0">1</ref> citations as of 2017. Boolean queries on such databases are bound to retrieve a large amount of documents, hence the need for automation in such a task. This, however, is a complex problem, due to the imbalance of the data (few relevant documents, too many non-relevant documents) and the misclassification cost, where not including a relevant document might have a great toll on the final decision making.</p><p>Task 2 <ref type="bibr" coords="2,183.89,253.08,10.52,8.74" target="#b0">[1]</ref> of CLEF 2018 eHealth lab <ref type="bibr" coords="2,321.84,253.08,10.52,8.74" target="#b1">[2]</ref> focuses on the first two parts of the systematic review process. Our approach consists of phrase extraction and querying for the document retrieval step, as well as a hybrid classification model for the title and abstract screening step, which initially ranks the retrieved documents using Learning-to-Rank (LTR) features and then uses relevance feedback to iteratively re-rank them, based on simple text representations.</p><p>The rest of this paper is organized as follows: we briefly describe Task 2 of CLEF 2018 eHealth lab in Section 2, and in Section 3 we analyze our approaches. Section 4 contains the results and the submitted runs, and finally Section 5 concludes and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Overview</head><p>This year, CLEF eHealth's Task 2 was split into two sub-tasks. Sub-Task 1 was about searching in PubMed for relevant documents given a piece of text, while Sub-Task 2 was the same as last year's CLEF eHealth Task 2.</p><p>Sub-Task 1 aims to bypass the first part of a Systematic Review -the construction of the boolean query, that would later on be submitted in a database to retrieve possibly relevant documents.</p><p>Given 40 topics as training set and 30 as test set, participants were asked to return a ranking with a maximum of 5000 documents per topic. Each topic contained its id, title and objective, as well as a protocol that described that particular topic. Each topic protocol had 6 fields, with another objective field that was slightly different than the topic's one: For each topic, participants were also provided with a date cut-off. This cut-off was also used in the Boolean Queries that were constructed by Cochrane experts to retrieve relevant documents.</p><p>Sub-Task 2 concerns the efficient ranking of the possibly relevant documents retrieved. Given a topic, its query and the documents retrieved, the goal is to rank the documents so that most relevant ones appear first, as well as to find a threshold, after which no documents will be shown to the user. The training set consisted of 42 topics, where each topic contained:</p><p>1. A unique topic ID 2. A title 3. An Ovid MEDLINE boolean query, constructed by Cochrane experts 4. The PubMed IDs as returned from the execution of the boolean query For both tasks, the relevant document PIDs (PubMed IDs) were provided as well, for abstract and content relevance. This enabled the use of algorithms that requested relevance feedback from the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>For both sub-tasks, we used last year's model <ref type="bibr" coords="3,341.21,352.69,10.52,8.74" target="#b2">[3]</ref> with some enhancements, as well as some modifications for Sub-Task 1. It consists of two models:</p><p>1. An inter-topic XGBoost <ref type="bibr" coords="3,257.53,384.94,10.52,8.74" target="#b3">[4]</ref> classifier that is trained on LTR features between a topic and a document and produces an initial ranking of the documents. This inter-topic model is trained on all the training topics. 2. An intra-topic Support Vector Machine (SVM) classifier that is iteratively trained on TF-IDF vectors after asking feedback for documents that are ranked the highest by the inter-topic model. This intra-topic model is trained for each of the test topics using relevance feedback at prediction time.</p><p>Algorithm 1 describes the re-ranking algorithm employed by the intra-topic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sub-Task 1: No Boolean Search</head><p>The first step for Sub-Task 1 was to find the initial relevant documents. For each topic, we used its title and objective to create queries that were later submitted to PubMed. To construct the queries, we tokenized both pieces of text, removed the stop-words, and extracted phrases from the resulting word lists. Figure <ref type="figure" coords="3,475.61,572.43,4.98,8.74" target="#fig_2">1</ref> shows an example of this process.</p><p>The phrases we extracted were n-grams (n ∈ {2, 3, 4, 5, 6}) of the words of each piece of text. Each phrase was then submitted to PubMed, with the date cut-off as given for each topic, for which we retrieved a maximum of 2500 documents.</p><p>For the query construction, we also experimented with TextRank <ref type="bibr" coords="3,451.90,644.16,9.96,8.74" target="#b4">[5]</ref>, an algorithm for keyword extraction. After extracting the keywords from both the title and the objective, we created the queries the same way as described above, where the text of each topic was now its keywords. This process did not seem to work well, as it decreased the total recall. We further experimented with the number of maximum allowed documents per query, where we had to trade between recall and number of documents retrieved. The 2500 limit proved to be a good fit, since retrieving more documents would not increase recall significantly, but would require our models to rank many more documents.</p><p>After retrieving the possibly relevant documents per topic, we use the intertopic and intra-topic models to rank them. The LTR features used for the intertopic model were computed using the title and abstract of each document and the different fields of each topic protocol, as well as the topic's title and objective. Table <ref type="table" coords="4,196.99,608.30,4.98,8.74">1</ref> shows the features employed by our model. For the inter-topic model, we use an Easy Ensemble <ref type="bibr" coords="4,281.64,620.25,10.52,8.74" target="#b5">[6]</ref> of 10 XGBoost classifiers, where each classifier is trained on all the relevant documents and a subset of the non-relevant documents, randomly sampled, sampling 5 non-relevant documents per each relevant. After getting an initial ranking from the inter-topic model, we use the intratopic model to re-rank up to the first 20,000 documents, and keep the first 5000, as per the task's limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sub-Task 2: Abstract and Title Screening</head><p>For the second sub-task, we also employed last year's model with a few modifications on both the inter-topic and the intra-topic model.</p><p>Inter-Topic Model On the Inter-Topic model, we included some semantic information using additional LTR features. Table <ref type="table" coords="5,361.25,455.14,4.98,8.74">2</ref> shows the features, with which we previously experimented, along with the new semantic features. We further advanced our model by removing the stop-words and fixed some minor issues with the BM25 <ref type="bibr" coords="5,231.92,491.01,10.52,8.74" target="#b6">[7]</ref> features.</p><p>Features 1-24 are the same as last year's submission. We distinguish between two topic fields -the query, which is a list of Medical Subject Headings (MeSH) terms extracted from the topic's Ovid Medline query and the title. MeSH terms are semantic annotations added manually on PubMed documents. The notation used for the LTR features is as follows:</p><p>1. t is a topic field 2. d is a document field 3. c(t i , d) counts the number of times the term t i appears on the document field term m i appears on the document field d 5. |C| is the total number of documents in the collection 6. df (t i ) is the number of documents that contain the term t i 7. levenshtein(m i , d j ) is the levenshtein distance betwen the MeSH term m i and the term d j</p><p>For features 25 and 26, we applied Latent Semantic Analysis to the TF-IDF vectors of the titles and the abstracts of each document, keeping 200 components. Then for each document in a topic, we computed the cosine similarity of their LSA vectors (topic title -document title and topic title -document abstract).</p><p>Features 27 and 28 use Word2Vec <ref type="bibr" coords="6,307.29,550.01,10.52,8.74" target="#b7">[8]</ref> vectors, obtained from the BioASQ challenge<ref type="foot" coords="6,174.34,560.39,3.97,6.12" target="#foot_1">2</ref> . These vectors were trained on 10,876,004 abstracts from PubMed, with a vocabulary of 1,701,632 words and a dimensionality of 200. For each piece of text, we sum up all its word vectors and average, which results in a single vector representing the document. Then, we compute the cosine similarities between a topic and a document using these vectors.</p><p>Features 29 and 30 use the Word2Vec vectors again, this time to compute the Word Mover's Distance <ref type="bibr" coords="6,257.11,634.35,10.52,8.74" target="#b8">[9]</ref> between pieces of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>In this paper, we described our approaches for both sub-tasks of Task 2 of CLEF eHealth 2018. We introduced new features and tweaked last year's models to improve performance, with a tendency towards semantic features.</p><p>As future work, we believe that more improvements can be made in both subtasks. For Sub-Task 1, the query construction stage could benefit from filtering out words that are not medically relevant, in order to reduce the number of queries and consequently reduce the number of retrieved documents. For the ranking model (sub-tasks 1 and 2), more semantic features could benefit the inter-topic model, while a better strategy for asking feedback in the intra-topic model could boost the metrics. Finally, it would be interesting to apply deep learning techniques to the task, and try to use word embeddings in a more efficient way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,142.24,121.70,323.41,8.77;4,142.24,133.68,26.29,8.74;4,149.71,145.35,274.87,7.89;4,189.63,156.33,272.00,7.86;4,189.63,167.29,258.08,8.35;4,189.63,178.25,189.49,8.35;4,149.71,189.18,227.88,7.89;4,140.21,200.17,94.54,7.86;4,411.92,200.81,61.20,7.47;4,140.21,211.10,80.21,7.89;4,140.21,223.43,4.51,6.14;4,165.05,222.08,84.72,7.86;4,140.21,236.87,39.20,7.86;4,140.21,247.81,330.38,7.89;4,140.21,260.13,4.51,6.14;4,165.05,258.79,48.25,7.86;4,140.21,271.09,4.51,6.14;4,165.05,269.75,93.94,8.68;4,140.21,284.51,332.91,8.37;4,140.21,296.84,4.51,6.14;4,165.05,295.50,92.05,7.86;4,279.72,296.14,193.00,7.47;4,170.03,307.10,235.37,7.47;4,135.70,318.76,9.03,6.14;4,165.05,317.42,307.67,8.11;4,170.03,328.37,280.06,8.11;4,135.70,340.67,9.03,6.14;4,165.05,339.31,155.12,7.89;4,135.70,351.63,9.03,6.14;4,180.39,350.29,61.18,7.86;4,135.70,362.59,9.03,6.14;4,165.05,361.23,16.87,7.89;4,135.70,373.55,9.03,6.14;4,180.39,372.21,82.56,8.35;4,135.70,388.34,9.03,6.14;4,165.05,386.97,104.33,7.89;4,135.70,399.30,9.03,6.14;4,180.39,397.96,148.10,8.68;4,135.70,416.55,107.35,7.89"><head>Algorithm 1 : 6 k</head><label>16</label><figDesc>The Iterative relevance feedback algorithm of the intra-topic model Input : The ranked documents R, of length n, as produced by the inter-review model, initial training step k, initial local training step stepinit, secondary local training step step secondary , step change threshold tstep, final threshold t f inal (optional) Output: Final ranking of documents R -f inalRanking 1 f inalRanking ← () ; // empty list 2 for i = 1 to k do 3 f inalRankingi ← Ri 4 k ← k; 5 while not f inalRanking contains both relevant and irrelevant documents do ← k + 1; 7 f inalRanking k = R k ; 8 while not length(f inalRanking) == n OR length(f inalRanking) == t f inal do 9 train(f inalRanking) ; // Train a local classifier by asking for abstract or document relevance for these documents 10 localRanking = rerank(R -f inalRanking) ; // Rerank the rest of the initial list R from the predictions of the local classifier 11 if length(f inalRanking) &lt; tstep then 12 step = stepinit; 13 else 14 step = step secondary ; 15 for i = k to k + step do 16 f inalRankingi ← localRanking i-k ; 17 return f inalRanking;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,169.73,280.99,275.89,7.89;5,152.06,115.83,311.25,150.39"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of the query construction pipeline of sub-task 1.</figDesc><graphic coords="5,152.06,115.83,311.25,150.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,151.70,607.78,5.19,8.74;5,138.97,619.91,341.62,9.65"><head>d 4 .</head><label>4</label><figDesc>c(m i , d) counts the number of times the MeSH (Medical Subject Headings)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.44,282.44,7.47"><p>https://www.nlm.nih.gov/bsd/licensee/2017_stats/2017_LO.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,657.44,84.73,7.47"><p>http://bioasq.org/</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>Semantic Features 25 cos(LSA(tf-idf))</head><p>T -D Title Title 26 cos(LSA(tf-idf)</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Description</head><p>Category Topic field Document field 1 </p><p>Feature 31 uses document vector representations which we obtained by training a Doc2Vec <ref type="bibr" coords="7,199.78,602.87,15.50,8.74" target="#b9">[10]</ref> model on the documents collected from the training set. The model was trained on each document's title and abstract. The vectors for documents not in the model's training set were inferred.</p><p>The new semantic features seemed to improve performance, but some of them proved to be better than others. For the final runs, from the semantic features we kept only 25, 26, 29 and 30, which use the Latent Semantic Analysis and the Word Mover's Distance.</p><p>Apart from adding new LTR features, we experimented with a variety of other techniques. First, we tried expanding the title query with more words, to obtain a bigger piece of text, so as to compute more accurate similarities. For each word in the title, we found its K most similar words using cosine similarity on the Word2Vec embeddings and added them to the title. Even for small values of K (e.g. 2) this did not seem to improve performance. We further tested to provide the document vectors (query title, document) from Doc2Vec directly to the inter-topic model, either concatenated or subtracted one from another, which still did not improve performance. Lastly, we experimented with undersampling techniques -specifically Easy Ensemble and SMOTE <ref type="bibr" coords="8,434.04,254.66,14.61,8.74" target="#b10">[11]</ref>, which did not improve performance either. On the contrary, Easy Ensemble works well for the first sub-task, where the number of non-relevant documents is on average an order of magnitude larger.</p><p>Intra-Topic Model For the intra-Topic model, we relaxed the C parameter of the SVM, which controls how "strict" the hyperplane will be in avoiding misclassification to allow for a bigger margin. The intuition for this came from the fact that due to the sheer class imbalance, finding a hyperplane with a bigger margin will probably fit the data better than finding a strict one which may lead to overfitting. This relaxation seemed to improve the model's predictions in our evaluations.</p><p>Additionally, we experimented with different SVM kernels, but they proved much slower and less efficient than the linear one. We also added n-grams (2, 3) but they did not give better results either. Finally, we tried to use embeddings for this task as well, by using the average Word2Vec vectors or the document vectors from Doc2Vec as input instead of the simple TF-IDF representations, to no avail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Both sub-tasks of CLEF E-health Task 2 supported both thresholded and nonthresholded runs. Our models, however, do not apply a threshold to the final ranking automatically -instead, we submitted thresholded runs on fixed handpicked thresholds.</p><p>The metrics used for evaluation were multiple and they are described in detail in the task's website 3 . The primary ones (as mentioned in the task's website) are the Mean Average Precision and the Recall, on which we focus below. Note that in the official evaluation script 4 , which we used to produce the following results, Mean Average Precision is computed on the whole ranking, without taking the threshold into account.</p><p>Table <ref type="table" coords="9,177.55,142.90,4.98,8.74">3</ref> shows our results for sub-task 1. The reranking parameters for the intra-topic model of HybridSVM are:</p><p>The Threshold column refers to the hand-picked threshold mentioned above, and the Train Relevance column refers to which relevances were used for trainingabstract or content. For evaluation, content relevance was used as per the competition's guideline. We submitted runs 1, 2 and 3, since we only found that training with abstract relevance gave slightly better results after the submission deadline. This is, however, an interesting observation -since there are more relevant documents at abstract level than at content level, the class imbalance was slightly less effective when training with the abstract relevance, thus producing slightly better results. Table <ref type="table" coords="9,178.16,449.22,4.98,8.74">4</ref> shows our results for the second sub-task. The only threshold we used is at 1000 documents. The last 4 columns refer to the parameters of the intra-topic's re-ranking algorithm. From the runs shown, we submitted runs 1, 5 and 7. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,328.74,337.63,7.86;10,151.52,339.70,329.07,7.86;10,151.52,350.66,271.28,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,409.70,328.74,70.89,7.86;10,151.52,339.70,228.83,7.86">CLEF 2018 Technology Assisted Reviews in Empirical Medicine Overview</title>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rene</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,400.23,339.70,80.36,7.86;10,151.52,350.66,193.07,7.86">CLEF 2018 Evaluation Labs and Workshop: Online Working Notes</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,361.53,337.64,7.86;10,151.52,372.48,329.07,7.86;10,151.52,383.44,329.07,7.86;10,151.52,394.40,329.07,7.86;10,151.52,405.36,211.80,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,288.58,383.44,192.01,7.86">Overview of the CLEF eHealth Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rene</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aurlie</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lionel</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joao</forename><surname>Palotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,175.91,394.40,267.29,7.86">CLEF 2018. In 8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,451.64,394.40,28.95,7.86;10,151.52,405.36,137.85,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,416.23,337.64,7.86;10,151.52,427.19,329.07,7.86;10,151.52,438.15,329.07,7.86;10,151.52,449.11,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,196.09,427.19,284.49,7.86;10,151.52,438.15,256.56,7.86">Combining inter-review learning-to-rank and intra-review incremental training for title and abstract screening in systematic reviews</title>
		<author>
			<persName coords=""><forename type="first">Antonios</forename><surname>Anagnostou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Athanasios</forename><surname>Lagopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.71,438.15,47.88,7.86;10,151.52,449.11,181.16,7.86">CLEF 2017 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,459.98,337.63,7.86;10,151.52,470.94,329.07,7.86;10,151.52,481.90,293.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,299.35,459.98,34.71,7.86">XGBoost</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,358.08,459.98,122.51,7.86;10,151.52,470.94,329.07,7.86;10,151.52,481.90,35.36,7.86">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,492.77,337.64,7.86;10,151.52,503.73,329.07,7.86;10,151.52,514.69,218.74,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,284.11,492.77,144.23,7.86">TextRank: Bringing order into texts</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,447.41,492.77,33.18,7.86;10,151.52,503.73,325.03,7.86;10,151.52,514.69,53.75,7.86">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
	<note>EMNLP 2004</note>
</biblStruct>

<biblStruct coords="10,142.96,525.56,337.63,7.86;10,151.52,536.51,329.07,7.86;10,151.52,547.47,82.42,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,351.98,525.56,128.61,7.86;10,151.52,536.51,101.81,7.86">Exploratory Undersampling for Class Imbalance Learning</title>
		<author>
			<persName coords=""><forename type="first">Xu-Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,261.49,536.51,215.03,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="539" to="550" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,558.34,337.64,7.86;10,151.52,569.30,329.07,7.86;10,151.52,580.26,329.07,7.86;10,151.52,591.22,70.65,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,242.97,569.30,237.63,7.86;10,151.52,580.26,149.10,7.86">A probabilistic model of information retrieval: development and comparative experiments Part 2</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,310.48,580.26,165.22,7.86">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="809" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,602.09,337.64,7.86;10,151.52,613.05,329.07,7.86;10,151.52,624.01,246.20,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,391.27,602.09,89.33,7.86;10,151.52,613.05,149.92,7.86">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,308.34,613.05,172.25,7.86;10,151.52,624.01,168.74,7.86">Proceedings of the International Conference on Learning Representations (ICLR 2013)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,634.88,337.63,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,193.11,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,434.48,634.88,46.11,7.86;10,151.52,645.84,146.13,7.86">From Word Embeddings To Document Distances</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Nicholas I Kolkin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.23,645.84,175.36,7.86;10,151.52,656.80,115.20,7.86">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,119.67,337.97,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,255.34,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,268.18,119.67,212.41,7.86;11,151.52,130.63,22.01,7.86">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName coords=""><forename type="first">Qv</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,193.36,130.63,287.23,7.86;11,151.52,141.59,44.56,7.86">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,152.55,337.98,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,91.80,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,151.52,163.51,214.18,7.86">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,374.61,163.51,105.98,7.86;11,151.52,174.47,63.55,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
