<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.18,115.96,325.00,12.62;1,246.63,133.89,122.11,12.62">CUNI team: CLEF eHealth Consumer Health Search Task 2018</title>
				<funder ref="#_gNArpqB">
					<orgName type="full">Czech Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.20,171.79,50.92,8.74"><forename type="first">Shadi</forename><surname>Saleh</surname></persName>
							<email>saleh@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName coords="1,316.81,171.79,55.35,8.74"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
							<email>pecina@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics</orgName>
								<orgName type="department" key="dep2">Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.18,115.96,325.00,12.62;1,246.63,133.89,122.11,12.62">CUNI team: CLEF eHealth Consumer Health Search Task 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC87C54BFF4A2757B8941FD30B18D4D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multilingual information retrieval</term>
					<term>statistical machine translation</term>
					<term>hypotheses reranking</term>
					<term>term reranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our participation in CLEF Consumer Health Search Task 2018, mainly, its monolingual and multilingual subtasks: IRTask1 and IRTask4. In IRTask1, we use language-model based retrieval model, vector-space model and Kullback-Leiber divergence query expansion mechanism to build our runs. In IRTask4, we submitted 4 runs for each language of Czech, French and German. We follow query-translation approach in which we employ a Statistical Machine Translation (SMT) system to get a ranked list of translation hypotheses in English. We use this list for two systems: the first one uses 1-best-list translation to construct queries, and the second one uses a hypotheses reranker to select the best translation (in terms of retrieval performance) to construct queries. We also present our term reranking model for query expansion, in which we deploy feature set from different resources (the document collection, Wikipedia articles, translation hypotheses). These features are used to train a logistic regression model that can predict the performance when a candidate term is added to a base query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Internet searches for medical topics had been increasing recently, and have gotten the attention of information retrieval researchers. Fox <ref type="bibr" coords="1,382.29,548.52,10.52,8.74" target="#b2">[3]</ref> reported that about 80% of Internet users in the United States look for medical information online. The main challenge in the medical information retrieval systems that people with different experience express their information need in different way <ref type="bibr" coords="1,462.33,584.39,14.61,8.74" target="#b13">[14]</ref>. Laypeople express their medical information need using non-medical terms, while medical experts tend to use advanced medical terms, thus, information retrieval systems need to be stable for such different query variations. The significant increasing of non-English digital content on the World Wide Web has been followed by an increase in looking for this information by internet users. Grefenstette and Nioche <ref type="bibr" coords="1,167.63,656.12,10.52,8.74" target="#b7">[8]</ref> presented an estimation of language size in 1996, late 1999 and early 2000 for documents captured from the internet. Their study showed that the English content has grown 800%, German 1500%, and Spanish 1800% in the same period. Furthermore, users started to look for information needs that is represented in documents which are not available in their native languages.</p><p>The system that searches for information in a language different from the one of user is called Cross-Lingual (multilingual) Information Retrieval (CLIR) system. It enables users to write queries (information need) represented in a language (lang. A), and returns results from a document collection written in a different language (lang. B). Usually, the baseline system in CLIR is to take 1-best-list translations which are returned by a statistical machine translation (SMT) system and perform the retrieval as shown in the CLEF eHealth Information Retrieval tasks before <ref type="bibr" coords="2,269.98,254.75,9.96,8.74" target="#b5">[6]</ref>. Nikoulina et al. <ref type="bibr" coords="2,360.89,254.75,15.50,8.74" target="#b9">[10]</ref> presented an approach to develop Cross-lingual information retrieval (CLIR) system which is based on reranking the hypotheses given from the SMT system. Saleh and Pecina <ref type="bibr" coords="2,465.09,278.66,15.50,8.74" target="#b19">[20]</ref> considered Nikoulina's work as a starting point and expanded it by adding a rich set of features for training. They presented approach covered translating queries from Czech, French and German into English and rerank the alternative translations to predict the hypothesis that gives better CLIR performance.</p><p>In this paper, we describe our participation at the CLEF 2018 eHealth consumer health search task <ref type="bibr" coords="2,246.37,354.63,14.61,8.74" target="#b22">[23]</ref>. We focus in our participation in the multilingual IR Task. We present our machine learning model which reranks the alternative translations given by the machine translation system for better IR results. We also present our new approach to expand translated queries using our machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>CLEF eHealth Consumer Health Search Task 2018 <ref type="bibr" coords="2,359.19,500.70,10.52,8.74" target="#b8">[9]</ref> is similar to the IR tasks in the previous years <ref type="bibr" coords="2,227.74,512.66,23.06,8.74">(2013)</ref><ref type="bibr" coords="2,250.80,512.66,4.61,8.74">(2014)</ref><ref type="bibr" coords="2,250.80,512.66,4.61,8.74">(2015)</ref><ref type="bibr" coords="2,250.80,512.66,4.61,8.74">(2016)</ref><ref type="bibr" coords="2,255.41,512.66,23.06,8.74">(2017)</ref>. The participants this year are required to retrieve relevant web pages from the provided document collection in response to users' queries. These queries represent information need in the medical domain. The IR task consists of IRTask1 which is a standard ad-hoc monolingual search task. IRTask2 is a similar task of the personalised search task in 2017 <ref type="bibr" coords="2,436.26,560.48,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="2,453.43,560.48,7.01,8.74" target="#b6">7]</ref>, the retrieved documents are personalised to match user expertise (how likely the user is able to understand the content of the retrieved documents). IRTask 3 contains query variations for the same information need, and the participants have to design a search system that is steady when the same information need is expressed in different query variations. In the multilingual ad-hoc search task (IRTask4 ), the monolingual English queries were translated by experts into Czech, French and German, and the participants are asked to design a search system to retrieve relevant documents to these queries from the English document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Collection</head><p>Document collection in the CLEF 2018 consumer health search task is created using CommonCrawl platform<ref type="foot" coords="3,270.35,148.25,3.97,6.12" target="#foot_0">1</ref> . First, the query set (described in Section 2.2) is submitted to Microsoft Bing APIs, and a list of domains is extracted from the top retrieved results. This list is extended by adding reliable health websites, at the end clefehealth2018 B (which we use in this work) contained 1, 653 sites, after excluding non-medical websites such as news websites. After preparing the domain list, these domains are crawled and provided as an indexed collection to the participants. Two indexes are provided, in the first one, documents are stemmed and a stop-word list is used, while no preprocessing is done in the second index. The collection contains 5, 560, 074 documents, the stemmed index contains 14, 213, 903 vocabularies, while the non-stemmed index contains 15, 298, 904 ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Queries</head><p>The query set this year includes 50 English queries. This set is a subset of 150 medical queries that were created from HON and TRIP query logs within the Khresmoi project <ref type="bibr" coords="3,213.67,340.99,9.96,8.74" target="#b3">[4]</ref>. Table <ref type="table" coords="3,257.60,340.99,4.98,8.74" target="#tab_0">1</ref> shows the average number of terms in the 50 test queries in all languages. Although the average number of terms in the English queries is 5.64, there are queries that are much longer (e.g. query 199001 ), as shown in Table <ref type="table" coords="3,204.37,376.85,3.87,8.74" target="#tab_1">2</ref>. Queries might contain typos since they are constructed from real query logs, as shown in query 175001, which contains Emugel instead of Emulgel. feeling of fullness with hiccups with a feeling of a lump in the back of the throat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The training data</head><p>The data that we use to train our systems was presented by the CLEF eHealth 2014 Task 3 -Information Retrieval <ref type="bibr" coords="4,296.53,153.72,10.52,8.74" target="#b4">[5]</ref> and CLEF eHealth 2015 Task 2: User-Centred Health Information Retrieval <ref type="bibr" coords="4,304.98,165.68,14.61,8.74" target="#b14">[15]</ref>. It is almost identical to the collection used in CLEFeHealth 2013 Task 3 -User-Centred Health Information Retrieval, which contained a few additional documents which were excluded from the 2014/2015 collection due to license issues. The document collection includes a total of 1,104,298 web pages in HTML, automatically crawled from various English medical websites such as Genetics Home Reference, ClinicalTrial.gov and Diagnosia. To clean the HTML pages in the collection, we follow the work of Saleh and Pecina <ref type="bibr" coords="4,226.74,249.36,14.61,8.74" target="#b18">[19]</ref>. The queries have also been adopted from the CLEF eHealth series and include all the test queries from the IR task of 2013 (50 queries), 2014 (50 queries), and 2015 (66 queries). We joined them to create a more representative and balanced sample for IR experiments. The set of all 166 queries was split into 100 queries for training and 66 queries for testing. The two sets are stratified in terms of distribution of the year of origin, number of relevant/not-relevant documents, and query length (number of words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Translation system</head><p>For the multilingual task (IRTask4 ), we follow the query translation approach, in which a query is translated into the collection language (English), then the retrieval is conducted. Query translation approach reduces the task into monolingual task (both queries and documents are expressed in the same language). We use Khresmoi statistical machine translation (SMT) system <ref type="bibr" coords="4,428.70,441.21,9.96,8.74" target="#b1">[2]</ref>, for language pairs: Czech-English, French-English and German-English, to translate the queries into English. Khresmoi SMT system was trained to translate queries, and tuned on parallel and monolingual data taken from the medical domain resources like Wikipedia, UMLS concept descriptions and UMLS metathesaurus. Such domain specific data made Khresmoi perform better when translating sentences in the medical domain like the queries in our case. Generally, feature weights in SMT systems are tuned toward BLEU <ref type="bibr" coords="4,354.64,524.90,14.61,8.74" target="#b16">[17]</ref>, a method for automatic evaluation of SMT systems correlates with human judgments. It is not necessary to have correlation between the quality of general SMT system and the quality of CLIR performance <ref type="bibr" coords="4,266.58,560.76,14.61,8.74" target="#b17">[18]</ref>; therefore Khresmoi SMT system was tuned using MERT <ref type="bibr" coords="4,193.44,572.72,15.50,8.74" target="#b11">[12]</ref> towards PER (position-independent word error rate), because it does not penalise word reorder; which is not important for the performance of IR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hypotheses reranking</head><p>Khresmoi SMT system produces a list of ranked translations in the target language, for each sentence in the source language, this list is called n-best-list.</p><p>However, this n-best-list is ranked based on the translation quality rather than the retrieval performance. Saleh and Pecina <ref type="bibr" coords="5,324.08,130.95,15.50,8.74" target="#b19">[20]</ref> presented an approach to rerank an n-best-list and predict a translation that gives the best retrieval performance in terms of P@10. The reranker is a generalized linear regression model that uses a set of features which can be divided according to their sources into: 1)</p><p>The SMT system: This includes features that are derived from the verbose output of the Khresmoi SMT system (e.g. phrase translation model, the target language model, the reordering model and word penalty). 2) Document collection: This includes IDF scores and features that are based on the blindrelevance feedback approach. 2) External resources: Resources like Wikipedia articles and UMLS metathesaurus <ref type="bibr" coords="5,287.44,238.55,15.50,8.74" target="#b21">[22]</ref> are employed to create a rich set of features for each query hypothesis. 3) Retrieval status value (RSV): RSV is the score of the retrieval scoring function when constructing a query from a translation hypothesis. It helps to involve more information from the collection in the reranking process by assigning to each hypothesis the score from the retrieval function. This feature is based on the work of Nottelman et al. <ref type="bibr" coords="5,411.27,298.32,14.61,8.74" target="#b10">[11]</ref>, where they investigated the correlation between RSV and relevance probability. To train the model, we join the training and test sets that we presented in Section 3 in one set, then calculate feature values from each language, and merge them from all seven languages in one training set. The test set is the CLEF eHealth 2018 query set in Czech, French and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query Expansion</head><p>Query expansion is a process that reformulates user's initial queries as an attempt to represent more information to improve retrieval performance eventually. In this section, we present our approach to reformulate user's query in the CLIR task using machine learning model, based on the presented work of Saleh and Pecina <ref type="bibr" coords="5,187.18,452.69,14.61,8.74" target="#b20">[21]</ref>. This approach is based on expanding a query by adding candidate terms from an existing pool. This is done by reranking candidate terms using machine learning model towards better IR performance and adding the top ranked terms to the original query. To create a pool of candidate terms for each query, we use two main resources:</p><p>-Translation hypotheses: This pool is built by merging n-best-list translations for each query, after filtering stopwords and terms that already appeared in the 1-best-list translation. -Wikipedia titles: First, we index English Wikipedia articles (titles and abstracts without any preprocessing) using Terrier <ref type="bibr" coords="5,362.03,565.98,15.50,8.74" target="#b12">[13]</ref> and its implementation of Dirichlet language model as an IR model, then we conduct retrieval for each query's 1-best-list translation from this index, then the top 10 ranked Wikipedia articles are selected and their titles are added to the pool.</p><p>To train the model, we use the training data that we presented in Section 3, while for testing, we use the provided queries from the CLEF eHealth 2018 IR task in Czech, French and German languages. After building a pool of candidate terms, we generate the following features for each term:</p><p>-IDF The inverse document frequency which is calculated from the relevant document collection. -Translation pool frequency This feature represents how many times a term appeared in the translation pool. When a term appears in multiple hypotheses, this means that the probability of being a relevant translation to one of the terms in the original query is high. -Wikipedia frequency The frequency of a term in the top 10 retrieved Wikipedia articles. Retrieval is conducted using the 1-best-list translation for the query that we want to expand with the candidate term. -Retrieval Status Value difference To calculate this feature, we conduct two retrievals, the first one using the original query (1-best-list translation ), and the second one using the original query expanded with the candidate term, then we take the score of the highest ranked document in each retrieval and calculate the difference between them. This feature tells us the contribution of the candidate term to the retrieval status value. -Similarity To calculate the similarity between a candidate term t m and the query terms, we use a trained model of word2vec embeddings on 25 millions articles from PubMed<ref type="foot" coords="6,285.12,321.21,3.97,6.12" target="#foot_1">2</ref> . First, we get the word embeddings for each term in the original query and we sum these embeddings to get a vector that represents the entire query. Then we take the embeddings for t m , and calculate the cosine similarity between the query vector and t m vector. -Co-occurrence frequency The co-occurrences of a candidate term t m and the query terms t i ∈ Q indicates how likely t m is related to the original query Q. We sum up the co-occurrence frequency for each term in query Q and the candidate term t m in all documents d j in the collection C, as shown in the Equation <ref type="formula" coords="6,223.52,418.56,3.87,8.74" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="6,228.75,445.19,251.84,20.14">co(t m , Q) = dj ∈C,ti∈Q tf (d j , t i )tf (d j , t m )<label>(1)</label></formula><p>-Term frequency First, we perform retrieval from the collection using a query that is constructed from the 1-best-list translation, then we calculate the term frequency of a candidate term t m in the top 10 ranked documents from the retrieval result. -Medical term count This feature represents how many times a term appeared in the UMLS lexicon, as an attempt to give more weight to the medical terms.</p><p>Our goal is to design a model that can predict the performance of the retrieval when expanding a query with a term from the terms pool, and add terms that can improve the performance. To train the model, we perform the following steps:</p><p>-Generate a pool of candidate terms for each query in the training and test set.</p><p>-Add one term from the pool to the query that we want to expand (1-best-list translation) and perform the retrieval using the baseline system (Dirichlet model) -Calculate the feature values for each term as we described above.</p><p>-For training queries, we evaluate the performance for each expanded query considering P @10 as a main metric, P @10 being the objective function for our model. -Merge training queries from the 7 languages together to enrich the training set with more instances. -After preparing the training set, we normalise feature values using standard scaling by removing the mean and scaling them to have unit variance. This is done independently on each feature, then we use the scaler coefficient to standardise the test set. Scaling is important since the range of the feature values varies widely.</p><p>The term reranker is a generalised linear regression model which predicts P @10 value for each term when expanding the original query with, we choose the term that has the highest predicted value of P @10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Systems</head><p>We submit runs for the monolingual task (IRTask1) and the multilingual task (IRTask4), as we present in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monolingual system</head><p>In the monolingual task, we submit four runs:</p><p>-Run 1 In this system, we use the Terrier's index that is provided by the organisers without applying any data preprocessing. Terrier's implementation of Dirichlet smoothing language model is used as the retrieval model with its default parameters. -Run 2 This system also uses the same retrieval model as in Run 1, while as an index, we use Terrier's index that uses Porter-stemming method and English stop-word list. -Run 3 This system uses Terrier's implementation of TF-IDF model, for the purpose of comparing between a vector-space model and an LM model (the one that is used in Run 1), we use the same index as in Run 1. -Run 4 In this run, we use Terrier's implementation of Kullback-Leiber divergence (KLD) <ref type="bibr" coords="7,225.77,584.39,10.52,8.74" target="#b0">[1]</ref> for query expansion, with number of top documents is set to 10 and number of terms for expansion is set to 3. These 3 terms are selected as following: first, an initial retrieval is done using the base query and the top 10 documents are chosen as pseudo-relevant documents. Then each term in these documents is scored as shown in Equation <ref type="formula" coords="7,419.18,632.21,3.87,8.74" target="#formula_1">2</ref>, where P r (t) is the probability of term t in the pseudo-relevant documents (these documents are treated as a bag-of-words), and P c (t) is probability of term t in the document collection c. Finally the top 3 scored terms are added to the base query and a final retrieval is done using the new expanded query.</p><formula xml:id="formula_1" coords="8,249.55,153.10,231.04,23.22">Score(t) = P r (t) • log P r (t) P c (t)<label>(2)</label></formula><p>5.2 Cross-lingual system -Run 1 In this run, we translate the queries in the source languages into English and get 1-best-list translations. Retrieval is conducted using Dirichlet model, and non-stemmed index. The same retrieval settings are used in the following runs. -Run 2 This run uses hypotheses reranking approach, in which each query is translated into English and from the 15-best-list translations, the 1-best-list (in terms of IR quality) translation is selected for the retrieval as described in Section 4.2 -Run 3 First we translate the queries into English and the 1-best-list that is produced by the SMT system is chosen as a base query, then this query is expanded by one term using the term reranking approach that is presented in Section 4.3 -Run 4 This run is similar to Run1, the only difference is that Google Translate<ref type="foot" coords="8,171.07,373.79,3.97,6.12" target="#foot_2">3</ref> is used to translate the queries into English. Table <ref type="table" coords="8,162.61,551.81,4.98,8.74" target="#tab_2">3</ref> shows the percent of similar documents that are retrieved (among the highest 10 ranked ones) by different runs. It is clear from the table that different approaches tend to retrieve different documents, for example, run 3 uses query expansion based approach. Query expansion means that a query will be expanded by more terms to include more information, leading to retrieve different documents, that is the reason why this run has the lowest similarity to the other runs. Both of run 1 and run 4 use 1-best-list translation from two different machine translation systems (Khresmoi and Google Translate respectively) to construct the queries. This explains why these two systems share similar documents more than all other systems. Run 2 uses hypotheses reranking approach to select best translation to be used for retrieval, while run1 uses 1-best-list translation as it is selected from the SMT system to construct queries. According to further analysis we performed between the difference between the retrieved documents by these two runs, we found that 23 queries (out of 50) have 100% similarity of the top 10 retrieved documents, and this correlates with what was shown by Saleh and Pecina <ref type="bibr" coords="9,258.20,202.68,14.61,8.74" target="#b19">[20]</ref>, that an SMT system fails in 50% of the cases to select the best translation to perform the best performance for the retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented our participation in CLEF eHealth Consumer Health Search Task 2018 (monolingual and multilingual subtasks). Four runs were submitted to the monolingual task, two runs use a language-model IR with Dirichlet smoothing, they differ in the used index (one uses a stemmed index and one uses an index without stemming). As for the multilingual task, we submitted four runs for each language of Czech, French and German. The first one uses 1-best-list translation from a statistical machine translation system, the second run uses hypotheses translations reranking, the third run is an implementation of query expansion using term reranker model, while the last run uses Google Translate to translate the provided queries into English. Our results analysis shows that similar approaches tend to share more similar retrieved documents than different approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,431.73,345.83,49.23"><head>Table 1 .</head><label>1</label><figDesc>The average number of terms in the query test set of the CLEF eHealth 2018</figDesc><table coords="3,134.77,442.71,235.56,38.25"><row><cell>IR task</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EN</cell><cell>CS</cell><cell>FR</cell><cell>DE</cell></row><row><cell>5.64</cell><cell>5.28</cell><cell>6.08</cell><cell>4.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,528.28,345.83,110.75"><head>Table 2 .</head><label>2</label><figDesc>Query samples from the English query test of the CLEF eHealth 2018 IR task</figDesc><table coords="3,183.45,558.27,248.45,80.77"><row><cell>id</cell><cell>title</cell></row><row><cell>156001</cell><cell>food allergy test</cell></row><row><cell>168001</cell><cell>hiv vaccine phase</cell></row><row><cell>175001</cell><cell>Voltaren Emugel 1%</cell></row><row><cell>199001</cell><cell>why is there a minimum drinking age and what are the consequences of underage drinking ?</cell></row><row><cell>200001</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,419.19,345.83,104.03"><head>Table 3 .</head><label>3</label><figDesc>Similarity</figDesc><table coords="8,134.77,419.22,345.83,104.00"><row><cell cols="4">(in percent) of top 10 retrieved documents between the submitted</cell></row><row><cell>runs in IRTask4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>runs</cell><cell>CS</cell><cell>DE</cell><cell>FR</cell></row><row><cell>run1-run2</cell><cell>48.80</cell><cell>50.20</cell><cell>55.60</cell></row><row><cell>run1-run3</cell><cell>38.00</cell><cell>26.60</cell><cell>35.20</cell></row><row><cell>run1-run4</cell><cell>52.80</cell><cell>54.40</cell><cell>62.80</cell></row><row><cell>run2-run3</cell><cell>32.20</cell><cell>22.00</cell><cell>33.80</cell></row><row><cell>run2-run4</cell><cell>46.20</cell><cell>43.20</cell><cell>43.40</cell></row><row><cell>run3-run4</cell><cell>27.80</cell><cell>14.4</cell><cell>28.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,103.97,7.86"><p>http://commoncrawl.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,656.80,292.24,7.86"><p>https://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/IRET/DATASET/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,144.73,656.80,82.25,7.86"><p>translate.google.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Czech Science Foundation</rs> (grant n. <rs type="grantNumber">P103/12/G084</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gNArpqB">
					<idno type="grant-number">P103/12/G084</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,534.15,337.64,7.86;9,151.52,545.11,329.07,7.86;9,151.52,556.07,116.26,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,313.18,534.15,167.41,7.86;9,151.52,545.11,122.07,7.86">Query difficulty, robustness, and selective application of query expansion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,295.15,545.11,181.87,7.86">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,567.73,337.63,7.86;9,151.52,578.69,329.07,7.86;9,151.52,589.65,329.07,7.86;9,151.52,600.61,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,446.80,567.73,33.79,7.86;9,151.52,578.69,208.16,7.86">Machine translation of medical texts in the Khresmoi project</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,380.43,578.69,100.16,7.86;9,151.52,589.65,180.14,7.86">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,612.26,337.63,7.86;9,151.52,623.22,159.87,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,186.00,612.26,290.79,7.86">Health Topics: 80% of internet users look for health information online</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Tech. rep. ; Pew Research Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,634.88,337.63,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,76.10,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,461.78,634.88,18.81,7.86;9,151.52,645.84,300.24,7.86">D7.2 Meta-analysis of the first phase of empirical and user-centered evaluations</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,121.64,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,244.78,130.63,235.81,7.86;10,151.52,141.59,147.30,7.86">ShARe/CLEF eHealth Evaluation Lab 2014, Task 3: Usercentred health information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,322.60,141.59,110.80,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,164.36,337.63,7.86;10,151.52,175.32,329.07,7.86;10,151.52,186.28,329.07,7.86;10,151.52,197.24,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,215.89,175.32,188.94,7.86">Overview of the CLEF eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Néváol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,447.76,175.32,32.83,7.86;10,151.52,186.28,179.10,7.86">The 6th Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,209.05,337.64,7.86;10,151.52,220.01,329.07,7.86;10,151.52,230.97,329.07,7.86;10,151.52,241.93,175.55,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,284.59,220.01,177.51,7.86">CLEF 2017 eHealth evaluation lab overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nvol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,230.97,254.63,7.86">CLEF 2017 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,413.87,230.97,66.72,7.86;10,151.52,241.93,100.21,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,253.74,337.64,7.86;10,151.52,264.70,329.07,7.86;10,151.52,275.66,329.07,7.86;10,151.52,286.62,81.50,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,265.64,253.74,214.95,7.86;10,151.52,264.70,32.54,7.86">Estimation of english and non-english language use on the www</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nioche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,206.39,264.70,188.29,7.86">Content-Based Multimedia Information Access</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>RIAO, Centre de hautes etudes internationales d&apos;informatique documentaire</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,298.43,337.64,7.86;10,151.52,309.39,329.07,7.86;10,151.52,320.35,240.08,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,385.28,298.43,95.31,7.86;10,151.52,309.39,132.37,7.86">Overview of the CLEF 2018 consumer health search task</title>
		<author>
			<persName coords=""><forename type="first">Zuccon</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,304.05,309.39,176.54,7.86;10,151.52,320.35,87.99,7.86">CLEF 2018 Evaluation Labs and Workshop: Online Working Notes</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,332.16,337.98,7.86;10,151.52,343.12,329.07,7.86;10,151.52,354.08,329.07,7.86;10,151.52,365.04,275.07,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,348.89,332.16,131.70,7.86;10,151.52,343.12,310.24,7.86">Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nikoulina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kovachev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lagos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,354.08,329.07,7.86;10,151.52,365.04,119.56,7.86">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,376.85,337.98,7.86;10,151.52,387.81,279.29,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,257.91,376.85,222.68,7.86;10,151.52,387.81,114.30,7.86">From retrieval status values to probabilities of relevance for advanced IR applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nottelmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,272.80,387.81,83.77,7.86">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="388" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,399.62,337.98,7.86;10,151.52,410.58,329.07,7.86;10,151.52,421.54,194.95,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,194.35,399.62,248.12,7.86">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.04,399.62,17.56,7.86;10,151.52,410.58,329.07,7.86">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,433.35,337.97,7.86;10,151.52,444.31,329.07,7.86;10,151.52,455.27,329.07,7.86;10,151.52,466.23,47.61,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,440.94,433.35,39.64,7.86;10,151.52,444.31,247.69,7.86">Terrier: A high performance and scalable information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.51,444.31,59.08,7.86;10,151.52,455.27,197.65,7.86">Proceedings of Workshop on Open Source Information Retrieval</title>
		<meeting>Workshop on Open Source Information Retrieval<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,478.04,337.98,7.86;10,151.52,489.00,329.07,7.86;10,151.52,499.96,250.74,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,370.40,478.04,110.20,7.86;10,151.52,489.00,329.07,7.86;10,151.52,499.96,74.83,7.86">How users search and what they search for in the medical domain -understanding laypeople and experts through query logs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E K</forename><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,233.02,499.96,70.95,7.86">Inf. Retr. Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="189" to="224" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,511.77,337.98,7.86;10,151.52,522.73,329.07,7.86;10,151.52,533.69,329.07,7.86;10,151.52,544.65,65.62,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,211.48,522.73,121.27,7.86;10,358.21,522.73,122.38,7.86;10,151.52,533.69,99.91,7.86">Task 2: Retrieving information about medical symptoms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.61,533.69,94.54,7.86">CLEF (Working Notes)</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Spriner</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
	<note>CLEF eHealth Evaluation Lab</note>
</biblStruct>

<biblStruct coords="10,142.62,556.46,337.98,7.86;10,151.52,567.42,329.07,7.86;10,151.52,578.38,329.07,7.86;10,151.52,589.34,110.47,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,189.10,567.42,274.25,7.86">CLEF 2017 task overview: The IR Task at the eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,578.38,291.62,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,601.15,337.97,7.86;10,151.52,612.11,329.07,7.86;10,151.52,623.07,329.07,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,349.20,601.15,131.39,7.86;10,151.52,612.11,133.70,7.86">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.56,612.11,174.03,7.86;10,151.52,623.07,167.34,7.86">Proceedings of the 40th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,634.88,337.98,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,293.90,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,151.52,645.84,329.07,7.86;10,151.52,656.80,61.44,7.86">Adaptation of machine translation for multilingual information retrieval in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hlavářová</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,220.64,656.80,134.17,7.86">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="185" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,119.67,337.98,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,160.00,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,235.87,119.67,207.54,7.86">CUNI at the ShARe/CLEF eHealth Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,130.63,307.58,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="226" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,152.55,337.98,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,329.07,7.86;11,151.52,183.16,254.66,10.13" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,235.16,152.55,245.43,7.86;11,151.52,163.51,114.32,7.86">Reranking hypotheses of machine-translated queries for crosslingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,288.66,163.51,191.93,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,70.67,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. The 7th International Conference of the CLEF Association, CLEF 2016</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="54" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,196.39,337.97,7.86;11,151.52,207.34,329.07,7.86;11,151.52,218.30,258.46,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,236.87,196.39,224.61,7.86">Task3 patient-centred information retrieval: Team CUNI</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,207.34,324.14,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="11,151.52,218.30,119.44,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,229.26,337.97,7.86;11,151.52,240.22,329.07,7.86;11,151.52,251.18,151.56,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,393.08,229.26,87.51,7.86;11,151.52,240.22,230.94,7.86">The UMLS Metathesaurus: representing different views of biomedical concepts</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Schuyler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">T</forename><surname>Hole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Sherertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,389.11,240.22,91.48,7.86;11,151.52,251.18,79.37,7.86">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,262.14,337.98,7.86;11,151.52,273.10,329.07,7.86;11,151.52,284.06,329.07,7.86;11,151.52,295.02,329.07,7.86;11,151.52,305.98,94.86,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,442.94,273.10,37.65,7.86;11,151.52,284.06,148.25,7.86">Overview of the CLEF eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nèvèol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,343.08,284.06,137.52,7.86;11,151.52,295.02,117.19,7.86">CLEF 2018 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="11,276.12,295.02,160.00,7.86">Lecture Notes in Computer Science LNC</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
