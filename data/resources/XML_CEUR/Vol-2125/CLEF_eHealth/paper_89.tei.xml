<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.70,115.96,291.95,12.62;1,156.60,133.89,302.16,12.62;1,261.39,151.82,92.58,12.62">Technology-Assisted Review in Empirical Medicine: Waterloo Participation in CLEF eHealth 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.80,189.49,88.41,8.74"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.90,189.49,88.66,8.74"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.70,115.96,291.95,12.62;1,156.60,133.89,302.16,12.62;1,261.39,151.82,92.58,12.62">Technology-Assisted Review in Empirical Medicine: Waterloo Participation in CLEF eHealth 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">763820401F2CA2083EA6A3C58EB37029</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Screening articles for studies to include in systematic reviews is an application of technology-assisted review ("TAR"). In this work, we applied the Baseline Model Implementation ("BMI") from the TREC Total Recall Track (2015-2016)  to the CLEF eHealth 2018 task of screening MEDLINE abstracts to identify articles reporting studies to be considered for inclusion. We employed exactly the same approach for Sub-Task 1 and Sub-Task 2, which was in turn exactly the same approach employed for the CLEF 2017 eHealth Lab. The only difference was that for Sub-Task 1, the entire Pubmed/MedLine database was searched; whereas for Sub-Task 2, the only records searched were those identified by CLEF using Boolean queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Waterloo participated in Task 2, Technologically Assisted Reviews in Empirical Medicine <ref type="bibr" coords="1,259.12,423.19,14.61,8.74" target="#b10">[11]</ref>, of the CLEF 2018 eHealth Evaluation Lab <ref type="bibr" coords="1,462.33,423.19,14.61,8.74" target="#b12">[13]</ref>. Task 2 was divided into Sub-Task 1 and Sub-Task 2, which simulate, respectively, the first and second phases, and only the second phase, in a prototypical threephase workflow to identify studies for inclusion in a systematic review:</p><p>1. Search: First, Boolean queries are used to identify as many articles as possible that may describe studies that should be included; 2. Screening: Second, titles and abstracts of the articles identified in the search phase are examined to eliminate those which could not possibly describe studies that should be included; and 3. Selection: Finally, articles that survived the screening phase are read in full to determine whether or not they meet the systematic review inclusion criteria.</p><p>The overall objective of our research is to improve the human efficiency, as well as the effectiveness, of workflows to identify studies for inclusion in systematic reviews. Our CLEF experiments investigate the following hypotheses:</p><p>1. Can continuous active learning ("CAL") can substantially improve the human efficiency of screening, without substantially compromising its effectiveness. 2. Does CAL obviate the use of keywords to select a universe of documents for screening?</p><p>Task 2 is essentially the Technology-Assisted Review ("TAR") task addressed by the TREC 2015 and TREC 2016 Total Recall Tracks <ref type="bibr" coords="2,360.95,177.96,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,378.11,177.96,7.01,8.74" target="#b8">9]</ref>. For our participation in CLEF 2018, we reprised our Total Recall efforts, and also our efforts from CLEF 2017 <ref type="bibr" coords="2,188.04,201.88,10.52,8.74" target="#b6">[7]</ref> using the same apparatus.</p><p>At TREC, the systems under test were given, at the outset, a corpus of documents and a set of topics. For each topic, a system under test repeatedly submitted documents from the corpus to a server, and in return, was given a simulated human assessment of "relevant" or "not relevant" for each document.</p><p>The objective was to identify as many relevant documents as possible, while submitting as few non-relevant documents as possible. The tension between these two criteria was evaluated using rank-based measures (e.g., recall as a function of the number of documents submitted), as well as set-based measures (e.g., recall at a point when a certain number of documents, specified contemporaneously by the system, had been submitted).</p><p>Prior to TREC, we made available a Baseline Model Implementation ("BMI"), <ref type="foot" coords="2,175.03,357.63,3.97,6.12" target="#foot_0">1</ref> to illustrate the client-server protocol, as well as to provide baseline results for comparison. BMI, which encapsulates our AutoTAR Continuous Active Learning ("CAL") method <ref type="bibr" coords="2,284.98,383.11,9.96,8.74" target="#b0">[1]</ref>, yielded rank-based results that compared favorably will all systems under test. During the course of our participation in TREC, we developed and tested the "knee method" stopping procedure <ref type="bibr" coords="2,448.49,407.02,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,460.67,407.02,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,470.07,407.02,7.01,8.74" target="#b4">5]</ref>, with the purpose of achieving high recall with high probability. Sub-Task 2, which was the only task for CLEF 2017, differed operationally from the TREC Total Recall Track in that a list of document identifiers, rather than a corpus, was supplied at the outset, and a complete set of relevance assessments, rather than an assessment server were used to simulate human assessments. Sub-Task 2 also differed substantively from the Total Recall Track in that the corpus for each topic was narrowed by a search phase specific to that topic, and therefore yielded a much smaller set that was richer in relevant documents. Sub-Task 2 differed further in that two sets of relevance assessments were available: the assessments from a previously conducted screening phase, and the assessments from a previously conducted selection phase, raising the question of which assessments (or combination of assessments) should be used to simulate relevance feedback, and which should be used to evaluate the results (cf. <ref type="bibr" coords="2,454.56,567.06,10.30,8.74" target="#b5">[6]</ref>).</p><p>Sub-Task 1, new for CLEF 2018, resembles the TREC Total Recall Track, in that no topic-specific culling of the document set is done; each search applies to the entire 30M-document Pubmed/MEDLINE collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Corpora</head><p>The corpus for each topic consisted of abstracts from MEDLINE/Pubmed<ref type="foot" coords="3,451.95,161.81,3.97,6.12" target="#foot_1">2</ref> identified by PMID. On April 1, 2018, we fetched the entire MEDLINE dataset consisting of 28,256,688 XML files, each containing the titles, abstracts, and metadata for an article. We used the raw XML files as documents in the corpora that were supplied at the outset to BMI.</p><p>For Sub-Task 1, we applied BMI to the entire corpus of 28,256,688 files, thus combining the search and screening phases. In a pilot experiment on the test topics, we found that no assessments were available for many, if not most, of the highly ranked documents returned by BMI. To our eye, these documents were indistinguishable from those for which "relevant" assessments were provided. We investigated, without success, the reasons why these documents were not retrieved by the previously conducted search phase. For example, the documents in question were neither newer nor older than those for which assessments were available, and appeared to contain relevant terms from the search query. Nonetheless, for Sub-Task 1, we treated any document for which no qrel was available to be "not relevant" for the purpose of feedback.</p><p>In a separate manual run, the authors used their own judgement to assess the relevance of abstracts returned by BMI, in order to provide relevance feedback.</p><p>For Sub-Task 2, we used a common corpus consisting of all documents that were assessed for any of the 30 test topics. That is, for any given topic, the corpus consisted of all the documents assessed for that topic, as well as all the documents assessed for each of the other 29 topics. Our rationale was that including documents retrieved for all topics would introduce enough diversity to unskew sufficiently the term-frequency statics. This approach appeared to achieve the efficiency of using reduced corpora and the effectiveness of using the full dataset, and was chosen for our official tests: For the official tests, the corpus consisted of all documents assessed for any of the 30 test topics; any unassessed document was considered "not relevant."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relevance Feedback</head><p>For Sub-Task 2, we used two modes of relevance feedback:</p><p>1. Relevance feedback based on the screening-phase assessments (Method UWA.Task2 for official testing); 2. Relevance feedback based on a hybrid of screening-phase and selection-phase assessments (Method UWB.Task2 for official testing).</p><p>The first method is straightforward: When BMI identifies a document for assessment, the judgment returned to BMI is that supplied by CLEF for either the screening phase (the "abstract qrels"). The second method operates in two phases: At the outset, the judgment returned to BMI is that of the abstract qrels.</p><p>The abstract qrels continue to be used until BMI identifies one document that is relevant not only according to the abstract qrels, but also according to the content qrels. Thereafter, the judgment returned to BMI is that of the content qrels.</p><p>For Sub-Task 3, we used three modes of relevance feedback:</p><p>1. Relevance feedback based on the screening-phase assessments (Method UWA .Task1 for official testing); 2. Manual feedback based on the authors' relevance assessments (Method UWG.Task1 for official testing); 3. Manual feedback based on the authors' relevance positive assessments, followed by relevance feedback based on the screening-phase assessments (Method UWX.Task1 for official testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stopping Criterion</head><p>For threshold-based evaluation, it was necessary to implement a stopping procedure to terminate screening when the best compromise between recall and effort had been achieved, for some definition of "best." In our opinion, technologyassisted review should be considered a satisfactory alternative to manual review only if it yields comparable or superior recall, with high probability. Toward this end, we deployed our knee method with default parameters (ρ = 156 -min(relret, 150), β = 100 <ref type="bibr" coords="4,284.48,390.54,10.30,8.74" target="#b2">[3]</ref>), which interprets a sharp fall-off in the slope of the gain curve (recall vs. review effort) as evidence that substantially all relevant documents have been identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AutoTAR</head><p>In 2015, we published the details and rationale for AutoTAR <ref type="bibr" coords="4,399.74,469.49,9.96,8.74" target="#b0">[1]</ref>, which remains, to this date, the most effective TAR method of which we are aware. BMI implements AutoTAR exactly as described above, except for the substitution of Sofia-ML logistic regression in place of SVM light (see <ref type="bibr" coords="4,377.22,505.35,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="4,392.05,505.35,50.62,8.74">Section 3.1]</ref>). It has no dataset-or topic-specific tuning parameters; except for modifications to incorporate the CLEF corpora and relevance assessments, and our knee-method stopping procedure, we used BMI "out of the box."</p><p>The AutoTAR/BMI algorithm, as modified for CLEF, is detailed in Algorithm 1, which is reproduced from <ref type="bibr" coords="4,287.42,565.13,10.52,8.74" target="#b0">[1]</ref> with the following changes:</p><p>-In Step 1, AutoTAR gives the option of starting with a relevant document, or with a synthetic document. Here, we used a synthetic document consisting of the title of the topic, and nothing else. -In Step 7, we introduced two different ways to simulate user feedback, corresponding to Method A and Method B, described above in Section 3.2. -In Step 10, we introduced the option to terminate the process when the knee-method stopping criterion was met.</p><p>Algorithm 1 The AutoTAR Continuous Active Learning ("CAL") Method, as Implemented by the TREC Baseline Model Implementation ("BMI") and deployed by Waterloo for the CLEF Technologically Assisted Review Task. Internally, BMI constructs a normalized TF-IDF ((1 + log tf ) • log N df ) wordvector representation of each document in the corpus (which, as noted in Section 3.1, consists of raw XML files), where a word is considered to be any sequence of two or more alphanumeric characters not containing a digit, that occurs at least twice in the corpus. Scoring is effected by Sofia-ML<ref type="foot" coords="5,420.03,429.83,3.97,6.12" target="#foot_2">3</ref> with parameters "--learner type logreg-pegasos --loop type roc --lambda 0.0001 --iterations 200000." As noted above, these parameters were fixed when BMI was created in 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Table <ref type="table" coords="5,162.83,519.01,4.98,8.74" target="#tab_0">1</ref> shows the average number of documents reviewed and recall achieved when the stopping criterion is met. We note that the CAL approach achieves similar high recall for each subtask, at the expense of more assessment effort. This additional assessment effort must be balanced against the effort to construct a precise yet inclusive Boolean query, as well as the risk of missing relevant documents that are not matched by the query.</p><p>There is some question as to how realistic the simulated assessments were for Subtask 1, as unassessed documents were deemed to be not relevant. Had all documents actually been assessed, the simulated effort may have been lower.</p><p>We believe that both sets of the CLEF assessments are incomplete with respect to the overall objective of identifying all studies that should be included in Therefore, from the assessments, it is impossible to determine whether an article not retrieved by the search phase, or an article eliminated during the screening phase, describes a study that should have been included in the review. The CLEF architecture tacitly assumes that no such articles exist; in other words, that the search and screening phases used to generate the relevance assessments were infallible, and each attained 100% recall. Such an assumption is unrealistic, and limits the recall of any simulated TAR method to that of the manual review to which it is compared <ref type="bibr" coords="6,411.79,352.12,9.96,8.74" target="#b5">[6]</ref>. As noted in the Cochrane Handbook <ref type="bibr" coords="6,244.35,364.07,15.50,8.74" target="#b9">[10]</ref> with regard to the search phase: "[T]here comes a point where the rewards of further searching may not be worth the effort required to identify the additional references." And with regard to the screening phase: "Using at least two authors may reduce the possibility that relevant reports will be discarded <ref type="bibr" coords="6,192.68,411.89,79.09,8.74">(Edwards 2002 [8]</ref>)."</p><p>Our hypothesis that our TAR runs found relevant articles that were missed by the search phase, or incorrectly discarded in the screening phase, is based on results from other domains <ref type="bibr" coords="6,275.44,448.10,9.96,8.74" target="#b5">[6]</ref>, where TAR acting as a "second assessor" was able to identify potentially relevant documents that had been judged "nonrelevant" by a human assessor. When we applied Method A to the 30 topics, it identified 9,250 potentially relevant articles for which the abstract qrel was "not relevant." Acquiring a second opinion on each of these documents would increase the cost of the TAR review by approximately 12%, and would, we believe, yield a substantial number of relevant documents, over and above the 670 identified in the abstract qrels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,139.55,156.43,341.04,7.86;5,151.70,167.39,108.37,7.86;5,139.55,178.35,142.25,7.86;5,139.55,189.31,341.04,7.86;5,151.70,200.27,200.34,7.86;5,139.55,211.22,189.57,7.86;5,139.55,222.18,207.22,7.86;5,139.55,233.14,320.90,7.86;5,139.55,244.10,324.82,7.86;5,151.88,257.05,289.61,7.86;5,151.37,268.01,329.22,7.86;5,168.64,278.97,132.55,7.86;5,139.55,291.92,201.72,7.86;5,139.55,302.88,68.02,7.86;5,217.29,301.25,5.54,5.24;5,216.55,307.97,7.31,5.24;5,229.76,302.88,2.56,7.86;5,134.95,313.84,173.71,7.86;5,151.88,326.79,265.02,7.86;5,151.37,337.75,305.77,7.86"><head>1 .</head><label>1</label><figDesc>The initial training set consists of a synthetic document containing only the topic title, labeled as "relevant." 2. Set the initial batch size B to 1. 3. Temporarily augment the training set by adding 100 random documents from the collection, provisionally labeled as "not relevant." 4. Apply logistic regression to the training set. 5. Remove the random documents added in step 3. 6. Select the highest-scoring B documents that have not yet yet been screened. 7. Label each of the B documents as "relevant" or "not relevant" by consulting: (a) Previous "abstract" assessments supplied by CLEF [Method A]; or, (b) Previous "document" assessments, once the first "relevant" document assessment is encountered [Method B]. 8. Add the labeled documents to the training set. 9. Increase B by B 10 . 10. Repeat steps 3 through 10 until either: (a) All documents have been screened [for ranked evaluation]; or, (b) The "knee-method" stopping criterion is met [for threshold evaluation].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,115.91,345.83,148.96"><head>Table 1 .</head><label>1</label><figDesc>Average Recall vs. Number of Documents Reviewed at Threshold The screening assessments are available only for documents retrieved by the search phase; the selection assessments are available only for documents retrieved by the search phase, and judged relevant during the screening phase.</figDesc><table coords="6,134.77,136.71,231.41,104.25"><row><cell cols="2">Subtask Run #Docs Recall</cell></row><row><cell>1</cell><cell>UWA 3559 0.951</cell></row><row><cell>1</cell><cell>UWG 3612 0.962</cell></row><row><cell>1</cell><cell>UWX 3613 0.951</cell></row><row><cell>2</cell><cell>UWA 2926 0.990</cell></row><row><cell>2</cell><cell>UWB 1764 0.927</cell></row><row><cell>the review:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.84,37.38,7.86;2,208.73,645.84,23.06,7.86;2,258.43,645.84,21.05,7.86;2,306.09,645.84,31.32,7.86;2,364.05,645.84,25.73,7.86;2,416.40,645.84,29.37,7.86;2,472.40,645.84,8.19,7.86;2,144.73,656.80,150.33,7.86"><p>Available under GNU General Public License at http://cormack.uwaterloo.ca/trecvm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,215.31,7.86"><p>See https://www.nlm.nih.gov/bsd/pmresources.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,656.80,173.16,7.86"><p>See https://github.com/glycerine/sofia-ml.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,590.70,337.64,7.86;6,151.52,601.66,329.07,7.86;6,151.52,612.62,20.99,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,317.61,590.70,162.99,7.86;6,151.52,601.66,183.36,7.86">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,623.92,337.63,7.86;6,151.52,634.88,329.07,7.86;6,151.52,645.84,329.07,7.86;6,151.52,656.80,45.44,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,313.54,623.92,167.05,7.86;6,151.52,634.88,126.23,7.86">Waterloo (Cormack) participation in the TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,299.63,634.88,180.96,7.86;6,151.52,645.84,126.32,7.86">Proceedings of The Twenty-Fourth Text REtrieval Conference, TREC 2015</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference, TREC 2015<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">November 17-20, 2015, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,119.67,337.63,7.86;7,151.52,130.63,329.07,7.86;7,151.52,141.59,329.07,7.86;7,151.52,152.55,195.27,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,324.09,119.67,156.50,7.86;7,151.52,130.63,104.65,7.86">Engineering quality and reliability in technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,276.79,130.63,203.80,7.86;7,151.52,141.59,324.74,7.86">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">July 17-21, 2016. 2016</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,163.51,337.63,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,329.07,7.86;7,151.52,196.39,276.44,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,306.35,163.51,174.24,7.86;7,151.52,174.47,148.95,7.86">Scalability of continuous active learning for reliable high-recall text classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,323.06,174.47,157.52,7.86;7,151.52,185.43,324.74,7.86">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 24-28, 2016. 2016</date>
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,207.34,337.64,7.86;7,151.52,218.30,329.07,7.86;7,151.52,229.26,329.07,7.86;7,151.52,240.22,73.66,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,308.93,207.34,171.66,7.86;7,151.52,218.30,179.79,7.86">When to stop&quot; Waterloo (Cormack) participation in the TREC 2016 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,350.31,218.30,130.28,7.86;7,151.52,229.26,157.46,7.86">Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference, TREC 2016<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 15-18, 2016, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,251.18,337.63,7.86;7,151.52,262.14,329.07,7.86;7,151.52,273.10,329.07,7.86;7,151.52,284.06,274.95,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,318.26,251.18,162.33,7.86;7,151.52,262.14,211.98,7.86">Navigating imprecision in relevance assessments on the road to total recall: Roger and me</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.40,262.14,94.19,7.86;7,151.52,273.10,329.07,7.86;7,151.52,284.06,107.64,7.86">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2017</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2017<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 7-11, 2017, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,295.02,337.63,7.86;7,151.52,305.98,329.07,7.86;7,151.52,316.93,74.80,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,318.01,295.02,162.58,7.86;7,151.52,305.98,215.72,7.86">Technology-assisted review in empirical medicine: Waterloo participation in clef ehealth 2017</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,377.42,305.98,97.74,7.86">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,327.89,337.64,7.86;7,151.52,338.85,329.07,7.86;7,151.52,349.81,303.53,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,459.86,327.89,20.74,7.86;7,151.52,338.85,329.07,7.86;7,151.52,349.81,106.59,7.86">Identification of randomized controlled trials in systematic reviews: accuracy and reliability of screening records</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Diguiseppi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,266.11,349.81,85.33,7.86">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1635" to="1640" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,360.77,337.64,7.86;7,151.52,371.73,329.07,7.86;7,151.52,382.69,267.88,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,356.88,360.77,123.71,7.86;7,151.52,371.73,33.25,7.86">TREC 2016 Total Recall Track overview</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.59,371.73,275.01,7.86;7,151.52,382.69,17.31,7.86">Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference, TREC 2016<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 15-18, 2016, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,393.65,337.98,7.86;7,151.52,404.61,172.77,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,263.52,393.65,217.07,7.86;7,151.52,404.61,18.40,7.86">Cochrane handbook for systematic reviews of interventions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,415.56,337.98,7.86;7,151.52,426.52,329.07,7.86;7,151.52,437.48,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,358.90,415.56,121.69,7.86;7,151.52,426.52,152.68,7.86">CLEF technologically assisted reviews in empirical medicine overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,322.47,426.52,158.12,7.86;7,151.52,437.48,111.25,7.86">CLEF 2018 Evaluation Labs and Workshop: Online Working Notes</title>
		<title level="s" coord="7,269.55,437.48,145.99,7.86">CEUR Workshop Proceedings. CEUR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,448.44,337.97,7.86;7,151.52,459.40,329.07,7.86;7,151.52,470.36,329.07,7.86;7,151.52,481.32,20.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,432.56,448.44,48.03,7.86;7,151.52,459.40,106.66,7.86">TREC 2015 total recall track overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,282.23,459.40,198.36,7.86;7,151.52,470.36,98.53,7.86">Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2015</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference, TREC 2015<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">November 17-20, 2015, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,492.28,337.98,7.86;7,151.52,503.24,329.07,7.86;7,151.52,514.19,329.07,7.86;7,151.52,525.15,287.13,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,389.02,503.24,91.56,7.86;7,151.52,514.19,91.43,7.86">Overview of the CLEF ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,289.07,514.19,191.52,7.86;7,151.52,525.15,70.33,7.86">CLEF 2018 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="7,229.74,525.15,141.41,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
