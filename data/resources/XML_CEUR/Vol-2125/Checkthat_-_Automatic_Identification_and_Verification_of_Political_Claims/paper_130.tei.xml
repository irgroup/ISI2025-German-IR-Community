<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.64,115.96,270.07,12.62;1,154.29,133.89,306.78,12.62;1,216.83,151.82,181.70,12.62">UPV-INAOE-Autoritas -Check That: An Approach based on External Sources to Detect Claims Credibility</title>
				<funder ref="#_DktaDGQ">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
				<funder ref="#_UJ8AM7D">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,214.21,189.52,60.26,8.74"><forename type="first">Bilal</forename><surname>Ghanem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">PRHLT Research Center</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.02,189.52,108.88,8.74"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofìsica</orgName>
								<address>
									<settlement>Puebla</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.73,201.47,73.97,8.74"><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">PRHLT Research Center</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Autoritas Consulting</orgName>
								<address>
									<settlement>Valencia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,334.47,201.47,52.69,8.74"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic</email>
							<affiliation key="aff0">
								<orgName type="department">PRHLT Research Center</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.64,115.96,270.07,12.62;1,154.29,133.89,306.78,12.62;1,216.83,151.82,181.70,12.62">UPV-INAOE-Autoritas -Check That: An Approach based on External Sources to Detect Claims Credibility</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD97E42DCE09D7CB1698E52B0F4F16A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Claims Credibility</term>
					<term>English</term>
					<term>Arabic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the uncontrolled increasing of fake news, untruthful claims, and rumors over the web, recently different approaches have been proposed to address this problem. In this paper, we present a credibility detector of factual claims in presidential debates. Our approach captures the distribution of the results from the search engines to infer the credibility of the claims. We participated in the CLEF-2018 Check That lab for Task 2, obtaining acceptable results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A massive amount of information is spread on the web. One of the main disadvantages of this data growth is the uncontrollability of their veracity. The existence of social media networks has helped the increase of untruthful news. Recently, different attempts have addressed this issue to propose solutions for fact checking, for instance for presidential debates. During the debates, multiple claims are made by the candidates about previous facts. Some of these claims could be untruthful: the claimer makes it in an attempt to weaken the other candidate. These untruthful claims pose a real risk on the elections results. In this paper, we present our approach for detecting the factuality or of claims in the presidential debates, where in <ref type="bibr" coords="1,299.11,560.45,9.96,8.74" target="#b0">[1]</ref>, we presented our approach for task 1 (detecting check worthy claims).</p><p>This task concerns with investigating claims veracity in presidential debates. Therefore, a set of presidential debates from the US presidential elections are presented. In our approach, we used results from search engines to infer the veracity of the claims. The idea behind our approach is to infer the veracity by measuring the similarity between the claims and the search engines results, with extracting the results' sources reliability. Also, we modeled the distribution of these features in each search engine. In the following section, we present an overview of the literature. In Section 3, we present our approach, with giving a view on the task. The experiments with the results are presented in Section 4. Finally, in Section 5, we draw some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Researchers in the literature studied several features from different aspects to address this research problem. These features addressed different characteristics of the claims on the web. Authors in <ref type="bibr" coords="2,296.12,226.16,10.52,8.74" target="#b1">[2]</ref> have combined two different features to infer the credibility of text. They used language stylistic features to capture the presence of specific language style. Also, they checked the reliability of the text sources using two measures: Amazon AlexaRank and Google PageRank. Authors in <ref type="bibr" coords="2,146.50,273.98,10.52,8.74" target="#b2">[3]</ref> used a different way to assess the credibility of claims. In their approach, they created a query from claims using the main sentence components, and they passed it to Google and Bing search engines. From the obtained snippets' results, they trained an LSTM network and they used its encoding to represent the results. These encodings were combined with other similarity features to train two classifiers: a Support Vector Machine (SVM) and a Neural Network. Another set of features were proposed in <ref type="bibr" coords="2,271.00,345.71,9.96,8.74" target="#b3">[4]</ref>, where the authors studied different aspects of the claims in Twitter to infer their credibility. The authors used features based on the text characteristics, user-based, topic-based, and tweets propagation-based features. In <ref type="bibr" coords="2,187.13,381.58,10.52,8.74" target="#b4">[5]</ref> a continuous conditional random field model was used to exploit several signals of interaction between a set of features. In their approach, the authors used features from the language of the news, source trustworthiness, and users confidence. An alternative statement collection approach was proposed in <ref type="bibr" coords="2,134.77,429.40,9.96,8.74" target="#b5">[6]</ref>. The authors collected alternative statements of the original claim by changing the doubt unit. Moreover, they inferred the veracity based on different rankers. The only weakness of their approach is that the process is not fully automatic: when a claim is entered, the user should choose the doubt unit which will be investigated. Authors in <ref type="bibr" coords="2,240.90,477.22,10.52,8.74" target="#b6">[7]</ref> proposed linguistic, credibility and semantic features to infer the credibility of Bulgarian news. They trained a word2vec model on DBPedia to model the semantics of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of our Approach</head><p>As we mentioned, this task concerns with the claims from the presidential debates. The claims that are unworthy for checking have not been annotated and kept in the debates to keep the context. Factual claims have been tagged as True, False, and Half-True. These debates are provided in two languages: English and Arabic, where the Arabic text is translated from the original English debates. The dataset that was provided is imbalanced, where the total number of factual claims is 81; claims as True: 19, Half-True: 22, False: 41. The task goal is to detect the credibility of the provided claims. The macro F1 score was used as the performance measure. More details of the task are mentioned in <ref type="bibr" coords="2,434.51,656.12,14.61,8.74" target="#b9">[10]</ref>.</p><p>Hypothesis: Factual claims have been discussed and mentioned in online news agencies. In our approach, we used the distribution of these claims in the search engines results. Furthermore, we supposed that truthful claims have been mentioned more by trusted web news agencies than the untruthful ones. Therefore, our approach depends on modeling the returned results from search engines using similarity measures and with extracting the reliabilities of the results' sources (dependent features). Also, we captured the distribution of these previous features from each search engine (independent features).</p><p>At the beginning, we started to reformulate each claim into a query. We fed this query to the Google and the Bing search engines to obtain a set of results. In our approach, we use only the returned snippets and we do not investigate more the original web pages. Given the search engine results, we used in our approach the first N results for the feature extraction. Next, we built the representation of the features: Independent features: For each returned result, we extracted the following three features:</p><p>Cosine over embedding: we used pre-trained Google News word2vec embedding to measure the cosine similarity between each snippet and the query. We used the main sentence components, discarding the stopwords. In the same way, we built this feature for the Arabic language, but we used fastText pre-trained embedding <ref type="bibr" coords="3,311.04,369.87,10.52,8.74" target="#b7">[8]</ref> since Google news word2vec is not available.</p><p>AlexaRank : For each result, we used Amazon Alexa Rank to retrieve the rank of its site. The sites that have lower values are the sites that have higher reliability.</p><p>Text similarity: we used another text similarity measure, but using the full sentence components (similarity over tokens), and without text embedding. For the English part, we used the Spacy python library<ref type="foot" coords="3,427.37,463.75,3.97,6.12" target="#foot_0">4</ref> , while for the Arabic language, since this library is not available, we implemented the text similarity approach that used in <ref type="bibr" coords="3,325.89,489.24,10.52,8.74" target="#b8">[9]</ref> for plagiarism detection.</p><p>As we mentioned, we considered the first N results from the search engines, thus, we ended with a features vector of size 3×N.</p><p>Dependent features: We extracted a set of features based on the previous independent features. These features model the distribution of the previous feature set, that has been extracted using Average (Avg) and Standard Deviation (Std).</p><p>Avg and Std of AlexaRank feature: We computed both Avg and Std features for the Alexa values that were extracted for the first N results.</p><p>Avg and Std of the Cosine over embedding feature: Similarly, we computed also the Avg and the Std features for the cosine similarities values that were extracted.</p><p>At the end, our representation has (3×N) + 4 features.</p><p>All of these previous dependent and independent features were extracted twice, one from the Google and another one from the Bing search engines. In the following section, we will investigate their importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>As we mentioned in the previous section, we built our features based on the first N results from the search engines. Experimentally, we found that choosing the first 5 results (N=5) has produced the highest results. Based on that, our feature vector length is 38 features. In Figure <ref type="figure" coords="4,194.36,500.70,3.87,8.74" target="#fig_0">1</ref>, we show the information gain of these features for each search engine. From these results, we can infer that the features that were obtained by the Google search engine are more important than the Bing features. Based on that, we can notice that the Google results can improve the performance more than the Bing results. At the beginning of our experiments, we tried also to combine Yahoo results, but unfortunately, in all of our experiments, Yahoo results had a lower performance.</p><p>Since our approach is search engines-based, for the Arabic task, we found that these claims did not exist because they were written originally in English and translated into Arabic for this task. Therefore, we translated back these claims into English to retrieve results. After that, the results and the query were translated back to Arabic.</p><p>During our experiments, many classifiers were tested. We found that the Random Forest classifier achieved the highest results. By using the K-Fold stratified technique, we achieved 0.34 of macro F1 score. The chosen value of K is 5, where we have a small number of data instances and an imbalanced dataset. Thus, higher values of K may lead to absence of some classes in one of the training/testing cycles. We tried also to build a different type of queries, using main sentence components, or phrase queries, but we found that when we changed the query, the results were affected negatively, especially when we used the phrase query, we noticed that the search engines snippets became meaningless (phrases appeared in the snippets but as small text clips connected using "..." characters and combined into the main snippet, where the semantic meaning of the main snippet became biased). For this reason, we passed the queries without any modification, letting the search engines to retrieve the most appropriate results for each one.</p><p>For the official testing phase the Mean Absolute Error (MAE) was used as performance measure. In Table <ref type="table" coords="5,273.28,274.41,3.87,8.74" target="#tab_0">1</ref>, the results of the task are shown. In the English part, our approach obtained the 3rd best results, while for the Arabic part, only two teams have submitted their runs. We can observe that the results are low, showing the difficulty of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented our simple approach for detecting the veracity of claims in the presidential debates. As we mentioned, our approach uses search engine results to infer the claims veracity. In our approach, we extracted two different types of features, dependent and independent, to model the distribution of claims' results. In general, the results of the task are low, knowing that our approach during the tuning phase has achieved good results comparing to the official one. Also, we can conclude that our feature set has improved the results with respect to the other participants approaches and to the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,457.24,345.82,7.89;4,134.77,468.23,236.88,7.86;4,134.77,271.50,345.84,170.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Information Gain values of the feature set. The features that started with BI are the ones built using Bing, similarly, GO for Google.</figDesc><graphic coords="4,134.77,271.50,345.84,170.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,158.41,293.33,298.55,85.45"><head>Table 1 .</head><label>1</label><figDesc>Official results for the Task 2, released using the MAE measure.</figDesc><table coords="5,221.05,314.13,174.14,64.65"><row><cell>Team</cell><cell>English</cell><cell>Arabic</cell></row><row><cell>Copenhagen</cell><cell>0.705</cell><cell></cell></row><row><cell>FACTR</cell><cell>0.913</cell><cell>0.657</cell></row><row><cell>UPV-INAOE</cell><cell>0.949</cell><cell>0.820</cell></row><row><cell>bigIR</cell><cell>0.964</cell><cell></cell></row><row><cell>Check It Out</cell><cell>0.964</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,656.80,153.49,7.86"><p>https://spacy.io/, visited in May 2018</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The authors acknowledge the <rs type="grantNumber">SomEMBED TIN2015-71147-C2-1-P</rs> <rs type="projectName">MINECO</rs> research project. The work on the data in Arabic as well as this publication were made possible by NPRP grant #<rs type="grantNumber">9-175-1-033</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). The statements made herein are solely the responsibility of the last two authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_UJ8AM7D">
					<idno type="grant-number">SomEMBED TIN2015-71147-C2-1-P</idno>
					<orgName type="project" subtype="full">MINECO</orgName>
				</org>
				<org type="funding" xml:id="_DktaDGQ">
					<idno type="grant-number">9-175-1-033</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,142.59,342.24,7.86;6,146.91,153.55,333.68,7.86;6,146.91,164.51,333.68,7.86;6,146.91,175.46,193.98,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,457.43,142.59,23.16,7.86;6,146.91,153.55,333.68,7.86;6,146.91,164.51,25.94,7.86">UPV-INAOE-Autoritas -Check That: Preliminary Approach for Checking Worthiness of Claims</title>
		<author>
			<persName coords=""><forename type="first">Bilal</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes-Y-Gòmez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,192.16,164.51,288.43,7.86;6,146.91,175.46,70.41,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CLEF &apos;18</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date>September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,186.42,342.24,7.86;6,146.91,197.38,333.68,7.86;6,146.91,208.34,333.68,7.86;6,146.91,219.30,93.94,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,146.91,197.38,216.66,7.86">Credibility Assessment of Textual Claims on the Web</title>
		<author>
			<persName coords=""><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jannik</forename><surname>Strtgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.03,197.38,96.56,7.86;6,146.91,208.34,313.44,7.86">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2173" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,230.26,342.25,7.86;6,146.91,241.22,333.68,7.86;6,146.91,252.18,333.68,7.86;6,146.91,263.14,181.10,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,209.77,241.22,232.12,7.86">Fully Automated Fact Checking Using External Sources</title>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,463.03,241.22,17.56,7.86;6,146.91,252.18,333.68,7.86;6,146.91,263.14,100.62,7.86">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
	<note>RANLP 2017</note>
</biblStruct>

<biblStruct coords="6,138.35,274.09,342.25,7.86;6,146.91,285.05,333.68,7.86;6,146.91,296.01,100.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,386.11,274.09,94.49,7.86;6,146.91,285.05,41.52,7.86">Information Credibility on Twitter</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,206.34,285.05,269.70,7.86">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,306.97,342.24,7.86;6,146.91,317.93,333.68,7.86;6,146.91,328.89,333.68,7.86;6,146.91,339.85,48.38,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,345.44,306.97,135.16,7.86;6,146.91,317.93,168.94,7.86">Leveraging Joint Interactions for Credibility Analysis in News Communities</title>
		<author>
			<persName coords=""><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.97,317.93,145.63,7.86;6,146.91,328.89,275.88,7.86">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,350.81,342.24,7.86;6,146.91,361.77,333.68,7.86;6,146.91,372.72,107.12,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,314.58,350.81,166.01,7.86;6,146.91,361.77,43.10,7.86">T-verifier: Verifying Truthfulness of Fact Statements</title>
		<author>
			<persName coords=""><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiyi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,207.69,361.77,100.83,7.86;6,336.59,361.77,144.01,7.86;6,146.91,372.72,8.19,7.86">IEEE 27th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct coords="6,138.35,383.68,342.24,7.86;6,146.91,394.64,333.68,7.86;6,146.91,405.60,198.48,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,369.46,383.68,106.36,7.86">In Search of Credible News</title>
		<author>
			<persName coords=""><forename type="first">Momchil</forename><surname>Hardalov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,159.22,394.64,321.37,7.86;6,146.91,405.60,48.96,7.86">International Conference on Artificial Intelligence: Methodology, Systems, and Applications</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,416.56,342.24,7.86;6,146.91,427.52,333.68,7.86;6,146.91,438.48,207.58,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,451.00,416.56,29.58,7.86;6,146.91,427.52,183.73,7.86">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,339.00,427.52,141.59,7.86;6,146.91,438.48,106.72,7.86">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,449.44,342.24,7.86;6,146.91,460.40,333.68,7.86;6,146.91,471.35,333.68,7.86;6,146.91,482.31,50.68,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,438.30,449.44,42.29,7.86;6,146.91,460.40,200.03,7.86">HYPLAG: Hybrid Arabic Text Plagiarism Detection System</title>
		<author>
			<persName coords=""><forename type="first">Bilal</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Labib</forename><surname>Arafeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-</forename><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,367.10,460.40,113.49,7.86;6,146.91,471.35,235.88,7.86">International Conference on Applications of Natural Language to Information Systems</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,493.27,337.98,7.86;6,146.91,504.23,333.68,7.86;6,146.91,515.19,333.68,7.86;6,146.91,526.15,333.68,7.86;6,146.91,537.11,333.68,7.86;6,146.91,548.07,162.08,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,303.25,515.19,177.34,7.86;6,146.91,526.15,329.83,7.86">Overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification of Political Claims. Task 2: Factuality</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wajdi</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Spas</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,158.74,537.11,321.85,7.86;6,146.91,548.07,38.53,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CLEF &apos;18</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date>September</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
