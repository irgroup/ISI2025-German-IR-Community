<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.79,116.95,307.77,12.62;1,134.77,134.89,345.83,12.62;1,156.74,152.82,301.88,12.62;1,164.29,170.75,286.78,12.62;1,259.36,188.68,96.64,12.62">The Copenhagen Team Participation in the Factuality Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.21,226.35,75.74,8.74"><forename type="first">Dongsheng</forename><surname>Wang</surname></persName>
							<email>wang@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (DIKU))</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.51,226.35,94.94,8.74"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
							<email>simonsen@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (DIKU))</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.00,226.35,59.37,8.74;1,390.37,224.78,1.83,6.12"><forename type="first">Birger</forename><surname>Larsen</surname></persName>
							<email>birger@hum.aau.dk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Communication</orgName>
								<orgName type="institution">Aalborg University Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.60,226.35,71.07,8.74"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
							<email>c.lioma@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (DIKU))</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.79,116.95,307.77,12.62;1,134.77,134.89,345.83,12.62;1,156.74,152.82,301.88,12.62;1,164.29,170.75,286.78,12.62;1,259.36,188.68,96.64,12.62">The Copenhagen Team Participation in the Factuality Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14EB7FA35D752A757C88FBE7DCA9415D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>political debates</term>
					<term>RNN</term>
					<term>CNN</term>
					<term>fact checking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a set of political debate claims that have been already identified as worth checking, we consider the task of automatically checking the factuality of these claims. In particular, given a sentence that is worth checking, the goal is for the system to determine whether the claim is likely to be true, false, half-true or that it is unsure of its factuality. We implement a variety of models, including Bayes, SVM, RNN, to either step-wise assist our model or work as potential baselines. Then, we develop additional multi-scale Convolutional Neural Networks (CNNs) with different kernel sizes that learn from external sources whether a claim is true, false, half-true or unsure as follows: we treat claims as search engine queries and step-wise retrieve the top-N documents from Google with as much original claim as possible. We strategically select most relevant but sufficient documents with respect to the claims, and extract features, such as title, total number of results returned, and snippet to train the prediction model. We submitted results of SVM and CNNs, and the overall performance of our techniques is successful, achieving the overall best performing run (with lowest error rate 0.7050 from our SVM and highest accuracy 46.76% from our CNNs) in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Copenhagen team participated in both Tasks 1 and 2 of the CLEF-2008 Fact Checking Lab for the English language. This report details our methods and results for Task 2, the task description paper by the organizers with all background and details specified can be found in <ref type="bibr" coords="1,351.72,621.25,9.96,8.74" target="#b1">[2]</ref>. Our participation in Task 1 is described in <ref type="bibr" coords="1,208.79,633.20,9.96,8.74" target="#b2">[3]</ref>.</p><p>Given a set of political debate claims that have been already identified as worth checking, the aim of the second task is to check the factuality of these claims. In particular, given a sentence that is worth checking, the goal is for the system to determine whether the claim is likely to be true, half-true, false or that it is unsure of its factuality.</p><p>One of the two examples given by organizers <ref type="bibr" coords="2,349.69,155.86,10.52,8.74" target="#b7">[8]</ref> is shown in Table <ref type="table" coords="2,443.83,155.86,3.87,8.74" target="#tab_0">1</ref>, where Hillary Clinton mentions Bill Clinton's work in the 1990s, followed by a claim made by Donald Trump stating that for president Clinton approved the North American Free Trade Agreement (NAFTA). This last statement by Trump is judged to be HALF-TRUE because it was George H.W. Bush who signed the approval for NAFTA, but Bill Clinton who signed it into law. As CLEF provides limited data (only 82 unique claims with labels), but the task of fact checking relies on labeled data to train prediction models, finding suitable datasets for training is the first basic step. Furthermore, the task at hand is more complex than traditional binary prediction (True/False) as graded truth values must be predicted, including the difficult "Half-True". There are primarily three objectives that we take into consideration:</p><p>1. Select external claims with labels and suitable proportion of samples 2. Retrieving most relevant but suitable amount of external sources (documents) for claims 3. Find the best models and parameters and tune them to their best performance</p><p>The three objectives are met by proceeding in a stepwise manner. Selecting external claims of high quality is the basis of the following steps. The multiple labels and their proportional samples have to be taken into account when selecting datasets with different labeling. Subsequently, retrieving most relevant but adequate documents for these claims are significant to support the building of training models. Finally, selected features of documents should be fitted well into different models, of which the parameters should be tuned to improve the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches Used and Progress Beyond State-of-the-Art</head><p>Our approach is as follows: we use a step-wise modeling from data selection, preprocessing, retrieval, to training modeling, with the aim of choosing a suitable proportion of samples with labels and supporting documents that we are going to employ. Specifically, we take advantage of a simple Bayesian model, to analyze label impact and data sufficiency in the data processing stage, and stepwise search in the stage of supporting document retrieving.</p><p>For training the model, we employ CNN and RNN models, as well as an SVM. RNN model is employed in similar tasks such as the work of detecting rumors from microblogs by Ma et al. <ref type="bibr" coords="3,309.56,181.22,10.52,8.74" target="#b5">[6]</ref> through capturing the variation of contextual information of relevant posts over time in microblogs. Closer to our aims, Karadzhov et al. <ref type="bibr" coords="3,239.54,205.13,10.52,8.74" target="#b3">[4]</ref> investigate a fact checking task, and we implement a similar framework as shown in Figure <ref type="figure" coords="3,310.91,217.09,3.87,8.74" target="#fig_1">2</ref>; we use two simplifications compared to <ref type="bibr" coords="3,147.20,229.04,9.96,8.74" target="#b3">[4]</ref>: (a) we only use 5 Google snippets while the original author uses 4 units consisting of one Google snippet, one Bing snippet and two triplets of rolling sentences from Google and Bing respectively; and (b) we only calculate one similarity, namely pairwise TF-IDF cosine similarity, whereas <ref type="bibr" coords="3,407.14,264.91,10.52,8.74" target="#b3">[4]</ref> calculates the average of cosine with TF-IDF, cosine over embeddings, and containment.</p><p>CNNs are adopted in the sentence-level classification by Kim et al. <ref type="bibr" coords="3,450.20,290.27,10.52,8.74" target="#b4">[5]</ref> and they have been demonstrated to improve performance on NLP classification tasks. In our CNN model, as shown in Figure <ref type="figure" coords="3,345.60,314.18,3.87,8.74" target="#fig_0">1</ref>, inspired by <ref type="bibr" coords="3,411.29,314.18,14.61,8.74" target="#b9">[10]</ref>, we employ multi-scale CNNs with different kernel sizes to overcome the drawback of simple convolutional kernel with fixed window size over encoded semantics of documents. It is hard to determine window size using simple convolutional kernels, because small window normally requires deeper networks to gain critical information and large window sizes result in loss of local information. Therefore, multi-scale CNNs, together with the other feature (total return), are designed to represent the comprehensive contextual information of the text. Specifically, we encode the semantics with word2vec <ref type="bibr" coords="3,309.79,409.82,10.52,8.74" target="#b6">[7]</ref> for documents of concatenated snippets on the first layer into low-dimensional vector. Then, we perform multi-scale CNNs with different kernel sizes on the second layer over the embedded word vectors. In our experiment settings, we concatenate four CNNs with 1, 2, 3 and 4 kernel sizes respectively, followed by max-pooling layer and dropout after each of them. We set the channel as static word vectors. In CNN representation layer, we add total return of the first search (step-wise search is discussed in Section 3.2) for each claim as additional information. Because the total return has a large numeric range, we discretize it into eight equally-sized categories based on the statistics of training samples.</p><p>While each step of our approach uses only simple, well-known methods, our progress beyond state-of-the-art methods consists of the combination of the following:</p><p>1. We use step-wise modeling, instead of using a mixed model in the final step, i.e., we use traditional Bayes models for data prepossessing, including data selection (label mapping) and external source analysis (sufficiency analysis), and then build a CNN model based on previous conclusion. 2. We employ step-wise searching in retrieving supporting documents with as much of the original claim as possible while strategically retrieving enough documents, instead of just using keywords. 3. we employ multi-scale CNNs with multiple kernel sizes, together with discrete total return, to represent the contextual information. We assume that multi-scale CNNs can obtain comprehensive information and the total return of a claim represents the intensity of attention, which to some degree reflect its hidden status. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resources Employed</head><p>We describe how to collect claims with labels from Politifact in Section 3.1, and how we retrieve supporting documents for these claims in Section 3.2, and we give a short description concerning the word embedding we utilize in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Claims with labels from Politifact</head><p>Due to the small dataset of claim samples (a total of 82 unique claims with labels) provided by CLEF-2018 Fact Checking Lab, we use Politifact as an external source to collect claims and their labels. Specifically, we crawl Politifact Truth-O-Meter statements from www.politifact.com that are operated by editors and reporters from the Tampa Bay Times. For the Truth-O-Meter webpage, Politifact staffers research U.S. politics statements and label them as "true", "mostly true", "half true", "mostly false", "false", "and pants on fire" (the latter for outrageously false claims). We obtain a total of 4,604 statements/claims from Politifact as demonstrated in Table <ref type="table" coords="5,291.95,237.52,3.87,8.74" target="#tab_1">2</ref>. The task of CLEF-2018 Fact Checking Lab requires us to predict claims as one of the three labels: "true", "false" and "half-true". Therefore, we map the six Politifact labels into three categories and remove some ambiguous labels. Table <ref type="table" coords="5,162.53,382.47,4.98,8.74" target="#tab_2">3</ref> shows three examples of label mapping; for Map1, we map all six into three labels; remove Mostly False for Map2; and Mostly-true as well in Map3. In experiment part (Section 4.1), we would compare the performance for each mapping and obtain the best one as training dataset. In addition, we try to discover how many overlaps does Politifact have with that in test set. If we check the exactly same claims, no claims are found, i.e., no overlap exists. If we use Levenshtein distance to detect similar claims, there are still no same claims exist when similarity is set below 0.8, whereas there are only three claims that are, to some degree, similar when the similarity is set below 0.8.</p><p>It is noted that of all the retrieved URLs, there were a total of 1,310 bad URLs out of 84,451 URLs (A ratio of 1.55%) in our training dataset. For model building and training with our existing dataset, we use the politifact dataset without further processing or filtering. For testing data from CLEF, we used the urlfilters function provided by CLEF to filter bad urls when retrieving supporting documents from Google, and output the prediction result. According to our internal testing, the performance does not seem to be negatively impacted with or without bad URLs in training dataset, and is sometime even slightly better after filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Documents Retrieval from Search Engine</head><p>We retrieved supporting documents and texts in order to train the prediction models, in addition to the claim texts themselves. To that end, we retrieve top-N documents from Google. Compared to the classical approach of analyzing and shortening claims into keywords, we used a step-wise searching method to maintain the semantics as much as possible.</p><p>The reason for this is that we conjecture that using the whole sentence could keep more of the original semantics, including speaking habits and commonlyused sequences of words. We thus use each whole claim verbatim as a Google query, at the risk of retrieving only few documents. We subsequently apply stepwise searching to fill up a list of documents as follows. First, we initialize a set with zero documents for each claim. We then use the whole claim as a query to retrieve documents and populate the set with the results. If the set has less than N documents (N=20 in our concrete experiments), we remove the stop words of the claim and search again, populating the set with retrieved documents. If the set contains fewer than N documents, we search again using only the nouns, verbs and adjectives obtained by using part-of-speech (POS) (obtained with the Stanford POS tagger <ref type="bibr" coords="6,229.75,420.58,10.30,8.74" target="#b8">[9]</ref>), populate the set in the same way as with the second search. We ended up being able to retrieve 20 documents for each of the claim. It is noted that we do not necessarily use all of them as we re-rank the documents with cosine similarity and employ, for example, top 4 or 5 snippets among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-trained embedding</head><p>For CNN and RNN, We employ existing pre-trained word vectors -word2vec <ref type="bibr" coords="6,470.08,505.97,10.52,8.74" target="#b6">[7]</ref> for our word embedding. word2vec is published by Google who trained it on 100 billion words of Google news with continuous bag-of-words model and generated the vectors of 300-dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of the Results</head><p>In Section 4.1 and 4.2, we perform some experiments with a Bayesian classifier to investigate how to map the six labels into three, and determine how many documents (snippets) we need to fit models. We do not use more advanced classifiers such as RNNs or CNNs to conduct this analysis as the word embedding layer is hidden, hard to explain, and the proper neural network layer are sensitive to parameters rather than semantics of texts. Conversely, it is usually easier to understand how a trained Bayesian classifier based on bag of words and n-gram reflect the semantics of texts in a simple and straightforward way. In Section 4.3, we give the comparison of performance of different models on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Label mapping</head><p>The six Politifact labels must be mapped into three: True, False, and Half-True. Some labels are ambiguous, e.g., Mostly-True can be either True or Half-True. Therefore, we tune the mapping using a Bayes classification model on three combination cases listed in Table <ref type="table" coords="7,283.51,251.95,3.87,8.74" target="#tab_2">3</ref>. As shown in Figure <ref type="figure" coords="7,383.93,251.95,3.87,8.74" target="#fig_2">3</ref>, Map3 has the same highest accuracy with Map1, the highest macro F1 and the highest macro recall. Therefore, we apply Map3 mapping case as our training data. We discard the claims with labels not listed in Map3, and the new statistics is shown in table <ref type="table" coords="7,134.77,299.77,3.87,8.74">4</ref>, which is applied to all other models as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Contents Sufficiency Analysis</head><p>We manipulate the number of document snippets that are concatenated and compare their performance (accuracy, macro F1, etc.) to determine the number of snippet texts needed. We rank the documents according to their pairwise TF-IDF similarity with the claims and select the top-N (we test N=1 to N=10 in our experiments) to concatenate. As shown in Figure <ref type="figure" coords="8,238.58,198.98,3.87,8.74" target="#fig_3">4</ref>, using two snippets leads to the highest performance, 5 snippets the second-best, and 4 and 8 the third-best. In short, the rank of "2,5,4,8, etc." is the order of numbers of snippets we learned that we can refer to utilize. As we know, training deep learning model is time-consuming, such analysis narrows the scope of choices and enables us to focus on parameters of models themselves. However, note that the results by number of snippets are quite unstable, and that no firm conclusions can be drawn. In our experiment, we primarily conduct our experiment on 2 and 5 snippets (highest and second highest) and attempt to obtain the models of parameters and number of snippets with the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prediction comparison</head><p>For Bayes and SVM classifiers, we employ the Bag of Words (BOW) model for English texts, tokenizing them and removing stop words; also, we adopt TF-IDF (Term Frequency times inverse document frequency) for term weight. We use grid search to tune the parameters of each model. We rank the documents of each claim according to their similarity with the claim and concatenate the first five snippets as a whole document. As shown in Table <ref type="table" coords="8,348.40,645.16,3.87,8.74" target="#tab_3">5</ref>, Naive bayes with grid search could reach its best performance to 53.98% and 43.90% on Politifact samples (20% of Politifact samples=743 samples) and CLEF (82 samples), respectively. We produce two best models, with CNNs and RNN, on Politifact and CLEF test respectively, titled as CNN1, CNN2, RNN1, RNN2, shown in the table. For CNN and RNN, we observe that RNN has worse performance than CNN on Politifact samples, with 47.22% and 46.88%, respectively. The performance on CLEF samples with 45.12% and 46.34% is similar. In contrast, using CNN results in an accuracy of 55.56% and 51.56%, respectively, on Politifact samples, but only 42.68% and 48.78% on CLEF samples.</p><p>We submitted the SVM+Gridsearch and CNN models because SVM is more stable across most test cases, and CNN has a relatively good overall performance, but is less stable. We run the two models on the final test dataset without label from CLEF and output the prediction result. The test result from CLEF is shown in Table <ref type="table" coords="9,204.49,263.45,3.87,8.74" target="#tab_4">6</ref>. Within our two groups of results, we observe that for metric MAE (mean absolute error) SVM outperforms CNN while for accuracy CNN2 outperforms SVM. However, either one group of our results outperforms those of all the other teams for each single metric. There are several further empirical phenomena evident from our experiments:</p><p>1. The RNN model is more unstable than other models and sensitive not only to parameters but also to epochs. 2. The traditional models, Bayes and SVM, sometimes have worse performance than the neural network-based approaches, but are much more robust in terms of performance.</p><p>3. We originally conjectured that claims in documents could be concerned with temporal information which could be exploited well by an RNN model. Hence, we also try to re-rank the documents of a claim according to year, and fit on RNN model. This does not improve performance. One possible reason is that some documents do not have time information, and placing them in a ranked list (in front or rear) just introduces uncertainty. Another reason might be that we adopt only a few documents so the ranking is not as apparent as we assumed.</p><p>5 Perspectives for Future Work</p><p>Our current work only employs lexical and syntactic context. In future work, we plan to add information about semantic structures and argumentation flow; we believe that this will aid our methods in identifying some of the most egregious common examples of poor reasoning or argumentation (e.g., logical fallacies). One similar work that could be referred to is by Ba el at. <ref type="bibr" coords="10,391.32,308.54,10.52,8.74" target="#b0">[1]</ref> who extract entities and relations from web and Twitter and gathers the conflicting information.</p><p>Secondly, while we have found that using combinations of simplifications of several methods found in the literature, we aim to investigate whether using tuned versions of the original methods (e.g., <ref type="bibr" coords="10,302.14,356.36,10.79,8.74" target="#b3">[4]</ref>) may improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This work is funded by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 721321.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,236.11,126.87,143.14,7.89;4,169.35,135.76,276.67,150.59"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our CNN model framework</figDesc><graphic coords="4,169.35,135.76,276.67,150.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,245.52,401.09,124.31,7.89;4,169.35,409.98,276.66,147.24"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. RNN model framework</figDesc><graphic coords="4,169.35,409.98,276.66,147.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,193.43,345.70,228.51,7.89;7,169.35,354.59,276.66,164.44"><head>Fig. 3 .FALSE</head><label>3</label><figDesc>Fig. 3. Performance with different Label Mapping Cases</figDesc><graphic coords="7,169.35,354.59,276.66,164.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,205.28,347.69,204.80,7.89;8,169.35,356.58,276.66,166.82"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance with different snippet number</figDesc><graphic coords="8,169.35,356.58,276.66,166.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,139.75,247.08,334.46,69.73"><head>Table 1 .</head><label>1</label><figDesc>Example of a spurious claim</figDesc><table coords="2,139.75,271.07,83.82,7.86"><row><cell>Speaker</cell><cell>Sentence</cell></row></table><note coords="2,139.75,287.03,272.08,7.86;2,139.75,297.99,334.46,7.86;2,139.75,308.94,229.86,7.86"><p>CLINTON: I think my husband did a pretty good job in the 1990s. CLINTON: I think a lot about what worked and how we can make it work again... TRUMP: Well, he approved NAFTA... (HALF-TRUE)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,137.81,267.86,339.74,46.07"><head>Table 2 .</head><label>2</label><figDesc>Distribution of labels of claims</figDesc><table coords="5,137.81,290.10,339.74,23.82"><row><cell>True</cell><cell>Mostly True Half True Mostly False</cell><cell>False</cell><cell>Pants on Fire All</cell></row><row><cell cols="4">654 (14.2%) 863 (18.7%) 974 (21.2%) 776 (16.9%) 911 (19.8%) 426 (9.3%) 4,604</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,152.56,450.61,310.24,79.73"><head>Table 3 .</head><label>3</label><figDesc>Different Combination of Label Mapping</figDesc><table coords="5,152.56,474.57,310.24,55.77"><row><cell cols="2">Mapping FALSE</cell><cell>TRUE</cell><cell>HALF-TRUE</cell></row><row><cell>Map1</cell><cell cols="3">False,Pants on Fire!,Mostly False True,Mostly True Half-True</cell></row><row><cell>Map2</cell><cell>False,Pants on Fire!</cell><cell cols="2">True,Mostly True Half-True</cell></row><row><cell>Map3</cell><cell>False,Pants on Fire!</cell><cell>True</cell><cell>Half-True</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,138.88,330.16,337.59,135.98"><head>Table 5 .</head><label>5</label><figDesc>Test Result with different models (%)</figDesc><table coords="9,138.88,354.62,337.59,111.52"><row><cell>Test dataset</cell><cell cols="4">Politifact CLEF (82 samples)</cell></row><row><cell>Metrics</cell><cell cols="4">Accuracy Accuracy Macro F1 Macro Recall</cell></row><row><cell>zeroR (majority voting)</cell><cell>50.00</cell><cell>39.60</cell><cell>\</cell><cell>\</cell></row><row><cell>ngram+SVM (baseline)</cell><cell>50.64</cell><cell>39.02</cell><cell>29.97</cell><cell>31.87</cell></row><row><cell>ngram+SVM+GridSearch</cell><cell>53.85</cell><cell>43.90</cell><cell>30.14</cell><cell>33.02</cell></row><row><cell>Naive Bayes+GridSearch</cell><cell>53.98</cell><cell>43.90</cell><cell>36.10</cell><cell>37.47</cell></row><row><cell cols="2">RNN+LSTM (best on Politifact) 47.22</cell><cell>45.12</cell><cell>35.95</cell><cell>36.66</cell></row><row><cell>RNN+LSTM (best on CLEF)</cell><cell>46.88</cell><cell>46.34</cell><cell>37.75</cell><cell>39.56</cell></row><row><cell>CNN1 (best on Politifact)</cell><cell>55.56</cell><cell>42.68</cell><cell>28.42</cell><cell>32.67</cell></row><row><cell>CNN2 (best on CLEF)</cell><cell>51.56</cell><cell>48.78</cell><cell>39.66</cell><cell>39.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,173.14,513.33,269.07,51.38"><head>Table 6 .</head><label>6</label><figDesc>Evaluated Result by CLEF</figDesc><table coords="9,173.14,534.13,269.07,30.58"><row><cell>Copenhagen</cell><cell cols="3">MAE Macro MAE ACC Macro F1 Macro Recall</cell></row><row><cell cols="2">primary(SVM) 0.7050 0.6746</cell><cell>0.4317 0.4008</cell><cell>0.4502</cell></row><row><cell cols="2">cont.(CNN2) 0.7698 0.7339</cell><cell>0.4676 0.4681</cell><cell>0.4721</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,479.81,337.63,10.13;10,151.52,493.04,329.07,7.86;10,151.52,503.99,329.07,7.86;10,151.52,514.95,193.40,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,402.06,482.08,78.53,7.86;10,151.52,493.04,153.66,7.86">VERA: A platform for veracity estimation over web data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Berti-Équille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Hammady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,328.71,493.04,151.88,7.86;10,151.52,503.99,190.69,7.86">Proceedings of the 25th International Conference on World Wide Web, WWW 2016</title>
		<meeting>the 25th International Conference on World Wide Web, WWW 2016<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">April 11-15, 2016. 2016</date>
			<biblScope unit="page" from="159" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,526.04,337.64,7.86;10,151.52,537.00,329.07,7.86;10,151.52,547.96,329.07,7.86;10,151.52,558.91,329.07,7.86;10,151.52,569.87,329.07,7.86;10,151.52,580.83,329.07,7.86;10,151.52,591.79,131.88,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,413.44,537.00,67.15,7.86;10,151.52,547.96,329.07,7.86;10,151.52,558.91,97.89,7.86">Overview of the clef-2018 checkthat! lab on automatic identification and verification of political claims, task 2: Factuality</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nakov</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,174.79,569.87,305.80,7.86;10,151.52,580.83,251.19,7.86">CLEF 2018 Working Notes. Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,602.87,337.63,7.86;10,151.52,613.83,329.07,7.86;10,151.52,624.79,329.07,7.86;10,151.52,635.75,263.96,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,385.22,602.87,95.37,7.86;10,151.52,613.83,329.07,7.86;10,151.52,624.79,329.07,7.86;10,151.52,635.75,53.47,7.86">The Copenhagen Team Participation in the Check-Worthiness Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF Fact Checking Lab</note>
</biblStruct>

<biblStruct coords="10,142.96,646.84,337.64,7.86;10,151.52,657.79,329.07,7.86;11,151.52,120.67,329.07,7.86;11,151.52,131.63,329.07,7.86;11,151.52,142.59,124.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,460.24,646.84,20.36,7.86;10,151.52,657.79,191.29,7.86">Fully automated fact checking using external sources</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,185.12,120.67,295.47,7.86;11,151.52,131.63,139.67,7.86">Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Angelova</surname></persName>
		</editor>
		<meeting>the International Conference Recent Advances in Natural Language Processing, RANLP 2017<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2017">September 2 -8, 2017. 2017</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,153.55,337.64,7.86;11,151.52,164.51,329.07,7.86;11,151.52,175.46,329.07,7.86;11,151.52,186.42,205.88,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,188.37,153.55,226.93,7.86">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,434.71,153.55,45.88,7.86;11,151.52,164.51,329.07,7.86;11,151.52,175.46,53.27,7.86;11,345.35,175.46,135.24,7.86;11,151.52,186.42,104.20,7.86">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">October 25-29, 2014. 2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>, A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct coords="11,142.96,197.38,337.64,7.86;11,151.52,208.34,329.07,7.86;11,151.52,219.30,329.07,7.86;11,151.52,230.26,147.24,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,451.02,197.38,29.57,7.86;11,151.52,208.34,237.34,7.86">Detecting rumors from microblogs with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,408.64,208.34,71.94,7.86;11,151.52,219.30,324.70,7.86">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3818" to="3824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,241.22,337.64,7.86;11,151.52,252.18,329.07,7.86;11,151.52,263.14,223.53,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,408.18,241.22,72.41,7.86;11,151.52,252.18,233.92,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.58,252.18,76.02,7.86;11,151.52,263.14,123.09,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,274.09,337.64,7.86;11,151.52,285.05,329.07,7.86;11,151.52,296.01,329.07,7.86;11,151.52,306.97,329.07,7.86;11,151.52,317.93,183.59,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,368.06,285.05,112.54,7.86;11,151.52,296.01,311.08,7.86">Overview of the CLEF-2018 lab on automatic identification and verification of claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martino</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,306.97,329.07,7.86;11,151.52,317.93,38.53,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CLEF &apos;18</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,328.89,337.63,7.86;11,151.52,339.85,329.07,7.86;11,151.52,350.81,329.07,7.86;11,151.52,361.77,329.07,7.86;11,151.52,372.73,329.07,7.86;11,151.52,383.68,239.31,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,303.46,328.89,177.13,7.86;11,151.52,339.85,157.93,7.86">Enriching the knowledge sources used in a maximum entropy part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,327.20,339.85,153.40,7.86;11,151.52,350.81,329.07,7.86;11,151.52,361.77,329.07,7.86;11,151.52,372.73,113.49,7.86;11,318.32,372.73,48.11,7.86">Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;00</note>
</biblStruct>

<biblStruct coords="11,142.61,394.64,337.97,7.86;11,151.52,405.60,151.41,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,291.69,394.64,188.90,7.86;11,151.52,405.60,122.77,7.86">Densely connected cnn with multi-scale feature attention for text classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
