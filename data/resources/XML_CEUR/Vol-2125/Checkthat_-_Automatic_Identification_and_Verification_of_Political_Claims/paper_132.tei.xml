<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,217.54,115.96,180.28,12.62">IRIT at CheckThat! 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.78,153.63,58.53,8.74"><forename type="first">Romain</forename><surname>Agez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université P. Sabatier de Toulouse</orgName>
								<address>
									<settlement>UPS</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.87,153.63,59.97,8.74"><forename type="first">Clement</forename><surname>Bosc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université P. Sabatier de Toulouse</orgName>
								<address>
									<settlement>UPS</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.39,153.63,74.95,8.74"><forename type="first">Cedric</forename><surname>Lespagnol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université P. Sabatier de Toulouse</orgName>
								<address>
									<settlement>UPS</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,180.56,165.58,63.01,8.74"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">ESPE</orgName>
								<orgName type="laboratory" key="lab2">IRIT</orgName>
								<orgName type="laboratory" key="lab3">UMR5505</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.73,165.58,69.60,8.74"><forename type="first">Noemie</forename><surname>Petitcol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université P. Sabatier de Toulouse</orgName>
								<address>
									<settlement>UPS</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,217.54,115.96,180.28,12.62">IRIT at CheckThat! 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2BDA96C85C3CB9CBD2E558D2B908200A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>fact-checking</term>
					<term>information nutritional label</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2018 CLEF CheckThat! is composed of two tasks: (1) Check-Worthiness and (2) Factuality. We participated to task (1) only which purpose is to evaluate the check-worthiness of claims in political debates. Our method to achieve this goal is to represent each claim by a vector of five computed values that correspond to scores on five criteria. These vectors are then used with machine learning algorithms to classify claims as check-worthy or not. We submitted three runs using different machine learning algorithms. The best result we achieved using the official measure MAP ranks our run that uses non linear SVM the 12 th over the 16 submitted runs. Our run that uses linear SVMis ranked 2 nd with the Mean Precision@1 measure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF CheckThat! first task aims at predicting which claims in political debates should be prioritized for fact-checking. All the background and detailed information about the task are available on the task description paper provided by the organizers of the task <ref type="bibr" coords="1,262.78,524.61,9.96,8.74" target="#b9">[8]</ref>.</p><p>To achieve this goal, the task organizers released several textual transcripts of political debates with each sentence being annotated according to whether it is check-worthy or not.</p><p>This paper describes the participation of the Université de Toulouse team (official name RNCC) at CLEF 2018 CheckThat! pilot task for check-worthiness.</p><p>We preprocessed the data by representing each sentence corresponding to a transcription of what a speaker said in the debate by a vector containing the score of this sentence for five different criteria. We then trained three classifiers using these vectors to submit three different runs.</p><p>The remaining of this paper is organized as follows: Section 2 gives a description of the pilot task. Section 3 details the model we developed and the submitted runs. Then Section 4 details the results we obtained. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Objectives</head><p>The Check-Worthiness task aims to predict which statements in a political debate should be fact-checked. Indeed, nowadays, information objects are spreading faster and faster on the Internet and especially on social networks. This spreading is named the virality of the information <ref type="bibr" coords="2,355.83,244.75,9.96,8.74" target="#b2">[1]</ref>.</p><p>During a political debate, any of the statements made by the participants can be reused without checking its factuality and it even can become viral. CheckThat! aims at providing journalists with a list of statements members of the debate made that should be checked before they are reused by others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>There are two datasets : one to train the model and one to test it. Both sets consist of political debates transcribed into texts.</p><p>They are annotated so that each row indicates the sentence number, the speaker, the transcription of the sentence that the speaker said. The training dataset includes in addition a label that indicates whether this sentence is to be fact-checked or not. The training set contains three political debates while the test set contains seven debates <ref type="bibr" coords="2,271.47,415.90,9.96,8.74" target="#b9">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation metric</head><p>The task has been evaluated according to different measures. The official measure is MAP which calculates the usual mean of the average precision. Then, other measures were used as Mean Reciprocal Rank which allows to obtain reciprocals of rank of the first relevant document as well as Mean Precision at x which performs the average of x best candidates. Details on the measures used can be found in the task overview <ref type="bibr" coords="2,253.85,526.97,9.96,8.74" target="#b9">[8]</ref>.</p><p>Evaluations are carried out on primary and contrastive runs. Primary run corresponds to the results file of the participant's main model ; the decision of the main run was the participant's decision. Contrastive runs match the secondary models the participant used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method and runs</head><p>We computed five of the criteria from the Information Nutritional Label for Online Documents proposed by <ref type="bibr" coords="2,271.95,644.16,9.96,8.74" target="#b4">[3]</ref>. These criteria and the methods we developed in this work to calculate their score are as follows:</p><p>-Factuality and Opinion : Determines whether a sentence represents a fact or a personal opinion. These two features are based on the same algorithm. Each value is the opposite of the other, it is either 0 or 1. We use a Multi-layer Perceptron classifier, using LBFGS gradient descent <ref type="bibr" coords="3,438.79,154.86,14.61,8.74" target="#b11">[10]</ref>. This neural network is composed of 500 neurons in the first hidden layer and 5 neurons in the second hidden layer. The activation function used is the rectified linear unit function ("relu"). We used a MLP classifier because it was the best performing classifier over Random Forest, Support Vector Machine and Linear Regression. The datasets to train the neural network come from various Wikipedia articles<ref type="foot" coords="3,286.20,225.02,3.97,6.12" target="#foot_0">3</ref> for factual sentences and from Opinosis 4 for opinion sentences. The features used to classify a sentence are fine-grained part-of-speech tags extracted with spaCy 5 . -Controversy : Determines the degree of controversy in a text. We count the number of controversial issues in the text based on the Wikipedia Article List of controversial issues 6 . For each issue referenced in the wiki article, we also take in account the anchor text labels 7 to find the synonyms and other appellations of the issues in all of the Wikipedia database. For example : Donald Trump is in the list of controversial issues. Other names can link to his Wikipedia page such as "45 th President of America". These names are called anchor text labels and will be recognized as a controversial issue. -Emotion : Determines the intensity of emotion in a sentence. We use the list of 2, 477 emotional words and valuation from AFINN 8 <ref type="bibr" coords="3,410.87,369.40,10.52,8.74" target="#b10">[9]</ref> (ex : abusive = -3, proud = 2). We sum the absolute value of the positive and negative valuations of the emotional words found in the sentence and we divide it by the total number of words in the sentence :</p><p>( posW ordV alue + |negW ordV alue|)/totalN umberW ords -Technicality : Determines the degree of technicality in a text. We count the number of domain-specific terms in the text. For that, we use NLTK 9 <ref type="bibr" coords="3,470.08,458.87,10.52,8.74" target="#b3">[2]</ref> to perform part of speech tagging (adjective = JJ, name = NN, etc.). Then, we use the RE library 10 to match, from tags, a regular expression defined in <ref type="bibr" coords="3,163.08,494.73,10.52,8.74" target="#b7">[6]</ref> which identifies the terminological noun phrases (NPs). NPs represent domain-specific terms in the text. We extract all the NPs from the text and keep those which appear more than once. We then calculate the ratio of the number of these NPs over the number of words in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( N P s)/totalN umberW ords</head><p>We decided to use only these criteria as features because our goal was to test the Information Nutritional Label on a concrete task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models</head><p>Each of our three runs uses its own model to compute a check-worthiness score. For each of our models, we preprocessed the data using the criteria previously described. We computed the five features for each sentence that has to be evaluated for check-worthiness. These sentences are then represented by a vector containing five features, one for each criterion score.</p><p>For our INL SVM RBF (primary run) and INL SVM Lin (first contrastive) runs, we decided to use the Support Vector Machine in sklearn<ref type="foot" coords="4,429.27,318.77,7.94,6.12" target="#foot_1">11</ref> with the probability setting set to "True". We used a RBF kernel for INL SVM RBF run and a linear kernel for the INL SVM Lin run. For our INL RF (second contrastive) run, we used the random forest classifier in sklearn.</p><p>To train our models, we used the three annotated debates provided by the clef2018-factchecking github repository<ref type="foot" coords="4,304.24,379.00,7.94,6.12" target="#foot_2">12</ref> .</p><p>To obtain a score of check-worthiness, we computed the probability for each sentence to be check-worthy using the classifiers. The score of a sentence was then normalized by the highest score obtained for this sentence divided by the highest probability computed, so that the scores are between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Seven teams submitted runs to this task for a total of 16 runs.</p><p>Table <ref type="table" coords="4,177.95,501.60,4.98,8.74">1</ref> presents the results of our three runs and the best submitted run according to the MAP measure, which is from the Copenhagen team <ref type="bibr" coords="4,438.13,513.55,9.96,8.74" target="#b5">[4]</ref>.</p><p>Table <ref type="table" coords="4,162.85,546.51,4.13,7.89">1</ref>. Results for each of our runs and the best run submitted. Values in parenthesis correspond to the ranks of our runs over the 16 that were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>MAP MRR Mean Prec@1 INL SVM RBF .0632 (16) .3775 <ref type="bibr" coords="4,332.21,589.63,11.78,7.86" target="#b10">(9)</ref> .2857 (6) INL SVM Lin .0886 (12) .4844 <ref type="bibr" coords="4,332.21,600.98,11.78,7.86">(5)</ref> .4286 (2) INL RF</p><p>.0747 (15) .2198 (15) .0000 (14) Copenhagen <ref type="bibr" coords="4,252.18,623.70,9.73,7.86" target="#b5">[4]</ref> .1810 <ref type="bibr" coords="4,288.98,623.70,11.77,7.86" target="#b2">(1)</ref> .6224 <ref type="bibr" coords="4,332.21,623.70,11.78,7.86" target="#b2">(1)</ref> .5714 <ref type="bibr" coords="4,375.44,623.70,11.78,7.86" target="#b2">(1)</ref> Overall, the INL SVM Lin run obtained better results than the INL SVM RBF run; that was somehow unexpected since non linear kernel have been shown to work better in other information retrieval applications. The INL SVM Lin run has been ranked twelfth according to the main measurement (Mean Average Precision), but obtained better rank when considering other measures: it is ranked fifth according to the Mean Reciprocal Rank and second according to the Mean Precision@1. These ranks mean that our INL SVM Lin run would be good if the purpose of the task was finding the most check-worthy claim instead of finding all the check-worthy claims. However, we need to deeper analyse the results to understand why.</p><p>Post-hoc experiments showed that the least important criterion is Technicality. This may be due to the fact that the method we use to compute this feature was meant to work with large texts and it is not appropriate for a single sentence. The most important criterion is Emotion. We can assume that a claim has greater chances to be check-worthy if it is highly emotional. The speaker thinks less about what he says and it is more likely that his claims are not fully accurate. We will check this hypothesis in future work.</p><p>Table <ref type="table" coords="5,177.70,322.38,4.98,8.74" target="#tab_0">2</ref> presents the weight of the 5 features for our INL SVM Lin model. The weights of the features for our INL RF model are similar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and perspectives for future works</head><p>In this paper we proposed three models to solve the CLEF2018 CheckThat! challenge (task 1 Check Worthiness) which deals with the evaluation of the check-worthiness of statements in political debates. We used random forest and support vector machine to learn models that make use of the Information Nutritional Label features <ref type="bibr" coords="5,228.73,560.40,9.96,8.74" target="#b4">[3]</ref>. We show that these models perform pretty well when considering the Mean Precision@1 measure, which ranks our run that uses a support vector machine with a linear kernel 2nd over 16 submitted runs. We are currently working on better calculation of the five features. We would like to complete the representations of the texts by using content-based components like it is done in <ref type="bibr" coords="5,235.89,620.25,9.96,8.74">[5]</ref>. While the objective is different (virality prediction), some of the features may also be useful for the task tackled by CheckThat!. To improve more our models, we would also like to investigate the use of wordembedding since we are using successfully this approach in other tasks <ref type="bibr" coords="5,450.27,656.12,10.52,8.74" target="#b8">[7]</ref> and this approach also worked well according to Hansen et al. <ref type="bibr" coords="6,387.52,118.99,10.52,8.74" target="#b5">[4]</ref> in the CheckThat! context. As future work, we will also take in consideration the sentences around the one to be classified and who said these sentences.</p><p>Finally, we will test these models on other datasets such as social networks. For example, we will consider a Twitter-based dataset where each tweet would have a score indicating its worthiness for fact-checking taking into account hashtags and tweet sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,170.35,365.98,274.66,74.09"><head>Table 2 .</head><label>2</label><figDesc>Weights for the features used in our INL SVM Lin model.</figDesc><table coords="5,214.57,386.76,186.21,53.32"><row><cell>Feature</cell><cell>Weight</cell></row><row><cell>Controversy</cell><cell>-2.08e-05</cell></row><row><cell cols="2">Factuality and Opinion -1.03e-05 and 1.03e-05</cell></row><row><cell>Technicality</cell><cell>2.22e-06</cell></row><row><cell>Emotion</cell><cell>2.56e-05</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,144.73,525.29,323.08,7.86;3,144.73,536.25,335.86,7.86"><p>Each of the following URL should be preceded by https://en.wikipedia.org/wiki /World War I, /Industrial Revolution, /October Revolution, /Fermi paradox,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_1" coords="4,144.73,645.84,196.60,7.86"><p>http://scikit-learn.org/stable/modules/svm.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_2" coords="4,144.73,656.80,389.75,7.86"><p>https://github.com/clef2018-factchecking/clef2018-factchecking/tree/master/data/task1/English</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,144.73,547.21,335.86,7.86;3,144.73,558.17,252.37,7.86;3,137.50,567.36,3.65,5.24;3,144.73,569.13,148.11,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,445.76,547.21,34.83,7.86;3,144.73,558.17,97.28,7.86">Triangular trade, /Song dynasty</title>
		<ptr target="http://kavita-ganesan.com/opinosis/" />
	</analytic>
	<monogr>
		<title level="m" coord="3,149.68,547.21,52.21,7.86;3,216.98,547.21,57.76,7.86;3,254.04,558.17,71.84,7.86">Nanking Massacre</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Amazon (company</publisher>
		</imprint>
	</monogr>
	<note>Steam engine</note>
</biblStruct>

<biblStruct coords="3,144.73,580.09,335.86,7.86;3,144.73,591.04,236.60,7.86;3,144.73,602.00,68.45,7.86;3,137.50,611.20,3.65,5.24;3,144.73,612.96,274.13,7.86;3,137.50,622.15,3.65,5.24;3,144.73,623.92,172.62,7.86;3,137.50,633.11,3.65,5.24" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,144.73,580.09,256.58,7.86">spaCy is a library for Natural Language Processing in Python</title>
		<ptr target="https://en.wikipedia.org/wiki/Anchortext8" />
	</analytic>
	<monogr>
		<title level="m" coord="3,409.87,580.09,70.72,7.86;3,144.73,591.04,232.09,7.86">It provides NER, POS tagging, dependency parsing, word vectors and more</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,246.20,337.63,7.86;6,151.52,257.13,124.62,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,260.07,246.20,130.56,7.86">What makes online content viral</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Milkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,397.05,246.20,83.54,7.86;6,151.52,257.16,32.61,7.86">Journal of marketing research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,268.12,323.34,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,271.61,268.12,166.03,7.86">Natural language processing with python</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,279.08,337.63,7.86;6,151.52,290.03,329.07,7.86;6,151.52,300.99,316.80,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,352.36,290.03,128.22,7.86;6,151.52,300.99,82.29,7.86">An information nutritional label for online documents</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,255.53,300.99,77.70,7.86">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="46" to="66" />
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,311.95,337.64,7.86;6,151.52,322.91,329.07,7.86;6,151.52,333.87,329.07,7.86;6,151.52,344.83,329.07,7.86;6,151.52,355.79,329.07,7.86;6,151.52,366.75,227.31,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,346.63,311.95,133.97,7.86;6,151.52,322.91,329.07,7.86;6,151.52,333.87,329.07,7.86;6,151.52,344.83,13.54,7.86">The Copenhagen Team Participation in the Check-Worthiness Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,398.28,344.83,82.31,7.86;6,151.52,355.79,212.91,7.86">CLEF 2018 Working Notes, Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="6,372.32,355.79,108.27,7.86;6,151.52,366.75,42.29,7.86">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
		<respStmt>
			<orgName>-WS.org</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,377.71,337.63,7.86;6,151.52,388.67,329.07,7.86;6,151.52,399.62,169.59,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,282.40,377.71,198.19,7.86;6,151.52,388.67,131.65,7.86">Predicting information diffusion on twitteranalysis of predictive features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jocs.2017.10.010</idno>
		<ptr target="https://doi.org/10.1016/j.jocs.2017.10.010" />
	</analytic>
	<monogr>
		<title level="j" coord="6,294.58,388.67,150.24,7.86">Journal of Computational Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,410.58,337.64,7.86;6,151.52,421.52,329.07,7.89;6,151.52,432.50,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,263.31,410.58,217.28,7.86;6,151.52,421.54,155.01,7.86">Technical terminology: some linguistic properties and an algorithm for identification in text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Justeson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,314.31,421.54,120.00,7.86">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,443.46,337.64,7.86;6,151.52,454.42,329.07,7.86;6,151.52,465.38,329.07,7.86;6,151.52,476.34,98.02,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,267.33,443.46,55.90,7.86">IRIT at TRAC</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ramiandrisoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,344.28,443.46,136.31,7.86;6,151.52,454.42,329.07,7.86;6,151.52,465.38,118.26,7.86">Proceedings of the First Workshop on Trolling, Aggression and Cyberbulling (TRAC), 27th International Conference on Computational Linguistics</title>
		<meeting>the First Workshop on Trolling, Aggression and Cyberbulling (TRAC), 27th International Conference on Computational Linguistics<address><addrLine>COLIN</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee on Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,487.30,337.64,7.86;6,151.52,498.25,329.07,7.86;6,151.52,509.21,329.07,7.86;6,151.52,520.17,329.07,7.86;6,151.52,531.13,329.07,7.86;6,151.52,542.09,329.07,7.86;6,151.52,553.05,329.07,7.86;6,151.52,564.01,249.37,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,384.29,498.25,96.30,7.86;6,151.52,509.21,329.07,7.86;6,151.52,520.17,25.94,7.86">Overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification of Political Claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,347.84,531.13,132.75,7.86;6,151.52,542.09,329.07,7.86;6,151.52,553.05,207.85,7.86">Proceedings of the Nineth International Conference of the CLEF Association (CLEF 2018)</title>
		<title level="s" coord="6,367.42,553.05,113.18,7.86;6,151.52,564.01,63.87,7.86">Lecture Notes in Computer Science (LNCS)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bellot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Nineth International Conference of the CLEF Association (CLEF 2018)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11018</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,142.96,573.38,326.19,9.45" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Å</forename><surname>Nielsen</surname></persName>
		</author>
		<ptr target="http://www2.imm.dtu.dk/pubdb/p.php?6010" />
	</analytic>
	<monogr>
		<title level="j" coord="6,209.82,574.97,22.26,7.86">Afinn</title>
		<imprint>
			<date type="published" when="2011-03">mar 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,585.93,337.98,7.86;6,151.52,596.88,329.07,7.86;6,151.52,607.84,329.07,7.86;6,151.52,618.78,325.87,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,394.26,607.84,86.33,7.86;6,151.52,618.80,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,232.82,618.80,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
