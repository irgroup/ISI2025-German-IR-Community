<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.79,115.96,307.77,12.62;1,145.67,133.89,324.02,12.62;1,152.93,151.82,309.49,12.62;1,145.78,169.76,323.80,12.62;1,250.51,187.69,114.35,12.62">The Copenhagen Team Participation in the Check-Worthiness Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 CheckThat! Lab</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.29,225.36,63.27,8.74"><forename type="first">Casper</forename><surname>Hansen</surname></persName>
							<email>c.hansen@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen (DIKU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.83,225.36,73.50,8.74"><forename type="first">Christian</forename><surname>Hansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen (DIKU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.60,225.36,92.84,8.74"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
							<email>simonsen@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen (DIKU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.99,225.36,71.07,8.74"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
							<email>c.lioma@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen (DIKU)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.79,115.96,307.77,12.62;1,145.67,133.89,324.02,12.62;1,152.93,151.82,309.49,12.62;1,145.78,169.76,323.80,12.62;1,250.51,187.69,114.35,12.62">The Copenhagen Team Participation in the Check-Worthiness Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 CheckThat! Lab</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">694DBC7FFFFC373D7699AB1C6E429ECB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>political debates</term>
					<term>recurrent neural networks</term>
					<term>sentence embedding</term>
					<term>check-worthiness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We predict which claim in a political debate should be prioritized for fact-checking. A particular challenge is, given a debate, how to produce a ranked list of its sentences based on their worthiness for fact checking. We develop a Recurrent Neural Network (RNN) model that learns a sentence embedding, which is then used to predict the checkworthiness of a sentence. Our sentence embedding encodes both semantic and syntactic dependencies using pretrained word2vec word embeddings as well as part-of-speech tagging and syntactic dependency parsing. This results in a multi-representation of each word, which we use as input to a RNN with GRU memory units; the output from each word is aggregated using attention, followed by a fully connected layer, from which the output is predicted using a sigmoid function. The overall performance of our techniques is successful, achieving the overall second best performing run (MAP: 0.1152) in the competition, as well as the highest overall performance (MAP: 0.1810) for our contrastive run with a 32% improvement over the second highest MAP score in the English language category. In our primary run we combined our sentence embedding with state of the art check-worthy features, whereas in the contrastive run we considered our sentence embedding alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Tasks Performed</head><p>The Copenhagen team participated in both Tasks 1 and 2 of the CLEF 2008 Fact Checking Lab for the English language. This report details our methods and results for Task 1, where we focused on the English task. Our participation in Task 2 is described in <ref type="bibr" coords="1,244.27,620.25,9.96,8.74" target="#b4">[5]</ref>.</p><p>The aim of Task 1 is to identify sentences in a political debate that should be prioritized for fact-checking: given a debate, the goal is to produce a ranked list of all sentences based on their worthiness for fact checking.</p><p>One of the two examples given by the lab organizers <ref type="bibr" coords="2,388.24,118.99,10.52,8.74" target="#b3">[4]</ref> is shown in Table <ref type="table" coords="2,134.77,130.95,3.87,8.74" target="#tab_0">1</ref>, where Hillary Clinton mentions Bill Clinton's work in the 1990s, followed by a claim made by Donald Trump stating that president Clinton approved the North American Free Trade Agreement (NAFTA). This last statement by Trump is worth checking and is flagged as such. We refer to the competition description <ref type="bibr" coords="2,134.77,178.77,10.52,8.74" target="#b3">[4]</ref> for further details on the competition and the provided dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Main Objectives of Experiments</head><p>The task of check-worthiness is different from the typical task of fact-checking because distilled claims are not provided, but rather the complete debate dialog is given. This means that most sentences will not be worthy of checking. Consequently, we reason that a rich representation of a sentence is needed in order to determine whether it is check-worthy.</p><p>The main objective of our experiments is to consider check-worthiness as two points that should both be satisfied:</p><p>1. The sentence content should be factually interesting 2. The sentence content should be possible to check</p><p>The first point requires that check-worthy sentences should contain semantic information that is interesting to verify as being true or not. For example, it is interesting to determine if Bill Clinton did approve NAFTA in the 1990s, but it would not be interesting to know what he had for breakfast. If a sentence is interesting, then later fact-checking is only possible if it is actually feasible to check the statement of the sentence, which is the requirement of the second point. Consequently when both points are satisfied, it follows that the sentence, or a part of it, should be syntactically structured in a way that resembles that of a check-able claim. This latter point, as well as the two original points, motivates our approach for learning a combined semantic and syntactical sentence embedding. Our approach to modelling this is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches Used and Progress Beyond State-of-the-Art</head><p>Our approach in this task is to learn a sentence embedding that can be used to predict the check-worthiness of a sentence. To do so, we encode the semantics and the syntactic dependencies, both of which will be explained next. We encode the semantics of a sentence by the use of word embeddings, specifically word2vec <ref type="bibr" coords="3,134.77,142.90,9.96,8.74" target="#b2">[3]</ref>, where each word is mapped into a vector space with the property that semantically similar words are close to each other. Furthermore, we employ Partof-Speech (POS) tagging to better model the role of each word in the sentence. To encode the syntactic structure of a sentence we employ syntactic dependency parsing with the purpose of better encoding the syntactic structure a sentences needs to resemble in order to be checkable. Specifically, the dependency parsing maps each word to its dependency (as a tag) in relation to the sentence structure. Both POS tags and syntactic dependencies are represented using a one-hotencoding.</p><p>For each word in a sentence, we thus have 3 distinct encodings that, together, represent the word. We use this multi-representation of each word as input to a recurrent neural network (See Figure <ref type="figure" coords="3,295.42,274.52,4.43,8.74" target="#fig_0">1</ref>) with GRU memory units, where the output from each word is aggregated using attention, followed by a fully connected layer, from which the output is predicted using a sigmoid function. GRU was chosen as opposed to the popular LSTM unit, since they have fewer parameters and therefore better suited to a small data setting as in this competition. Related Check-Worthiness methods Existing state-of-the-art check-worthiness methods are based on feature engineering, by extracting a number of varied features from each sentence and potentially its context. So far actually learning (sentence) embeddings have not been done. ClaimBuster <ref type="bibr" coords="3,383.94,644.16,10.52,8.74" target="#b1">[2]</ref> focus on extracting features of sentiment, sentence length, TF-IDF, Part-of-Speech tags, and en-tity extraction. The features are made on a sentence-level, and no information about the context of the sentence with regards to its surrounding sentences is used. Gencheva et al. <ref type="bibr" coords="4,235.45,142.90,10.52,8.74" target="#b0">[1]</ref> extend ClaimBuster by incorporating context-aware features into the representation, as well as more sentence-level features. Among other things, they extract contextual features such as the sentence position in a speaker segment, whether the speaker mentions the opponent, audience reactions, and a sentence's similarity to the segments around it. However, one of the features employed by Gencheva et al. is the largest overlap between the sentence, and sentences from PolitiFact and the training corpus. Due to the rules of the competition, the feature of the largest overlap from a sentence to PolitiFact claims is not included as a feature. Compared to the above, our approach can be considered a sentence-level feature, since we do not include information from other sentences when generating the embedding. Thus, we also evaluate the combination of our sentence embedding with the context-aware baseline. Our choice of focusing the embedding entirely on the sentence and not the context was due to the low amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Resources Employed</head><p>In our sentence embedding based approach we use the model described in Figure <ref type="figure" coords="4,134.77,394.76,3.87,8.74" target="#fig_0">1</ref>, where the GRU has 100 hidden units and the densely connected layer has 25 neurons and uses a ReLU activation function. For the word embedding we use a pretrained word2vec model based on Google News<ref type="foot" coords="4,364.04,417.10,3.97,6.12" target="#foot_0">1</ref> , and keep it fixed during training to avoid overfitting. For POS tagging and syntax dependency parsing we use spaCy<ref type="foot" coords="4,192.64,441.01,3.97,6.12" target="#foot_1">2</ref> , and for the neural network implementation Keras<ref type="foot" coords="4,416.32,441.01,3.97,6.12" target="#foot_2">3</ref> . We apply no preprocessing to the raw sentences, except for tokenization, and do not remove stopwords. The reason for this is stopwords might contain important information for representing the syntactical structure of the sentence.</p><p>For ClaimBuster and the context-aware baseline we implement the features described in the original papers. Claimbuster provides an online API which we do not use due to it having worse performance compared to self-training <ref type="bibr" coords="4,447.48,515.83,11.49,8.74" target="#b0">[1]</ref>. For the classification algorithm we use the best observed model from <ref type="bibr" coords="4,420.41,527.79,9.96,8.74" target="#b0">[1]</ref>, which is a feedforward neural network with 2 layers of 200 and 50 neurons respectively, both with ReLU activations. Following the original papers we apply stopword removal before generating the TF-IDF bag-of-words features, but generate all remaining features without stopwords removed. When evaluating the combination of our sentence embedding with the context-aware baseline, we combine the models by concatenating the second last layers, such that the final prediction is based on a 75 dimensional vector.</p><p>For evaluation of the models we use 3-fold evaluation where 2 of the 3 debates act as training data and the remaining as test. We follow the competition guidelines and report the MAP measure for each of the test debates individually and as an average. Additionally, we compute the MAP scores on each speaker by only considering the speakers sentences. This is done in order to determine if some of the models are better at adapting to certain individuals.</p><p>The performance for each model evaluated on the debates is shown in Table <ref type="table" coords="5,134.77,225.91,3.87,8.74" target="#tab_1">2</ref>. ClaimBuster, based on sentence-level features, performs the worst. We see that by context-aware features from the model proposed by Gencheva et al. <ref type="bibr" coords="5,147.96,249.82,10.52,8.74" target="#b0">[1]</ref> the MAP is improved in the first debate, but similar in the remaining. Our approach of training a combined semantic and syntactic sentence embedding performs better, and the lowest MAP score is increased from around 0.04 to 0.105 on the vice-presidential debate. However, it performs 0.09 worse on the first debate, and 0.09 better on the second. By combining our approach with the context-aware model we obtain the best model and obtain an average MAP of 0.158.</p><p>The performance of each model evaluated on the sentences by the individual speakers (without the moderators) is shown in Table <ref type="table" coords="5,361.47,345.46,3.87,8.74" target="#tab_2">3</ref>. In this experiment we use the same train and test setup as before, but report the AUC for each speaker across the three runs. As before, the context-aware model is in most cases a direct improvement upon ClaimBuster. We observe that our sentence embedding performs notably better on the speakers from the vice presidential debate, and on Donald Trump, but worse on Hillary Clinton. By combining the embedding with the context-aware model the average MAP is slightly improved compared to only using the context-aware model, but using only our embedding overall obtains the best MAP score averaged over all speakers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis</head><p>Across the debates we observe a large variation in performance depending on the used model: ClaimBuster obtains the lowest average MAP, but also uses the most simple features. The introduction of context-aware features provided a direct improvement by significantly improving the performance on the first In our approach we do not directly encode specific features that might generalize poorly, but instead focus purely on deriving an embedding based on the semantics and syntactic dependencies of each sentence. The goal of this was to improve generalizability, which we also observe in the results, where we are able to increase the performance of the vice presidential debate by more than 0.06. When comparing our approach to the context-aware model we observe that they have orthogonal strengths, in the sense that our approach obtains higher scores when the context-aware model performs worse and vice versa. In the combination of the two approaches the performance on the two presidential debates increases and the highest MAP scores are obtained, however the performance on the vice presidential debate remains at the level of the context-aware model. We further consider the generalizability of the models by inspecting the performance on each of the individual speakers. It is interesting to consider that our approach perform noticeable better on the Trump/Pence duo compared to Clinton/Kaine, whereas the context-aware model and the combined approach favors Trump and Clinton compared to the vice presidential candidates. When considering the average MAP scores, then we similarly observe an increased generalizability of our method, since our sentence embedding alone obtains the best score averaged over all speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion of test data performance</head><p>Donald Trump was present in 6 out of the 7 provided testing debates and speeches, Hillary Clinton in 2, and a person not in the training data (Bernie Sanders) only in 1. For the speakerwise performances we believed the combined approach to be the optimal choice, since this obtained the best combined performance on both Trump and Clinton. As the contrastive run we submitted our proposed sentence embedding alone as it performed very similar to the combined approach and due to the previous arguments of improved generalizability, in the case that the Trump speeches are significantly different from the debates he participated in. The final results on the test data can be seen in Table <ref type="table" coords="6,452.73,632.21,3.87,8.74" target="#tab_3">4</ref>. The combined approach ranked second in the task with a MAP of 0.1152, whereas our sentence embedding alone obtained the highest overall performance in the task with a MAP of 0.1810. Compared to the second highest performing submission in the English language category our sentence embedding outperformed it by 32.5% (MAP: 0.1366). We believe the reason for the large performance difference is due to the improved generalizability of our sentence embedding, as the difference between the training debates compared to the debates and speeches in the testing data proved to be non negligible.</p><p>Our method does not use any context-based or global information, but is entirely reliant on the sentence that is being ranked. When inspecting the training data manually it is easy to see that an awareness of the overall context is necessary for ranking many of the sentences. Since our method which is working entirely on the sentences alone perform the best, it shows that the context-aware models may be prone to overfitting, and with the amount of training data available, it is better to focus entirely on what can be achieved from the sentence in isolation. The test data also contains multiple speeches in comparison to only debates in the training data. How the context should be treated in speeches and debates are very likely different, and therefore the models trained on context in debates does most likely not generalize well on all the test data. In the future we plan to extend our model by incorporating context. At the moment our sentence embedding is based on each sentence alone, but as discussed previously, many check-worthy sentences are dependent on their context. One way to do this would be to create a context window embedding around the sentence in focus, such that the model is aware of what is said before and after. Inspired by Gencheva et al. <ref type="bibr" coords="7,262.84,533.40,9.96,8.74" target="#b0">[1]</ref>, this could also be based on a person-specific context window embedding, by modelling what each debate participator has individually said. Naturally this leads to a more complex model, thus most likely requiring more training data than was provided in this competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,555.94,345.83,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Network architecture for our sentence embedding for a sentence with M words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,139.75,210.25,334.46,69.73"><head>Table 1 .</head><label>1</label><figDesc>Example of Check-Worthiness Speaker Sentence CLINTON: I think my husband did a pretty good job in the 1990s. CLINTON: I think a lot about what worked and how we can make it work again...</figDesc><table /><note coords="2,139.75,272.12,227.59,7.86"><p>TRUMP: Well, he approved NAFTA... (check-worthy)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,151.50,472.26,309.28,81.88"><head>Table 2 .</head><label>2</label><figDesc>MAP scores for each model across the provided training debates.</figDesc><table coords="5,151.50,496.24,309.28,57.90"><row><cell>Model</cell><cell cols="4">1st-president 2nd-president vice-president Average</cell></row><row><cell>ClaimBuster [2]</cell><cell>0.159</cell><cell>0.109</cell><cell>0.043</cell><cell>0.104</cell></row><row><cell>Gencheva et al. [1]</cell><cell>0.193</cell><cell>0.114</cell><cell>0.039</cell><cell>0.115</cell></row><row><cell>Our</cell><cell>0.109</cell><cell>0.206</cell><cell>0.105</cell><cell>0.140</cell></row><row><cell cols="2">Our + Gencheva et al. [1] 0.225</cell><cell>0.207</cell><cell>0.043</cell><cell>0.158</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,115.91,345.82,145.76"><head>Table 3 .</head><label>3</label><figDesc>MAP scores for each model across the speakers From these types of features we see an indication of limited generalizability, since they are not able to obtain good performance on the vice presidential debate, where the speakers are not present in the training debates.</figDesc><table coords="6,134.77,139.90,299.48,97.86"><row><cell>Model</cell><cell>Trump Clinton Pence Kaine Average</cell></row><row><cell>ClaimBuster [2]</cell><cell>0.128 0.139 0.074 0.041 0.095</cell></row><row><cell>Gencheva et al. [1]</cell><cell>0.143 0.201 0.057 0.039 0.110</cell></row><row><cell>Our</cell><cell>0.225 0.077 0.169 0.086 0.139</cell></row><row><cell cols="2">Our + Gencheva et al. [1] 0.245 0.149 0.054 0.050 0.125</cell></row><row><cell>presidential debate.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,341.59,309.46,117.62"><head>Table 4 .</head><label>4</label><figDesc>MAP scores for the submitted models on the testing data</figDesc><table coords="7,134.77,365.57,245.74,93.63"><row><cell>Model</cell><cell>Test MAP</cell></row><row><cell cols="2">Our + Gencheva et al. [1] 0.1152</cell></row><row><cell>Our</cell><cell>0.1810</cell></row><row><cell>6 Perspectives for Future Work</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,635.53,202.42,7.47"><p>https://code.google.com/archive/p/word2vec/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,646.48,80.03,7.47"><p>https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,657.44,80.03,7.47"><p>https://keras.io/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,623.92,342.24,7.86;7,146.91,634.88,333.68,7.86;7,146.91,645.84,333.67,7.86;7,146.91,656.80,227.66,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,438.24,623.92,42.35,7.86;7,146.91,634.88,282.24,7.86">A contextaware approach for detecting worth-checking claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,447.41,634.88,33.18,7.86;7,146.91,645.84,333.67,7.86;7,146.91,656.80,71.16,7.86">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
	<note>RANLP 2017</note>
</biblStruct>

<biblStruct coords="8,138.35,119.67,342.25,7.86;8,146.91,130.63,333.68,7.86;8,146.91,141.59,333.68,7.86;8,146.91,152.55,120.62,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,345.94,119.67,134.65,7.86;8,146.91,130.63,217.08,7.86">Toward automated fact-checking: Detecting check-worthy factual claims by claimbuster</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,386.11,130.63,94.48,7.86;8,146.91,141.59,329.24,7.86">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1803" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,163.51,342.24,7.86;8,146.91,174.47,333.67,7.86;8,146.91,185.43,223.54,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,407.85,163.51,72.74,7.86;8,146.91,174.47,236.58,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.88,174.47,76.71,7.86;8,146.91,185.43,123.09,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,196.39,342.24,7.86;8,146.91,207.34,333.67,7.86;8,146.91,218.30,333.67,7.86;8,146.91,229.26,333.68,7.86;8,146.91,240.22,183.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,367.00,207.34,113.59,7.86;8,146.91,218.30,314.63,7.86">Overview of the CLEF-2018 lab on automatic identification and verification of claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martino</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,229.26,333.68,7.86;8,146.91,240.22,38.53,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CLEF &apos;18</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,251.18,342.25,7.86;8,146.91,262.14,333.68,7.86;8,146.91,273.10,333.68,7.86;8,146.91,284.06,201.89,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,357.48,251.18,123.11,7.86;8,146.91,262.14,333.68,7.86;8,146.91,273.10,329.17,7.86">The Copenhagen Team Participation in the Factuality Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF Fact Checking Lab</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
