<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.43,115.96,290.50,12.62;1,215.18,133.89,185.00,12.62;1,244.30,151.82,126.76,12.62">Overview of the CLEF 2018 Personalised Information Retrieval Lab (PIR-CLEF 2018)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.11,189.49,61.45,8.74"><forename type="first">Gabriella</forename><surname>Pasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Milano Bicocca</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.11,189.49,81.65,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.32,189.49,54.88,8.74"><forename type="first">Keith</forename><surname>Curtis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.76,189.49,74.25,8.74"><forename type="first">Stefania</forename><surname>Marrara</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Consorzio C2T</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.14,201.45,73.61,8.74"><forename type="first">Camilla</forename><surname>Sanvitto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Milano Bicocca</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.31,201.45,73.38,8.74"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research Labs</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.25,201.45,55.94,8.74"><forename type="first">Procheta</forename><surname>Sen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Consorzio C2T</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.43,115.96,290.50,12.62;1,215.18,133.89,185.00,12.62;1,244.30,151.82,126.76,12.62">Overview of the CLEF 2018 Personalised Information Retrieval Lab (PIR-CLEF 2018)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">770E38A39601C9D83148C458924668C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> At CLEF 2018, the Personalised Information Retrieval Lab  (PIR-CLEF 2018)  <p>has been conceived to provide an initiative aimed at both providing and critically analysing a new approach to the evaluation of personalization in Information Retrieval (PIR). PIR-CLEF 2018 is the first edition of this Lab after the successful Pilot lab organised at CLEF 2017. PIR CLEF 2018 has provided registered participants with the data sets originally developed for the PIR-CLEF 2017 Pilot task; the data collected are related to real search sessions over a subset of the ClueWeb12 collection, undertaken by 10 users by using a novel methodology. The data were gathered during the search sessions undertaken by 10 volunteer searchers. Activities during these search sessions included relevance assessment of a retrieved documents by the searchers. 16 groups registered to participate at PIR-CLEF 2018 and were provided with the data set to allow them to work on PIR related tasks and to provide feedback about our proposed PIR evaluation methodology with the aim to create an effective evaluation task.</p><p>Recent years have seen increasing interest in the study of contextual search: in particular, several research contributions have addressed the task of personalizing search by incorporating knowledge of user preferences into the search process 1 http://trec.nist.gov/data/session.html</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The PIR CLEF Lab organized within CLEF 2018 aims to provide a framework for the evaluation of Personalised Information Retrieval (PIR). PIR systems seek to enhance traditional IR systems to better satisfy the users information needs by providing search results that are not only relevant to the query but more specifically to the interests of the user who submitted the query. In order to provide a personalised service, a PIR system can leverage various kinds of information about the current user and their preferences and interests. These can be stated directly or be inferred through a variety of interactions of the user with the system. This information is then represented in a user model, which can be employed to either improve the user's query or to re-rank a set of retrieved results list so that documents more relevant to the user are presented in the top positions of the list.</p><p>Evaluating the effectiveness of personalised approaches to search has been investigated for many years within studies of interactive information retrieval.</p><p>In this work, the notion of relevance has been user centered with potential variation during a search session, depending both on the task at hand and on the user's interactions with the search system. This work has mostly based on user studies; this approach involves real users undertaking search tasks in a supervised environment. By placing the user at the centre of the evaluation activity these studies have produced valuable insights and feedback. However, while this methodology has the advantage of enabling the detailed study of the activities of real users, it has the significant drawback of not being easily reproducible, thus greatly limiting the scope for algorithmic exploration of technologies for search personalisation. Among some previous attempts to define PIR benchmark tasks based on the Cranfield paradigm, the closest experiment to the PIR Lab is the TREC Session track 1 conducted annually between 2010 and 2014. This track focused on stand-alone search sessions, where a "session" is a continuous sequence of query reformulations on the same topic, along with any user interaction with the retrieved results in service of satisfying a specific information need; however no details of the searcher undertaking the task have been made available. Thus, the TREC Session track did not exploit any user model to personalise the search experience, nor did it allow user actions over multiple search session to be taken into consideration in the ranking of the search output.</p><p>The PIR-CLEF 2018 Lab has provided search data gathered in search sessions carried out ten volunteer users: the provided data were originally collected for the Pilot Lab run in 2017. We plan in the future to gather data across multiple sessions to enable the construction and exploitation of persistent user behaviour data collected from the user across the multiple search sessions. This year the data were provided to the 16 groups registered to task. with the objective of allowing them to attempt the proposed tasks. An evaluation using this collection was run to allow research groups working on PIR to both experience with and provide feedback about our proposed PIR evaluation methodology. Two papers were submitted and accepted for presentation at the workshop related to the Lab; both papers report on the usage of the collected data to perform different tasks; the work reported in these papers is summarized later in this overview. The papers give some useful suggestions to improve the data gathering process, which will give rise to interesting discussions during the Lab.</p><p>The remainder of this paper is organised as follows: Section 2 outlines existing related work, Section 3 provides an overview of the PIR-CLEF 2018 task, Section 4 discusses the metrics available for the evaluation of the task, Section 5 overviews papers submitted by task participants, and Section 6 concludes the paper.</p><p>2 Related Work <ref type="bibr" coords="3,134.77,118.99,9.96,8.74" target="#b1">[2]</ref>. This user-centered approach to search has raised the related issue of how to properly evaluate the effectiveness of personalized search in a scenario where relevance is strongly dependent on the interpretation of the individual user. To this purpose several user-based evaluation frameworks have been developed, as discussed in <ref type="bibr" coords="3,189.73,166.81,9.96,8.74" target="#b2">[3]</ref>.</p><p>A first category of approaches aimed at evaluating personalized search systems (PIRS, Personalized Information Retrieval Systems) are focused on performing a user-centered evaluation by providing a kind of extension to the laboratory based evaluation paradigm. The TREC Interactive track <ref type="bibr" coords="3,430.89,218.48,10.52,8.74" target="#b3">[4]</ref> and the TREC HARD track <ref type="bibr" coords="3,223.41,230.43,10.52,8.74" target="#b5">[5]</ref> are examples of this kind of evaluation framework, which aimed at involving users in interactive tasks to get additional information about them and the query context. The evaluation was done by comparing a baseline run ignoring the user/topic metadata with another run considering it.</p><p>The more recent TREC Contextual Suggestion track <ref type="bibr" coords="3,385.09,282.09,10.52,8.74" target="#b6">[6]</ref> was proposed with the purpose of investigating search techniques for complex information needs that are highly dependent on both context and users interests. Participants in the track were given, as input, a set of geographical contexts and a set of user profiles that contain a list of attractions the user has previously rated. The task was to produce a list of ranked suggestions for each profile-context pair by exploiting the given contextual information. However, despite these extensions, the overall evaluation was still system controlled and only a few contextual features were available in the process.</p><p>TREC also introduced a Session track <ref type="bibr" coords="3,321.28,393.53,10.52,8.74" target="#b7">[7]</ref> the focus of which was to exploit user interactions during a query session to incrementally improve the results within that session. The novelty of this task was the evaluation of system performance over entire sessions instead of a single query. However, the above tasks have various limitations to the satisfactory injection of user behaviour into the evaluation proces; for this reason the problem of defining a standard approach to the evaluation of personalized search is a hot research topic, which needs effective solutions.</p><p>A first attempt to create a collection satisfactorily accounting for the user behaviour in search was done in the FIRE Conference held in 2011. The Personalised and Collaborative Information Retrieval track <ref type="bibr" coords="3,366.60,520.77,10.52,8.74" target="#b8">[8]</ref> was organised with the aim of extending a standard IR ad-hoc test collection by gathering additional meta-information during the topic development process to facilitate research on personalised and collaborative IR. However, since no runs were submitted to this track, only preliminary studies have been carried out and reported using it.</p><p>Within CLEF 2017, we launched the PIR-CLEF benchmark with a pilot study and workshop (PIR CLEF 2017). for the purpose of providing a forum for the exploration of the evaluation of PIR. The Pilot Lab provided a preliminary edition of the 2018 PIR-CLEF Lab. One of the achievements of the PIR-CLEF 2017 Pilot Task was the setting up of an evaluation benchmark which seeks to combine user-centered methods with the Cranfield evaluation paradigm, with the key potential benefit of producing evaluation results that are easily reproducible.</p><p>The Pilot task was based on search sessions over a subset of the ClueWeb12 document collection, undertaken by 10 users by using a clearly defined and novel methodology. The collection was defined by relying on data gathered from activities undertaken during the search sessions by each participant, including details of relevant documents as marked by the searchers. An important point to be outlined is that the collection was prepared but not used by any group participating at the pilot task. For this reason at PIR-CLEF 2018 we relied on this data collection. We distributed it to the 16 groups registered to the Lab. We have also prepared a second collection, as well as a prototype system for the comparative evaluation of systems developed by participating groups. The data collection is described in more detail in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of the task</head><p>The goal of the PIR-CLEF 2018 Task is to investigate the use of a laboratorybased method to enable comparative evaluation of PIR methods. The collection defined within the PIR-CLEF 2017 Pilot Study and the PIR-CLEF 2018 Lab was created with the cooperation of volunteer users, and was organized into two sequential phases:</p><p>-Data gathering. This phase involved the volunteer users carrying out a taskbased search session during which a set of activities performed by the user were recorded (e.g, formulated queries, bookmarked documents, etc.). Each search session was composed of a phase of query development, refinement and modification, and associated search with each query on a specific topical domain selected by the user, followed by a relevance assessment phase where the user indicated the relevance of documents returned in response to each query and a short report writing activity based on the search activity undertaken. -Data cleaning and preparation. This phase took place once the data gathering had been completed, and did not involve any user participation. It consisted of filtering and elaborating the information collected in the previous phase in order to prepare a dataset with various kinds of information related to the specific user's preferences. In addition, a bag-of-words representation of the participant's user profile was created to allow comparative evaluation of PIR algorithms using the same simple user model.</p><p>For the Task we made available the user profile data and raw search data produced by guided search sessions undertaken by the 10 volunteer users as detailed in section 3.1.</p><p>The aim of the task was to use the provided information to improve the ranking of a search results list over a baseline ranking of documents judged relevant to the query by the user who entered the query.</p><p>The Task data was provided in csv format to registered participants in the task. Access to the search service for the indexed subset of the ClueWeb12 collection was provided by Dublin City University via an API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>For the PIR-CLEF 2018 Task we made available both user profile data and raw search data produced by guided search sessions undertaken by the 10 volunteer users. The data provided included the submitted queries, the baseline ranked lists of documents retrieved in response to each query by using a standard search system, the items clicked by the user in the result list, and the documents relevance assessments provided by the user on a 4-grade scale. Each session was performed by the user on a topic of her choice selected from a provided list of broad topics, and search carried out over a subset of the ClueWeb12 web collection.</p><p>The data was extracted and stored in csv format in 7 csv files in a zip folder which was provided to participants. The details of the contents of the csv files are as follows:  username: the user who performed the session query session: id of the query session within the search was performed category: the top level search domain query text: the submitted query document id: the document on which a particular action was performed rank: the retrieval rank of the document on which a particular action is performed action type: the type of the action executed by the user (query submission, open document, close document, bookmark) time stamp: the timestamp of the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>csv3:</head><p>The file user's assessment contains the relevance assessments of a pool of documents with respect to every single query developed by each user to fulfill the given task:</p><p>username: the user who performed the session query session: id of the query session within the evaluation was performed query text: the query on which the evaluation is based document id: the document id for which the evaluation was provided rank: the retrieval rank of the document on which a particular action is performed relevance score: the relevance of the document to the topic (1 off-topic, 2 not relevant, 3 somewhat relevant, 4 relevant).</p><p>csv4: The file user's info contains some personal information about the users:</p><p>username age range gender occupation native language.</p><p>The file user's topic (csv5) contains the TREC-style final topic descriptions about the users information needs that were developed in the final step of each search session:</p><p>username, the user who formulated the topic query session, id of the query session which the topic refers to title, a small phrase defining the topic provided by the user description, a detailed sentence describing the topic provided by the user narrative, a description of which documents are relevant to the topic and which are not, provided by the user csv6: The file simple user profile for each user contains the following information (simple version -the applied indexing included tokenization, shingling, and index terms weighting):</p><p>username: the user whose interests are represented category: the search domain of interest a list of triples constituted by: • a term: a word or n-grams related to the users searches • a normalised score: term weight computed as the mean of the term frequencies in the users documents of interests, where term frequency is the ratio of the number of occurrences of the term in a document and the number of occurrences of the most frequent term in the same document.</p><p>csv6b: The file complex user profile contains, for each user, the same information provided in csv6a, with the difference that the applied indexing was enriched by also including stop word removal:</p><p>username, the user whose interests are represented category, the search domain of interest a list of triples constituted by: • term, a word or a set of words related to the users searches • normalised score, Task participants had the possibility of contribute in two different ways:</p><p>the two user profile files (csv6a and csv6b) provide bag-of words profiles for each of the 10 volunteer searchers in the data collection. The profiles were created by applying different indexing procedures to the documents that the searcher assessed as relevant during the search session. The searcher's log file (cvs2) contains all the queries she formulated during the query session. Task participants could compare the results obtained by applying their personalisation algorithms on these queries with the results obtained and evaluated by the searchers on the same queries (and included in the user assessment file csv3). The search had to be carried out on the ClueWeb12 collection, by using the API provided by DCU. Then, by using the 4-graded scale evaluations of the documents (relevant, somewhat relevant, non relevant, off topic) provided by the users and contained in the user assessment file csv3, it was possible to compute Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG) using the standard NIST trec eval tool. Note that documents that do not appear in csv3 were considered non-relevant. -The challenge here was to use the raw data provided in csv1, csv2, csv3, csv4, and csv5 to create user profiles. A user profile is a formal representation of the user interests and preferences; the more accurate the representation of the user model, the higher is the probability to improve the search process. In the approaches proposed in the literature, user profiles are formally represented as bags of words, as vectors, or as conceptual taxonomies, generally defined based on external knowledge resources (such as the WordNet and the ODP Open Directory Project). The task request here was more research oriented: are the provided information sufficient to create a useful profile? Which information is missing? The outcome here was a report up to 6 pages by the participant discussing the theme of user information to profiling aims, by proposing possible integrations of the provided data and by suggesting a way to collect them in a controlled Cranfield style experiment.</p><p>We encouraged participants to be involved in this task by using existing or new algorithms and/or to explore new ideas. We also welcomed contributions that make an analysis of the task and/or of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Analysis</head><p>The metrics and methodology used to evaluate and analyze the PIR-CLEF task pose significant challenges. It is not at all obvious how we might properly compare and contrast the behaviour of alternative methods of integrating personalization into search sessions. While we can start off using stand metrics, such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG) for individual these will not be sufficient to enable a detailed session based analysis.</p><p>As a starting for point for the development of formal methodology for analysis and evaluation of our framework for laboratory-based evaluation of PIR, we have developed a prototype evaluation tool which we describe in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PIR Evaluation Tool</head><p>Our proposed evaluation tool is designed to provide a repeatable approach for the evaluation of PIR. A few description of the analysis, design and implementation of the current version of this tool is provided in <ref type="bibr" coords="8,345.36,164.66,14.61,8.74" target="#b12">[11]</ref>.</p><p>This evaluation tool consists of three sequential phases:</p><p>1. File Extraction: During this phase the data is extracted from standard TREC format results files created for each retrieval query operation. The extracted data is stored in efficient data structures to accelerate the next steps. 2. Metric Calculation: During this phase a set of standard IR evaluation measures are calculated, including Precision, Recall, Precision@K, NDCG, etc Ṅovel approaches for defining a retrieval are also defined. 3. Output Generation: During this phase the evaluation tool produces a report which shows a set of standard IR evaluation measures. These evaluation measures are computed and compared in such a way as to highlight different performance measures between alternative results files. Additionally, a set of charts are included in results which graphically display the evolution of the performance of each evaluation measure through a retrieval session.</p><p>We now describe each phase in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>File Extraction</head><p>In this phase all information contained in the results files is extracted and efficient data structures are created to accelerate the evaluation process. These files contain the required information to estimate the performance of a system, therefore the files need to be created in a specified format:</p><p>-TREC format results file: containing the runs performed by a PIR system using the personalised data collection. This tool supports concurrent execution of multiple results files. -Relevance judgments: containing the relevant documents mark by the volunteer searchers. -Search logs: containing all searcher activities recorded during the search sessions. -Commands file: allowing for the results to be tailored.</p><p>TREC Format Results File: The evaluation tool requires TREC format results files containing the ranked lists computed by the PIR system. The TREC format has been chosen because it a well known in the IR community. Results files must have the following fields: user id, topic id, query id, document id, rank, name.</p><p>Relevance Judgments File: The relevance judgments file contains the relevant documents marked by participants during the third phase of the PIR-CLEF experiment. This file has the following fields: user id, topic id, query id, document id, relevance score.</p><p>Search Logs: The search logs contain the user information gathered during the search sessions. This information is used by the PIR system to create the user representation that will be used in the retrieval process and by the evaluation tool to simulate the user behaviour.</p><p>Commands File: This allows for the tailoring of the evaluation process by specifying which measures and charts are to be computed by the tool.</p><p>Metric Calculation During this phase metrics are computed to enable the performance of the PIR systems to be evaluated and compared. The evaluation tool first computes per-query measures which allow the evaluation of the effectiveness considering a single ranked list. It then computes novel approaches for evaluation of retrieval sessions considering multiple ranked lists.</p><p>We next describe the process undertaken by the evaluation tool computing per query measures. We then detail the approaches defined for the evaluation of retrieval sessions.</p><p>Per-Query Measures The evaluation tool computes the following measures for each query in the personalised data collection: Precision, Recall, Precision@K, Recall@K, F-measure, R-precision, Average Precision, and NDCG.</p><p>The computation process of each metric follows these steps:</p><p>1. A ranked list set containing the ranked lists related to the queries are extracted from the result file provided by the PIR system, so as to have a ranked list for each query. 2. The relevance judgments set containing the relevance judgment documents for each query is extracted from the relevance judgments file. 3. The evaluation measure is calculated by comparing the retrieved documents list and the relevance judgments set for each query.</p><p>For each triple measure-user-topic, the evaluation tool generates both a line chart and a bar chart to show the evolution of the measure through the retrieval session as well as to compare the performance of the different input algorithms.</p><p>The created charts have a y-axis, which represents the measure value, and an x-axis, which represents the queries belonging to the same topic sorted by time using the timestamp contained in the search logs.</p><p>Session Measures Real users often begin an interaction with a search engine with a query which they need to reformulate one or more times. The ability of the PIR system to improve results after query reformulation cannot be easily assessed by the measures normally used for measuring system effectiveness, but requires new approaches taking into consideration the user behaviour triggered by the system. Three alternative approaches are proposed to simulate user behaviour through a retrieval session. Two participant papers were accepted for presentation at the PIR-CLEF session at the CLEF 2018 conference. The papers have made use of the provided data collection to examine two different tasks, and both present interesting and useful findings and suggestions on how to improve the PIR-CLEF dataset.</p><p>The paper titled ECNU at CLEF PIR 2018: Evaluation of personalised information retrieval <ref type="bibr" coords="11,221.61,524.61,15.50,8.74" target="#b14">[13]</ref> presents a study exploring the potential of the dataset provided by PIR CLEF. The authors report in their paper two different experiments based on two distinct baselines, i.e. "query level baseline" and "session level baseline"; in the first baseline each single query in a session is evaluated independently, while in the second one all queries in a session are summed up to define a single query. The authors then report on the experiments they made, in which they applied two methods for query expansion and an approach to define a "topic sensitive user model" based on search sessions. In the reported discussion, the following suggestions to improve the dataset are made: i) to provide more numerous relevance labels, ii) to increase the number of provided user-related information (i.e. to have information related to more users profiles), and iii) to define richer query logs.</p><p>The paper titled PIR based on explicit and implicit feedback <ref type="bibr" coords="12,420.64,118.99,15.50,8.74" target="#b15">[14]</ref> addresses Task 2 of the PIR-CLEF lab, i.e. user profiling. More specifically, the authors explored the use of explicit and implicit feedback to define user profiles. Concerning explicit feedback, the subjective relevance judgments provided by the searchers for a given set of documents is employed to train a text classifier, thus exploiting the PIR task as a text classification task. Concerning implicit feedback, the correlation between information that is inferred form the data and relevance judgments provided by the users has been analyzed. Several analyses and useful remarks are reported in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work the PIR-CLEF 2018 Personalised Information Retrieval task was presented. This task is the first edition of a lab dedicated to the theme of personalised search after the successful pilot held at CLEF 2017. This is the first evaluation benchmark in this field based on the Cranfield paradigm, with the significant benefit of producing results easily reproducible. The PIR-CLEF 2018 workshop has provided a Lab task based on a test collection that has been generated by using a well defined methodology. An evaluation using this collection has been run to allow research groups working on PIR to both experience with and provide feedback about our proposed PIR evaluation methodology. We also introduced our current work on a prototype system for the comparative analysis of PIR systems across search sessions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,274.52,345.82,8.77;5,134.77,286.51,300.10,8.74;5,140.99,306.20,211.46,8.77;5,140.99,318.12,219.25,8.77;5,140.99,330.03,234.87,8.77;5,140.99,341.95,267.61,8.77;5,140.99,353.86,205.88,8.77;5,140.99,365.78,198.85,8.77;5,140.99,377.69,239.62,8.77;5,140.99,389.61,308.97,8.77"><head>csv 1 :</head><label>1</label><figDesc>The file user's session contains the information about each phase of the query sessions performed by each user. Each row of the csv contains:username: the user who performed the session query session: id of the performed query session category: the top level search domain of the session task: the description of the search task fulfilled by the user start time: starting time of the query session close time: closing time of the search phase evaluated time, closing time of the assessment phase end time: closing time of the topic evaluation and the whole session.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,413.66,345.83,8.77;5,134.77,425.65,283.16,8.74"><head>csv 2 :</head><label>2</label><figDesc>The file user's log contains the search logs of each user, i.e. every search event that was triggered by a users action. The file row contains:</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,151.70,644.13,328.89,8.77;9,151.70,656.12,78.45,8.74"><p>Using logs file: User behaviour is simulated using information contained in the search logs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.35,439.66,342.24,7.86;12,146.91,450.62,333.68,7.86;12,146.91,461.58,333.68,7.86;12,146.91,472.54,219.79,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,367.49,439.66,113.09,7.86;12,146.91,450.62,163.62,7.86">A Laboratory-Based Method for the Evaluation of Personalised Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sanvitto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,317.40,450.62,163.19,7.86;12,146.91,461.58,333.68,7.86;12,146.91,472.54,104.18,7.86">Proceedings of the Seventh International Workshop on Evaluating Information Access (EVIA 2016), a Satellite Workshop of the NTCIR-12 Conference</title>
		<meeting>the Seventh International Workshop on Evaluating Information Access (EVIA 2016), a Satellite Workshop of the NTCIR-12 Conference<address><addrLine>Tokyo Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-07">June 7, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,483.16,342.25,7.86;12,146.91,494.12,97.15,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,182.33,483.16,176.83,7.86">Issues in personalising information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,365.86,483.16,114.73,7.86;12,146.91,494.12,30.83,7.86">IEEE Intelligent Informatics Bulletin</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,504.74,342.25,7.86;12,146.91,515.69,333.68,7.86;12,146.91,526.65,152.94,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,366.08,504.74,114.52,7.86;12,146.91,515.69,263.44,7.86">Evaluation of contextual information retrieval effectiveness: overview of issues and research</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tamine-Lechani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,418.11,515.69,62.49,7.86;12,146.91,526.65,81.48,7.86">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,537.27,302.94,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,199.29,537.27,193.52,7.86">Overview of the fourth text retrieval conference</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.75,537.27,34.60,7.86">TREC-4)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,445.13,537.27,35.45,7.86;12,146.91,548.23,333.68,7.86;12,146.91,559.19,165.70,7.86" xml:id="b4">
	<monogr>
		<title level="m" coord="12,214.88,548.23,142.35,7.86">TREC, volume Special Publication</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,569.81,342.25,7.86;12,146.91,580.77,333.68,7.86;12,146.91,591.73,197.85,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,185.69,569.81,294.91,7.86;12,146.91,580.77,27.02,7.86">HARD track overview in TREC 2003: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,193.96,580.77,282.53,7.86">Proceedings of The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>The Twelfth Text REtrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">2437</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,602.34,342.24,7.86;12,146.91,613.30,333.68,7.86;12,146.91,624.26,53.12,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,189.74,613.30,234.18,7.86">Overview of the TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
		<editor>Voorhees and Bucklan</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,634.88,342.25,7.86;12,146.91,645.84,333.68,7.86;12,146.91,656.80,185.25,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,388.12,634.88,92.47,7.86;12,146.91,645.84,71.55,7.86">Overview of the TREC 2014 session track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,235.91,645.84,244.69,7.86;12,146.91,656.80,53.50,7.86">Proceedings of The Twenty-Third Text REtrieval Conference (TREC 2014)</title>
		<meeting>The Twenty-Third Text REtrieval Conference (TREC 2014)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,119.67,342.24,7.86;13,146.91,130.63,333.67,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,413.17,119.67,67.42,7.86;13,146.91,130.63,317.17,7.86">Overview of the personalized and collaborative information retrieval (PIR) track at FIRE-2011</title>
		<author>
			<persName coords=""><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,146.91,141.59,333.68,7.86;13,146.91,152.55,333.67,7.86;13,146.91,163.51,333.67,7.86;13,146.91,174.47,333.68,7.86;13,146.91,185.43,333.68,7.86;13,146.91,196.39,267.33,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,425.54,141.59,55.06,7.86;13,146.91,152.55,119.84,7.86">Venkata Subramaniam, Danish Contractor</title>
		<author>
			<persName coords=""><forename type="first">Prasenjit</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pushpak</forename><surname>Bhat-Tacharyya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,380.27,152.55,100.32,7.86;13,146.91,163.51,329.48,7.86;13,332.94,174.47,118.12,7.86">Multilingual Information Access in South Asian Lan-guages -Second International Workshop, FIRE 2010</title>
		<title level="s" coord="13,146.91,196.39,141.41,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Gandhinagar, India; Bombay, India</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">February 19-21, 2010. FIRE 2011. December 2-4, 2011. 2011</date>
			<biblScope unit="volume">7536</biblScope>
			<biblScope unit="page">227240</biblScope>
		</imprint>
	</monogr>
	<note>Third International Workshop. Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="13,138.35,207.34,342.25,7.86;13,146.91,218.30,333.68,7.86;13,146.91,229.26,59.26,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,418.72,207.34,61.88,7.86;13,146.91,218.30,265.72,7.86">Overview of the ImageCLEF 2016 Handwritten Scanned Document Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.51,218.30,48.08,7.86;13,146.91,229.26,35.20,7.86">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,240.22,337.97,7.86;13,146.91,251.18,333.68,7.86;13,146.91,262.14,281.32,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,211.03,240.22,173.18,7.86">A new interpretation of Average Precision</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,404.44,240.22,76.15,7.86;13,146.91,251.18,333.68,7.86;13,146.91,262.14,82.92,7.86">Proceedings of the International ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;08)</title>
		<meeting>the International ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="689" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,273.10,337.97,7.86;13,146.91,284.06,203.13,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,206.19,273.10,229.34,7.86">Comparative Evaluation of Personalised Search Systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Angiolillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Milano, Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universit degli Studi di Milano Bicocca</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,295.02,337.97,7.86;13,146.91,305.98,333.68,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,277.78,295.02,202.81,7.86;13,146.91,305.98,47.88,7.86">Rank-biased precision for measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,200.82,305.98,201.90,7.86">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,316.93,337.97,7.86;13,146.91,327.89,333.67,7.86;13,146.91,338.85,237.65,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,303.93,316.93,176.66,7.86;13,146.91,327.89,139.45,7.86">ECNU at CLEF PIR 2018 : Evaluation of Personalized Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,305.45,327.89,175.13,7.86;13,146.91,338.85,136.96,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,349.81,337.98,7.86;13,146.91,360.77,333.68,7.86;13,146.91,371.73,285.03,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,146.91,360.77,183.84,7.86">PIR Based in Explicit and Implicit Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andreu-Marn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martnez-Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Urea-Lpez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Daz-Galiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,351.16,360.77,129.44,7.86;13,146.91,371.73,184.34,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
