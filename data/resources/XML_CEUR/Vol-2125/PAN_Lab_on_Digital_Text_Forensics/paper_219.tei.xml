<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.68,115.90,300.01,12.90;1,221.72,133.83,171.92,12.90;1,223.43,153.68,168.50,10.75">Combining textual and visual representations for multimodal author profiling Notebook for PAN at CLEF 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.88,190.08,64.47,8.64"><forename type="first">Sebastian</forename><surname>Sierra</surname></persName>
							<email>ssierral@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="department">Computing Systems and Industrial Engineering Dept</orgName>
								<orgName type="institution">Universidad Nacional de Colombia Bogotá</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.18,190.08,74.83,8.64"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<email>fagonzalezo@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="department">Computing Systems and Industrial Engineering Dept</orgName>
								<orgName type="institution">Universidad Nacional de Colombia Bogotá</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.68,115.90,300.01,12.90;1,221.72,133.83,171.92,12.90;1,223.43,153.68,168.50,10.75">Combining textual and visual representations for multimodal author profiling Notebook for PAN at CLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23ACF106C5CBD3EE6E36B9132D8722ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social media data allows researchers to establish relationships between everyday language and people's sociodemographic variables, such as gender, age, language variety or personality. Author Profiling studies the common use of language inside those demographic groups. This work describes our proposed method for the PAN 2018 Author Profiling shared task. This year's task consisted of evaluating gender using multimodal information (text and images) which was extracted from Twitter users. We trained separate models for text, image and multimodal approaches. In multimodal approaches we explored early, late and hybrid approaches. We found experimentally that early approaches obtained the best performance. We obtained 0.80, 0.74 and 0.81 of accuracy in the multimodal scenario for the test partition for English, Spanish and Arabic respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, there is a large amount of information generated by users on various social networks. Facebook, Twitter, Instagram, among others generate a high amount of information in which users write their opinion about a topic, upload a photo about a relevant topic to them or simply record a video of what they are doing at that moment. Key applications can be derived from the generation of automatic analysis methods, which can handle properly the multimodal nature of social media information. Due to the increasing amount of social media information, several tasks for social media automatic analysis have acquired a greater importance. One of those tasks is Author Profiling (AP). AP can be seen as the study of the use of language in different demographic groups (profiles). For instance, gender-based profiles [3], age-based [22], native country-based [20], among others. Gender detection is one of the most popular subtask in Author Profiling <ref type="bibr" coords="1,216.21,560.80,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="1,235.51,560.80,12.45,8.64" target="#b21">22,</ref><ref type="bibr" coords="1,251.50,560.80,7.47,8.64" target="#b3">4,</ref><ref type="bibr" coords="1,262.50,560.80,7.47,8.64" target="#b7">8,</ref><ref type="bibr" coords="1,273.49,560.80,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="1,289.48,560.80,12.45,8.64" target="#b12">13,</ref><ref type="bibr" coords="1,305.46,560.80,12.45,8.64" target="#b14">15,</ref><ref type="bibr" coords="1,321.44,560.80,11.83,8.64" target="#b17">18]</ref>. However, most of the work in AP has been devoted to the use of texts for categorizing correctly the profile of an author. Gender identification based on the images that an user posts in his/her social media is a task that has been gaining interest <ref type="bibr" coords="1,272.66,596.66,10.79,8.64" target="#b5">[6,</ref><ref type="bibr" coords="1,286.06,596.66,12.45,8.64" target="#b22">23,</ref><ref type="bibr" coords="1,301.11,596.66,12.45,8.64" target="#b30">30,</ref><ref type="bibr" coords="1,316.17,596.66,11.83,8.64" target="#b26">27]</ref>. Most of these works take the images of a social media user, extract the visual concepts, for instance, if a bag is present in the image and finally associate the presence of this concepts to the gender of the user (profile). Shigenaka et al. [23]  interestingly propose a neural architecture which learns a proper representation for the images while associates it with the visual concepts which are extracted from the images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Information fusion</head><p>Information fusion considers the problem of merging correctly two different representations of the same concept <ref type="bibr" coords="2,247.19,155.06,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="2,260.76,155.06,8.33,8.64" target="#b6">7]</ref>.Atrey et al. <ref type="bibr" coords="2,319.32,155.06,11.62,8.64" target="#b4">[5]</ref> considers three levels of information fusion: feature level or early fusion, decision level or late fusion, and hybrid approaches. For this work, feature level consists of extracting text and visual representations and combining them into a single learning method. These combinations ignore the intrinsic correlation between modalities <ref type="bibr" coords="2,262.94,202.88,15.27,8.64" target="#b16">[17]</ref>. Decision level consists of combining the output decisions of previously learned classifiers for each modality. Hybrid approaches consist of methods that create a joint space for representing the different modalities of a concept, for instance for solving image captioning tasks <ref type="bibr" coords="2,348.52,238.74,15.77,8.64" target="#b25">[26,</ref><ref type="bibr" coords="2,367.18,238.74,11.83,8.64" target="#b27">28]</ref>. Multimodal approaches for Author Profiling have been considered by <ref type="bibr" coords="2,322.54,250.70,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,336.57,250.70,12.45,8.64" target="#b13">14,</ref><ref type="bibr" coords="2,352.26,250.70,11.83,8.64" target="#b24">25]</ref>. Álvarez-Carmona et al. <ref type="bibr" coords="2,468.97,250.70,11.62,8.64" target="#b0">[1]</ref> extend the PAN AP 2014 corpus by extracting a large set of tweets and images from the original users of this corpus. While their fusion strategy consists of an early fusion of text and image features. Taniguchi et al. <ref type="bibr" coords="2,329.55,286.56,16.60,8.64" target="#b24">[25]</ref> propose a hybrid fusion strategy, where visual concepts are extracted using a CNN, but each concept has a probability of being associated to a dimension of the profile (male or female). Text representation is extracted as the probability of a document to belong to a female user or a male user. At the very end, all the probabilities are concatenated and fed to a logistic regression classifier. It is worth to mention that these approaches are very recent and AP using multimodal strategies is becoming a very important topic in the scientific community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">PAN Author Profiling 2018 Shared Task</head><p>PAN-AP 2018 shared task consisted of classifying correctly the gender of an user of Twitter <ref type="bibr" coords="2,165.90,427.79,15.27,8.64" target="#b18">[19]</ref>. Two modalities were considered for representing an user: text and image. For each user, 100 tweets and 10 posted images were extracted. Also, users were selected from different languages: English, Spanish and Arabic. 1500 users were collected for the Arabic split, while 3000 users were collected separately for English and 3000 users for Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we describe our submission to the PAN-AP 2018 shared task. Each subsection in this methodology describes the preprocessing, the feature extraction process and the learning algorithm used to classify an author as male or female.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text representation</head><p>Twitter text representation strategies were explored at two levels. At the first level, several preprocessing strategies were considered: removing URLs, removing stopwords, lowercasing tweets, filtering retweets, usernames, hashtags and stripping of accents. Then, these main representations were explored:</p><p>-Bag-of-Words using unigrams (WORD_UNI): BoW representation is built using only unigrams with higher document frequency than 10 documents. Preprocessing steps for unigrams were URL removal, lowercasing, retweet filtering, usernames filtering, stopwords removal and accent striping. -Bag-of-Words using bigrams (WORD_BI): Word bigrams with a higher document frequency than 10 documents were extracted from the training corpora. Preprocessing steps were the same as WORD_UNI, but stopwords were not removed. -Bag-of-Words using character n-grams (CHAR): N-grams of characters were extracted using Scikit-Learn. 2-grams, 3-grams and 4-grams were used for representing stylistic features from the documents. Only preprocessing steps were: URL removal, hashtag and usernames filtering.</p><p>-Concatenation of all bag of words representations (ALL_BOW): WORD_UNI, WORD_BI and CHAR representations are concatenated. -Average of fastText representations (AVG_FAST): Each word of each document was represented using a pretrained model of fastText <ref type="bibr" coords="3,367.73,286.87,15.27,8.64" target="#b9">[10]</ref>. Then, each author was represented by the feature-wise average of the word embeddings extracted from each fastText model. For each language a separate model was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual representation</head><p>Extracting visual features from a set of images is not an easy task. In the recent years, Convolutional Neural Networks (CNN) have gained a lot of attention by their competitive performance for several computer vision tasks. CNNs are built upon the idea of building high-level features using a compositional hierarchy of low-level features <ref type="bibr" coords="3,461.50,396.94,15.27,8.64" target="#b11">[12]</ref>. This means, first layers are expected to capture low-level patterns like edges, while higher layers are expected to learn domain specific features, which combine properly the low-level features. In image classification, the last layer of a CNN is commonly a SoftMax Layer with a size depending on the number of classes that it attempts to predict. Deep learning models like CNNs require a large amount of data for training, however they present two additional advantages: they can capture a pattern regardless of its location in the image, and the learned patterns can be transferred to solve a related image classification task. As described by Yosinski et al. <ref type="bibr" coords="3,360.48,492.58,3.82,8.64">[</ref> Before any image is fed to the network, they are scaled without losing their aspect ratio. Both networks produce non-negative features. For representing an author, all his/her images were propagated through the network and the resulting feature vectors are averaged across each feature. This means, every author is represented by a vector of size 2048 or 4096, depending on the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal representation</head><p>As stated in Section 1, information fusion strategies can be categorized into: early fusion or feature fusion, late fusion or decision fusion, and hybrid approaches. In this work, we use one implementation of each category in order to assess the best strategy for fusing both modalities:</p><p>Figure <ref type="figure" coords="4,161.50,408.02,3.36,8.06">1</ref>. Methodology describing our approach to multimedia author profiling. Tweets from one author are gathered, then multimodal information is extracted from his/her tweets. Image representation is extracting using an average VGG16/ResNet50, while textual representation is built using bag representation. Finally, a multimodal representation is learned using GMUs.</p><p>-Early Fusion (CONCAT): Best features from each modality (text and image) are stored, then each feature is scaled so it has zero mean and standard deviation of one. Finally a classifier is trained on top of these standardized features. -Late Fusion (VOTING): The best classifiers from each modality are stores using joblib library. This includes storing Scikit-Learn pipelines of transformation of data. Then, image and text are propagated through their respective classifier. Finally, the output probabilities are averaged and the max value is chosen as the predicted class. -Hybrid Fusion (GMU): For each language, a GMU <ref type="bibr" coords="4,368.87,572.75,11.62,8.64" target="#b1">[2]</ref> is trained using the best features per modality. Training split was divided again in training and development splits. The development split was used for validating the hyperparameters of the GMU. GMUs have the advantage of learning a a multimodal representation, while attempting to solve a supervised task (gender prediction). In Figure <ref type="figure" coords="4,423.08,620.57,3.74,8.64">1</ref>, we describe the methodology of our multimodal approach. Also, the best features in the textual modality had a large dimensionality, therefore a PCA was applied to retain th 99$ of variance.</p><p>For each language, the dataset was split into training and validation. 70% of the dataset was used for training and the remainder for validation. When the best performing features were extracted, a model was trained again using the complete dataset. The code for extracting the features was saved using joblib and was deployed in the TIRA evaluation system, where the evaluation on the test split was carried on. For the text modality, the following results were obtained using the proposed features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Character n-grams worked very well for identifying gender on English and Arabic, as can be seen on Table <ref type="table" coords="5,224.60,620.57,3.74,8.64" target="#tab_1">1</ref>. While for Spanish, the word unigrams worked better. In the image modality, ResNet50 outperformed VGG16 as a feature extractor as can be seen on Table <ref type="table" coords="5,171.73,644.48,3.74,8.64" target="#tab_2">2</ref>. However the strategy for combining the extracted features of the images of one author consisted only of a feature-wise average.</p><p>One of the main motivations of the work, was to learn a multimodal representation using GMUs. Strong regularization using dropout and batch normalization per modality was used for training GMUs. Although for the case of English, only 2100 samples were fed to the GMU. While for Arabic, the number of samples decreased to 1050. Although we used strong regularization, our model overfitted quickly. In Table <ref type="table" coords="6,411.10,167.13,4.98,8.64" target="#tab_3">3</ref> we showed that early fusion approaches obtained the best results for the multimodal Future work involves improving the way that image representations are extracted and combined. Taniguchi et al. <ref type="bibr" coords="6,261.99,203.00,16.60,8.64" target="#b24">[25]</ref> provide a very good strategy for extracting visual information from the posted images. Also, we are interested in applying successfully GMUs to the Author Profiling task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,492.58,345.83,138.44"><head></head><label></label><figDesc><ref type="bibr" coords="3,364.29,492.58,11.45,8.64" target="#b28">29]</ref>, we can use the activation values of pre-trained CNNs for extracting features in images that belong to different classification domains. Since 2012, CNNs have been the state-of-the-art methods for image recognition tasks. In this work, we use two CNN architectures: VGG<ref type="bibr" coords="3,446.44,528.44,16.60,8.64" target="#b23">[24]</ref> and ResNet50<ref type="bibr" coords="3,176.61,540.40,10.58,8.64" target="#b8">[9]</ref>. Both had a top performance in the ImageNet Large Scale Visual Recognition Challenge<ref type="bibr" coords="3,203.77,552.35,16.60,8.64" target="#b20">[21]</ref> during 2014 and 2015. Both VGG and ResNet50 are easy to use in Keras, so they were chosen as feature extractors:</figDesc><table coords="3,141.74,586.07,338.86,44.94"><row><cell>-ResNet50: Receives a RGB image of size 224 × 224 and produces an output of</cell></row><row><cell>2048 non-negative values.</cell></row><row><cell>-VGG16: Receives a RGB image of size 224 × 224 and produces an output of 4096</cell></row><row><cell>non-negative values.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,174.40,237.37,266.56,84.29"><head>Table 1 .</head><label>1</label><figDesc>Results on validation for gender task using only the text modality</figDesc><table coords="5,234.73,258.34,145.90,63.32"><row><cell cols="4">Text Features English Spanish Arabic</cell></row><row><cell cols="2">WORD_UNI 0.79</cell><cell>0.79</cell><cell>0.55</cell></row><row><cell>WORD_BI</cell><cell>0.79</cell><cell>0.73</cell><cell>0.54</cell></row><row><cell>CHAR</cell><cell>0.81</cell><cell>0.76</cell><cell>0.79</cell></row><row><cell cols="2">AVG_FAST 0.70</cell><cell>0.68</cell><cell></cell></row><row><cell cols="2">ALL_BOW 0.81</cell><cell>0.78</cell><cell>0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,170.10,370.06,275.16,51.41"><head>Table 2 .</head><label>2</label><figDesc>Results on validation for gender task using only the image modality</figDesc><table coords="5,230.99,391.03,153.38,30.44"><row><cell cols="4">Visual Features English Spanish Arabic</cell></row><row><cell>ResNet50</cell><cell>0.78</cell><cell>0.77</cell><cell>0.7</cell></row><row><cell>VGG16</cell><cell>0.75</cell><cell>0.7</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,171.85,469.87,271.66,62.37"><head>Table 3 .</head><label>3</label><figDesc>Results on validation for gender task using multimodal approaches</figDesc><table coords="5,214.13,490.84,187.09,41.40"><row><cell cols="4">Multimodal Approaches English Spanish Arabic</cell></row><row><cell>CONCAT</cell><cell>0.82</cell><cell>0.80</cell><cell>0.79</cell></row><row><cell>VOTING</cell><cell>0.80</cell><cell>0.75</cell><cell>0.78</cell></row><row><cell>GMU</cell><cell>0.81</cell><cell>0.77</cell><cell>0.79</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,156.34,171.12,320.11,8.64;7,156.34,183.07,324.24,8.64;7,156.34,195.03,284.45,8.64;7,156.34,206.98,224.98,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,473.39,183.07,7.19,8.64;7,156.34,195.03,236.87,8.64">A visual approach for age and gender identification on Twitter</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Álvarez-Carmona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pellegrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sánchez-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villatoro-Tello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,400.68,195.03,40.12,8.64;7,156.34,206.98,113.74,8.64">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3133-3145</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,218.57,323.93,8.64;7,156.34,230.52,285.26,8.64;7,156.34,242.48,272.06,8.64;7,156.34,254.43,125.12,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,406.94,218.57,73.34,8.64;7,156.34,230.52,113.66,8.64">Gated Multimodal Units for Information Fusion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.01992" />
	</analytic>
	<monogr>
		<title level="m" coord="7,291.38,230.52,150.23,8.64;7,156.34,242.48,157.94,8.64">International Conference on Learning Representations ICLR 2017 -Workshop</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02">feb 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,266.01,313.65,8.64;7,156.34,277.97,313.32,8.64;7,156.34,289.93,92.96,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,358.90,266.01,111.09,8.64;7,156.34,277.97,117.08,8.64">Gender, Genre, and Writing Style in Formal Written Texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Shimoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,280.98,277.97,188.69,8.64;7,156.34,289.93,39.84,8.64">Text -Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,301.51,315.01,8.64;7,156.34,313.46,324.24,8.64;7,156.34,325.42,212.91,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,376.98,301.51,94.38,8.64;7,156.34,313.46,134.79,8.64">Automatically Profiling the Author of an Anonymous Text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<idno type="DOI">10.1145/1461928.1461959</idno>
		<ptr target="http://doi.acm.org/10.1145/1461928.1461959" />
	</analytic>
	<monogr>
		<title level="j" coord="7,298.59,313.46,118.18,8.64">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="123" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,337.00,322.99,8.64;7,156.34,348.96,303.25,8.64;7,156.34,360.91,209.31,8.64;7,156.34,372.87,210.05,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,404.88,337.00,74.45,8.64;7,156.34,348.96,130.91,8.64">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00530-010-0182-0</idno>
		<ptr target="http://dx.doi.org/10.1007/s00530-010-0182-0http://link.springer.com/10.1007/s00530-010-0182-0" />
	</analytic>
	<monogr>
		<title level="j" coord="7,294.94,348.96,82.20,8.64">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010-04">apr 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,384.45,312.38,8.64;7,156.34,396.40,228.29,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,258.96,384.45,209.76,8.64;7,156.34,396.40,38.73,8.64">Gender prediction using individual perceptual image aesthetics</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gavrilova</surname></persName>
		</author>
		<ptr target="https://otik.zcu.cz/handle/11025/21646" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,407.99,286.14,8.64;7,156.34,419.94,274.80,8.64;7,156.34,431.90,181.09,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,267.32,407.99,175.16,8.64;7,156.34,419.94,40.49,8.64">Multimedia data mining: state of the art and challenges</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-010-0645-5</idno>
		<ptr target="http://dx.doi.org/10.1007/s11042-010-0645-5" />
	</analytic>
	<monogr>
		<title level="j" coord="7,204.47,419.94,141.17,8.64">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="76" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,443.48,308.21,8.64;7,156.34,455.43,297.73,8.64;7,156.34,467.39,288.52,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,362.73,443.48,101.83,8.64;7,156.34,455.43,27.15,8.64">Discriminating gender on Twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Burguer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zarrella</surname></persName>
		</author>
		<ptr target="https://dl.acm.org/citation.cfm?id=2145568" />
	</analytic>
	<monogr>
		<title level="m" coord="7,204.51,455.43,249.56,8.64;7,156.34,467.39,81.07,8.64">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,478.97,324.01,8.64;7,156.34,490.93,284.42,8.64;7,156.34,502.88,129.22,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,298.86,478.97,177.51,8.64">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,169.90,490.93,270.86,8.64;7,156.34,502.88,43.88,8.64">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,514.47,307.38,8.64;7,156.34,526.42,233.30,8.64" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,358.62,514.47,105.10,8.64;7,156.34,526.42,74.15,8.64">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1607.01759" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,538.00,312.26,8.64;7,156.34,549.96,305.46,8.64;7,156.34,561.91,198.93,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,324.86,538.00,143.74,8.64;7,156.34,549.96,94.21,8.64">Automatically Categorizing Written Texts by Author Gender</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Shimoni</surname></persName>
		</author>
		<idno type="DOI">10.1093/llc/17.4.401</idno>
		<ptr target="http://dx.doi.org/10.1093/llc/17.4.401" />
	</analytic>
	<monogr>
		<title level="j" coord="7,258.69,549.96,139.20,8.64">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2002-11">nov 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,573.50,324.24,8.64;7,156.34,585.45,179.77,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,296.18,573.50,54.69,8.64">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/nature14539" />
	</analytic>
	<monogr>
		<title level="j" coord="7,358.22,573.50,27.11,8.64">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,597.03,314.72,8.64;7,156.34,608.99,315.87,8.64;7,156.34,620.94,313.39,8.64;7,156.34,632.90,279.94,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,232.45,608.99,239.77,8.64;7,156.34,620.94,95.15,8.64">Discriminative subprofile-specific representations for author profiling in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0950705115002427" />
	</analytic>
	<monogr>
		<title level="j" coord="7,259.56,620.94,109.05,8.64">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="134" to="147" />
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,156.34,644.48,310.90,8.64;7,156.34,656.44,308.23,8.64;8,156.34,119.31,315.16,8.64;8,156.34,131.27,209.97,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,320.61,644.48,146.64,8.64;7,156.34,656.44,242.70,8.64">You are what you tweet...pic! gender prediction based on semantic analysis of social media images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7177499/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,420.58,656.44,43.99,8.64;8,156.34,119.31,232.82,8.64">2015 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,143.15,301.26,8.64;8,156.34,155.10,294.18,8.64;8,156.34,167.06,311.85,8.64;8,156.34,179.01,81.62,8.64;8,156.34,190.97,283.81,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,206.70,155.10,243.82,8.64;8,156.34,167.06,175.05,8.64">Emphasizing personal information for Author Profiling: New approaches for term selection and weighting</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Ortega-Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Franco-Arcega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0950705118300224" />
	</analytic>
	<monogr>
		<title level="j" coord="8,339.22,167.06,109.05,8.64">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="169" to="181" />
			<date type="published" when="2018-04">apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,202.84,318.73,8.64;8,156.34,214.80,324.24,8.64;8,156.34,226.75,321.51,8.64;8,156.34,238.71,269.82,8.64;8,156.34,250.66,181.37,8.64" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,366.35,202.84,108.73,8.64;8,156.34,214.80,103.74,8.64">Predicting Age and Gender in Online Social Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peersman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Vaerenbergh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2065023.2065035</idno>
		<ptr target="http://doi.acm.org/10.1145/2065023.2065035" />
	</analytic>
	<monogr>
		<title level="m" coord="8,281.74,214.80,198.84,8.64;8,156.34,226.75,175.15,8.64;8,383.03,226.75,42.55,8.64">Proceedings of the 3rd International Workshop on Search and Mining User-generated Contents</title>
		<meeting>the 3rd International Workshop on Search and Mining User-generated Contents<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note>SMUC &apos;11</note>
</biblStruct>

<biblStruct coords="8,156.34,262.54,313.65,8.64;8,156.34,274.50,311.40,8.64;8,156.34,286.45,215.43,8.64;8,156.34,298.41,293.33,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,287.38,262.54,182.61,8.64;8,156.34,274.50,115.95,8.64">Unsupervised multimodal feature learning for semantic image segmentation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6706748" />
	</analytic>
	<monogr>
		<title level="m" coord="8,293.71,274.50,174.04,8.64;8,156.34,286.45,103.41,8.64">The 2013 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-08">aug 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,310.28,324.08,8.64;8,156.34,322.24,208.08,8.64;8,156.34,334.19,279.94,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,243.29,310.28,182.28,8.64">On the impact of emotions on author profiling</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0306457315000783" />
	</analytic>
	<monogr>
		<title level="j" coord="8,432.84,310.28,47.59,8.64;8,156.34,322.24,107.91,8.64">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="92" />
			<date type="published" when="2016-01">jan 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,346.07,321.20,8.64;8,156.34,358.03,320.13,8.64;8,156.34,369.98,319.90,8.64;8,156.34,381.94,323.67,8.64;8,156.34,393.89,123.78,8.64" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,427.87,346.07,49.67,8.64;8,156.34,358.03,320.13,8.64;8,156.34,369.98,27.15,8.64">Overview of the 6th Author Profiling Task at PAN 2018: Multimodal Gender Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,415.89,369.98,60.35,8.64;8,156.34,381.94,292.56,8.64">Working Notes Papers of the CLEF 2018 Evaluation Labs. CEUR Workshop Proceedings</title>
		<title level="s" coord="8,455.66,381.94,24.36,8.64;8,156.34,393.89,38.85,8.64">CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,405.77,296.44,8.64;8,156.34,417.72,320.37,8.64;8,156.34,429.68,316.14,8.64;8,156.34,441.63,322.56,8.64;8,156.34,453.59,293.78,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,373.22,405.77,79.56,8.64;8,156.34,417.72,320.37,8.64;8,156.34,429.68,37.39,8.64">Overview of the 5th Author Profiling Task at PAN 2017: Gender and Language Variety Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Rangel Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,437.87,429.68,34.62,8.64;8,156.34,441.63,192.25,8.64">Working Notes Papers of the CLEF 2017 Evaluation Labs</title>
		<title level="s" coord="8,355.46,441.63,119.18,8.64;8,198.76,453.59,65.70,8.64">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
	<note>CLEF and CEUR</note>
</biblStruct>

<biblStruct coords="8,156.34,465.47,311.09,8.64;8,156.34,477.42,323.78,8.64;8,156.34,489.38,313.97,8.64;8,156.34,501.33,114.27,8.64;8,156.34,513.29,210.05,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,415.30,477.42,64.83,8.64;8,156.34,489.38,141.77,8.64">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="http://link.springer.com/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="8,305.99,489.38,164.32,8.64">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,525.16,322.55,8.64;8,156.34,537.12,291.94,8.64;8,156.34,549.07,160.77,8.64;8,156.34,561.03,313.68,8.64" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,372.36,525.16,106.53,8.64;8,156.34,537.12,47.64,8.64">Effects of Age and Gender on Blogging</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Papers/Symposia/Spring/2006/SS-06-03/SS06-03-039.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,225.52,537.12,222.76,8.64;8,156.34,549.07,72.46,8.64">AAAI spring symposium: Computational approaches to analyzing weblogs</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,572.91,295.99,8.64;8,156.34,584.86,311.41,8.64;8,156.34,596.82,314.35,8.64;8,156.34,608.77,209.97,8.64" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,318.33,572.91,134.01,8.64;8,156.34,584.86,269.87,8.64">Content-Aware Multi-task Neural Networks for User Gender Inference Based on Social Media Images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shigenaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuboshita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7823607/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,447.83,584.86,19.93,8.64;8,156.34,596.82,211.38,8.64">2016 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-12">dec 2016</date>
			<biblScope unit="page" from="169" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,620.65,316.61,8.64;8,156.34,632.60,233.85,8.64" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="8,277.68,620.65,195.28,8.64;8,156.34,632.60,70.72,8.64">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,156.34,644.48,324.24,8.64;8,156.34,656.44,295.46,8.64;9,156.34,119.31,321.42,8.64;9,156.34,131.27,97.91,8.64;9,156.34,143.22,227.70,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="8,433.06,644.48,47.53,8.64;8,156.34,656.44,276.80,8.64">A Weighted Combination of Text and Image Classifiers for User Gender Inference</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shigenaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuboshita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
		<ptr target="http://www.anthology.aclweb.org/W/W15/W15-2814.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,156.34,119.31,272.25,8.64">Proceedings of the 2015 Workshop on Vision and Language (VL&apos;15)</title>
		<meeting>the 2015 Workshop on Vision and Language (VL&apos;15)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,155.18,314.27,8.64;9,156.34,167.13,276.32,8.64" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="9,344.01,155.18,126.60,8.64;9,156.34,167.13,72.13,8.64">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.4555" />
	</analytic>
	<monogr>
		<title level="j" coord="9,236.29,167.13,24.92,8.64">CoRR</title>
		<imprint>
			<date type="published" when="2014-11">nov 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,179.09,318.32,8.64;9,156.34,191.04,312.66,8.64;9,156.34,203.00,283.39,8.64;9,156.34,214.95,181.75,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="9,310.58,179.09,164.09,8.64;9,156.34,191.04,133.20,8.64">Gender estimation for SNS user profiling using automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuboshita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/6890569/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,310.74,191.04,158.26,8.64;9,156.34,203.00,174.46,8.64">2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,226.91,323.09,8.64;9,156.34,238.86,319.90,8.64;9,156.34,250.82,160.54,8.64" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m" coord="9,156.34,238.86,315.89,8.64">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,156.34,262.77,313.08,8.64;9,156.34,274.73,313.80,8.64;9,156.34,286.69,296.35,8.64;9,156.34,298.64,145.37,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="9,341.88,262.77,127.55,8.64;9,156.34,274.73,89.53,8.64">How transferable are features in deep neural networks?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,278.08,286.69,174.61,8.64;9,156.34,298.64,33.22,8.64">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,304.21,298.64,124.50,8.64;9,156.34,310.60,313.46,8.64;9,156.34,322.55,52.19,8.64" xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,334.51,324.24,8.64;9,156.34,346.46,309.53,8.64;9,156.34,358.42,293.88,8.64;9,156.34,370.37,181.75,8.64" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="9,301.29,334.51,179.30,8.64;9,156.34,346.46,190.42,8.64">The Eyes of the Beholder: Gender Prediction Using Images Posted in Online Social Networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7022709/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,368.48,346.46,97.39,8.64;9,156.34,358.42,152.39,8.64">2014 IEEE International Conference on Data Mining Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-12">dec 2014</date>
			<biblScope unit="page" from="1026" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
