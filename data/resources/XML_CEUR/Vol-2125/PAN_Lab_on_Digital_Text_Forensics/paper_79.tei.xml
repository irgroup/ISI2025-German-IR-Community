<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,224.01,115.90,214.83,12.90;1,223.43,135.75,168.50,10.75">Profiling based on Text and Images Notebook for PAN at CLEF 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.23,172.15,42.57,8.64"><forename type="first">Luka</forename><surname>Stout</surname></persName>
							<email>l.stout@anchormen.nl</email>
							<affiliation key="aff0">
								<address>
									<settlement>Anchormen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.21,172.15,59.88,8.64"><forename type="first">Robert</forename><surname>Musters</surname></persName>
							<email>r.musters@anchormen.nl</email>
							<affiliation key="aff0">
								<address>
									<settlement>Anchormen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.78,172.15,42.35,8.64"><forename type="first">Chris</forename><surname>Pool</surname></persName>
							<email>c.pool@anchormen.nl</email>
							<affiliation key="aff0">
								<address>
									<settlement>Anchormen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,224.01,115.90,214.83,12.90;1,223.43,135.75,168.50,10.75">Profiling based on Text and Images Notebook for PAN at CLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">528A1A9D05E41DE204126C44C4173235</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in the PAN 2018 shared task of Author Profiling. In this task we identify the gender of authors based on written text and shared images. We describe our approaches to the text-based, imagebased and the combined task. The presence of three different languages raises the question whether a single model architecture can be built that works well on all three languages. We also propose a way to combine multiple predictions on shared content into a single prediction on user-level. Our final system for text is an ensemble of a Naive Bayes model and a RNN with attention. The image classification is done by finding selfies and predicting the gender of the person on those images using CNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the gaining influence and importance of social media it becomes more and more relevant to gain insights into the authors of content, mostly made up of images and text. Because social media networks allow people to create anonymous accounts it becomes of greater interest to the research community to get to know users on social media. Knowing specific details about a user, like gender, age, native language or emotional state is an interesting challenge for the marketing, forensic and security sectors. Author Profiling <ref type="bibr" coords="1,169.64,453.13,11.62,8.64" target="#b0">[1]</ref> is the task of determining an author's features like gender, age, language variety by understanding their online persona. In addition to the tweets the shared task of 2018 <ref type="bibr" coords="1,152.79,477.04,13.52,8.64" target="#b1">[2]</ref> includes images that were shared by the authors as well. The goal is to infer the gender of an author given one-hundred of their tweets and ten images, in three different languages, English, Spanish and Arabic. The presence of three different languages raises the question whether a single model architecture can be built that works well on all three languages. The shared task is divided into three subtasks: Infer the gender based on tweets, based on their shared images and a combination of the two. We have focused on the text-based task, however we have also developed an image-based approach to also participate in the combined task. For this we experimented with traditional techniques, such as tf-idf and Naive Bayes <ref type="bibr" coords="1,285.38,572.68,12.82,8.64" target="#b2">[3]</ref>, as well as deep learning techniques, such as Recurrent Neural Networks (RNNs) <ref type="bibr" coords="1,275.19,584.64,14.39,8.64" target="#b3">[4]</ref> and Convolutional Neural networks (CNNs) <ref type="bibr" coords="1,462.33,584.64,13.69,8.64" target="#b4">[5]</ref>. In this paper we describe our final systems and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Description and Preprocessing</head><p>The PAN 2018 Author Profiling <ref type="bibr" coords="1,265.45,644.48,11.62,8.64" target="#b5">[6]</ref> training set consists of text in three different languages and images grouped by authors, who are labeled by gender and language. The number of authors per gender is balanced in every language. This training set was used for feature engineering, parameter tuning and training of the classification model. For the languages English, Spanish and Arabic we received a dataset containing 100 tweets and 10 images per author. For English and Spanish there are 3,000 authors and for Arabic 1,500 authors. This gives a total of 750,000 tweets and 75,000 images. The goal of the task is to predict the gender of a user given these 100 tweets and 10 images. We have chosen to create models on tweet and image level and combine the predictions to create a single prediction for every author.</p><p>The following preprocessing steps were performed, the two additional preprocessing steps for Arabic can be found online * :</p><p>-Replaced numbers, URLs, hashtags, mentions, emojis and smileys with their own unique tokens. -Used a tokenizer to filter out punctuation and tokenize sentences into a list of lowercase words. -Expanded contractions. (For English) -Normalization of tokens, namely unifying the orthography of alifs, hamzas, and yas/alif maqsuras. (For Arabic) -Noise removal, i.e. removing short vowels and other symbols (harakat). (For Arabic)</p><p>After preprocessing and tokenization, the maximum number of words in a tweet is 39 for English. For the other languages there are fewer than 200 tweets longer than 39 words. As this only accounts for 0.02% of all the tweets in the data set and to keep the models consistent across languages, we have decided to cap the number of words in a sentence to 39.</p><p>Basile et al. <ref type="bibr" coords="2,199.85,418.99,10.65,8.64" target="#b6">[7]</ref> note that augmenting the tweet dataset with the data of previous Author Profiling tasks <ref type="bibr" coords="2,223.15,430.94,11.52,8.64" target="#b7">[8,</ref><ref type="bibr" coords="2,237.71,430.94,8.30,8.64" target="#b8">9]</ref> does not improve the performance of the resulting classifiers. They emphasize that this is due to temporal differences in the data. We have seen that topics reflect events from 2017 are definitely present in the data. While the data from previous years contains data with events from 2016 and before. As such we have decided not to include additional datasets to limit the effects of these differences.</p><p>For the image classification task we have used additional data to create our classifier: a selfie dataset <ref type="bibr" coords="2,220.59,502.68,16.10,8.64" target="#b9">[10]</ref> and the MIRFLICKR dataset <ref type="bibr" coords="2,357.43,502.68,15.58,8.64" target="#b10">[11]</ref>. Their use is explained in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prediction Strategies</head><p>There are two ways to predict gender based on an author's social media content. The first is to treat all of the content as a single item and create a single prediction based on the entirety of the data. This is analogous to a bag of words approach in the case of text. However, concatenating or summing up the images at pixel level is not straightforward and does not make intuitive sense. As such we have chosen a different approach. The approach is to make predictions on the item-level and combine these predictions somehow.</p><p>There are multiple ways of constructing an author-level prediction based on tweetlevel predictions, whether it is text, images or a combination of both. We have used three different strategies. <ref type="bibr" coords="3,214.82,143.22,11.62,8.64" target="#b0">(1)</ref> The first strategy is using the majority class of all predictions.</p><p>(2) The second strategy is to use the mean probability of all predictions. (3) The last strategy is to only use predictions where the model is very sure that an input indicates a certain gender. With the latter strategy a weighted average of the predictions where the weights are zero for predictions that are within a certain range is used:</p><formula xml:id="formula_0" coords="3,252.30,215.45,228.29,23.30">w i = 0 if α &lt; P i (f emale) &lt; β, 1 otherwise<label>(1)</label></formula><formula xml:id="formula_1" coords="3,214.81,246.49,261.91,30.32">P (f emale) = 1 N N i=1 w i P i (f emale) (<label>2</label></formula><formula xml:id="formula_2" coords="3,476.72,257.22,3.87,8.64">)</formula><p>where P is the prediction by a single model for a single author, P i is the prediction for the i-th tweet or image of the author and N the number of tweets or images for the author. If no such prediction exists we fall back to the second strategy, the mean strategy. We have found α = 0.25 and β = 0.75 to be good default values. Our usage of the third prediction strategy improved our accuracy on a validation set, as illustrated in Table <ref type="table" coords="3,170.55,348.49,3.74,8.64" target="#tab_0">1</ref>. The rationale is that the predictions where the model is sure that a certain input points towards a specific gender are the only ones that have any influence on the author-level prediction. The model was trained on tweet level and then the strategies were applied to a tweet set of unseen authors. This gives an example of the performance increase gained by using a different prediction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Text classification 4.1 Features</head><p>For author profiling, it has been shown that tf-idf weighted n-gram features, both in terms of characters and words, are very successful in inferring gender <ref type="bibr" coords="3,414.29,632.53,12.36,8.64" target="#b8">[9]</ref>. As such we have decided to use character 2-to 7-grams and word 1-to 3-grams with tf-idf weighting with sublinear term frequency scaling <ref type="bibr" coords="3,284.50,656.44,15.77,8.64" target="#b11">[12]</ref>.</p><p>Word embeddings are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems <ref type="bibr" coords="4,318.96,143.22,17.62,8.64" target="#b12">[13,</ref><ref type="bibr" coords="4,339.79,143.22,11.83,8.64" target="#b13">14]</ref>. They work in such a way that words with similar meaning get a similar representation in a lower dimensional space. These embeddings are trained on huge corpora of text to have the most context-specific information. For English and Arabic we used the pretrained fastText embeddings <ref type="bibr" coords="4,458.17,179.09,17.93,8.64" target="#b14">[15]</ref>. For Spanish the pretrained embeddings we used were trained on the Spanish Billion Word Corpus Embeddings <ref type="bibr" coords="4,236.35,203.00,18.37,8.64" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Neural Network</head><p>RNNs <ref type="bibr" coords="4,155.64,249.96,15.65,8.64" target="#b3">[4]</ref> are used to model sequences where the order is important. They have an internal memory that keeps track of the examples they have seen so far in the current sequence. Text is one of the clear use cases for RNNs <ref type="bibr" coords="4,355.49,273.87,20.75,8.64" target="#b16">[17]</ref> because of its sequential nature.</p><p>A challenge with using a recurrent neural networks is the vanishing gradient problem. In this problem long dependencies get lost over time. The problem was explored in depth in <ref type="bibr" coords="4,168.49,321.69,15.77,8.64" target="#b17">[18,</ref><ref type="bibr" coords="4,186.17,321.69,13.28,8.64" target="#b18">19]</ref> who found some fundamental reasons why it might be difficult to retain these dependencies. One solution to this problem is to use multiple gates as the atomic units within a recurrent neural network. Multiple versions of these gates exist, such as Long Short Term Memory-units (LSTM) <ref type="bibr" coords="4,298.53,357.56,19.92,8.64" target="#b19">[20]</ref> and Gated Recurrent Units (GRU) <ref type="bibr" coords="4,457.41,357.56,18.54,8.64" target="#b20">[21]</ref>. Chung et al. <ref type="bibr" coords="4,185.38,369.51,16.60,8.64" target="#b21">[22]</ref> note that both LSTM and GRU are superior over recurrent neural networks with traditional tanh units. LSTMs are, in theory, better able to remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations. An advantage of GRUs over LSTMs is that they are computationally more efficient because they have fewer within the units. As noted in Section 2 the texts are small in length and as such we do not need the additional power of LSTMs and have decided to use GRUs.</p><p>Another way to solve the long term dependency problem is to use an attention mechanism. They were recently demonstrated to have success in a wide range of tasks <ref type="bibr" coords="4,464.72,465.15,15.86,8.64" target="#b22">[23,</ref><ref type="bibr" coords="4,134.77,477.11,12.45,8.64" target="#b23">24,</ref><ref type="bibr" coords="4,150.16,477.11,12.45,8.64" target="#b24">25,</ref><ref type="bibr" coords="4,165.54,477.11,11.83,8.64" target="#b25">26]</ref>. We use a modification of the mechanism proposed by Zhou et al. <ref type="bibr" coords="4,450.82,477.11,15.27,8.64" target="#b26">[27]</ref>, in which we have not used the weighted sum but instead have taken the global maximum and the global average over the attention matrix and have concatenated the two.</p><p>Bidirectional RNNs <ref type="bibr" coords="4,224.38,512.97,20.34,8.64" target="#b27">[28,</ref><ref type="bibr" coords="4,247.03,512.97,13.28,8.64" target="#b28">29]</ref> are a combination of two seperate RNNs. The input sequence is fed in the normal order for one network, and in reverse order for the other. The outputs of the two networks are usually concatenated at each time step. This structure allows the networks to have both backward and forward information about the sequence at every time step. Human understanding of text works in the same way, we use the context of words to determine their meaning. In our work the seperate RNNs have the same configuration.</p><p>Recurrent neural networks can require millions of parameters to sufficiently model tasks. This high dimensional parameter space translates to a high chance of overfitting on the training data set. Because large networks are slow to use, creating an ensemble of many large networks is infeasible. One technique to reduce the overfitting is to add dropout <ref type="bibr" coords="4,164.52,644.48,17.01,8.64" target="#b29">[30,</ref><ref type="bibr" coords="4,183.51,644.48,13.28,8.64" target="#b30">31]</ref> to the network. We used different amounts of dropout in different places in the network. Between the embeddings and the recurrent layer of our network we use spatial dropout <ref type="bibr" coords="5,212.21,221.24,17.31,8.64" target="#b31">[32]</ref> instead of normal dropout. The benefit of this is that entire embedding channels can be dropped with a certain probability, which is better than removing random points in the embedding matrix. We used 300-dimensional word embeddings as the input for our network. Spatial Dropout with a rate of 0.4 is applied to the word embeddings. We used a bidirectional GRU with 256 units for each direction. The GRU had a tanh activation function, an output dropout-rate of 0.35 and an internal dropout-rate of 0.1. After the recurrent layer the attention mechanism was applied. Global average and max pooling were applied to this layer to get a single vector for every input text. The pooling operations are concatenated together as input to a dense network with a dropout rate of 0.5 between every dense block. A dense block consists of a fully connected layer with a PReLU <ref type="bibr" coords="5,418.18,340.79,20.66,8.64" target="#b32">[33]</ref> activation function. We also applied the tanh activation function on the output of the PReLU and concatenated it together with the original, as seen in Figure <ref type="figure" coords="5,372.97,364.70,3.74,8.64" target="#fig_0">1</ref>. Three such dense blocks were used with respectively 256, 128 and 64 neurons. Because of the concatenation the output size of these blocks is twice the number of neurons. The final output was a single neuron with the sigmoid function. We optimize the model with the Adam <ref type="bibr" coords="5,419.73,400.57,20.47,8.64" target="#b33">[34]</ref> optimizer and as the loss function we chose binary cross entropy. The network architecture can be seen in Figure <ref type="figure" coords="5,193.70,424.48,3.74,8.64" target="#fig_1">2</ref>. The layout for the recurrent neural networks we used for classifying the text. The gray layer is the input text. The orange layer is an embedding layer which replaces the words by their embedding. The magenta layer is the shape of the output of the bidirectional recurrent layer. The green layer is the attention mechanism. The blue layer is a global max pooling layer. The red layer is a global average pooling layer. The white layers are the dense blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ensemble</head><p>To do our final predictions on the texts we make use of an ensemble of two models. The ensemble is a combination of a traditional model and a deep-learning model. The deep-learning model (GRU) is described in the previous section.</p><p>The traditional model is a multinomial Naive Bayes <ref type="bibr" coords="6,362.64,119.31,13.49,8.64" target="#b2">[3]</ref> classifier (NB) using the character and word n-grams with tf-idf weighting on tweet level. Naive Bayes is a family of classification algorithms based on the assumption that every feature being classified is independent of any other feature given the class. The Naive Bayes classifier considers each word in a piece of text to contribute independently to the probability that the author is female (or male), regardless of any correlations between features. Although it is based on independence assumptions that often do not hold in the real world, Naive Bayes can often obtain surprisingly good results <ref type="bibr" coords="6,352.98,203.00,15.03,8.64">[35]</ref>.</p><p>The ensemble uses a weighted average to combine the output of different models. The weights and models within the ensemble have the same architecture, regardless of language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Image classification</head><p>After inspecting the images in the dataset we found that a lot of users post selfies. Our hypothesis is that if we could identify selfies, and detect the gender of a person on that selfie we could predict the gender of the author of the picture. If we do not find selfies for a user this pipeline will give a random prediction for the user.</p><p>Our image model is not our main approach to this shared task and as such we hope to improve our score in the combined task using it. Because of this it does not really make an impact on our results if a user does not post selfies.</p><p>For this approach we need a dataset consisting of selfies, and a dataset without selfies. For the selfies class we used the selfie dataset provided by Kaleyeh et al. <ref type="bibr" coords="6,451.02,393.01,15.27,8.64" target="#b9">[10]</ref>. In this research 46.836 selfies where collected and annotated with 36 different attributes. The focus of this research was to predict the popularity of a selfie. For the no-selfie class we used the MIRFLICKR dataset <ref type="bibr" coords="6,297.05,428.87,15.58,8.64" target="#b10">[11]</ref>. This dataset consists of 25.000 images from Flickr. The images are annotated with tags. We removed images containing the tags 'person', 'portrait' or 'selfie' resulting in 23.500 images.</p><p>In 2012 Krizhevsky et al. won the ImageNet competition with a CNN <ref type="bibr" coords="6,430.07,464.95,20.06,8.64" target="#b34">[36]</ref>. Since then they have been the default architecture to tackle computer vision problems. We have used a CNN to detect selfies and if it is we predict the gender of this selfie with a different CNN with the same architecture. The architecture we used is shown in Figure <ref type="figure" coords="6,134.77,512.77,3.74,8.64" target="#fig_2">3</ref>. There are 64 filters in every convolutional layer. The kernel-sizes are 3 × 3 and the max-pooling size is 2 × 2. In every layer except the last we used the ReLU <ref type="bibr" coords="6,460.12,524.72,20.47,8.64" target="#b35">[37]</ref> activation function. In the last dense layer it is a sigmoid. The selfie detection was trained for 20 epochs using the Adam <ref type="bibr" coords="6,279.95,548.63,20.47,8.64" target="#b33">[34]</ref> optimizer on 150px by 150px versions of the input images with a batch size of 256. We augmented the dataset by rescaling, zooming and shearing and horizontal flipping of the images. We got a 96 percent accuracy of correctly identifying a selfie on a validation set of our created dataset. We found that on a small sample over 80% of the users post images that get classified as selfies. For this model we got an accuracy of 86% on just selfies. The model does not perform well on images that are not selfies.</p><p>One caveat of this approach is that not every picture with a face posted is of the author themselves. However we hypothesize that more often than not women will post pictures of themselves or other women and likewise with men. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Combining models</head><p>To combine the text models and the image models we use a weighted average. Overall, the text models were vastly outperforming the image models, however the addition of the image models did improve the overall performance of the system.</p><p>We chose to keep a single configuration for all languages. The weighted mean between the text models is a 1:4 ratio in favor of the RNN model. This is also the case for the combination of the image model and the text models where the ratio is in favor of the text models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>As in previous years with this shared task the models are compared using accuracy of correctly predicting the gender of an author. For every language the accuracy is calculated. Then, the accuracies are averaged to obtain a final score for our submission. The results in this section are evaluated on the PAN 2018 Author profiling evaluation set.</p><p>Table <ref type="table" coords="7,174.33,536.80,4.98,8.64" target="#tab_1">2</ref> shows the accuracy of our text models and ensemble. We achieve an accuracy of 76% for Arabic, 78.5% for English and 74.1% for Spanish on the evaluation set using the ensemble. There is a big difference in performance between Arabic and English, and Spanish. This might be because we have done additional preprocessing for Arabic and English. The GRU model outperforms the Naive Bayes model. The ensemble has a higher accuracy than the models separately for Spanish. For Arabic and English this is not the case, here the GRU model has the highest performance. To prevent overfitting on the very small test set we used for tweaking we did not alter our ensemble based on these results.</p><p>Using only the selfie model we get accuracies upwards of 62% of the different languages, as can be seen in Table <ref type="table" coords="7,274.40,656.44,3.74,8.64" target="#tab_1">2</ref>. This low accuracy might be because not all users The text models are: NB, GRU and NB+GRU. We hypothesize that the big difference between languages for the text models comes from the preprocessing used varying amount of preprocessing that is done on the dataset. The image column shows the accuracy of the gender prediction pipeline using our selfie detection algorithm on the evaluation set. The joint column shows the results of the weighted combination of the NB+GRU and image models on the evaluation set. The joint model has a slight improvement over using just the text models.</p><p>post selfies so our model does not know what to predict. Another reason might be that the selfies in the shared task dataset are different from the ones in the selfie dataset. It might also be the case that the MIRFLICKR dataset might not be sufficiently diverse. The images in this dataset are all high quality photos, which is not necessarily the case for the images shared in the PAN '18 dataset. We note that the accuracy on the images shared by Spanish users is a lot higher than with the Arabic and English users. We postulate that Spanish users might post more selfies or images representative for gender. For this reason we could have chosen to make the weight of the image model higher in the combined model case. However, to prevent overfitting, we have not done this.</p><p>The addition of the image models to the text models did give a very small improvement to the accuracy of our models (0.3%). This is because there is a big difference between the performance of the two approaches. If the performance of our image models would be on the same level as our text models we would see a significant improvement by using an ensemble of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have used a combination of text models and image models to create gender predictions for three different languages. We have done the predictions on individual tweets and images and then used multiple strategies to combine these predictions to create a single prediction on user level. We have also chosen to keep a single configuration of the system across the languages.</p><p>As such our performance on the individual languages is not as high as it could have been, had we optimized every combination of models for the different regions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,169.77,188.08,275.81,8.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The dense block that we use instead of a single activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,519.51,345.82,8.12;5,134.77,530.82,345.82,7.77;5,134.77,541.78,345.82,7.77;5,134.77,552.74,345.83,7.77;5,134.77,563.70,277.33,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. The layout for the recurrent neural networks we used for classifying the text. The gray layer is the input text. The orange layer is an embedding layer which replaces the words by their embedding. The magenta layer is the shape of the output of the bidirectional recurrent layer. The green layer is the attention mechanism. The blue layer is a global max pooling layer. The red layer is a global average pooling layer. The white layers are the dense blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,134.77,268.78,345.83,8.12;7,134.77,280.08,345.82,7.77;7,134.77,291.04,345.82,7.77;7,134.77,302.00,158.62,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The layout for the convolutional neural networks used in the selfie identification and gender prediction. The gray layer is the input image. The magenta layers are convolutional layers. The blue layers are max-pooling layers. The green layer is a flattened version of the layer before it. The white layers are normal dense layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,409.82,345.82,69.87"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table coords="3,221.09,409.82,173.18,45.94"><row><cell></cell><cell>acc</cell><cell></cell><cell>σ</cell></row><row><cell>(1) Majority</cell><cell>0.767</cell><cell>±</cell><cell>0.003</cell></row><row><cell>(2) Mean</cell><cell>0.780</cell><cell>±</cell><cell>0.011</cell></row><row><cell>(3) Sure</cell><cell>0.790</cell><cell>±</cell><cell>0.008</cell></row></table><note coords="3,167.87,471.92,312.72,7.77"><p>3-fold validation accuracy of using the Recurrent Neural Network on the English text.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,119.49,345.83,98.56"><head>Table 2 .</head><label>2</label><figDesc>The table shows the accuracy of 5 different models: 3 text models, 1 image model and 1 joint model per language.</figDesc><table coords="8,181.81,119.49,251.73,63.66"><row><cell></cell><cell>NB  †</cell><cell>GRU  †</cell><cell>NB+GRU</cell><cell>Image</cell><cell>Joint</cell></row><row><cell>Arabic</cell><cell>0.660</cell><cell>0.800</cell><cell>0.760</cell><cell>0.623</cell><cell>0.764</cell></row><row><cell>English</cell><cell>0.660</cell><cell>0.790</cell><cell>0.785</cell><cell>0.658</cell><cell>0.788</cell></row><row><cell>Spanish</cell><cell>0.640</cell><cell>0.720</cell><cell>0.741</cell><cell>0.623</cell><cell>0.743</cell></row><row><cell>Average</cell><cell>0.653</cell><cell>0.770</cell><cell>0.762</cell><cell>0.635</cell><cell>0.765</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="8,150.48,622.15,1.72,5.24;8,152.69,624.21,327.90,7.77;8,134.77,635.17,345.82,7.77;8,134.77,646.13,345.82,7.77;8,134.77,657.08,50.55,7.77"><p>†  The results of the NB and GRU models are obtained by evaluating the models on a small test set of 100 users as it was not possible to run the models on the evaluation set used. As such they might not be entirely representable for the performance of our models. We show these results for completeness.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An ensemble of a RNN and a bag of words model did improve performance on the English language, with respect to just using the RNN, but it does not improve on the other languages.</p><p>On the evaluation set, we got accuracy scores between 62.3% and 78.8% depending on language and whether we used models that classify based on text or on images. On our small test set our non-ensemble models showed an improved performance, however the test set only contained 100 users and as such were not be representable for the distributions shown in the evaluation set.</p><p>To conclude: we successfully defined an ensemble of deep-learning and traditional models capable of good performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,278.38,337.98,7.77;9,150.95,289.34,329.64,7.77;9,150.95,299.95,329.64,8.12;9,150.95,311.26,32.87,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,246.42,289.34,234.17,7.77;9,150.95,300.30,135.10,7.77">Improving feature representation based on a neural network for author profiling in social media texts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Posadas-Durán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Sanchez-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chanona-Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,295.27,300.30,164.26,7.77">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,321.77,337.98,7.77;9,150.95,332.73,329.64,7.77;9,150.95,343.69,329.64,7.77;9,150.95,354.65,329.64,7.77;9,150.95,365.61,329.64,7.77;9,150.95,376.57,156.50,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,170.25,332.73,310.35,7.77;9,150.95,343.69,20.70,7.77">Overview of PAN-2018: Author Identification, Author Profiling, and Author Obfuscation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.16,354.65,215.44,7.77;9,150.95,365.61,271.48,7.77">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 9th International Conference of the CLEF Initiative (CLEF 18)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bellot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,387.09,337.98,7.77;9,150.95,397.70,53.04,8.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,223.80,387.09,139.99,7.77">Idiot&apos;s bayes -not so stupid after all?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,367.61,387.09,112.98,7.77">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,408.56,337.98,7.77;9,150.95,419.52,93.86,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,298.01,408.56,182.58,7.77;9,150.95,419.52,63.21,7.77">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,429.69,297.37,8.12" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,278.14,430.04,49.27,7.77">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,334.15,430.04,24.40,7.77">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,440.56,337.98,7.77;9,150.95,451.51,329.64,7.77;9,150.95,462.47,329.64,7.77;9,150.95,473.43,295.73,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,393.73,440.56,86.86,7.77;9,150.95,451.51,273.22,7.77">Overview of the 6th Author Profiling Task at PAN 2018: Multimodal Gender Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,294.24,462.47,186.36,7.77;9,150.95,473.43,129.18,7.77">Working Notes Papers of the CLEF 2018 Evaluation Labs. CEUR Workshop Proceedings</title>
		<title level="s" coord="9,286.21,473.43,59.13,7.77">CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,483.95,337.98,7.77;9,150.95,494.91,172.08,7.77" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rawee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<title level="m" coord="9,426.02,483.95,54.57,7.77;9,150.95,494.91,123.49,7.77">N-GrAM: New Groningen Author-profiling Model</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,505.43,337.98,7.77;9,150.95,516.38,329.64,7.77;9,150.95,527.34,329.64,7.77;9,150.95,538.30,225.57,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,435.35,505.43,45.24,7.77;9,150.95,516.38,231.33,7.77">Overview of the 4th author profiling task at pan 2016: Cross-genre evaluations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,400.65,516.38,79.94,7.77;9,150.95,527.34,240.06,7.77">Working Notes Papers of the CLEF 2016 Evaluation Labs. CEUR Workshop Proceedings</title>
		<title level="s" coord="9,458.67,527.34,21.92,7.77;9,150.95,538.30,48.22,7.77;9,225.01,538.30,59.13,7.77">CLEF and CEUR-WS</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">2016/09 2016</date>
		</imprint>
	</monogr>
	<note>CLEF and CEUR</note>
</biblStruct>

<biblStruct coords="9,142.61,548.82,337.98,7.77;9,150.95,559.78,329.64,7.77;9,150.95,570.74,61.27,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,320.66,548.82,159.93,7.77;9,150.95,559.78,229.19,7.77">Overview of the 5th author profiling task at pan 2017: Gender and language variety identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,388.60,559.78,92.00,7.77;9,150.95,570.74,35.12,7.77">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,581.26,338.35,7.77;9,150.95,592.21,329.64,7.77;9,150.95,603.17,102.25,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,337.84,581.26,95.28,7.77">How to take a good selfie?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Seifu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.72,581.26,31.87,7.77;9,150.95,592.21,266.67,7.77">Proceedings of the 23rd ACM International Conference on Multimedia. MM &apos;15</title>
		<meeting>the 23rd ACM International Conference on Multimedia. MM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="923" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,613.69,338.35,7.77;9,150.95,624.65,329.64,7.77;9,150.95,635.61,84.90,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,249.37,613.69,120.56,7.77">The mir flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,390.24,613.69,90.35,7.77;9,150.95,624.65,284.01,7.77">MIR &apos;08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,646.13,338.35,7.77;9,150.95,657.08,146.57,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,304.17,646.13,130.80,7.77">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,119.96,338.35,7.77;10,150.95,130.92,329.64,7.77;10,150.95,141.88,56.78,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,373.82,119.96,106.77,7.77;10,150.95,130.92,158.85,7.77">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.79,130.92,152.81,7.77;10,150.95,141.88,26.81,7.77">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,153.42,338.35,7.77;10,150.95,164.38,265.22,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,311.17,153.42,165.88,7.77">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,163.16,164.38,222.04,7.77">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,175.92,338.35,7.77;10,150.95,186.53,150.18,8.12" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,371.22,175.92,109.37,7.77;10,150.95,186.88,34.29,7.77">Learning word vectors for 157 languages</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1802.06893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,198.43,288.89,7.77" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardellino</surname></persName>
		</author>
		<title level="m" coord="10,207.23,198.43,172.11,7.77">Spanish Billion Words Corpus and Embeddings</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,207.94,338.35,9.80;10,150.95,220.93,329.64,7.77;10,150.95,231.89,130.76,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,402.80,209.97,77.79,7.77;10,150.95,220.93,100.41,7.77">Recurrent neural network based language model</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,273.73,220.93,206.86,7.77;10,150.95,231.89,100.90,7.77">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,243.43,338.35,7.77;10,150.95,254.04,121.91,8.12" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,208.38,243.43,187.24,7.77">Untersuchungen zu dynamischen neuronalen netzen</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma</note>
</biblStruct>

<biblStruct coords="10,142.24,265.93,338.35,7.77;10,150.95,276.89,220.66,7.77" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m" coord="10,385.76,265.93,94.83,7.77;10,150.95,276.89,194.52,7.77">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,288.08,338.35,8.12;10,150.95,299.39,40.35,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,269.27,288.43,88.04,7.77">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,365.43,288.43,71.59,7.77">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,310.93,338.35,7.77;10,150.95,321.89,329.64,7.77;10,150.95,332.85,205.89,7.77" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,182.44,321.89,298.15,7.77;10,150.95,332.85,60.87,7.77">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="10,142.24,344.40,338.35,7.77;10,150.95,355.35,260.68,7.77" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,315.64,344.40,164.95,7.77;10,150.95,355.35,113.44,7.77">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.24,366.90,338.35,7.77;10,150.95,377.86,191.04,7.77" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="10,283.38,366.90,197.21,7.77;10,150.95,377.86,44.55,7.77">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.24,389.40,338.35,7.77;10,150.95,400.36,329.64,7.77;10,150.95,411.32,141.96,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,184.14,400.36,157.64,7.77">Teaching machines to read and comprehend</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,363.24,400.36,117.35,7.77;10,150.95,411.32,69.21,7.77">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1693" to="1701" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,422.86,338.35,7.77;10,150.95,433.82,329.64,7.77;10,150.95,444.78,13.45,7.77" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,394.84,422.86,85.75,7.77;10,150.95,433.82,78.57,7.77">Attention-based models for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,248.13,433.82,182.57,7.77">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="577" to="585" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,456.32,338.35,7.77;10,150.95,467.28,329.64,7.77;10,150.95,478.24,268.21,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,336.60,456.32,143.99,7.77;10,150.95,467.28,181.27,7.77">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,350.72,467.28,129.88,7.77;10,150.95,478.24,195.74,7.77">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,489.78,338.35,7.77;10,150.95,500.74,329.64,7.77;10,150.95,511.70,329.64,7.77;10,150.95,522.66,96.47,7.77" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,374.80,489.78,105.79,7.77;10,150.95,500.74,218.94,7.77">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.72,500.74,87.87,7.77;10,150.95,511.70,235.21,7.77">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="10,142.24,534.20,338.35,7.77;10,150.95,544.81,158.89,8.12" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,255.26,534.20,139.58,7.77">Bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,402.13,534.20,78.46,7.77;10,150.95,545.16,64.01,7.77">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,556.70,338.35,7.77;10,150.95,567.66,329.64,7.77;10,150.95,578.62,160.14,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,305.85,556.70,174.74,7.77;10,150.95,567.66,87.84,7.77">Bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.03,567.66,223.57,7.77;10,150.95,578.62,105.25,7.77">Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 29th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,590.16,338.35,7.77;10,150.95,601.12,329.64,7.77;10,150.95,611.73,123.26,8.12" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="10,438.33,590.16,42.26,7.77;10,150.95,601.12,199.59,7.77">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.39,601.12,122.20,7.77;10,150.95,612.08,32.86,7.77">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,623.62,338.35,7.77;10,150.95,634.58,117.57,7.77" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="10,305.06,623.62,145.33,7.77">Recurrent neural network regularization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.24,646.13,338.35,7.77;10,150.95,657.08,259.36,7.77" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="10,375.40,646.13,105.19,7.77;10,150.95,657.08,109.58,7.77">Efficient Object Localization Using Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4280[cs</idno>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,119.96,338.35,7.77;11,150.95,130.92,329.64,7.77;11,150.95,141.88,140.22,7.77" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,277.32,119.96,203.27,7.77;11,150.95,130.92,129.49,7.77">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,300.94,130.92,179.66,7.77;11,150.95,141.88,68.03,7.77">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,152.84,247.55,7.77;11,134.77,163.80,194.11,7.77" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="11,229.65,152.84,160.14,7.77;11,134.77,163.80,7.47,7.77">Adam: A method for stochastic optimization 35</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.78,163.80,105.19,7.77">The optimality of naive bayes</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,174.76,338.35,7.77;11,150.95,185.71,329.64,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="11,309.64,174.76,170.95,7.77;11,150.95,185.71,55.66,7.77">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,225.59,185.71,182.66,7.77">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,196.67,338.35,7.77;11,150.95,207.63,201.91,7.77" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<title level="m" coord="11,281.12,196.67,199.47,7.77;11,150.95,207.63,50.26,7.77">Empirical evaluation of rectified activations in convolutional network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
