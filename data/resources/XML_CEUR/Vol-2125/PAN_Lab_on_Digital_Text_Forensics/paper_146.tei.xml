<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.28,117.03,316.79,13.99;1,232.52,134.97,150.32,13.99;1,223.04,161.45,169.29,9.77">Authorship attribution with neural networks and multiple features Notebook for PAN at CLEF 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,275.16,196.11,65.05,9.69"><forename type="first">Łukasz</forename><surname>Gągała</surname></persName>
							<email>lukaszgagala@stud.uni-goettingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Göttingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.28,117.03,316.79,13.99;1,232.52,134.97,150.32,13.99;1,223.04,161.45,169.29,9.77">Authorship attribution with neural networks and multiple features Notebook for PAN at CLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">24F70F43AE827838770CAD1D87CD8DF4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stylometry</term>
					<term>Authorship Attribution</term>
					<term>Artificial Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However neural networks are receiving more and more attention from different fields of computer-aided research, application of this approach to stylometry and authorship attribution is still relatively infrequent in comparison to other domains of natural language processing. In this paper we present our attempt to analyse frequencies of different types of linguistic data (Part-of-speech, most frequent words, n-grams and skip-grams) with the means of simple neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The concept of artificial neural network (ANN) is quite old <ref type="bibr" coords="1,393.93,422.86,10.96,9.69" target="#b0">[1,</ref><ref type="bibr" coords="1,404.90,422.86,7.31,9.69" target="#b1">2]</ref> and its massive application in contemporary research should be mostly attributed to progress in hardware development (parallel computing, GPU-acceleration <ref type="bibr" coords="1,423.57,446.77,10.52,9.69" target="#b2">[3]</ref> ) allowing to faster project, prototype and employ ANN architectures on diverse types of data. Stylometry and authorship attribution <ref type="bibr" coords="1,332.92,470.68,10.52,9.69">[4]</ref> seem to be hardly fitted to this kind of inquiry, for most focus of the domain lies on attribution of shorter text fragments in noisy environment (e.g. different genre and topics of texts) with minimal amount of data (e.g. micro-corpora with very similar authors). ANN's require the opposite as many approaches in the field of machine learning. It does not, however, prevent researches from attempting to apply ANN to the question of authorship attribution and style analysis <ref type="bibr" coords="1,330.03,542.41,10.80,9.69" target="#b3">[5,</ref><ref type="bibr" coords="1,340.83,542.41,7.20,9.69" target="#b4">6,</ref><ref type="bibr" coords="1,348.03,542.41,7.20,9.69" target="#b5">7]</ref>. Moreover, the PAN contest has already seen successful applications of ANN <ref type="bibr" coords="1,351.21,554.37,10.52,9.69" target="#b6">[8]</ref> . Encouraged by that fact, we present in this paper our efforts to apply ANN-based solutions to the corpus of the PAN competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Selection of features</head><p>Selection of linguistic features for stylometric analysis heavily hinges upon a structure of particular language <ref type="bibr" coords="1,283.27,645.16,11.63,9.69" target="#b7">[9,</ref><ref type="bibr" coords="1,294.89,645.16,11.63,9.69" target="#b8">10]</ref> and as far as the tongues of the Indo-European family are considered, research of stylometry focuses on most frequent words (MWF), part-of-speech tags (PoS-tags) and function words being a preselected subgroup of the group of most frequent words <ref type="bibr" coords="2,362.73,131.95,14.62,9.69" target="#b9">[11]</ref>. Also n-grams of letters may be involved in successful stylometric analysis, since they may grasp relevant linguistic information encapsulated in prefixes and suffixes (otherwise accessible through a more specific description of parts of speech and morphology) or short words very often belonging to the group of function words.</p><p>Stylometric features can be combined together in so-called n-grams being groups of n-neighbouring elements, mostly part-of-speech tags or characters, because they render linguistic structures of higher order (respectively, codependencies among parts of speech or stems with affixes). There is also a category of skipgrams being extension of the idea of n-grams <ref type="bibr" coords="2,376.89,239.54,16.39,9.69" target="#b10">[12,</ref><ref type="bibr" coords="2,393.27,239.54,12.29,9.69" target="#b11">13,</ref><ref type="bibr" coords="2,405.56,239.54,8.19,9.69">14</ref>] -a skipgram is a combination of at least two particular elements with a gap between them that may consist of one or more random elements. Both n-grams and skipgrams are supposed to catch linguistic patterns (mostly expressed by distribution of PoS-tags). Since we wanted to try out experimental settings of features we propose a fashion of combing PoS-tags with most frequent words used already for stylom-etry <ref type="bibr" coords="3,158.50,119.99,16.13,9.69" target="#b12">[15,</ref><ref type="bibr" coords="3,174.63,119.99,12.10,9.69" target="#b13">16]</ref> . We presume that most frequent words among which also function words are concealed may render together with PoS-tags stylistic structures of higher order what in turn may result in better attribution of authorship. This idea could also refer to the notion of a "functor" proposed for stylometric analysis <ref type="bibr" coords="3,156.98,167.81,14.62,9.69" target="#b14">[17]</ref>. In the training corpus we have texts in languages of a varying level of inflectional morphology, therefore we decided to lemmatise all of them.</p><p>To produce specific text forms of text representations (like PoS-tags) we used third-party software for tagging and lemmatisation. For all but the Polish language we tagged and lemmatised the training corpus with tools provided by the package SpaCy <ref type="bibr" coords="3,206.75,229.44,15.50,9.69">[18]</ref> for Polish texts alone we employed the morphological analyser Morfeusz 2 [19] (both in Python). The preprocessed text representations were stored in the form of lists, from which we calculated frequencies of particular elements and subsequently normalised them. At the example of text no. 5 by author no. 4 from problem corpus no. 1 we can demonstrate different results of preprocessing:</p><p>(1) Normal unprocessed text. "The funeral had been a nightmare. Being who he is, the security kept away those who wished to sabotage the ceremony, and allowed only a minimal number of people to enter the graveyard. It didn't stop some of the invited people from whispering in harsh tones insults that Mycroft chose to ignore."</p><formula xml:id="formula_0" coords="3,134.77,388.55,345.82,45.56">(2) Character trigrams. '_-t-h', 't-h-e', 'h-e-_', '_-f-u', 'f-u-n', 'u-n-e', 'n-e-r', 'e-r-a', 'r-a-l', 'a-l-_', '_-h- a', 'h-a-d', 'a-d-_', '_-b-e', 'b-e-e', 'e-e-n', 'e-n-_', '_-a-_', '_-n-i', 'n-i-g', 'i-g-h', 'g-h-t', 'h-t-m', 't-m-</formula><p>a', 'm-a-r', 'a-r-e', 'r-e-_' (only first sentence showed)</p><p>(3) PoS-tags mixed with 150 most frequent words in the lemma form. 'the', 'DT', 'NN', 'have', 'VBD', 'be', 'VBN', 'a', 'DT', 'NN', 'NN', 'be', 'VBG', 'who', 'WP', '-PRON-', 'PRP', 'be', 'VBZ', 'the', 'DT', 'NN', 'keep', 'VBD', 'away', 'RB', 'DT', 'who', 'WP', 'VBD', 'to', 'TO', 'VB', 'the', 'DT', 'NN', 'and', 'CC', 'VBD', 'only', 'RB', 'a', 'DT', 'JJ', 'NN', 'of', 'IN', 'NNS', 'to', 'TO', 'VB', 'the', 'DT', 'NN', '-PRON-', 'PRP', 'didn', 'VBZ', 't', 'NN', 'stop', 'VB', 'some', 'DT', 'of', 'IN', 'the', 'DT', 'VBN', 'NNS', 'from', 'IN', 'VBG', 'in', 'IN', 'JJ', 'NNS', 'NNS', 'that', 'IN', 'NN', 'VBD', 'to', 'TO', 'VB' (4) PoS-tags with corresponding lemma for each token. "'the', 'DT', 'funeral', 'NN', 'have', 'VBD', 'be', 'VBN', 'a', 'DT', 'nightmare', 'NN', 'be', 'VBG', 'who', 'WP', '-PRON-', 'PRP', 'be', 'VBZ', 'the', 'DT', 'security', 'NN', 'keep', 'VBD', 'away', 'RB', 'those', 'DT', 'who', 'WP', 'wish', 'VBD', 'to', 'TO', 'sabotage', 'VB', 'the', 'DT', 'ceremony'" (only first sentence showed) Preference to a PoS-based approach was supported by an intuition that author-specific grammar structures should vary less between different domains than, for instance, most frequent words containing a lot of domain-specific vo-cabulary dictated by the domain itself. Moreover, by mixing PoS-tags with few MWF we wanted to enhance information on sentence structure carried e.g. by some function words. The optimal solution would be, however, to construct a list of function words, or by extension functors -due to time limit and insufficient knowledge of some of the PAN corpus languages we resigned from that step.</p><p>From the corpus texts prepared in the described way we calculated frequencies of n-grams and skip-grams (both of size 2, 3 and 4) for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Networks</head><p>The general idea of Artificial Neural Networks (ANN) was introduced in the 1940's and after multiple ups and downs it is today a steadfastly growing branch of AI research. The term "deep learning" refers to a method of stacking many layers of artificial neurons together what improve their computational capabilities. In our approach we use simple architecture of so-called dense layers already proposed for stylometric analysis with n-grams of characters <ref type="bibr" coords="4,406.53,343.37,14.62,9.69" target="#b15">[20]</ref>. We enhance this approach with various categories of features (see Section 2 of this paper), since the authorial fingerprint is thought to be present across different types of text characteristics (most frequent words, function words, part-of-speech tags).</p><p>Our network consists of individual branches for each category of linguistic features (e.g trigrams of PoS-tags) with an activation function. The formula of each hidden dense layer, that data are subsequently fed in, has the following form:</p><formula xml:id="formula_1" coords="4,277.93,467.33,202.67,9.72">y = W a + b (1)</formula><p>where x is a vector with either raw input data (for the first layer) or output data from an earlier layer (for n th layer, n &gt; 1) and W, b are network parameters, a matrix and a vector called respectively weights and bias, that are learnt while training the network. Since all neurons in subsequent layers are connected with each other this type of ANN-architecture is called "dense" or "fully connected" in the contrast to convolutional layers, which preselect data output from a previous layer by so-called filters.</p><p>Activation functions are vital part of any artificial neural network and provide a non-linear transformation of the input value. It guarantees that particular neurons of a network are activated by a particular values of data. For our architecture we choose the exponential linear unit <ref type="bibr" coords="4,356.47,621.25,14.62,9.69" target="#b16">[21]</ref>, since on initial tests of our architecture it seemed to outperform other very popular activation functions widely used in DL research: ReLU <ref type="bibr" coords="4,289.66,645.16,15.50,9.69" target="#b17">[22]</ref> and Tanh <ref type="bibr" coords="4,356.17,645.16,15.50,9.69" target="#b18">[23,</ref><ref type="bibr" coords="4,371.68,645.16,11.63,9.69" target="#b19">24]</ref>. The ELU transforms input as in <ref type="bibr" coords="4,185.20,657.11,12.73,9.69" target="#b1">(2)</ref> with its derivate in (3). </p><formula xml:id="formula_2" coords="5,236.61,407.31,243.98,24.04">f (x) = x if x &gt; 0 α(exp(x) -1) if x ≤ 0<label>(2)</label></formula><formula xml:id="formula_3" coords="5,233.81,441.18,246.78,24.04">f (x) = 1 if x &gt; 0 f (x) + α if x ≤ 0<label>(3)</label></formula><p>After the first activation layer we carry forward our data to a next hidden layer for each category, then the respective outputs run once again through the activation cells and are merged into one branch combing all categories together. Then this one block of data in the form of a single vector is fed in the activation cells and the final layer assigning the authors to data.</p><p>The training consists of iterative providing of input data and conditioning the weights and biases of the hidden layers of the network. The output of the last hidden layer is mapped into (0,1) by the softmax function and then we take the natural logarithm of that mapping. In this way we obtain log probabilities for each author. As a loss function (called also cost function) to measure how "bad" the network predicts a correct class we chose the negative log likelihood loss. At that point we compute also gradients for all parameters. The gradients can be thought of as vectors pointing to local minima of the loss function. The intention is to arrive at a global minimum by locally optimising particular parameters <ref type="bibr" coords="5,462.32,633.20,14.62,9.69" target="#b20">[25]</ref>. For network optimisation we employed the Adam algorithm widely used in deep learning research <ref type="bibr" coords="5,211.56,657.11,14.62,9.69" target="#b21">[26]</ref>. The proposed network architecture assumes a multi-branch type of input, where the sets of features described in Section 2 are simultaneously processed by each set-specific branch (e.g a branch for POS-tags, a branch for POS-tags enriched with most frequent lemma etc.). For the purpose of determining feature utility we investigated different combinations of those sets. Due to practical constraints (for n types of sets we have k n n k sub-combinations to investigate for k being the size of the smallest sub-combination) we focused on the most promising ones.</p><p>The task development corpus consisted of ten apart problems. Each problem had either five or twenty different authors, to which a set of anonymous texts of varying length of a couple of hundreds words. Furthermore, the texts in the corpus problems were written in one of five languages (English, French, Polish, Spanish, Italian) and the texts themselves were fragments of web fan fiction originating from different fandom milieus <ref type="bibr" coords="7,316.93,306.04,15.50,9.69" target="#b22">[27,</ref><ref type="bibr" coords="7,332.43,306.04,11.63,9.69" target="#b23">28]</ref>. The objective was to attribute authorship of prose fragments written by the same authors for different fandoms -the idea was to focus on a topic-independent (or more accurately, fandomindependent) method of classification.</p><p>The baseline approach proposed by the organisers was a Linear Support Machine Vectors classification with trigrams of letters -an appropriate Python code was also delivered by the organisers <ref type="bibr" coords="7,318.85,378.83,14.62,9.69">[29]</ref>. The frequency threshold was set on 5 meaning that all character trigrams occurring more than 5 times were taken into account. The training strategy was chosen to be the one-vs.-rest methodfor each class (a particular author from the set of authors) a single classifier was constructed. The final decision is made by comparison of confidence scores of all individual classifiers <ref type="bibr" coords="7,259.46,438.60,14.62,9.69" target="#b24">[30]</ref>. Below in Table <ref type="table" coords="7,358.34,438.60,4.98,9.69" target="#tab_0">1</ref> we present results of this baseline method with default parameters (frequency threshold of 5 and one-vs.rest). Noteworthy is a clear impact of the number of authors in each classification problem across all languages of the task corpus. To start with, we tried out our network just with singular sets with different frequencies:</p><p>1.) 1000, 2000, and 3000 most frequent trigrams of POS-tags enriched with 350 most frequent lemmas.</p><p>2.) 1000, 2000 and 3000 most frequent lemmas.</p><p>3.) 1000, 2000, and 3000 most frequent trigrams of POS-tags.</p><p>Since each element has a single input node assigned, frequencies directly correspond to the size of the input layer influencing the training time of the whole network. For each problem a new model is initialised with the same parameters across all the languages and the problems with the same language do not share their models or features. The input for each specified branch is a simple normalised table with frequencies of corresponding features (trigrams of PoS-tags, unigrams of lemmas etc.). We give an overview of the average F1 score of each combination of parameters in Table <ref type="table" coords="8,296.77,275.41,3.88,9.69" target="#tab_1">2</ref>. A superficial look at results in Table <ref type="table" coords="8,313.63,621.25,4.98,9.69" target="#tab_1">2</ref> reveals a simple and obvious correlation -the more features the network analyses, the better it can perform across different categories. Furthermore, the network fed only with lemmas seems to perform better than as fed with PoS-tags and 350 most frequent lemmas. Since an only size limitation for a neural network is the RAM of a particular machine, we constructed a multi-branch network as described in Figure <ref type="figure" coords="9,419.42,131.95,4.98,9.69" target="#fig_3">4</ref> for different features categories as described in Section 2. The single categories perform, however, significantly worse than the baseline method. All the described networks were trained until an arbitrary loss threshold of 0.01 was not achieved. Because texts in the training data sets were from different domains, we decided to resign from creating a validation subset of texts -this approach has of course numerous weak points, like the risk of over-fitting of the model. In future work on this approach it should be more accurately addressed.</p><p>Since our network can process different types of data, we extended it with 6 branches (unigrams of lemmas, trigrams of PoS-tags, trigrams of PoS-tags enriched with 350 MFLemmas, trigrams of characters, skipgrams of PoS-tags with 350 MFLemmas with the gap size of 1, 2 and 3). For each category we took 3000 most frequent features. During different experimental runs we noticed some significant divergence among results of the runs with similar parameters, so we decided to run our model 10-times with exactly the same parameters. As seen in Table <ref type="table" coords="9,198.94,499.58,3.88,9.69" target="#tab_2">3</ref>, the model does not yield the same scores for subsequent runs, meaning it may be guessing for some of the unknown texts. For the final test run we chose the above combination of features, however due to an unfortunate setting mistake the model was trained just one epoch instead of approximately tens of times. It obviously leaded to an inferior score. The test corpus consisted of 20 problems of the similar ratio of authors and languages.</p><p>In the post-evaluation phase we continued to scrutinise the performance of various parameter settings. The self-evident drawback of our method is the size of the network, since it ties one input neuron to the frequency of each element, making the whole network as wide as a whole frequency table multiplied by the number of feature categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Plans of feature work</head><p>We want to further test our approach and ameliorate the obvious disadvantages, like the size of a multi-branch network. Furthermore, we are convinced of utility of the multi-branch approach combining together different types of linguistic markers (PoS-tags alone, PoS-tags with MFLemmas, skipgrams of different size).</p><p>On the other hand one should think about simplification of layers, e.g. the number of nodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,587.74,345.82,8.83;2,134.77,598.73,345.83,8.74;2,134.77,609.69,95.62,8.74;2,134.77,328.47,345.52,244.29"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Concept of skipgrams with a gap of respectively one and two tokens. The redcoloured gap between selected words can vary to catch frequent tokens/lemmas/PoS surrounding each other.</figDesc><graphic coords="2,134.77,328.47,345.52,244.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,144.99,353.21,325.37,8.83;5,134.98,117.36,345.98,220.58"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic view of a dense layer. Not all connection arrows are depicted.</figDesc><graphic coords="5,134.98,117.36,345.98,220.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,202.39,524.82,210.57,8.83;6,155.95,252.16,308.18,257.75"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Schematic view of the classification network.</figDesc><graphic coords="6,155.95,252.16,308.18,257.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,565.91,345.82,8.83;8,134.77,576.89,345.82,8.74;8,134.77,587.85,184.28,8.74;8,134.77,306.36,345.82,244.56"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The network architecture for three selected feature branches with n features and k authors. The number of the features as well as the number of the branches are adjustable and vary for different experiments.</figDesc><graphic coords="8,134.77,306.36,345.82,244.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,145.47,508.27,324.41,154.58"><head>Table 1 .</head><label>1</label><figDesc>Results for the baseline method with SVM and trigrams of characters.</figDesc><table coords="7,198.37,529.18,215.54,133.67"><row><cell cols="3">Problem name Language Number of authors F1-score</cell></row><row><cell>problem00001 English</cell><cell>20</cell><cell>0.426</cell></row><row><cell>problem00002 English</cell><cell>5</cell><cell>0.588</cell></row><row><cell>problem00003 French</cell><cell>20</cell><cell>0.607</cell></row><row><cell>problem00004 French</cell><cell>5</cell><cell>0.82</cell></row><row><cell>problem00005 Italian</cell><cell>20</cell><cell>0.508</cell></row><row><cell>problem00006 Italian</cell><cell>5</cell><cell>0.517</cell></row><row><cell>problem00007 Polish</cell><cell>20</cell><cell>0.437</cell></row><row><cell>problem00008 Polish</cell><cell>5</cell><cell>0.822</cell></row><row><cell>problem00009 Spanish</cell><cell>20</cell><cell>0.612</cell></row><row><cell>problem00010 Spanish</cell><cell>5</cell><cell>0.636</cell></row><row><cell>Average</cell><cell></cell><cell>0.597</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,187.82,345.83,141.07"><head>Table 2 .</head><label>2</label><figDesc>Results of the initial experiment for different types of input data and for different network sizes.</figDesc><table coords="9,161.23,217.93,292.89,110.96"><row><cell>Type of input data</cell><cell cols="2">Number of features Average F1 score</cell></row><row><cell>lemmas</cell><cell>1000</cell><cell>0.486</cell></row><row><cell>lemmas</cell><cell>2000</cell><cell>0.504</cell></row><row><cell>lemmas</cell><cell>3000</cell><cell>0.504</cell></row><row><cell>POS trigrams</cell><cell>1000</cell><cell>0.302</cell></row><row><cell>POS trigrams</cell><cell>2000</cell><cell>0.316</cell></row><row><cell>POS trigrams</cell><cell>3000</cell><cell>0.302</cell></row><row><cell>POS and 350 MFLemmas trigrams</cell><cell>1000</cell><cell>0.420</cell></row><row><cell>POS and 350 MFLemmas trigrams</cell><cell>2000</cell><cell>0.445</cell></row><row><cell>POS and 350 MFLemmas trigrams</cell><cell>3000</cell><cell>0.473</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,543.49,345.82,74.57"><head>Table 3 .</head><label>3</label><figDesc>F1-scores of repetitive runs of the model with 6 different branches, 3000 features per branch.</figDesc><table coords="9,198.04,575.25,216.21,42.81"><row><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell></row><row><cell>0.544</cell><cell>0.528</cell><cell>0.544</cell><cell>0.503</cell><cell>0.542</cell></row><row><cell>#6</cell><cell>#7</cell><cell>#8</cell><cell>#9</cell><cell>#10</cell></row><row><cell>0.535</cell><cell>0.543</cell><cell>0.544</cell><cell>0.528</cell><cell>0.535</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,381.31,342.23,8.74;10,146.92,392.27,25.61,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,226.43,381.31,119.89,8.74">Cybernetic Predicting Devices</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Ivakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>CCM Information Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,403.54,342.24,8.74;10,146.92,414.50,333.67,8.74;10,146.92,425.46,113.62,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,462.29,403.54,18.30,8.74;10,146.92,414.50,272.61,8.74">IBM PowerAI: Deep Learning Unleashed on IBM Power Systems Servers</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ch</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tsukamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IBM Redbooks</publisher>
			<biblScope unit="page" from="3" to="5" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,436.73,342.24,8.74;10,146.92,447.69,118.06,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m" coord="10,311.41,436.73,57.95,8.74">Deep Learning</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,514.07,342.25,8.74;10,146.92,525.03,333.67,8.74;10,146.92,535.99,327.22,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,309.05,514.07,171.54,8.74;10,146.92,525.03,139.74,8.74">Authorship Attribution with Convolutional Neural Networks and POS-Eliding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rehbein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-4907" />
	</analytic>
	<monogr>
		<title level="m" coord="10,310.05,525.03,170.54,8.74;10,146.92,535.99,35.97,8.74">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2018-05-30">30 May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,547.26,342.25,8.74;10,146.92,558.22,333.67,8.74;10,146.92,569.18,333.67,8.74;10,146.92,580.14,81.13,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,355.47,547.26,125.12,8.74;10,146.92,558.22,167.54,8.74">Convolutional Neural Networks for Authorship Attribution of Short Texts</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,325.31,558.22,155.27,8.74;10,146.92,569.18,286.83,8.74">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,591.41,342.25,8.74;10,146.92,602.37,333.67,8.74;10,146.92,613.33,25.61,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,284.47,591.41,196.13,8.74;10,146.92,602.37,61.27,8.74">Authorship Attribution Using a Neural Network Language Model</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,218.52,602.37,262.08,8.74">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,624.60,342.24,8.74;10,146.92,635.56,168.56,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,227.36,624.60,253.23,8.74;10,146.92,635.56,19.25,8.74">Author Identification using multi-headed Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,176.94,635.56,114.46,8.74">Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,646.84,342.24,8.74;10,146.92,657.80,277.35,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,185.64,646.84,294.95,8.74;10,146.92,657.80,57.53,8.74">Style-markers in authorship attribution: a cross-language study of authorial fingerprint</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,225.35,657.80,113.89,8.74">Studies in Polish Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="99" to="114" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,120.67,337.97,8.74;11,146.92,131.63,333.67,8.74;11,146.92,142.59,25.61,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,234.70,120.67,245.88,8.74;11,146.92,131.63,101.46,8.74">Deeper Delta across genres and languages: do we really need the most frequent words?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rybicki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,264.99,131.63,140.61,8.74">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,153.55,337.97,8.74;11,146.92,164.51,333.67,8.74;11,146.92,175.47,25.61,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,213.01,153.55,218.09,8.74">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,439.29,153.55,41.30,8.74;11,146.92,164.51,248.52,8.74">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,186.42,337.98,8.74;11,146.92,197.38,207.62,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,364.50,186.42,116.10,8.74;11,146.92,197.38,126.25,8.74">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">)</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,208.34,337.98,8.74;11,146.92,219.30,252.73,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,288.73,208.34,191.87,8.74;11,146.92,219.30,170.46,8.74">word2vec Explained: deriving Mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,252.18,337.97,8.74;11,146.92,263.14,333.67,8.74;11,146.92,274.10,168.47,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,212.83,252.18,183.52,8.74">Authorship Attribution Using Text Distortion</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.66,252.18,58.93,8.74;11,146.92,263.14,333.67,8.74;11,146.92,274.10,68.12,8.74">Proceedings of the 15th Conference of the European Chapter of the Association for the Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for the Computational Linguistics<address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<publisher>EACL</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,285.05,97.08,8.74" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hitschler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,296.01,337.97,8.74;11,146.92,306.97,333.67,8.74;11,146.92,317.93,333.67,8.74;11,146.92,328.89,333.67,8.74;11,146.92,339.85,25.61,8.74;11,134.77,350.81,203.61,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,219.67,296.01,260.92,8.74;11,146.92,306.97,42.76,8.74">Function Words in Authorship Attribution. From Black Magic to Theory?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<ptr target="https://spacy.io" />
	</analytic>
	<monogr>
		<title level="m" coord="11,211.55,306.97,269.04,8.74;11,146.92,317.93,333.67,8.74;11,146.92,328.89,233.20,8.74">Proceedings of the Third Computational Linguistics for Literature Workshop, co-located with EACL 2014 -the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Third Computational Linguistics for Literature Workshop, co-located with EACL 2014 -the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-15">2014. 15 Jun 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,372.73,337.97,8.74;11,146.92,383.68,333.67,8.74;11,146.92,394.64,333.67,8.74;11,146.92,405.60,333.67,8.97;11,146.92,416.56,332.78,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,368.68,372.73,111.91,8.74;11,146.92,383.68,333.67,8.74;11,146.92,394.64,77.40,8.74">Deep learning and computational authorship attribution for ancient Greek texts. The case of the Attic Orators</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mambrini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Passarotti</surname></persName>
		</author>
		<ptr target="http://de.digitalclassicist.org/berlin/files/slides/2015-2016/dcsb_kestemont_mambrini_passarotti_20160216.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,235.96,394.64,110.92,8.74">Digital Classicist Seminar</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-16">16 February 2016. 15 Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,427.52,337.97,8.74;11,146.92,438.48,257.49,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,319.51,427.52,161.08,8.74;11,146.92,438.48,115.03,8.74">Fast and accurate deep network learning by Exponential Linear Units</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,269.64,438.48,23.08,8.74">ELUs)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,449.44,337.97,8.74;11,146.92,460.40,333.67,8.74;11,146.92,471.36,258.65,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,248.33,449.44,232.26,8.74;11,146.92,460.40,23.09,8.74">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,323.75,460.40,156.84,8.74;11,146.92,471.36,173.96,8.74">Proceedings of the 27th International Conference on Machine Learning (ICML10)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Furnkranz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,482.32,337.97,8.74;11,146.92,493.27,332.42,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,288.61,482.32,191.98,8.74;11,146.92,493.27,105.53,8.74">Eigenvalues of covariance matrices: Application to neural-network learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,259.39,493.27,95.71,8.74">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2396" to="2399" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,504.23,337.97,8.74;11,146.92,515.19,333.67,8.74;11,146.92,526.15,220.09,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,357.02,504.23,72.16,8.74">Efficient backprop</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,245.04,515.19,148.42,8.74">Neural Networks: Tricks of the Trade</title>
		<title level="s" coord="11,450.22,515.19,30.36,8.74;11,146.92,526.15,108.00,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,537.11,166.23,8.74" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="267" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,548.07,337.98,8.74;11,146.92,559.03,265.33,8.74" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Adam</surname></persName>
		</author>
		<title level="m" coord="11,268.75,548.07,211.85,8.74;11,146.92,559.03,188.57,8.74">A Method for Stochastic Optimization. 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,569.99,337.97,8.74;11,146.92,580.95,25.61,8.74" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hellekson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Busse</surname></persName>
		</author>
		<title level="m" coord="11,249.20,569.99,126.73,8.74">The Fan Fiction Studies Reader</title>
		<imprint>
			<publisher>University of Iowa Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,591.90,337.97,8.74;11,146.92,602.86,25.61,8.74;11,134.77,613.82,305.40,8.97" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="11,191.60,591.90,201.62,8.74">A Companion to Media Fandom and Fan Studies</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Booth</surname></persName>
		</author>
		<ptr target="https://pan.webis.de/clef18/pan18-code/pan18-cdaa-baseline.py" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,624.78,337.97,8.74;11,146.92,635.74,76.36,8.74" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="11,250.74,624.78,174.63,8.74">Pattern Recognition and Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">182</biblScope>
		</imprint>
	</monogr>
	<note>et seqq</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
