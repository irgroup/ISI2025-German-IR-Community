<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.27,115.90,260.83,12.90;1,225.58,133.83,164.19,12.90;1,223.43,154.26,168.50,10.75">A Parallel Hierarchical Attention Network for Style Change Detection Notebook for PAN at CLEF 2018</title>
				<funder ref="#_Kft9FmP">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,228.27,191.24,70.55,8.64"><forename type="first">Marjan</forename><surname>Hosseinia</surname></persName>
							<email>mhosseinia@uh.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.19,191.24,68.89,8.64"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.27,115.90,260.83,12.90;1,225.58,133.83,164.19,12.90;1,223.43,154.26,168.50,10.75">A Parallel Hierarchical Attention Network for Style Change Detection Notebook for PAN at CLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">218458E2C1E3930766775375F64055DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a model for the new problem of style change detection. Given a document, we verify if it contains at least one style change. In other words, the task is to investigate whether it is written by one or multiple authors. The model is composed of two parallel attention networks. Unlike the conventional recurrent neural networks that use the character or word sequences to learn the underlying language model of documents, our model focuses on the hierarchical structure of the language and observes the parse tree features of a sentence using a pre-trained statistical parser. Besides, our model is independent of style change positions although they are given during the training phase. The reason is to have a more applicable approach to the real world problems where such information is not available. PAN 2018 results show that it achieves 82% accuracy and stays at the second rank.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given one document, the problem of style change detection is to find if the writing style of the document has changed. In other words, we investigate whether it is written by multiple authors or one author. This is relatively a new problem in the area of mining writing style of text documents and is similar to the authorship verification and attribution problems where the task is to decide who has written them by comparing how they are written. Similarly, we need to study the writing style of a document by focusing on its linguistic aspects. Actually, the writing style expresses the selection of words and (grammatical) structure of a sentence.</p><p>According to the literature, there are two frequent NLP approaches for document representation. First, the bag-of-words model that is independent of the word order of a sentence and expresses the word selection <ref type="bibr" coords="1,313.55,572.17,10.58,8.64" target="#b4">[5]</ref>. Second, the sequence models such as word embedding that are sensitive to the order of words of a sentence <ref type="bibr" coords="1,425.53,584.13,10.58,8.64" target="#b5">[6]</ref>. Although the English language is recognized to have a latent hierarchical, tree-based structure <ref type="bibr" coords="1,134.77,608.04,10.58,8.64" target="#b0">[1]</ref>, none of the two approaches deploy the hierarchical structure of a sentence for its representation.</p><p>In the problem of style change detection, we consider the latent structure of English language to represent a sentence. We use a context-free grammar parser to predict the tree-based structure, Parse Tree, of a sentence. Then, we extract the ordered features of a parse tree to be used by a parallel hierarchical attention network to find the determinative parts of a sentence and a document. Finally, a fusion layer is used to compare a document with its reverse version to predict the class label. The results show that our model achieves promising results for PAN 2018 dataset.</p><p>The model is described in details in Section 2. In Section 3 we provide the results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parallel Hierarchical Attention Network</head><p>Our model is inspired by the two successful neural network architectures in authorship verification and document classification. Similar to <ref type="bibr" coords="2,344.33,548.84,11.62,8.64" target="#b1">[2]</ref> it has a parallel structure: two columns of recurrent neural networks, one fusion layer, and one softmax layer. However, each column is not a simple RNN anymore but is a hierarchical attention network with two levels of attention mechanism proposed in <ref type="bibr" coords="2,340.78,584.71,10.58,8.64" target="#b7">[8]</ref>. To be for specific, each column includes a Parse Tree Feature (PTF) embedding, a PTF-level LSTM, a PTF-level attention, a PT sentence-level LSTM, and a PT sentence-level attention layer. Here, the key difference is that the LSTM input is not the conventional character/word sequence but is the sequence of Parse Tree Features (PTFs) extracted from the tree-based structure of a sentence. The model architecture is shown in Figure <ref type="figure" coords="2,351.96,644.48,3.74,8.64" target="#fig_0">1</ref>. Each part will be described in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PTF Embedding</head><p>We use Stanford PCFG parser<ref type="foot" coords="3,256.64,355.36,3.49,6.05" target="#foot_0">1</ref> to retrieve the hierarchical structure of a sentence <ref type="bibr" coords="3,466.49,357.03,10.58,8.64" target="#b3">[4]</ref>. Figure <ref type="figure" coords="3,163.40,368.98,4.98,8.64" target="#fig_1">2</ref> shows the underlying parse tree of sentence "Computers defeated chess players in 1980s.". To use this tree structure by the following LSTMs in our model we need to preserve the word sequence of a sentence. We define Parse Tree Feature (PTF) of each word in a sentence as a path starting from the root to the corresponding leaf (word) of its parse tree. The path is a set of all rules of the form parent → child 1 ...child n from root to a leaf (word). Here, punctuation marks are considered as word unigrams. For example, the PTF of "computers" consists of three rules and is [S → NP VP ., NP → NNS, NNS → computers] and PTF of "chess" with four rules is [S → NP VP ., VP → NP, NP → NN, NN → chess] (Figure <ref type="figure" coords="3,308.71,464.62,3.60,8.64" target="#fig_1">2</ref>). We ignore rule ROOT → S because it is a shared rule among all PTFs. Accordingly, the Parse Tree (PT) representation of a sentence is the set of PTF of all its word unigram. For the above example, s has seven PTFs where P T F computer is the first and P T F . is the last feature i.e.,</p><formula xml:id="formula_0" coords="3,134.77,512.12,345.83,36.98">s = [[S → NP VP ., NP → NNS, NNS → computers], ..., [S → ., . → .]] Let d pt = [s i |i ∈ [0, n]] be Parse Tree (PT) representation of document d with n sentences where s = [P T F j |j ∈ [0, l s ]</formula><p>] is PT representation of sentence s with size l s . As we mentioned earlier, we preserve the order of sentences and words according to their occurrence in a document. We define d pt r to be the reverse PT representation of d pt . To make d pt r , first, we reverse the order of sentences of d pt then the order of PTFs of each sentence. In other words, the last PTF of the last sentence of d pt is the first PTF of d pt r . So,</p><formula xml:id="formula_1" coords="3,134.77,597.65,345.83,34.22">d pt r = [s r,i |i ∈ [n, 0]] where s r = [P T F j |j ∈ [l s , 0]] is the reverse of s. In our example, s r = [[S → ., . → .], ..., [S → NP VP ., NP → NNS, NNS → computers]].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LSTMs and Attention Mechanism</head><p>Here, the PT document representation and its reverse (d pt , d pt r ) are the inputs of our model, each for one of the two columns. Later, we will explain why the PT reverse version of a document is one of the two inputs. Each column is a hierarchical attention network proposed in <ref type="bibr" coords="4,218.09,172.16,10.58,8.64" target="#b7">[8]</ref>. It has two layers of LSTM each followed by an attention layer. However, the input of the network is no longer word unigrams but is the PTFs. Besides, we use unidirectional LSTM instead of a bidirectional as we feed the reverse version of a document, almost similar to the backward pass of a bidirectional LSTM, to the second column of the network. and includes a Parse Tree Feature (PTF) embedding, a PTF-level LSTM, a PTF-level attention, a PT sentence-level LSTM, and a PT sentence-level attention layer. Parse Tree Feature Embedding For a sentence s = {P T F 1 , ..., P T F ls } of l s PTFs, we embed PTFs using PTF matrix embedding W that is initialized randomly and learned during the training phase. Here, x i = WP T F i is the PTF embedding of the ith feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PTF-level LSTM and PTF-level Attention</head><p>We choose LSTM in our model that is known to achieve promising results for longterm dependencies while its forget and update gates control the flow of information effectively. Here, h i is the LSTM hidden state after ith feature of sentence s. Although we believe that PTFs express the writing style(s) of a given document, some are more determinative than others in predicting the class label. To highlight the importance of each PTF, we apply the attention mechanism proposed in <ref type="bibr" coords="4,361.86,375.40,11.62,8.64" target="#b7">[8]</ref> to the hidden state of PTFlevel LSTM at step i. This mechanism computes a weight vector α i using a Multi-Layer Perceptron and a softmax function. Here, W pt and b are weight matrix and bias vector respectively, u pt is a random context vector and will be adjusted during the training phase and s pt a is the sum of weighted hidden states of sentence s:</p><formula xml:id="formula_2" coords="4,261.42,441.14,219.17,9.65">u i = tanh(W pt h i + b)<label>(1)</label></formula><formula xml:id="formula_3" coords="4,262.89,459.73,217.70,25.06">α i = exp(u i u pt ) i exp(u i u pt )<label>(2)</label></formula><formula xml:id="formula_4" coords="4,277.50,489.79,203.10,21.98">s pt a = i α i h i<label>(3)</label></formula><p>PT Sentence-level LSTM and PT Sentence-level Attention The sequence of weighted sentences (s pt a ) are the inputs of the PT sentence-level LSTM. Again, we would like to find which part of a document is more important for classification. In other words, we need to know in which sentence the style of the document is changed significantly. So, the PT sentence-level attention layer is applied to the hidden state of the PT sentence-level LSTM <ref type="bibr" coords="4,287.16,576.34,10.58,8.64" target="#b7">[8]</ref>. Similarly, W s and b are weight matrix and bias vector respectively, u s is a random context vector and will be adjusted during the training phase, β j is the weight vector and d pt a is the final weighted document vector: </p><formula xml:id="formula_5" coords="4,260.56,617.88,220.03,12.69">u s j = tanh(W s h s j + b )<label>(4)</label></formula><formula xml:id="formula_6" coords="4,262.34,639.56,218.25,28.34">β j = exp(u s j u s ) j exp(u s j u s )<label>(5)</label></formula><formula xml:id="formula_7" coords="5,179.39,154.33,301.21,101.35">(-γ i [ (a i -b i ) 2 (a i +b i ) ]) Cosine similarity ab T ||a||||b|| Euclidean ( i (ai -bi) 2 ) 0.5 Linear kernel a T b RBF kernel exp(-γ||a -b|| 2 ) Mean of L1 norm n i |a i -b i | n Sigmoid kernel tanh(γa T b + c0) d pt a = j β j h s j<label>(6)</label></formula><p>The reverse version of a document passes the same process through the second column of layers simultaneously. At this step, we have two weighted document vectors, the original (d pt a ) and its reverse version (d pt a,r ), from two parallel columns of layers. Next, we explain how to use the two document vectors for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fusion and Output</head><p>The last and important step is to investigate the style change in a document. In this problem, the number of authors is unknown to us and it is an open set problem with respect to the authors. Indeed, learning writing styles and observing a change may not be applicable solely. So, we need a mechanism independent of the number of authors/writing styles. Here, learning the difference between the two versions of a document is the key to find the existence of a style change. To do so, we use the fusion layer of our previous work <ref type="bibr" coords="5,157.99,415.06,11.62,8.64" target="#b1">[2]</ref> where several similarity functions compute the similarity/difference between a pair of documents in a fully connected neural network layer. The similarity functions are listed in Table1. The weighted document vector and its reverse version d pt a , d pt r,a are compared in the fusion layer to learn the existence of a style change in documents by different authors and with various writing styles:</p><formula xml:id="formula_8" coords="5,260.62,479.56,219.97,12.69">V f = [sim i (d pt a , d pt r,a )] i<label>(7)</label></formula><p>where V f is the similarity vector and sim i belongs to one of the functions in Table1.</p><p>For documents with no style change, it compares the language model of one author between the forward and backward pass. However, for documents by multiple authors, there are two possible cases. In the first case, the order of different writing styles differs in both versions of a document. For example, document d by three authors d = [author 1 , author 2 , author 3 ] and its reverse d r = [author 3 , author 2 , author 1 ].</p><p>In the second case, the order of writing styles is the same in both regular and the reverse version of a document. For example, d = [author 1 , author 2 , author 1 ] and d r = [author 1 , author 2 , author 1 ]. For both cases, the fusion layer compares the language model of multiple authors and the model learns the transition from one writing style to another. However, in the first case, the PT representation of the two documents are much more different than the second case as the order of authors differs in both versions of the document. Finally, the similarity vector V f is given to a softmax function for binary classification. We participate in PAN 2018 style change detection task <ref type="bibr" coords="6,366.11,222.73,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="6,376.90,222.73,7.19,8.64" target="#b2">3]</ref>. The task contains two training and validation sets that are publicly available before the competition and one test set which is not visible to the participants and is used to evaluate the participating models including our parallel attention network. In the training phase, we use the negative log-likelihood as our loss function and RMSprop with learning rate = 1e -03 as the optimizer. We initialize the 100-dimensional PTF embedding from a uniform distribution over [0, 1). The size of the hidden layer of the two LSTMs is 8 with the batch size = 1. We also apply a dropout of 0.3 on the output of the fusion.</p><p>The accuracy and the size of each set are listed in Table2. It shows that our model achieves 82% accuracy on test dataset and 83.78% on the validation set and stays at the second rank in PAN 2018. As there is not much difference (less than 1.4%) between the accuracy of the two sets it indicates that the model is generalized well. Besides, our model is independent of style change positions although they are given during the training phase. The reason is to have a more applicable approach to the real world problems where such information is not available. To see the effect of utilizing PTFs in style change detection and fusion layer we do some experiments on PAN 2018 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PTF vs Word Unigram</head><p>To show that PTFs are effective elements to represent one's style of writing, we train our model using only word unigram features instead of PTFs. We keep all settings intact and take the advantage of Glove pre-trained word vectors<ref type="foot" coords="6,347.25,448.21,3.49,6.05" target="#foot_1">2</ref> as the input of embedding layer. Here, the reverse of a document is created as before and contains the set of reverse sentences from the last to the first sentence of the document. Results show that the accuracy of the validation set<ref type="foot" coords="6,251.12,484.07,3.49,6.05" target="#foot_2">3</ref> is 71%, almost 13% less than PTFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion vs Concatenation</head><p>The effect of the fusion layer that includes several well-known similarity metrics can be addressed with ablation. We replace the fusion layer with a fully connected layer that takes the concatenation of the two weighted document vectors produced from the two columns of attention networks. The accuracy on the validation dataset reduced by 10%.</p><p>The downside of this method is its expensive pre-processing phase that results in producing a huge PTF embedding dimensionality. The size of PTFs of the training set is around 1, 300, 000 compared to 70, 402 word unigram features which is almost 19 times larger. However, this huge dimensionality helps the attention mechanism to find and focus on discriminative features for class label prediction. <ref type="foot" coords="6,387.03,603.62,3.49,6.05" target="#foot_3">4</ref> We believe the model can be improved if we feed the style change positions to the network in an appropriate way. Or instead of using a pre-trained tree structure of a sentence we can learn the latent hierarchical structure of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a model to solve the new problem of style change detection in PAN 2018. We use parse tree features to deploy the hierarchical structure of a sentence and extract them such that the order of the corresponding words will be preserved to be used by a parallel hierarchical attention network. The results show that our model achieves promising results although we do not use the style change positions for training the model and only rely on the raw text of the dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,199.06,375.97,217.24,8.12;2,221.04,115.84,173.28,235.44"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Parallel hierarchical attention network architecture</figDesc><graphic coords="2,221.04,115.84,173.28,235.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,182.95,278.91,249.46,8.12;3,186.74,115.84,241.88,138.38"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. parse tree for "Computers defeated chess players in 1980s."</figDesc><graphic coords="3,186.74,115.84,241.88,138.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,152.92,115.88,309.52,50.62"><head>Table 1 .</head><label>1</label><figDesc>Similarity functions. a, b: document vectors, n: number of features in a and b</figDesc><table coords="5,179.39,140.03,256.58,26.46"><row><cell>Metric</cell><cell>Description</cell><cell>Metric</cell><cell>Description</cell></row><row><cell>Chi2 kernel</cell><cell>exp</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,115.83,256.02,94.17"><head>Table 2 .</head><label>2</label><figDesc>PAN2018 dataset statistics and results</figDesc><table coords="6,134.77,138.50,247.90,71.50"><row><cell></cell><cell cols="3">Train set Validation set Test set</cell></row><row><cell>Size</cell><cell>2980</cell><cell>1492</cell><cell>1352</cell></row><row><cell cols="2">Accuracy 100</cell><cell>83.78</cell><cell>82.47</cell></row><row><cell>3 Results and Analysis</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.08,139.13,7.77"><p>https://stanfordnlp.github.io/CoreNLP/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,623.83,138.71,7.77"><p>https://nlp.stanford.edu/projects/glove/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,634.98,101.51,7.77"><p>the test set was not released.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,645.84,335.87,8.06;6,144.73,657.08,335.86,7.77"><p>Producing PTFs using Stanford standalone parser made it slow and took around 10 hours in PAN evaluation process (test phase). It is much faster if one uses CoreNLP server available at</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">1527364</rs>. We also thank anonymous reviewers for their helpful feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Kft9FmP">
					<idno type="grant-number">1527364</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,364.41,185.95,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,198.06,364.41,65.87,7.77">Syntactic structure</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Mouton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,374.67,332.56,7.77;7,146.47,385.62,229.77,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,257.78,374.67,212.91,7.77;7,146.47,385.62,79.74,7.77">Experiments with neural networks for small and large scale authorship verification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hosseinia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06456</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,138.13,395.88,315.37,7.77;7,146.47,406.84,306.92,7.77;7,146.47,417.80,327.16,7.77;7,146.47,428.76,298.91,7.77;7,146.47,439.72,222.76,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,194.80,406.84,258.59,7.77;7,146.47,417.80,183.40,7.77">Overview of the Author Identification Task at PAN-2018: Cross-domain Authorship Attribution and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschugnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,208.87,428.76,236.52,7.77;7,146.47,439.72,81.11,7.77">Working Notes Papers of the CLEF 2018 Evaluation Labs. CEUR Workshop Proceedings</title>
		<title level="s" coord="7,233.66,439.72,59.13,7.77">CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,449.97,342.15,7.77;7,146.47,460.93,225.39,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,242.60,449.97,109.58,7.77">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,370.20,449.97,110.07,7.77;7,146.47,460.93,199.24,7.77">Proceedings of the 41st annual meeting of the association for computational linguistics</title>
		<meeting>the 41st annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,471.19,329.17,7.77;7,146.47,482.15,315.10,7.77;7,146.47,493.11,67.98,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,256.10,471.19,211.20,7.77;7,146.47,482.15,232.22,7.77">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,385.07,482.15,76.50,7.77">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,503.36,334.77,7.77;7,146.47,514.32,120.44,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,192.82,503.36,190.02,7.77">Statistical language models based on neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04-02">2nd April (2012</date>
			<pubPlace>Mountain View</pubPlace>
		</imprint>
	</monogr>
	<note>Presentation at Google</note>
</biblStruct>

<biblStruct coords="7,138.13,524.58,330.46,7.77;7,146.47,535.54,288.71,7.77;7,146.47,546.49,329.36,7.77;7,146.47,557.45,322.61,7.77;7,146.47,568.41,321.53,7.77;7,146.47,579.37,144.80,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,161.42,535.54,273.76,7.77;7,146.47,546.49,42.23,7.77">Overview of PAN-2018: Author Identification, Author Profiling, and Author Obfuscation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,271.48,557.45,197.60,7.77;7,146.47,568.41,282.97,7.77">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 9th International Conference of the CLEF Initiative (CLEF 18)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bellot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,589.63,332.35,7.77;7,146.47,600.59,327.95,7.77;7,146.47,611.55,327.11,7.77;7,146.47,622.50,79.94,7.77;7,144.73,646.13,335.86,7.77;7,144.73,657.08,203.14,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,357.52,589.63,112.96,7.77;7,146.47,600.59,96.09,7.77">Hierarchical attention networks for document classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="https://stanfordnlp.github.io/CoreNLP/corenlp-server.html.However" />
	</analytic>
	<monogr>
		<title level="m" coord="7,260.28,600.59,214.14,7.77;7,146.47,611.55,323.27,7.77">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>, we were not allowed to use any external resources during PAN evaluation phase</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
