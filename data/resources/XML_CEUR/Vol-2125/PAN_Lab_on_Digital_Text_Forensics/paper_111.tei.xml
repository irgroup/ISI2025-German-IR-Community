<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.99,115.90,301.38,12.90;1,285.76,133.83,43.83,12.90;1,223.43,153.68,168.50,10.75">Stacked Gender Prediction from Tweet Texts and Images Notebook for PAN at CLEF 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.27,190.08,71.82,8.64;1,213.09,188.18,1.36,6.12"><forename type="first">Giovanni</forename><surname>Ciccone</surname></persName>
							<email>giovanni.ciccone.1994@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.65,190.08,54.52,8.64;1,277.17,188.18,1.46,6.12"><forename type="first">Arthur</forename><surname>Sultan</surname></persName>
							<email>arthur.sultan@insa-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.26,190.08,48.41,8.64;1,345.67,188.18,1.36,6.12"><forename type="first">Léa</forename><surname>Laporte</surname></persName>
							<email>lea.laporte@insa-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.23,190.08,90.49,8.64;1,445.72,188.18,1.36,6.12"><forename type="first">Előd</forename><surname>Egyed-Zsigmond</surname></persName>
							<email>elod.egyed-zsigmond@insa-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,455.28,190.08,18.81,8.64;1,228.01,202.03,40.95,8.64;1,268.96,200.14,1.46,6.12"><forename type="first">Alaa</forename><surname>Alhamzeh</surname></persName>
							<email>alaa.alhamzeh@insa-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.93,202.03,72.76,8.64;1,378.68,200.14,1.36,6.12"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
							<email>michael.granitzer@uni-passau.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon -LIRIS UMR5205</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität</orgName>
								<address>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.99,115.90,301.38,12.90;1,285.76,133.83,43.83,12.90;1,223.43,153.68,168.50,10.75">Stacked Gender Prediction from Tweet Texts and Images Notebook for PAN at CLEF 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD09637FB5748C7B64E53F24599E1D78</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation at the PAN 2018 Author Profiling shared task. Given texts and images from some Twitter's authors, the goal is to estimate their genders. We considered all the languages (Arabic, English and Spanish) and all the prediction types (only from texts, only from images and combined). The final submitted system is a stacked classifier composed of two main parts. The first one, based on previous PAN Author Profiling editions, concerns gender prediction from texts. It consists in a pipeline of preprocessing, word n-grams from 1 to 2, TF-IDF with sublinear weighting, Linear Support Vector classification and probability calibration. The second part is formed by different layers of classifiers used for gender estimation from images: four base classifiers (object detection, face recognition, colour histograms, local binary patterns) in the first layer, a meta classifier in the second layer and an aggregation classifier as third layer. Finally, the two gender predictions, from texts and images, feed into the last layer classifier that provides the combined gender predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prediction of the gender of an author is part of a more general task called author profiling. Author profiling aims at predicting the characteristics of an author (age, gender, social background, etc...) based on content produced by the author. Author profiling is useful for marketing intelligence, to analyze customers characteristics <ref type="bibr" coords="1,422.61,524.93,16.60,8.64" target="#b9">[10]</ref>  <ref type="bibr" coords="1,441.47,524.93,16.60,8.64" target="#b11">[12]</ref> . Author profiling can also be used in forensics, in order to set up a suspect profile from a threat or sexual harassment document <ref type="bibr" coords="1,288.14,548.84,10.58,8.64" target="#b4">[5]</ref>. For example, in 2001, Roger Shuy analyzed a ransom note which led to the arrest of its author <ref type="bibr" coords="1,346.30,560.80,10.58,8.64" target="#b8">[9]</ref>. Another possible application for author profiling is in the field of security, for example to detect emails written by terrorists, from an established standard profile <ref type="bibr" coords="1,320.43,584.71,10.58,8.64" target="#b6">[7]</ref>.</p><p>Author profiling from tweets has been studied since at least 2013, through research tasks proposed by the PAN annual challenge <ref type="bibr" coords="1,318.59,608.62,15.27,8.64" target="#b10">[11]</ref>. However, until now, the prediction was based only on text taken from social medias, while this year, images were added to the available data. The objective of the 2018 Pan author profiling shared task and of our approach is thus to study if and how text and images taken from tweets can be used to predict the gender of the authors of those tweets <ref type="bibr" coords="1,328.72,656.44,15.27,8.64" target="#b12">[13]</ref>.</p><p>To do so, we built two independent classifiers, with for each, as output, the probability of an author to belong to the "male" or to the "female" class. The first classifier uses textual features whereas the second classifier uses image-based features. Finally, a meta-classifier combines the prediction of the text-based classifier and of the imagebased classifier in order to provide a prediction based on the combination of the textual and image-based features.</p><p>This paper is structured as follows. In section 2, we present consecutively the functioning of our text-based, image-based and meta classifiers. Section 3 deals with the results of our approach, obtained on the PAN 2018 author profiling test dataset. Finally, we draw the conclusion of our work in section 4.</p><p>2 Overview of Our Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gender Prediction from Tweet Texts</head><p>The requirement of predicting users' gender from textual Twitter data was proposed also in PAN 2017 edition of Author Profiling shared task so we based our work on the analyses performed by previous edition participants <ref type="bibr" coords="2,360.60,325.22,15.27,8.64" target="#b13">[14]</ref>. The idea is to grab hints from their works in order to reach similar results as quick as possible and devote the remaining part of the available time for handling the novelty of this year's challenge that concerns gender prediction from images. Regarding the text sub-task, we mainly based our work on the papers written by two teams of PAN 2017 Author Profiling task: the winner <ref type="bibr" coords="2,179.59,385.00,11.62,8.64" target="#b2">[3]</ref> and our research team LIRIS <ref type="bibr" coords="2,311.27,385.00,10.58,8.64" target="#b7">[8]</ref>.</p><p>The approach used by us in PAN 2018 challenge for gender prediction from tweets texts consists in a pipeline formed by: text preprocessing, n-gram Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF) weighting, Linear Support Vector Classification (LinearSVC) and probability calibration (CalibratedClassifierCV). Figure <ref type="figure" coords="2,163.18,444.77,4.98,8.64">1</ref> shows the proposed method. In the following we present in detail the different steps.</p><p>Preprocessing 's goal is to filter useless data in order to obtain a smaller dataset and consequently reduce resource usage and computation time. Firstly, we analyzed the provided data for having some hints regarding which preprocessing actions to implement and how. Table <ref type="table" coords="2,196.41,522.04,4.98,8.64">1</ref> describes counts of interesting preprocessing characteristics regarding the training dataset.</p><p>The provided dataset has two kinds of difficulties:</p><p>the first one is the presence of three very different languages (Arabic, English and Spanish). They differ for alphabet, syntax and grammar rules. the second issue is related to tweets nature, the shortness (maximum 140 characters), they often contain hashtags, user mentions, URLs, slang expressions, misspelled words, poor grammar.</p><p>We did some prototypes by using Python nltk tweets tokenizer, called TweetTokenizer. For Arabic language, we discovered that it has some difficulties in handling diacritics, that are a kind of accents used in Arabic language. Specifically, the tokenizer,   <ref type="table" coords="3,249.00,625.75,3.36,8.06">1</ref>. Preprocessing characteristics counts when finding a diacritic, splits the word in three tokens: the part before the diacritic, the diacritic itself and the part after. This behaviour leads to worse results therefore we implemented a script for Arabic text normalization and tokenization by taking into account this issue. For what concern preprocessing tweets features we did some considerations about URLs ('http://...') and user mentions (@user). The point is that they don't carry on information that can be used for inferring the author's gender, therefore we decided to filter them.</p><p>Several techniques have been proposed in literature for tweets preprocessing before further information extraction <ref type="bibr" coords="4,256.70,215.94,15.27,8.64" target="#b15">[16]</ref>. Considering those approaches and our analyses on the training dataset we decided to use the preprocessing architecture shown in figure <ref type="figure" coords="4,134.77,239.85,3.74,8.64" target="#fig_1">2</ref>. Independently from the language, we apply the HTML unescaping and filtering of URLs and user mentions. Since they are not correlated to author's gender and language, this operation can be done at the beginning independently from the language. Afterwards, for English and Spanish texts, the following actions are performed: removal of punctuation, repeating characters and stopwords. These operations are applied also to Arabic corpus in addition to textual normalization and diacritics removal. Feature extraction: This stage converts text data to vectors of floats representing the scores of tokens within each document. One document contains all the 100 tweets for a certain author. We considered the representation of tweets based on n-grams and TF-IDF. N-gram BOW model consists in representing a text as a multi-set (bag) of its tokens (for instance words) ignoring the grammar and the order of the words but taking in account only the multiplicity. TF-IDF is a well-known technique used in Information Retrieval that produces scores depending on the number of token occurrences within a document and on the number of distinct documents containing the tokens. TF-IDF simplifies learning algorithms in selecting more discriminative words. This technique has been widely used for the Author Profiling task. Our implementation relies on CountVectorizer and TfidfTransformer libraries of Python sklearn. For BoW we tested 2 methods:</p><p>1. the approach used by PAN 2017 winner that consists in a combination of character n-grams from 3 to 5 and word n-grams from 1 to 2 2. word n-grams from 1 to 2 According to our experiments, approach 1 gave more or less the same accuracy results as approach 2 but having the disadvantage of bigger data structures because of the great number of character based n-grams. CountVectorizer parameters analyzer and ngram_range are used for specifying the level of tokens (word level) and the ngrams range (from 1 to 2, that correspond to uni-grams and bi-grams), min_df = 2 means that all the tokens appearing only once are not considered and this causes a matrix dimensions reduction. The tokenizer is a reference to the Python method used for tokenizing the text, it can be either the nltk default one or a user defined method. In our case we defined one tokenizer per language reflecting the requirements specified in the figure <ref type="figure" coords="5,187.75,411.79,3.74,8.64" target="#fig_1">2</ref>. Concerning TfidfTransformer, sublinear_tf weighting conducted to an overall quality improving for each language corpus (around 2% in terms of average macro f score). Sublinear term frequency scaling means to replace tf with 1 + log(tf ) in TF-IDF formula.</p><p>Machine learning algorithms Regarding the classifier, we tried the most commonly ones in PAN past editions, that are Support Vector Machine, Random Forrest, Naive Bayes. As asserted by past teams, the best one for gender classification is Support Vector Machine. More precisely, the linear approach (LinearSVC) allows to reach better outcomes than the kernel approach, therefore we decided to use LinearSVC. However, it has the disadvantage of providing only the output labels without the associated probabilities, but in PAN 2018 scenario in which there are two different sub-tasks about texts and images it would be better to have the intermediate outputs (from texts and images) with the corresponding probabilities, this makes easier their combination for obtaining the final results. For this goal of probability calibration we used a CalibratedClassi-fierCV in cascade to LinearSVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gender Prediction from Tweet Images</head><p>Our classifier based on images is composed of 3 layers of classifiers. The first layer is composed of classifiers which we will call "low classifiers". Each low classifier is based on one only type of image feature and outputs the probability that the input image was posted by a male or a female. The second layer is composed of a meta-classifier which combines the prediction of the low classifiers, in order to provide an improved prediction, based on the classifier stacking principle <ref type="bibr" coords="6,341.29,155.18,10.58,8.64" target="#b5">[6]</ref>. Finally, the last layer is another meta-classifier which combines the predictions from the second layer, given from the 10 images associated to the author we try to predict the gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) Low Classifiers (Layer 1)</head><p>The first layer is composed of 4 independent classifiers. Each of them performs the prediction of the gender of the author of the input image, based on specific image representations. The 4 kind of methods we used are listed below.</p><p>-Object recognition: Images are represented based on the objects they contain, detected using an object recognition algorithm. The object recognition task is performed with the library YOLO <ref type="bibr" coords="6,280.85,296.02,10.58,8.64" target="#b3">[4]</ref>, with a confidence threshold of 0.2. We think that an increase the confidence threshold should give better prediction results, but we did not have enough time to study this phenomenon. The feature vector V object resulting from the object recognition task is such as:</p><formula xml:id="formula_0" coords="6,235.18,350.92,161.94,9.65">V object = {O 1 : I 1 , O 2 : I 2 , ..., O i : I i },</formula><p>with O i an object identified in the image and I i the "importance" associated to that label. The importance I of an object O is defined such as:</p><formula xml:id="formula_1" coords="6,262.92,398.82,106.46,30.20">I = n z=0 conf idence(O z ),</formula><p>with confidence(O z ), the confidence associated to the z th recognition of the object O by YOLO.</p><p>For example, for an image containing two cats and one person, the output of the object recognition task done by YOLO could be: [{ label: "cat", confidence: 0.8 }, { label: "cat", confidence: 0.6 }, { label: "person", confidence: 0.9 }]. The V object resulting feature vector would then be: V object = { "cat": 1.4, "person": 0.9 }. One important note, is that we did not spend a lot of time trying to find a good model for the computation of the importance I of an object O. This computation could hence certainly be improved in order to achieve better prediction results.</p><p>-Facial recognition: Images are represented by two features, respectively the number of men and women detected in the image. We used a neural network which was pre-trained to detect the gender of people in an image, based on their faces only <ref type="bibr" coords="6,151.70,593.82,15.27,8.64" target="#b16">[17]</ref>. We thus count the number of male and female faces identified in the input image. The resulting V face vector is such as:</p><p>V f ace = {M ale : x, F emale : y }, with x and y respectively the number of male and female faces identified in the image by the pre-trained network.</p><p>From a rough hand-made evaluation made on 500 images from the training dataset, in the recognition of male and female faces in the image, the pre-trained network performs a global accuracy of 96% , a recall of 50% for male faces and of 50% for female faces.</p><p>-Color histogram: Images are represented by a standard color histogram. The resulting V color vector is the 'flattened' version of the color histogram (i.e dim( V color =768)).</p><p>-Local binary patterns: Computation of a standard vector of local binary patterns, for 24 points and a radius of 8. To compute this vector, we used the skimage library <ref type="bibr" coords="7,151.70,249.97,10.58,8.64" target="#b0">[1]</ref>. The result is a vector V LBP such as dim(V LBP )=26.</p><p>Each low classifier was trained on 56% of the training dataset (42000 images). The images from the 3 language (arabic, english, spanish), where grouped together for the training: in the 42000 images, there were 8400 images from the arabic folder, 16800 from the english folder and 16800 from the spanish folder. We evaluated the performance of each of the 4 classifiers on the 56% of the training data mentioned above, thanks to a 20-fold cross-validation process. The results are shown in the following array (the classifiers in the "Classifier type" column are those from the sklearn [2] library): The second layer is composed of one only classifier called "meta-classifier". This meta-classifier takes as input the outputs of the low classifiers of the layer 1, i.e for each low classifier, the probability estimated by this low classifier that the analyzed image was posted by a male or a female. The meta-classifier thus aggregates the results of the first layer in order to provide an improved prediction of the gender of the author of the analyzed image, based on the idea of classifier stacking <ref type="bibr" coords="7,357.77,620.57,10.58,8.64" target="#b5">[6]</ref>.</p><p>This meta-classifier was trained on 16% of the training dataset (12000 images). The images from the 3 language (arabic, english, spanish), where grouped together for the training: in the 12000 images, there was 2400 images from the arabic folder, 4800 from the english folder and 4800 from the spanish folder. We evaluated the performance of the meta-classifier on the 16% of the training data mentioned above, thanks to a 20-fold cross-validation process. The results are a mean accuracy of 58.4%, a standard deviation of 2.2%, for a LinearSVC classifier with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Aggregation Classifier (Layer 3)</head><p>The third layer is composed of one only classifier called the "aggregation classifier". As a reminder, for each author of the training or the evaluation dataset, 10 images are associated to this author. The aggregation classifier takes as input the 10 probabilities that the author is a male or a female, given by the second layer on the 10 images associated to the author. The aim of the aggregation classifier is thus to predict the gender of the author, based on the whole set of genders predicted from the analysis of the 10 images associated to this author.</p><p>The aggregation classifier was trained on 8% of the training dataset (600 images). The images from the 3 language (arabic, english, spanish), where grouped together for the training: in the 600 images, there was 120 images from the arabic folder, 240 from the english folder and 240 from the spanish folder. We evaluated the performance of the aggregation classifier on the 8% of the training data mentioned above, thanks to a 20-fold cross-validation process. The results are a mean accuracy of 69.8%, a standard deviation of 7.7%, for a MultinomialNB classifier with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gender Prediction from both Texts and Images</head><p>Gender prediction from both text and images is done by a classifier we call the "final classifier". This classifier takes as input the outputs of the text and image classifiers (for images, the output of the third layer is used). The aim of the final classifier is hence to combine the gender prediction of the author based on the text associated to the author, and the gender prediction based on the image associated to the author, in order to output a final improved prediction, based on the classifier stacking idea <ref type="bibr" coords="8,393.27,489.06,10.58,8.64" target="#b5">[6]</ref>. The two inputs of this final classifier coming from the text and image classifiers are both probabilities.</p><p>The final classifier was trained on 20% of the training dataset (1500 authors): 300 arabic authors, 600 english authors and 600 spanish authors. The machine learning algorithm of the final classifier is LinearSVC with default parameters. We evaluated the performance of the final classifier on the 20% of the training data mentioned above, thanks to a 20-fold cross-validation process. We performed the same evaluation for the text classifier and the image classifier (i.e the "aggregation classifier"), but respectively on 80% and 8% of the training data. The results are shown in the following array:</p><p>From this table, we can say that our initial estimations of our classifiers performance, based on the training data, does not allow to conclude that our combined approach improved the prediction score. Indeed, the performance of the final classifier did not give a better prediction result than the prediction of the text classifier. Text related results are very close to the state of the art, represented by <ref type="bibr" coords="9,449.14,392.35,10.58,8.64" target="#b2">[3]</ref>. For images, we cannot compare with previous PAN editions because this is the first time in which images requirement is proposed. Those results stick are consistent with the evaluation of our classifiers on the training data, shown in the table 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Concerning the gender prediction based on text, the proposed method is a pipeline composed of text preprocessing, n-gram BoW, TF-IDF, Linear Support Vector Classification and probability calibration. Our implementation relies on results presented in <ref type="bibr" coords="9,443.61,512.52,15.26,8.64" target="#b13">[14]</ref>. Our goal was to reach the state of the art level as soon as possible in order to dedicate the remaining part of the time for approaching the novelty of this edition, that is the gender prediction based on images. Specifically, we based our text related work on papers <ref type="bibr" coords="9,459.01,548.38,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="9,469.80,548.38,7.19,8.64" target="#b7">8]</ref>, we focused mainly on the preprocessing step by implementing different tokenizers depending on the languages, one of the most noteworthy points is the diacritics handling for Arabic language. Our final score is around 80% and it is pretty near to the best result of previous PAN edition (82.53% by Basile et al <ref type="bibr" coords="9,329.51,596.20,10.45,8.64" target="#b2">[3]</ref>).</p><p>Regarding the gender prediction based on images only, we can conclude that our overall approach provides significant results, with an accuracy around 70%. Among the image-based features used, the most effective seems to be the "face recognition" feature. Regarding the "meta-classifier", we cannot conclude that stacking low classifiers were more efficient, since our meta-classifier only improves the prediction score of the classifier based on the face recognition features by 1.5%, with a standard deviation of 2.2% given by the cross-validation process. However, our approach of combining the 10 images associated to an author seems to be efficient, with an improvement of the accuracy around 11% compared to the accuracy given by a classifier (here the "meta-classifier") based on a single image.</p><p>To improve the prediction based on images only, one could add new image-based features, such as character recognition. Indeed, some images are photos or screenshots of text (for example a screenshot of a tweet). Another possibility would be to train one classifier for each language. Indeed, as mentioned in section 2.2, we grouped images from all languages during the training, but it is possible that cultural specificities exist among images of one language, which might be useful to predict the gender of an author of this language. Another possibility to improve the prediction based on images would be to improve the object detection process by using pre-trained network trained to detect more object classes (YOLO <ref type="bibr" coords="10,284.42,287.23,11.62,8.64" target="#b3">[4]</ref> can only detect 80 object classes).</p><p>Regarding the gender prediction based on the combination of text and images, we can conclude that our approach inspired of classifier stacking <ref type="bibr" coords="10,393.93,323.65,11.62,8.64" target="#b5">[6]</ref> does not seem to be efficient. Indeed, only a slight improvement of the prediction with the combined approach can be noted, compared to the text approach only, with an average increase of 0.43%, which is not significant enough but matches with the improvement of 0.48% obtained by Sakaki et al. in a similar work <ref type="bibr" coords="10,305.67,371.47,15.27,8.64" target="#b14">[15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,189.28,376.42,234.55,8.12;3,250.98,134.92,113.39,226.77"><head>Figure</head><label></label><figDesc>Figure 1. Proposed method for gender profiling from tweets texts</figDesc><graphic coords="3,250.98,134.92,113.39,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,216.13,631.13,183.09,8.12;4,165.95,332.91,283.47,283.48"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Text preprocessing proposed architecture</figDesc><graphic coords="4,165.95,332.91,283.47,283.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,216.02,376.42,207.81,257.38"><head></head><label></label><figDesc>1. Proposed method for gender profiling from tweets texts</figDesc><table coords="3,225.66,432.92,150.12,200.89"><row><cell>Name</cell><cell cols="2">Language Number</cell></row><row><cell>URLs</cell><cell cols="2">Arabic 44556</cell></row><row><cell>URLs</cell><cell cols="2">English 138128</cell></row><row><cell>URLs</cell><cell cols="2">Spanish 118957</cell></row><row><cell cols="3">users' mentions Arabic 66917</cell></row><row><cell cols="3">users' mentions English 238599</cell></row><row><cell cols="3">users' mentions Spanish 217703</cell></row><row><cell cols="3">punctuation signs Arabic 1039379</cell></row><row><cell cols="3">punctuation signs English 1826354</cell></row><row><cell cols="3">punctuation signs Spanish 1549798</cell></row><row><cell>stopwords</cell><cell cols="2">Arabic 237753</cell></row><row><cell>stopwords</cell><cell cols="2">English 1254179</cell></row><row><cell>stopwords</cell><cell cols="2">Spanish 1367597</cell></row><row><cell>diacritics</cell><cell cols="2">Arabic 120742</cell></row><row><cell>diacritics</cell><cell>English</cell><cell>-</cell></row><row><cell>diacritics</cell><cell>Spanish</cell><cell>-</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,392.15,339.81,153.30"><head>Table 2 .</head><label>2</label><figDesc>Low classifiers prediction results (on training data)</figDesc><table coords="7,134.77,417.87,339.81,127.58"><row><cell></cell><cell cols="2">Mean accuracy Standard deviation</cell><cell>Classifier type</cell></row><row><cell>Object detection</cell><cell>53.3%</cell><cell>1.3%</cell><cell>Linear SVC with default parameters</cell></row><row><cell>Face recognition</cell><cell>56.9%</cell><cell>1.2%</cell><cell>Linear SVC with default parameters</cell></row><row><cell>Color histogram</cell><cell>51.6%</cell><cell>1.6%</cell><cell>MultinomialNB with default parameters</cell></row><row><cell>LBP</cell><cell>53.1%</cell><cell>1.3%</cell><cell>Linear SVC with default parameters</cell></row><row><cell cols="2">b) Meta-Classifier (Layer 2)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,118.07,345.82,224.49"><head>Table 3 .</head><label>3</label><figDesc>Text, Image and final classifiers prediction results (on training data)3 Results on the Evaluation DatasetThe official results we obtained are shown in table4. As you can see, we got about 80% precision for gender prediction only from texts and approximately 70% for image based estimation. Concerning the combined approach we obtained scores slightly better than the ones about texts.</figDesc><table coords="9,166.95,118.07,281.46,224.49"><row><cell></cell><cell></cell><cell cols="2">Mean accuracy Standard deviation</cell></row><row><cell></cell><cell>Text classifier</cell><cell>80.5%</cell><cell>3.9%</cell></row><row><cell></cell><cell>Image classifier</cell><cell>69.8%</cell><cell>7.7%</cell></row><row><cell></cell><cell>Final classifier</cell><cell>80.1%</cell><cell>4.9%</cell></row><row><cell cols="4">Language Accuracy (only text) Accuracy (only images) Accuracy (combined)</cell></row><row><cell>Arabic</cell><cell>0.7910</cell><cell>0.7010</cell><cell>0.7940</cell></row><row><cell>English</cell><cell>0.8074</cell><cell>0.6963</cell><cell>0.8132</cell></row><row><cell>Spanish</cell><cell>0.7959</cell><cell>0.6805</cell><cell>0.8000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,196.42,345.29,220.29,8.12"><head>Table 4 .</head><label>4</label><figDesc>Official results for the PAN'18 Author Profiling task</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.61,432.41,126.49,7.77" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Skimage</surname></persName>
		</author>
		<ptr target="http://scikit-image.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,443.92,141.92,7.77" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Sklearn</surname></persName>
		</author>
		<ptr target="http://scikit-learn.org/stable/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,455.43,324.28,7.77;10,150.95,466.39,304.61,7.77;10,150.95,477.35,108.85,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,428.04,455.43,38.85,7.77;10,150.95,466.39,243.83,7.77">N-GRAM: New groningen author-profiling model: Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rawee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.65,466.39,23.91,7.77;10,150.95,477.35,82.71,7.77">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,488.85,193.36,7.77" xml:id="b3">
	<monogr>
		<ptr target="https://pjreddie.com/darknet/yolo/" />
		<title level="m" coord="10,150.96,488.85,55.11,7.77">Darknet</title>
		<imprint/>
	</monogr>
	<note>YOLO</note>
</biblStruct>

<biblStruct coords="10,142.61,500.36,308.86,7.77;10,150.95,511.32,239.47,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,333.43,500.36,118.04,7.77;10,150.95,511.32,80.72,7.77">Mining e-mail content for author identification forensics</title>
		<author>
			<persName coords=""><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mohay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,237.37,511.32,78.35,7.77">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,522.83,336.49,7.77;10,150.95,533.79,165.84,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,239.77,522.83,239.33,7.77;10,150.95,533.79,16.93,7.77">Is combining classifiers with stacking better than selecting the best one?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ženko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,170.12,533.79,63.00,7.77">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="273" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,545.30,219.10,7.77;10,150.95,556.26,328.62,7.77;10,150.95,567.21,115.54,7.77" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">N R</forename><surname>Fund</surname></persName>
		</author>
		<ptr target="https://www.prhlt.upv.es/wp/project/2017/arabic-author-profiling-for-cyber-security" />
		<title level="m" coord="10,204.26,545.30,153.66,7.77">Arabic Author Profiling for Cyber-Security</title>
		<imprint>
			<date type="published" when="2017-05-25">2017. 25 may 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,578.72,310.79,7.77;10,150.95,589.68,305.49,7.77;10,150.95,600.64,108.85,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,290.04,578.72,163.37,7.77;10,150.95,589.68,244.72,7.77">INSA Lyon and UNI passau&apos;s participation at PAN@CLEF&apos;17: Author Profiling task: Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Laporte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.53,589.68,23.91,7.77;10,150.95,600.64,82.71,7.77">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,612.15,333.75,7.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Mcmenamin</surname></persName>
		</author>
		<title level="m" coord="10,223.42,612.15,182.88,7.77">Forensic linguistics: Advances in forensic stylistics</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,623.66,332.33,7.77;10,150.95,634.62,318.53,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,268.00,623.66,166.00,7.77">Predicting movie sales from blogger sentiment</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,452.16,623.66,22.41,7.77;10,150.95,634.62,241.30,7.77">AAAI spring symposium: computational approaches to analyzing weblogs</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="155" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,646.13,131.35,7.77;10,150.95,657.08,242.89,7.77" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,172.80,646.13,78.38,7.77">PAN Author Profiling</title>
		<author>
			<persName coords=""><surname>Pan</surname></persName>
		</author>
		<ptr target="https://pan.webis.de/clef13/pan13-web/author-profiling.html" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,119.96,316.23,7.77;11,150.95,130.92,313.52,7.77;11,150.95,141.88,23.90,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,285.38,119.96,134.11,7.77">Author profiling for vietnamese blogs</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,437.55,119.96,20.92,7.77;11,150.95,130.92,74.95,7.77;11,254.30,130.92,125.75,7.77">IALP&apos;09. International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="190" to="194" />
		</imprint>
	</monogr>
	<note>Asian Language Processing</note>
</biblStruct>

<biblStruct coords="11,142.24,152.84,324.70,7.77;11,150.95,163.80,303.58,7.77;11,150.95,174.76,317.64,7.77;11,150.95,185.71,308.71,7.77;11,150.95,196.67,20.92,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,395.33,152.84,71.61,7.77;11,150.95,163.80,287.89,7.77">Overview of the 6th Author Profiling Task at PAN 2018: Multimodal Gender Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,341.19,174.76,127.40,7.77;11,150.95,185.71,190.22,7.77">Working Notes Papers of the CLEF 2018 Evaluation Labs. CEUR Workshop Proceedings</title>
		<title level="s" coord="11,347.26,185.71,59.13,7.77">CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,207.63,325.95,7.77;11,150.95,218.59,311.30,7.77;11,150.95,229.55,69.97,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,312.90,207.63,155.29,7.77;11,150.95,218.59,230.58,7.77">Overview of the 5th author profiling task at PAN 2017: Gender and language variety identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,399.47,218.59,62.79,7.77;11,150.95,229.55,43.83,7.77">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,240.51,334.39,7.77;11,150.95,251.47,327.37,7.77;11,150.95,262.43,142.66,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,349.86,240.51,126.77,7.77;11,150.95,251.47,170.25,7.77">Twitter user gender inference using combined analysis of text and image processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,339.34,251.47,138.99,7.77;11,150.95,262.43,74.22,7.77">Proceedings of the Third Workshop on Vision and Language</title>
		<meeting>the Third Workshop on Vision and Language</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,273.39,331.03,7.77;11,150.95,284.34,317.89,7.77;11,150.95,295.30,316.45,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,186.83,284.34,180.39,7.77">Overview of the 3rd Author Profiling Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stammatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,391.88,284.34,76.96,7.77;11,150.95,295.30,38.13,7.77">CLEF 2015 Labs and Workshops</title>
		<title level="s" coord="11,195.56,295.30,174.72,7.77">Notebook Papers. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">1391</biblScope>
			<biblScope unit="page" from="898" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,306.26,302.43,7.77" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Won</surname></persName>
		</author>
		<ptr target="https://github.com/wondonghyeon/face-classification" />
		<title level="m" coord="11,185.60,306.26,63.54,7.77">face-classification</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
