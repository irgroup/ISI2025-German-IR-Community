<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.70,152.67,303.68,12.64;1,270.05,170.67,55.23,12.64">ERTIM@MC2: Diversified Argumentative Tweets Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.42,209.70,59.04,8.96"><forename type="first">Kévin</forename><surname>Deturck</surname></persName>
							<email>kevin.deturck@viseo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">INaLCO ERTIM</orgName>
								<address>
									<postCode>75007</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Viseo Innovation</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.77,209.70,80.23,8.96"><forename type="first">Parantapa</forename><surname>Goswami</surname></persName>
							<email>parantapa.goswami@viseo.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Viseo Innovation</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.33,209.70,63.44,8.96"><forename type="first">Damien</forename><surname>Nouvel</surname></persName>
							<email>damien.nouvel@inalco.frfrederique.segond@inalco.fr</email>
						</author>
						<author>
							<persName coords="1,378.79,209.70,75.60,8.96"><forename type="first">Frédérique</forename><surname>Segond</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">INaLCO ERTIM</orgName>
								<address>
									<postCode>75007</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.70,152.67,303.68,12.64;1,270.05,170.67,55.23,12.64">ERTIM@MC2: Diversified Argumentative Tweets Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58C2722F2F6036C7DB98BD3C0337C3A2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argumentation</term>
					<term>Opinion</term>
					<term>Twitter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our participation to CLEF MC2 2018 edition for the task 2 Mining opinion argumentation. It consists in detecting the most argumentative and diverse Tweets about some festivals in English and French from a massive multilingual collection. We measure argumentativity of a Tweet computing the amount of argumentation compounds it contains. We consider argumentation compounds as a combination between opinion expression and its support with facts and a particular structuration. Regarding diversity, we consider the amount of festival aspects covered by Tweets. An initial step filters the original dataset to fit the language and topic requirements of the task. Then, we compute and integrate linguistic descriptors to detect claims and their respective justifications in Tweets. The final step extracts the most diverse arguments by clustering Tweets according to their textual content and selecting the most argumentative ones from each cluster. We conclude the paper describing the different ways we combined the descriptors among the different runs we submitted and discussing their results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF MC2 Lab 2018 <ref type="bibr" coords="1,215.21,614.75,11.58,8.96" target="#b0">[1]</ref> proposes an information retrieval task for festival organizers who would like to know what people think about their event on Twitter 1 . A user's query can be either in French or in English and also specifies a topic from a list of festival names. We design a system, based on linguistic information, which selects the 100 most argumentative and diverse Tweets associated to a user's query. An initial step filters Tweets according to languages and topics in order to reduce the amount of data to be processed. We first extract French and English Tweets performing language detection thanks to an external tool. Then, using regular expressions and key words, a topic filtering step extracts, for each language, sets of Tweets related to the different Festivals.</p><p>We perform a linguistic enrichment on the previously extracted sets of Tweets. We then use these Tweets enriched with linguistic information to compute the argumentativity score of each Tweet and measure diversity among Tweets.</p><p>Argumentation is a process of construction with arguments that are sets of premises, in other words facts chosen to support claims <ref type="bibr" coords="2,306.33,270.18,10.75,8.96" target="#b1">[2]</ref>. Claims are personal statements made by an individual about a topic. Thus, a claim is the expression of an individual's opinion as a polarity (negative, neutral, positive) considering a topic. We link argumentation and opinion in that the former supports the latter. As we said an argumentation is related to an opinion, we measure the argumentativity of a Tweet according the amount of opinion and argumentation it contains. Opinion mining is driven by subjectivity detection because subjectivity is the property of a personal expression and we said opinion is personal. We think the characterization of argumentation by factuality is a crucial marker <ref type="bibr" coords="2,155.16,366.21,10.70,8.96" target="#b2">[3]</ref>. Factuality measures how much facts are present in a discourse. A fact is the opposite of subjective content as it stands for a proposition which is true independently of its enunciator. As we mentioned argumentation is a process of construction, we also use discourse structuration markers to detect argumentation.</p><p>Diversity is measured on a set of Tweets according to the variety of festival aspects mentioned in the featured view-points. Therefore, the resulting Tweets from our system must be distant considering the aspects they contain. That is why we measure diversity as a distance among Tweets using clustering on their textual content.</p><p>In what follows, we present the general architecture of the system together with the different linguistic modules and resources we used. We also explain the different configurations of the runs we have submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Our approach to the detection of the most argumentative and diverse Tweets within MC2</p><p>The overall approach (see Fig. <ref type="figure" coords="2,248.86,558.23,4.03,8.96" target="#fig_0">1</ref>) consists in applying different filtering steps in order to reduce the original set of "Festival" Tweets to those relevant for the particular task context and to map the most relevant Tweets to user's queries according to their level of argumentativity and diversity. We reduce the original dataset by two pre-filtering steps to fit the particular task context. The original dataset contains other languages than English and French thus the initial challenge is to identify and to separate English and French Tweets by a language filtering step. A list of festival names is provided as topics for each language. We detect and extract Tweets which contain mentions of these festivals.</p><p>We perform data enrichment on the pre-filtered set using Natural Language Processing tools. It consists of morpho-syntactic and semantic information on which the calculation of the argumentativity score is based.</p><p>We compute argumentativity score of a Tweet as the amount of both opinion and argumentation it contains. For example, a Tweet with only one claim as "I love Hellfest." will get a lower argumentativity score than a Tweet which combines an opinion and an associated argumentation as in "I love Hellfest because it is ethic.".</p><p>We define the diversity of Tweets as the amount of different aspects they mention about the festivals. For example, a set of Tweets about Cannes festival that only contains Tweets like "I love Cannes festival because the introduction was great!" and "Beautiful introduction at Cannes!" is argumentative but not relevant for diversity as it only mentions one aspect. The more diverse Tweets are, the more individuals' critical criteria are provided so that festival organizers get a larger perspective on what people think and why. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language filtering</head><p>The language filtering step (see Fig. <ref type="figure" coords="3,277.30,477.11,4.17,8.96" target="#fig_1">2</ref>) is performed using the Python module "langid.py" 2 ; we choose this module because it combines state-of-the-art results and speed which is essential for processing such a massive dataset <ref type="bibr" coords="3,349.87,501.11,10.69,8.96" target="#b3">[4]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic filtering</head><p>The original dataset contains Tweets that are not only about the festivals from the particular task context. The next step consists, for each language, in detecting and grouping the Tweets into categories corresponding to the lists of festivals provided (see Fig. <ref type="figure" coords="4,456.82,194.22,3.63,8.96">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Principle of topic filtering module</head><p>Topic detection is performed using regular expressions based on key words representative of each festival. We select a set of "representative" key words associated with each festival based on the mentions in the topically categorized sample of Tweets provided by the organizers. For two festivals, Cannes and Avignon, we notice that the city name is often used alone (without "festival" like "Cannes" instead of "Festival de Cannes") so we decide to only look for "cannes" and "avignon". Regular expressions are built so that the tokens may appear in any case and any order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data enrichment</head><p>The goal of this intermediary step is to enrich the pre-filtered data with linguistic information. The output of this step is then stored in order to run the process just once preventing lost in performance. We first normalize each Tweet using the Python module "tweet-preprocessor" 3 . It is fully customizable, allowing us to specify parts of the Tweets we want to remove: URL, Mention, Emoji, Smiley. We decide to keep hashtags as they might contain important information. For example in "The sound is too loud! #FestivalCannes", it allows to identify the topic of the Tweet. We normalize hashtags removing the "#" character.</p><p>After text normalization, for each Tweet, we extract the following information using NLP tools we selected according to our needs; they are mostly bilingual and fast to handle the data size (see Table <ref type="table" coords="4,249.60,640.79,3.65,8.96">3</ref>). ─ List of tokens 3 https://pypi.org/project/tweet-preprocessor/ ─ List of lemmas ─ List of POS labels ─ Subjectivity score ─ Opinion polarity score Lists of tokens, lemmas and POS labels are obtained using the TreeTagger tool <ref type="foot" coords="5,454.90,205.11,3.24,5.83" target="#foot_1">4</ref> on the normalized Tweets. We use normalized Tweets because TreeTagger is meant to analyze regular texts while the original Tweets are noisy texts as they can contain Tweet-specific elements like smileys. Specifying as a parameter the language of the text to analyze (English or French), TreeTagger returns a list of lists containing each form, its POS label and lemma.</p><p>Subjectivity and opinion polarity scores are obtained using "TextBlob" library <ref type="foot" coords="5,450.46,277.14,3.24,5.83" target="#foot_2">5</ref> and an adaptation for French named "textblob-fr" <ref type="foot" coords="5,311.71,289.14,3.24,5.83" target="#foot_3">6</ref> . TextBlob computes the scores using lexical resources and pattern matching. We run it on the normalized Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Opinion and argumentation filtering</head><p>This step computes an argumentativity score for each Tweet according to the opinion and argumentation it contains. We have selected linguistic features that may represent both aspects.</p><p>For opinion detection, we use the subjectivity score as we consider the expression of an opinion to be a subjective content <ref type="bibr" coords="5,277.97,400.29,10.69,8.96" target="#b4">[5]</ref>. We consider that the higher its subjectivity score, the more "opinioned" a Tweet is. We also use the opinion polarity score, not for the polarity itself, but for its magnitude that may also indicate how much opinioned a Tweet is. These two scores are combined with their respective weights (specified in section 2.6) as a magnitude score described in equation <ref type="bibr" coords="5,347.95,448.29,10.66,8.96" target="#b1">(2)</ref>.</p><formula xml:id="formula_0" coords="5,174.86,467.28,295.84,9.96">𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑e(𝑡𝑤𝑒𝑒𝑡) = α * 𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑖𝑡𝑦 + β * |𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦| (1)</formula><p>where 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) is the opinion magnitude score (comprised between [0-1]) for 𝑡𝑤𝑒𝑒𝑡, 𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑖𝑡𝑦 the subjectivity score (comprised between [0-1]), 𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦 the polarity score in [0-1], 𝛼 and 𝛽 are their respective weights</p><p>We also use two lexical resources: one for English and one for French. For English, we use <ref type="bibr" coords="5,140.78,544.31,11.72,8.96" target="#b5">[6]</ref> which encodes the "arousal" property of 13,915 English lemmas. It associates a score to each lemma according to the affectivity it denotes; our hypothesis is that the more a Tweet contains high affectivity lemmas (high scores), the more opinioned it would be (see equation 3). For French, we use <ref type="bibr" coords="5,315.79,580.31,10.69,8.96" target="#b6">[7]</ref>, a French lexicon which associates to 14,129 non-neutral lemmas a binary polarity value ("positive" or "negative") and six binary values depending on whether each lemma evokes <ref type="bibr" coords="5,363.67,604.31,11.71,8.96" target="#b0">(1)</ref> or not (0) a sentiment among six different: joy, anger, surprise, sadness, disgust and fear. We consider sentiment as an internal psychologic state whose expression can serve the formulation of an opinion. Our hypothesis is that the more a French Tweet contains lemmas present in this lexicon (encoding only non-neutral lemmas) and with high number of sentiment denotations, the more opiniated it would be (see equation <ref type="bibr" coords="6,356.18,162.18,7.29,8.96" target="#b3">4)</ref>.</p><formula xml:id="formula_1" coords="6,208.13,179.07,262.57,20.49">𝑎𝑟𝑜𝑢𝑠𝑎𝑙(𝑡𝑤𝑒𝑒𝑡) = ∑ 𝑎𝑟𝑜𝑢𝑠𝑎𝑙(𝑙𝑒𝑚𝑚𝑎 𝑖 ) 𝑛 𝑖=1 𝑛<label>(2)</label></formula><p>where 𝑎𝑟𝑜𝑢𝑠𝑎𝑙(𝑡𝑤𝑒𝑒𝑡) is the arousal score comprised between [0-1] for 𝑡𝑤𝑒𝑒𝑡, 𝑎𝑟𝑜𝑢𝑠𝑎𝑙(𝑙𝑒𝑚𝑚𝑎 𝑖 ) the lexicon-based arousal score normalized comprised between [0-1] for the lemma 𝑙𝑒𝑚𝑚𝑎 𝑖 and 𝑛 the number of lemmas in 𝑡𝑤𝑒𝑒𝑡</p><formula xml:id="formula_2" coords="6,189.53,270.03,281.17,20.52">𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) = ∑ 𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑙𝑒𝑚𝑚𝑎 𝑖 ) 𝑛 𝑖=1 𝑛<label>(3)</label></formula><p>where 𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) is the expressivity score comprised between [0-1] for 𝑡𝑤𝑒𝑒𝑡, 𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑙𝑒𝑚𝑚𝑎 𝑖 ) is the expressivity of 𝑙𝑒𝑚𝑚𝑎 𝑖 computed following equation ( <ref type="formula" coords="6,165.41,323.85,3.90,8.96" target="#formula_4">5</ref>) and 𝑛 is the number of lemmas in 𝑡𝑤𝑒𝑒𝑡</p><formula xml:id="formula_3" coords="6,222.89,341.92,247.81,17.88">𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑙𝑒𝑚𝑚𝑎) = |𝑡𝑟𝑢𝑒| 7<label>(4)</label></formula><p>where 𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑙𝑒𝑚𝑚𝑎) is the lexicon-based expressivity score for 𝑙𝑒𝑚𝑚𝑎 according to |𝑡𝑟𝑢𝑒|, the number of valid properties among the presence in the lexicon and the six lexicon-annotated sentiments Besides these lexicon-based measures, we also detect the opinion in a Tweet by taking into account the proportion of adjectives regarding POS tags; our hypothesis is that the more a Tweet contains adjectives, the more opiniated it would be (see equation <ref type="bibr" coords="6,443.54,449.25,7.31,8.96" target="#b5">6)</ref>.</p><formula xml:id="formula_4" coords="6,213.77,479.22,256.93,17.88">𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) = |𝑎𝑑𝑗𝑒𝑐𝑡𝑖𝑣𝑒𝑠| 𝑛<label>(5)</label></formula><p>where 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) is the descriptivity score for 𝑡𝑤𝑒𝑒𝑡 according to |𝑎𝑑𝑗𝑒𝑐𝑡𝑖𝑣𝑒𝑠|, the number of tokens tagged as adjectives and 𝑛 the number of tokens in 𝑡𝑤𝑒𝑒𝑡 Regarding argumentation, we say that an argumentative text is particularly structured to effectively combine arguments and opinions. Conjunctions are discourse connectors thus we suppose they are particularly used to structure a text. We use POS tags to value the proportion of conjunctions in a Tweet (see equation 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟𝑎𝑡𝑖𝑜𝑛(𝑡𝑤𝑒𝑒𝑡) =</head><p>|𝑐𝑜𝑛𝑗𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠| 𝑛 <ref type="bibr" coords="6,458.98,625.07,11.72,8.96" target="#b5">(6)</ref> where 𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟𝑎𝑡𝑖𝑜𝑛(𝑡𝑤𝑒𝑒𝑡) is the structuration score comprised between [0-1] for 𝑡𝑤𝑒𝑒𝑡, |𝑐𝑜𝑛𝑗𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠| the number of conjunctions in 𝑡𝑤𝑒𝑒𝑡 and 𝑛 the number of tokens in 𝑡𝑤𝑒𝑒𝑡 For English Tweets, we compute a concreteness score (see equation 8) relying on the lexical resource <ref type="bibr" coords="7,206.68,162.18,10.78,8.96" target="#b7">[8]</ref>. It associates to nearly 40,000 English lemmas a score which indicates how much perceptible (by the five senses) their mean is. As we start from the hypothesis that an argumentative text is factual, thus independent from an individual's state of mind, we formulate a new hypothesis saying that it may contain more concrete lemmas.</p><formula xml:id="formula_5" coords="7,185.69,227.07,285.01,20.49">𝑐𝑜𝑛𝑐𝑟𝑒𝑡𝑒𝑛𝑒𝑠𝑠(𝑡𝑤𝑒𝑒𝑡) = ∑ 𝑐𝑜𝑛𝑐𝑟𝑒𝑡𝑒𝑛𝑒𝑠𝑠(𝑙𝑒𝑚𝑚𝑎 𝑖 ) 𝑛 𝑖 = 1 𝑛<label>(7)</label></formula><p>where 𝑐𝑜𝑛𝑐𝑟𝑒𝑡𝑒𝑛𝑒𝑠𝑠(𝑡𝑤𝑒𝑒𝑡) is the concreteness score comprised between [0-1] of 𝑡𝑤𝑒𝑒𝑡, 𝑐𝑜𝑛𝑐𝑟𝑒𝑡𝑒𝑛𝑒𝑠𝑠(𝑙𝑒𝑚𝑚𝑎 𝑖 ) the lexicon-based concreteness score of 𝑙𝑒𝑚𝑚𝑎 𝑖 (normalized between [0-1], 0 if lemma is missing) and 𝑛 the number of tokens in 𝑡𝑤𝑒𝑒𝑡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Diversity filtering</head><p>In this final step, we build a set of Tweets that maximizes the diversity criterion among the most argumentative Tweets. Diversity measures how much festival aspects the Tweets mention. Thus, we suppose diverse Tweets may contain words semantically distant. For example, we detect that "This festival is too expensive." and "Ticket price for this festival is too high." mention a similar aspect by the semantic proximity between the words "expensive" and "price". Inversely, the texts "This festival program is so good!" and "This festival proposes a good choice of beers!" are more distant due to the semantic distance between the words "program" and "beer".</p><p>As diversity is computed according to the lexical semantics distance between Tweets, we use word embeddings models from Sketch Engine<ref type="foot" coords="7,377.95,438.06,3.24,5.83" target="#foot_4">7</ref> to get a spatial representation of words, one for English and one for French. As we want to keep as much form-wise information as possible, we select for both languages word form models (without lowercasing). For English, we select the model based on British National Corpus because it is the lighter therefore it avoids memory problem at loading time. For French, the only one proposed is a model based on a Web corpus. We vectorize a Tweet by matching its tokens against the model using FastText Python module <ref type="foot" coords="7,418.18,510.08,3.24,5.83" target="#foot_5">8</ref> . We use Kmeans clustering via the ScikitLearn toolkit <ref type="foot" coords="7,301.27,522.08,3.24,5.83" target="#foot_6">9</ref> to compute the distance between vectorized Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Impacts of dataset pre-filtering</head><p>Table <ref type="table" coords="7,149.89,585.11,4.98,8.96" target="#tab_0">1</ref> shows the efficiency of the pre-filtering steps (regarding languages and topics) in reducing data size by evaluating compression ratios among selective properties for argumentativity. It includes linguistic properties (lemmas, subjectivity and opinion polarity) obtained as described in section 2.3. We consider properties which are relative indicators of point of view diversity in our data regarding sources (authors) and vocabulary (lemmas). We also select subjectivity and opinion polarity scores as a mean to measure inclination of authors to express and explain their point of view. The language filtering step removes more than 40% of the original data compressing the unique authors ratio, computed with equation <ref type="bibr" coords="8,325.70,198.18,10.71,8.96" target="#b7">(8)</ref>, by more than 50% for both languages. The dataset is much more practicable but with an impoverishment of sources. Only around 1% of the lemmas used in Tweets are different, which may be difficult for lexical approaches. Polarity and subjectivity average magnitudes ([0-1] interval scale) are low among the two languages; it may be positive to distinguish argumentative Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑟𝑎𝑡𝑖𝑜 =</head><p>𝑛𝑈𝑛𝑖𝑞𝐴𝑢𝑡ℎ𝑜𝑟𝑠 𝑛𝑇𝑤𝑒𝑒𝑡𝑠 <ref type="bibr" coords="8,458.98,280.65,11.72,8.96" target="#b7">(8)</ref> where 𝑟𝑎𝑡𝑖𝑜 is the ratio of the number of unique authors, 𝑛𝑈𝑛𝑖𝑞𝐴𝑢𝑡ℎ𝑜𝑟𝑠 to number of Tweets 𝑛𝑇𝑤𝑒𝑒𝑡𝑠 We can observe in Table <ref type="table" coords="8,241.35,339.45,4.98,8.96" target="#tab_0">1</ref> the evolution of author and vocabulary usage between the two first filtering steps. Unique authors ratio increases by around 80% for both languages; it is a considerable increase compared to the previous step and a positive result for the representativeness of the data. Vocabulary usage is poor in English (0.2% of different lemmas) and in French (0.5%); this may be a relevant concern for the diversity criterion. Polarity and subjectivity average magnitude stay low even if it increases for French Tweets; the selective power of this information may be preserved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs description</head><p>A run returns the 100 most argumentative and diverse Tweets for all languages and festival names from the particular task context. To get the 100 most argumentative and diverse Tweets, we run K-Means with k = 100 and select the Tweet with higher argumentativity score from each cluster. Each run results in a ranked set of Tweets with the most argumentative first. We have submitted three runs which differ by features and associated weights used for computing the argumentativity score of each Tweet. We combine scores, described in section 2.4, that use the same types of linguistic information (POS-based and lexical types). The purpose is to evaluate the impact of Mathematically, the combination is an arithmetic mean (see equations 9, 10 and 11).</p><formula xml:id="formula_6" coords="9,158.78,373.60,311.92,85.75">𝑝𝑜𝑠𝑆𝑐𝑜𝑟𝑒(𝑡𝑤𝑒𝑒𝑡) = 𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟𝑎𝑡𝑖𝑜𝑛(𝑡𝑤𝑒𝑒𝑡) + 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) 2 (9) 𝑙𝑒𝑥𝑖𝑐𝑎𝑙𝑆𝑐𝑜𝑟𝑒𝐸𝑛(𝑡𝑤𝑒𝑒𝑡) = 𝑎𝑟𝑜𝑢𝑠𝑎𝑙(𝑡𝑤𝑒𝑒𝑡) + 𝑐𝑜𝑛𝑐𝑟𝑒𝑡𝑒𝑛𝑒𝑠𝑠(𝑡𝑤𝑒𝑒𝑡) 2 (10) 𝑙𝑒𝑥𝑖𝑐𝑎𝑙𝑆𝑐𝑜𝑟𝑒𝐹𝑟(𝑡𝑤𝑒𝑒𝑡) = 𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡)<label>(11)</label></formula><p>In all runs, we set weights in magnitude equation ( <ref type="formula" coords="9,331.45,482.51,3.90,8.96" target="#formula_1">2</ref>) with 𝛼 = 0.75 and 𝛽 = 0.25 as we are more confident in the subjectivity analysis than in the polarity one, relying on our observations of some analyzed data. Even if we do not have a lot of confidence in the magnitude score according to our observations of some analyzed data, it is the feature that is the more directly related to argumentativity. Therefore we use it in all runs mostly with a minor weight (0.25). Run 1 uses all types of features while the two others interchange the uses of lexicon-based and POS-based scores to evaluate their respective impact on result quality. Run 1 uses all types of feature: POS, opinion score and the lexicon-based score (see equations 12 and 13). English lexical resources cover opinion and argumentation aspects whereas the French one only covers opinion. We decide to give a minor weight in the French run (0.25) than in the English one (0.50). We try to balance the lack of lexicon resource for argumentation in French giving a more important weight to POSbased score (0.50) in comparison with English (0.25). The magnitude score gets the 15 Obtained using https://github.com/sloria/textblob 16 Not designed to support all languages of the original dataset Polarity magnitude average 15 N/A 16 0.18 0.14 0.13 0.07 same weight for the two languages (0.25) as we do not have a lot of confidence in the tool that computes the score. </p><p>Run 2 uses the magnitude and lexicon-based scores (see equations 14 and 15). As we previously said, the French lexicon coverage of task aspects is not complete contrary to the English one so we set it with a smaller weight in French (0.50) than in English (0.75). In English, we give a major weight to the lexicon-based score because we attach more importance to the manually built lexical resource in comparison with the magnitude score automatically computed. </p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results, conclusions and perspectives</head><p>An overview of the results obtained by the different systems in the lab can be found in <ref type="bibr" coords="10,124.70,595.31,10.66,8.96" target="#b8">[9]</ref>. In this section, we logically focus on our system as it is the subject matter of the present paper and because we are in a relevant position to review its results. At this time, no diversity results have been provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Regarding argumentativity, the organizers used two measures to evaluate the quantity of argumentative content in runs. NDCG measures the relevance of Tweets with a discount function over the rank: in each run, the most relevant Tweets must appear first. NDCG measures the relevance of each run according to regular expressions which match argumentative content. Two references for argumentative content have been used: one manual prepared by annotators and another one obtained by a pooling of runs from the different participants' systems. A measure named "%arg" gives the percentage of argumentative content comparing to both pooling and manual references. Table <ref type="table" coords="11,465.68,254.22,4.98,8.96" target="#tab_1">2</ref> presents the results for all runs. We explain this difference by the different natures of lexical resources across the languages; as we suspected preparing the runs (see section 2.7), the lexical resource for English may be more related to argumentativity especially with the concreteness property while the French one is only about sentiment expression which may be useful for opinion mining (see section 2.4) but not sufficient to detect argumentative content.</p><p>Comparing the results in one language, run 2 in French is particularly low (NDCGpooling and %arg). This run in French may not include enough features related to argumentativity; the presence of opinion polarity, subjectivity or sentiments in a Tweet should indicate that it contains a personal expression but it does not imply that it is justified by an argumentation. However, the addition of the lexical sentiment feature allows run 1 to be the best in French (in comparison with run 3). We think that a personal content may be the base for argumentation as a supporting tool. In other words, particularly on Twitter, we suppose that there might not be argumentation without a personal content. We note that POS-based information considering the structuration is effective even on Tweets, probably due to relativity of POS-based scores among Tweets. In English, it is surprising to observe that the run with lexical and POS-based features (run 1) gets lower results than the run without POS-based information (run 2). Regarding the weights among the two runs (see section 2.7), the lexical feature is ¾ of the score in run 2 whereas it is ½ in run 1; we think that the lower results in run 1 compared to run 2 might be explained by the lower importance of the lexical feature in run 1 rather than the addition of the POS-based feature. It is supported by the result of run 3 which uses the POS-based feature and gets a better NDCG score than run 1.</p><p>Considering our position across the different participants' systems is interesting because we are at the first place by pooling for both languages (lowest scores are 0.00 in French and 0.05 in English) and at the last place for English (best score is 0.06) and penultimate for French (the lowest score is 2.28 from the baseline and the best score is 2.89). It means that our system does not correctly match the manual reference but extracts arguments not considered by the annotators or other participants. Maybe it reflects a divergence in what is considered as argumentative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conclusions and perspectives</head><p>The hypothesis of argumentation using words which denote concrete things seems to be validated by the importance of the corresponding lexical feature in English, getting a better score when it is used with a greater weight. In French, the discourse connectors feature gives the best results and validates the assumption of a more structured text when it is argumentative, even on short messages like Tweets.</p><p>As the lexicon encoding concreteness and arousal properties allows to get the best results, it may be relevant to build a corresponding resource in French; it could be achieved by a translation process. It would be interesting to view if we also get better results in French.</p><p>We would like to analyze similar features on other text media to compare their respective contribution. In particular, it would be interesting to evaluate the POS-based feature with structuration words on texts which are less bound by their size, maybe a longer text needs to be more structured.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,256.13,421.94,125.88,8.10;3,152.40,339.60,356.16,73.68"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System general architecture</figDesc><graphic coords="3,152.40,339.60,356.16,73.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,206.69,631.31,196.02,8.96;3,187.68,522.00,236.52,106.08"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Principle of the language filtering module</figDesc><graphic coords="3,187.68,522.00,236.52,106.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,124.70,193.27,279.06,9.96;10,124.70,205.15,217.18,9.96;10,453.94,206.10,16.76,8.96;10,124.70,225.43,282.90,9.96;10,124.70,237.19,216.22,9.96"><head></head><label></label><figDesc>𝑎𝑟𝑔𝑢𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑣𝑖𝑡𝑦𝐸𝑛(𝑡𝑤𝑒𝑒𝑡) = 0.25 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) + 0.50 * 𝑙𝑒𝑥𝑖𝑐𝑎𝑙𝑆𝑐𝑜𝑟𝑒𝐸𝑛(𝑡𝑤𝑒𝑒𝑡) + 0.25 * 𝑝𝑜𝑠𝑆𝑐𝑜𝑟𝑒(𝑡𝑤𝑒𝑒𝑡) (12) 𝑎𝑟𝑔𝑢𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑣𝑖𝑡𝑦𝐹𝑟(𝑡𝑤𝑒𝑒𝑡) = 0.25 * 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) + 0.25 * 𝑙𝑒𝑥𝑖𝑐𝑎𝑙𝑆𝑐𝑜𝑟𝑒𝐹𝑟(𝑡𝑤𝑒𝑒𝑡) + 0.50 * 𝑝𝑜𝑠𝑆𝑐𝑜𝑟𝑒(𝑡𝑤𝑒𝑒𝑡)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,124.70,361.30,283.98,9.96;10,124.70,373.18,102.80,9.96;10,453.94,374.13,16.76,8.96;10,124.70,393.46,285.06,9.96;10,124.70,405.22,92.96,9.96;10,453.94,406.17,16.76,8.96;10,136.10,438.21,334.57,8.96;10,124.70,450.21,345.65,8.96;10,124.70,462.23,346.07,8.96;10,124.70,474.23,308.32,8.96;10,142.58,493.32,109.88,9.96;10,214.61,505.32,238.18,9.96"><head></head><label></label><figDesc>𝑎𝑟𝑔𝑢𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑣𝑖𝑡𝑦𝐸𝑛(𝑡𝑤𝑒𝑒𝑡) = 0.25 * 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) + 0.75 * 𝑙𝑒𝑥𝑖𝑐𝑎𝑙𝑆𝑐𝑜𝑟𝑒𝐸𝑛(𝑇𝑤𝑒𝑒𝑡) (14) 𝑎𝑟𝑔𝑢𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑣𝑖𝑡𝑦𝐹𝑟(𝑡𝑤𝑒𝑒𝑡) = 0.50 * 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) + 0.50 * lexicalScoreFr(tweet) (15) Run 3 uses the POS-based score in association with the magnitude score (see equation 16). As the tool we used to extract the POS labels is the same for English and French, we give the same weight (0.75) to POS-based score for the two languages. It is a major score because of the lack of reliability we have for the opinion score. 𝑎𝑟𝑔𝑢𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑣𝑖𝑡𝑦(𝑡𝑤𝑒𝑒𝑡) = 0.25 * 𝑚𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒(𝑡𝑤𝑒𝑒𝑡) + 0.75 * 𝑝𝑜𝑠𝑆𝑐𝑜𝑟𝑒(𝑡𝑤𝑒𝑒𝑡)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,124.70,435.45,340.26,236.93"><head>Table 1 .</head><label>1</label><figDesc>Statistics on dataset through the pre-filtering steps</figDesc><table coords="8,124.70,455.85,340.26,216.53"><row><cell>Language</cell><cell></cell><cell>Initial multilin-</cell><cell>English</cell><cell></cell><cell>French</cell><cell></cell></row><row><cell></cell><cell></cell><cell>gualism</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>Initial</cell><cell>Language</cell><cell>Topic</cell><cell>Lan-</cell><cell>Topic</cell></row><row><cell></cell><cell></cell><cell>"Festival"</cell><cell>filtered</cell><cell>fil-</cell><cell>guage fil-</cell><cell>fil-</cell></row><row><cell></cell><cell></cell><cell>Tweets</cell><cell></cell><cell>tered</cell><cell>tered</cell><cell>tered</cell></row><row><cell>#Tweets</cell><cell></cell><cell>63M</cell><cell>34M</cell><cell>2M</cell><cell>3M</cell><cell>200k</cell></row><row><cell cols="2">#Unique authors</cell><cell>45M</cell><cell>9M</cell><cell>1M</cell><cell>1M</cell><cell>100k</cell></row><row><cell>#Tokens 10</cell><cell></cell><cell>960M</cell><cell>532M</cell><cell>25M</cell><cell>41M</cell><cell>3M</cell></row><row><cell>#Unique</cell><cell>lemma-</cell><cell>N/A 12</cell><cell>7M</cell><cell>61k</cell><cell>252k</cell><cell>15k</cell></row><row><cell cols="2">tized tokens 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Subjectivity magni-</cell><cell>N/A 14</cell><cell>0.28</cell><cell>0.28</cell><cell>0.26</cell><cell>0.15</cell></row><row><cell cols="2">tude average 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">10 Obtained using Unix 'wc' command</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">11 Lemmatized using http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/</cell><cell></cell></row><row><cell cols="5">12 14 Not designed to support all languages of the original dataset</cell><cell></cell><cell></cell></row><row><cell cols="4">13 Obtained using https://github.com/sloria/textblob</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,124.70,290.06,345.89,170.19"><head>Table 2 .</head><label>2</label><figDesc>Argumentativity resultsWe observe in Table2the best results are not obtained with the same types of features across the languages. All runs use the magnitude of subjectivity and polarity scores (see section 2.7) but in English the best results are obtained by addition with lexical features (run 2) while in French the best run combines lexical and POS-based features (run 1).</figDesc><table coords="11,130.34,319.17,320.86,68.48"><row><cell>Language</cell><cell>English</cell><cell></cell><cell></cell><cell>French</cell><cell></cell><cell></cell></row><row><cell>Measure</cell><cell>NDCG-</cell><cell>NDCG-</cell><cell>%arg</cell><cell>NDCG-</cell><cell>NDCG-</cell><cell>%arg</cell></row><row><cell></cell><cell>manual</cell><cell>pooling</cell><cell></cell><cell>manual</cell><cell>pooling</cell><cell></cell></row><row><cell>Run1</cell><cell>0.002</cell><cell>0.36</cell><cell>21.81</cell><cell>2.597</cell><cell>2.06</cell><cell>22.00</cell></row><row><cell>Run2</cell><cell>0.007</cell><cell>0.60</cell><cell>36.72</cell><cell>2.594</cell><cell>1.39</cell><cell>20.43</cell></row><row><cell>Run3</cell><cell>0.003</cell><cell>0.39</cell><cell>20.36</cell><cell>2.594</cell><cell>1.99</cell><cell>21.89</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,129.98,686.23,84.47,8.10"><p>http://www.twitter.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="5,136.10,664.27,217.95,8.10"><p>http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,136.10,675.19,134.41,8.10"><p>http://textblob.readthedocs.io/en/dev/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="5,136.10,686.23,129.15,8.10"><p>https://github.com/sloria/textblob-fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="7,136.10,664.27,202.28,8.10"><p>https://embeddings.sketchengine.co.uk/static/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="7,136.10,675.19,233.04,8.10"><p>https://github.com/facebookresearch/fastText/tree/master/python</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="7,136.10,686.23,140.95,8.10"><p>http://scikit-learn.org/stable/index.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,132.67,533.08,299.29,8.10" xml:id="b0">
	<monogr>
		<ptr target="http://www.mc2.talne.eu" />
		<title level="m" coord="12,141.74,533.08,97.79,8.10">CLEF MC2 Lab Homepage</title>
		<imprint>
			<date type="published" when="2018-05-24">2018/05/24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,544.12,337.71,8.10;12,141.74,555.04,328.75,8.10;12,141.74,566.08,169.50,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,246.24,544.12,224.14,8.10;12,141.74,555.04,89.02,8.10">Argumentation mining: the detection, classification and structure of arguments in text</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Palau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,249.29,555.04,221.20,8.10;12,141.74,566.08,70.96,8.10">Proceedings of the 12th international conference on artificial intelligence and law</title>
		<meeting>the 12th international conference on artificial intelligence and law</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,577.12,337.72,8.10;12,141.74,588.04,328.69,8.10;12,141.74,599.08,124.51,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,288.89,577.12,181.49,8.10;12,141.74,588.04,25.59,8.10">Argument mining on twitter: arguments, facts and sources</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.73,588.04,285.70,8.10;12,141.74,599.08,37.53,8.10">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2317" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,610.12,337.97,8.10;12,141.74,621.04,230.62,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,221.05,610.12,199.78,8.10">langid. py: An off-the-shelf language identification tool</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,438.51,610.12,32.13,8.10;12,141.74,621.04,161.37,8.10">Proceedings of the ACL 2012 system demonstrations</title>
		<meeting>the ACL 2012 system demonstrations</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,632.08,337.97,8.10;12,141.74,643.12,192.43,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,254.26,632.08,160.28,8.10">Survey on mining subjective data on the web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsytsarau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,433.44,632.08,37.19,8.10;12,141.74,643.12,107.73,8.10">Data Mining and Knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="514" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,654.07,337.65,8.10;12,141.74,665.11,306.10,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,315.02,654.07,155.30,8.10;12,141.74,665.11,95.21,8.10">Norms of valence, arousal, and dominance for 13,915 English lemmas</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,255.85,665.11,98.01,8.10">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,149.99,337.78,8.10;13,141.74,161.03,215.98,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,310.72,149.99,144.15,8.10">Feel: a french expanded emotion lexicon</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abdaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Azé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bringay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Poncelet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,141.74,161.03,131.14,8.10">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="833" to="855" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,172.07,337.70,8.10;13,141.74,182.99,320.53,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,312.18,172.07,158.18,8.10;13,141.74,182.99,118.79,8.10">Concreteness ratings for 40 thousand generally known English word lemmas</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,279.63,182.99,97.76,8.10">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="904" to="911" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,194.03,337.98,8.10;13,141.74,205.07,328.85,8.10;13,141.74,215.99,115.40,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,313.21,194.03,40.27,8.10">CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajjem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Latiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,371.71,194.03,98.94,8.10;13,141.74,205.07,281.29,8.10">International Conference of the Cross-Language Evaluation Forum for European Languages Proceedings</title>
		<title level="s" coord="13,430.04,205.07,22.91,8.10">LNCS</title>
		<meeting><address><addrLine>Avignon</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
