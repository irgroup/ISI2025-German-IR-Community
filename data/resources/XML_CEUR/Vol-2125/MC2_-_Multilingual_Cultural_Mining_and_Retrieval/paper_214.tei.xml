<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.76,115.96,303.83,12.62;1,160.53,133.89,294.30,12.62">Opinion argumentation based on combined Information Retrieval and topic modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,243.82,171.56,42.89,8.74"><forename type="first">Seif</forename><surname>Sendi</surname></persName>
							<email>sendiseif@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">ISAMM</orgName>
								<orgName type="institution">University of Manouba</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.87,171.56,53.20,8.74"><forename type="first">Chiraz</forename><surname>Latiri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FST</orgName>
								<orgName type="laboratory">LIPAH</orgName>
								<orgName type="institution">University of Tunis El Manar</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.76,115.96,303.83,12.62;1,160.53,133.89,294.30,12.62">Opinion argumentation based on combined Information Retrieval and topic modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">205E30E6145EB1F165EAD183CD82F272</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Opinion mining</term>
					<term>topic modeling</term>
					<term>information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Argumentation mining is a text mining task, which aims at automatically detecting the argumentative structure concealed in a huge amount of text data. Previous researches in this field have focused on the classification of text sentences as arguments and the detection of relations between them. Different corpora have been used, such as newspaper articles and online debates. Due to the explosion of social networks, microblogging platforms like Twitter and Facebook have become interesting tools to evaluate public opinion on different domains.</p><p>In this work, we propose a new pipeline process to achieve the goal of argumentation mining, based on 70 millions of twitter-microblogs released from MC2 CLEF-2018 lab dealing with cultural events. Our approach is based on Information Retrieval protocol combined with sentiment analysis and topic modeling using Latent Dirichlet Allocation (LDA).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social network became an important source of information considering the number of users and the data exchanged. Twitter is one of the most famous platforms in the world, millions of people express themselves every minute by simply sharing a 140 characters short text (tweet).SS The first question which should be asked is can we use this large data ? The answer is yes, thanks to the Twit-terAPI, people can have a free access to twitter's data. But that's not enough, unless we know what we are looking for.</p><p>Dealing with such data requires precautions : We must understand that a normal person can't analyse (manually) a huge volume of text data, that's why we need to ask computers to do it for us, using specific methods and algorithms. The analysis of twitter text data must be different from the other types of text, because tweets are pressed, noisy and often unstructured.</p><p>Our purpose is to explore twitter data and to identify the argumentative parts of it, but first we need to know how can we consider a tweet as an argument.</p><p>Mochales Palau and Moens <ref type="bibr" coords="2,256.08,118.99,10.52,8.74" target="#b0">[1]</ref> consider an argument as a set of premises, pieces of evidence (e.g. facts), offered in support of a claim. The claim is a proposition, an idea which is either true or false, put forward by somebody as true. Based on this definition, we want to extract the most ranked argumentative tweets to create their summary which present the relevant information about a given topic.</p><p>To achieve our goal, we create a pipeline containing three popular techniques : Information Retrieval, sentiment analysis and topic modeling. Each technique has it's own particularities; Information Retrieval, based on the indexing and the querying, allows a simple searching process of the whole data to get the result of the query. Sentiment analysis aims to categorize opinions about something (product, event, etc.). Topic modeling allows the discovery of hidden semantic topics into the text data. We believe that combining these techniques can achieve the aim of argumentation mining, by identifying the most argumentative tweets within a 70 millions microblogs dataset of cultural events. The results confirm that our pipeline succeeded to identify the most 100 argumentative tweets compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed approach</head><p>The MC2 CLEF2018's data contains 18 text files, each one represents a collection of tweets which contains 16 topics in different languages during one month. Each topic is a festival name, there are 12 festivals in english and 4 in french. Since our goal is to extract the most 100 argumentative tweets of each topic, we created a pipeline and we put these 18 files through it. This pipeline is composed of information retrieval, topic modeling and sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information retrieval</head><p>Information retrieval is the process of obtaining relevant information about a collection of text data, we choose the Indri platform to make our IR process. A specific format called TRECText (or TRECWeb) is required, any other format won't be recognized. As our input files are text documents, we need to get the correct format, that's why we have used Perl language to do the job. We wrote a program which accepts a .txt file as input and return a .trec file as output without changing the content of our tweets. Example of TRECText format :</p><p>Creating the index of our text data allows us to do the querying, for each topic we wrote a lexicon-based query in Indri language, example : Art festival # weight[m](1.0 # band(Art festival) 0.5 # or(abnormal aborted))</p><p>We demand the engine to return the best 500 tweets for each topic based on their scores which have been calculated by default with Indri. It uses a query likelihood function with Dirichlet <ref type="bibr" coords="3,283.32,222.26,10.52,8.74" target="#b1">[2]</ref> prior smoothing to weight terms. The formulation is given by : By default, mu is equal to 2500, which means that for the very small documents you're using, the score differences will be very small. <ref type="bibr" coords="3,396.40,448.46,10.52,8.74" target="#b2">[3]</ref> We have made this process twice, one for english topics and one for french, so we have two result files which are considered as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic modeling</head><p>The baseline from the previous step does not confirm that the 500 resulting tweets are talking about the same topic because it is an index-quey process, the system is not intelligent enough to provide a list of tweets which have exactly the same topic simply by a lexicon query. That is why we need to use a machine learning technique to train a model that confirms the matching result.</p><p>Topic modeling is a modern technique frequently used in machine learning and natural language processing, it aims to extract topics from a collection of text documents. It helps in understanding our data, organizing it, and discovering hidden information into it. There are many methods that are used to obtain topic models, in this work we focused on Latent Dirichlet Allocation (LDA).</p><p>LDA is a popular method of topic modeling which considers each document as a blend of topics, and each topic as a blend of words. We imagine that we have the following tweets (documents):</p><p>-Tweet 1 : Jazz festival is the best, i can't wait for the next year.</p><p>-Tweet 2 : Rock festival is always amazing !! -Tweet 3 : Jazz music is relaxing !! -Tweet 4 : I don't like rock music.</p><p>The LDA model discover the topics in these tweets, it may produce these results :</p><p>-Topic 1 : 50 % Festival, 15% Jazz, 15% Rock.. (This topic deals with festivals) -Topic 2 : 50 % Music, 15% Jazz, 15% Rock..(This topic deals with music) -Tweet 1 and 2 : 100% topic 1 -Tweet 3 and 4 : 100% topic 2</p><p>The hard part when working with LDA is how to find the number of topics. In general, the use of metrics, like Griffiths2004 <ref type="bibr" coords="4,346.06,340.44,10.52,8.74" target="#b3">[4]</ref> or Arun2010 [5], is required to calculate the fitting optimal number of topics. But in our case, this number is known (see previous section).</p><p>The preprocessing phase is needed because of the noise of tweets, we have removed punctuations, URLs, stop words (english and french), twitter identifiers and we have done the stemming process. The figure below presents a wordcloud of our tweets after cleaning, and it contains the most frequently words. As LDA supports two methods, VEM and Gibbs Sampling, we need to choose which fits our model.</p><p>The estimates for the topic distributions for the documents are included which are the estimates of the corresponding variational parameters for the VEM algorithm and the parameters of the predictive distributions for Gibbs sampling. In additional slots the objects contain the assignment of terms to the most likely topic and the log-likelihood which is log p(w | α, β) for LDA with VEM estimation and log z) for LDA using Gibbs sampling. <ref type="bibr" coords="5,391.27,178.77,13.02,8.74" target="#b4">[6]</ref> The estimation using Gibbs sampling requires specification of values for the parameters of the prior distributions, that's why we choose VEM.</p><p>The aim is to score each tweet by how much it belongs to his expected topic. We can examine the per-document-per-topic probabilities called Gamma. This will result k number of clusters or groups of words, we manually annotate these groups to do the matching between score and topic for every single tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentiment analysis</head><p>Sentiment analysis is a text classification tool that uses NLP and machine learning to analyse text and extract the writer's sentiment about a given topic, it is widely used in online debates and social media platforms. Since our data is a collection of tweets dealing with cultural events, we can apply the sentiment analysis on it to discover what people feel about each topic.</p><p>The same baseline is considered as our input, and since we have cleaned the tweets (see previous section), the data is ready to be analysed. We have used the R sentiment package, which perform sentiment analysis by scoring individual words based on the affinity score defined by NRC <ref type="foot" coords="5,354.03,412.87,3.97,6.12" target="#foot_0">3</ref> . To get a sentiment score of each tweet in our corpus, we applied the get nrc sentiment() function. This function doesn't stop at providing a positive or negative score for a tweet, it goes deeper to tell us the other emotions that a tweet contains (fear, joy, ..), in addition, it counts the frequency of each emotion word. With these sentiments, we can calculate a new score which reflects how the writer of the tweet have felt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>After finishing the process, we have three scores for each tweet, as our goal is not particularly one of them, we need to put these scores together. The new calculated score is the key to get a new ranking. Score = IR score + LDA score + Sentiment score Based on this score, we re-ranked the tweets. To test our approach, we need to compare the new results with the baseline. We have picked the top 3 tweets from Toronto festival, and located the same tweets in the baseline ranking. The whole ranking is changing after the process, besides w didn't show the 500 tweets of each topic, we just take the best 100 between them. It's remarkable that the top ranked tweets have become more argumentative than before.</p><p>Argumentation mining is a new research field which aims to automatically extract the argumentation structure hidden in a collection of text data. Previous studies have been through the classification of sentences into argument and non-argument, based on a manually annotated corpus and using static machine learning algorithms. In this paper, we tried to achieve the same goal with a different style, because we think that annotating a corpus of tweets is a hard task, besides it is not accurate due to human errors. We tried to combine computational techniques that made our results more specific, especially with the ranking method. Our model provides an easy result list which contains a summary of ranked tweets according to their scores.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,149.71,258.32,178.94,8.74;3,149.71,282.44,176.87,8.74;3,149.71,306.55,181.59,8.74;3,149.71,330.66,179.24,8.74;3,149.71,354.78,188.74,8.74;3,149.71,378.89,116.76,8.74;3,220.28,413.77,174.79,8.74"><head></head><label></label><figDesc>c(w;D) = count of word in the document c(w;C) = count of word in the collection | D | = number of words in the document | C | = number of words in the collection numerator = c(w;D) + mu * c(w;C) / | C | denominator = | D | + mu score = log( numerator / denominator )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,237.25,597.86,140.86,8.74;4,149.71,620.16,330.88,8.74;4,134.77,632.11,322.93,8.74;4,149.71,644.16,330.88,8.74;4,134.77,656.12,91.96,8.74;4,192.93,432.44,229.50,153.90"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Word cloud of our corpus After cleaning our tweets, we have used the LDA function provided by the topicmodels package in R. This function was fed with our baseline results.As LDA supports two methods, VEM and Gibbs Sampling, we need to choose which fits our model.</figDesc><graphic coords="4,192.93,432.44,229.50,153.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,238.47,611.97,138.42,8.74;5,168.63,448.34,278.10,152.10"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Histogram of sentiments.</figDesc><graphic coords="5,168.63,448.34,278.10,152.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,241.04,431.14,133.28,8.74;6,142.30,366.07,330.75,53.55"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Sample of the new rank</figDesc><graphic coords="6,142.30,366.07,330.75,53.55" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,144.73,634.88,335.87,7.86;5,144.73,645.84,335.86,7.86;5,144.73,656.80,176.03,7.86"><p>The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,142.59,342.24,7.86;8,146.91,153.55,124.56,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,382.95,142.59,93.33,7.86">Argumentation mining</title>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Mochales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,146.91,153.55,119.73,7.86">Artificial Intelligence and Law</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,175.46,342.25,7.86;8,146.91,186.42,333.68,7.86;8,146.91,197.38,45.62,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,300.10,175.46,180.49,7.86;8,146.91,186.42,161.59,7.86">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,316.40,186.42,164.20,7.86;8,146.91,197.38,17.25,7.86">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,219.30,313.86,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,300.25,219.30,148.31,7.86">Experiments using the Lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,241.22,296.17,7.86;8,134.77,263.14,242.92,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<title level="m" coord="8,339.05,241.22,95.46,7.86;8,134.77,263.14,3.58,7.86;8,235.01,263.14,142.67,7.86">Optimal Number of topics for LDA</title>
		<editor>
			<persName><forename type="first">Karthik</forename><surname>Arun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004. 2010</date>
		</imprint>
	</monogr>
	<note>Finding scientific topics 5</note>
</biblStruct>

<biblStruct coords="8,138.35,285.05,342.25,7.86;8,146.91,296.01,78.65,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,355.64,285.05,124.95,7.86;8,146.91,296.01,78.65,7.86">Topicmodels: an R package for fitting topic models</title>
		<author>
			<persName coords=""><forename type="first">Bettina</forename><surname>Grun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Kepler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
